<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python系列文章-jieba分词包使用]]></title>
    <url>%2F2022%2F06%2F17%2F2022-06-17-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-jieba%E5%88%86%E8%AF%8D%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 总结 参考文献及资料 背景第一部分https://github.com/fxsjy/jieba https://bbs.huaweicloud.com/blogs/252899 https://blog.csdn.net/u013982921/article/details/81122928 参考文献及资料1、变量官网介绍：]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[经济系列文章-央行资产负债表]]></title>
    <url>%2F2022%2F06%2F06%2F2022-06-06-%E7%BB%8F%E6%B5%8E%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E5%A4%AE%E8%A1%8C%E8%B5%84%E4%BA%A7%E8%B4%9F%E5%80%BA%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料1、]]></content>
      <categories>
        <category>Distance Measures</category>
      </categories>
      <tags>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python环境配置-pip更换国内源]]></title>
    <url>%2F2022%2F05%2F22%2F2022-05-22-Python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-pip%E6%9B%B4%E6%8D%A2%E5%9B%BD%E5%86%85%E6%BA%90%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Linux环境 第二部分 windows环境 第三部分 国内其他安装源 参考文献及资料 背景国内环境通过pip安装，默认使用境外的镜像服务器。由于特殊原因，速度较慢，所以需要调整为国内镜像源。例如清华镜像源。 清华pypi 镜像间隔 5 分钟同步一次。 第一部分 Linux环境1.1 临时使用1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package 注意，simple 不能少, 是 https 而不是 http 1.2 配置文件永久生效升级 pip 到最新的版本 (&gt;=10.0.0) 后进行配置： 12python -m pip install --upgrade pippip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple 如果您到 pip 默认源的网络连接较差，临时使用本镜像站来升级 pip： 1python -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip 如果需要手动配置，手动创建下面的文件：/root/.config/pip/pip.conf 注意这里是在root下面创建，可以根据情况在其他用户下创建，路径为：~/.pip/pip.conf 文件内容： 12[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple 第二部分 windows环境1.1 临时使用和Linux相同，命令中增加url参数： 1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package 1.2 配置文件永久生效 方法1（命令方式）： 执行执行下面的命令： 1pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple 回显如下（注意用户名为系统的用户）： 1Writing to C:\Users\用户名\AppData\Roaming\pip\pip.ini 或者手动方式创建配置文件，如下： 方法2（手动）： 在C:\Users\用户名\pip（如果没有路径创建，注意用户名用户根据自己系统调整），添加文件pip.ini，文件内容： timeout 是超时时间，可以适当调大些。 12345[global] timeout = 60000index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host = https://pypi.tuna.tsinghua.edu.cn 对于路径可以使用系统默认路径： 1C:\Users\用户名\AppData\Roaming\pip 第三部分 国内其他安装源其他国内数据源有： 阿里云 http://mirrors.aliyun.com/pypi/simple/ 中国科学技术大学 :https://pypi.mirrors.ustc.edu.cn/simple 清华：https://pypi.tuna.tsinghua.edu.cn/simple 豆瓣：http://pypi.douban.com/simple/ 华中理工大学 :http://pypi.hustunique.com/simple 山东理工大学 :http://pypi.sdutlinux.org/simple 参考文献及资料1、清华大学开源软件镜像站]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pyspark系列文章-Pyspark和Elasticsearch交互最佳实践]]></title>
    <url>%2F2022%2F05%2F22%2F2022-05-22-Pyspark%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Pyspark%E5%92%8CElasticsearch%E4%BA%A4%E4%BA%92%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Receiver接口模式 第二部分 Direct接口模式 第三部分 PySpark和Kafka交互 第四部分 任务提交 参考文献及资料 背景参考文献及资料1、Improvements to Kafka integration of Spark Streaming，]]></content>
      <categories>
        <category>pyspark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入了解Elasticsearch存储]]></title>
    <url>%2F2022%2F05%2F21%2F2022-05-21-%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3Elasticsearch%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境 第二部分 导入数据案例 参考文献及资料 背景In this article we’ll investigate the files written to the data directory by various parts of Elasticsearch. We will look at node, index and shard level files and give a short explanation of their contents in order to establish an understanding of the data written to disk by Elasticsearch. 本文对 Elasticsearch 7.10 适用 Elasticsearch 7.10 对应 Lucene 8.7 Lucene 8.7 关于扩展名的官方文档 https://lucene.apache.org/cor… 第一部分 路径Elasticsearch 运行前需要配置多种文件系统存储路径。分别在JVM启动参数或者配置文件中config/elasticsearch.yml中进行配置。 1.1 JVM中路径参数 path.home：运行 Elasticsearch 进程的用户的主目录。默认为 Java 系统属性user.dir，它是进程所有者的默认主目录。我们在bin/elasticsearch执行脚本中找到下面的Java启动参数： 1-Des.path.home="$ES_HOME" 而这个变量ES_HOME在bin/elasticsearch-env(source 这个变量文件)中定义如下： 12ES_HOME=`dirname "$SCRIPT"`ES_HOME=`cd "$ES_HOME"; pwd` 所以默认路径为bin同路径，例如我们测试Elasticsearch的path.home路径为：/usr/share/elasticsearch/bin。 path.conf: 服务的配置文件路径。和path.home定义方式相同，在bin/elasticsearch中： 1-Des.path.conf="$ES_PATH_CONF" 变量值在bin/elasticsearch-env中定义如下： 1if [ -z "$ES_PATH_CONF" ]; then ES_PATH_CONF="$ES_HOME"/config; path.plugins：Elasticsearch 插件目录。用于存放各类插件。位置为：$ES_HOME/plugins。 1.2 配置文件中路径 path.logs：Elasticsearch 运行日志存储路径。配置文件config/elasticsearch.yml可指定。 path.data：Elasticsearch 数据存储路径。 1.3 全部文件树12345678910111213# tree -L 1 elasticsearch/elasticsearch/|-- LICENSE.txt # 证书|-- NOTICE.txt # 提示|-- README.asciidoc # 说明|-- bin # 可执行文件|-- config # 配置路径|-- data # 数据路径|-- jdk # JDK路径，高版本为了解决jdk兼容问题，自带。减少用户自配置各类兼容问题。|-- lib # 依赖包路径|-- logs # 日志路径|-- modules # 各类模块|-- plugins # 插件路径 本文我们将详细介绍数据存储路径 ( path.data) 中的存储内容和用途。 第二部分 数据路径我们现在谈到全文检索，通常都是Elasticsearch或者solr。其实两者都是对搜索引擎Lucene的封装，Lucene成为一个依赖包。 我们查了一下Elasticsearch和solr两个词在Google趋势对比（2004-2022），明显Elasticsearch占优势（Elasticsearch 蓝色曲线，solr红色曲线）。 2.1 存储架构Elasticsearch 底层使用 Lucene 来处理分片级别的索引和查询，因此数据目录中的文件由 Elasticsearch 和 Lucene 编写。其中Lucene 负责读写维护 Lucene 索引文件，而 Elasticsearch 在 Lucene 之上读写管理相关的元数据，例如字段映射、索引设置等。 Elasticsearch中一个Shard分片就是一个Lucene Index，每个Lucene里的Segment为Lucene存储的最小管理单元。例如下图中对于分片I1P2进行了案例说明。 我们先看看 Elasticsearch 写入的数据的外部级别。 2.2 数据路径数据节点data/nodes/0 1234567# tree -L 3 data/data/`-- nodes `-- 0 |-- _state |-- indices `-- node.lock node.lock，文件用于确保一次只有一个 Elasticsearch 安装从单个数据目录读取/写入。 _state，目录中存放集群元数据信息。 indices，目录中存放集群index索引数据。 2.3 集群元数据1234567891011121314151617181920212223# tree _state/ _state/|-- _1q.fdt|-- _1q.fdx|-- _1q.fnm|-- _1q.si|-- _1q_2.liv|-- _1q_Lucene84_0.doc|-- _1q_Lucene84_0.tim|-- _1q_Lucene84_0.tip|-- _1u.cfe|-- _1u.cfs|-- _1u.si|-- _1x.cfe|-- _1x.cfs|-- _1x.si|-- _1z.cfe|-- _1z.cfs|-- _1z.si|-- manifest-2.st|-- node-2.st|-- segments_27`-- write.lock Name Extension Brief Description Segments File segments_N Stores information about a commit point Lock File write.lock The Write lock prevents multiple IndexWriters from writing to the same file.(写入锁，防止多个IndexWriters 同时写一个文件) Segment Info .si Stores metadata about a segment（segment的元数据信息，指明这个segment都包含哪些文件） Compound File .cfs, .cfe An optional “virtual” file consisting of all the other index files for systems that frequently run out of file handles. Fields .fnm Stores information about the fields Field Index .fdx Contains pointers to field data Field Data .fdt The stored fields for documents(Field数据文件) Term Dictionary .tim The term dictionary, stores term info Term Index .tip The index into the Term Dictionary Frequencies .doc Contains the list of docs which contain each term along with frequency（保留包含每个Term的文档列表） Positions .pos Stores position information about where a term occurs in the index Payloads .pay Stores additional per-position metadata information such as character offsets and user payloads Norms .nvd, .nvm Encodes length and boost factors for docs and fields Per-Docukment Values .dvd, .dvm Encodes additional scoring factors or other per-document information. Term Vector Index .tvx Stores offset into the document data file Term Vector Documents .tvd Contains information about each document that has term vectors Term Vector Fields .tvf The field level info about term vectors Live Documents .liv Info about what files are live 更有趣的是global-0.st-file。global-前缀表示这是一个全局状态文件，而扩展.st名表示这是一个包含元数据的状态文件。正如您可能已经猜到的那样，这个二进制文件包含有关集群的全局元数据，前缀后面的数字表示集群元数据版本，这是遵循集群的严格递增的版本控制方案。 虽然技术上可以在紧急情况下使用十六进制编辑器编辑这些文件，但强烈建议不要这样做，因为它会很快导致数据丢失。 https://www.shenyanchao.cn/blog/2018/12/04/lucene-index-files/ 2.4 索引数据12345678[root@f1f4420ca021 0]# tree -L 1 indices/indices/|-- Ce1hPxBFTc6xLH9zyzWRog|-- LFTAEix1Q6u79iWX5wFdAg|-- cNEZTRZGTtWn2f5r7zjveQ|-- pg9zI9o1QNKFav7KsxVQ-Q|-- r0lRikPvTUOWZaloPg7QXw`-- wHQ5gnEESM2GWgQsybPTKA 让我们创建一个单一的分片索引并查看 Elasticsearch 更改的文件： 1234567891011121314151617181920212223# tree cNEZTRZGTtWn2f5r7zjveQcNEZTRZGTtWn2f5r7zjveQ|-- 0| |-- _state| | |-- retention-leases-9.st| | `-- state-0.st| |-- index| | |-- _6.cfe| | |-- _6.cfs| | |-- _6.si| | |-- _6_1.fnm| | |-- _6_1_Lucene80_0.dvd| | |-- _6_1_Lucene80_0.dvm| | |-- _7.cfe| | |-- _7.cfs| | |-- _7.si| | |-- segments_6| | `-- write.lock| `-- translog| |-- translog-4.tlog| `-- translog.ckp`-- _state `-- state-2.st 我们看到已经创建了一个与索引名称对应的新目录。该目录有两个子文件夹：_state和0. 前者包含所谓的索引状态文件 ( indices/{index-name}/_state/state-{version}.st)，其中包含有关索引的元数据，例如其创建时间戳。它还包含唯一标识符以及索引的设置和映射。后者包含与索引的第一个（也是唯一一个）分片（分片 0）相关的数据。接下来，我们将仔细研究一下。 分片数据分片数据目录包含分片的状态文件，其中包括版本控制以及有关分片被视为主分片还是副本的信息。 12345678910$ tree -h data/elasticsearch/nodes/0/indices/foo/0data/elasticsearch/nodes/0/indices/foo/0├── [ 102] _state│ └── [ 81] state-0.st├── [ 170] index│ ├── [ 36] segments.gen│ ├── [ 79] segments_1│ └── [ 0] write.lock└── [ 102] translog └── [ 17] translog-1429697028120 在早期的 Elasticsearch 版本中，在分片数据目录中也可以找到单独的{shard_id}/index/_checksums-文件（和.cks-files）。在当前版本中，这些校验和现在位于 Lucene 文件的页脚中，因为 Lucene 已为其所有索引文件添加了端到端校验和。 该{shard_id}/index目录包含 Lucene 拥有的文件。Elasticsearch 通常不会直接写入此文件夹（在早期版本中发现较旧的校验和实现除外）。这些目录中的文件构成了任何 Elasticsearch 数据目录的大部分大小。 在我们进入 Lucene 的世界之前，我们将看一下 Elasticsearch 事务日志，不出所料，它位于每个分片translog目录中，前缀为translog-. 事务日志对于 Elasticsearch 的功能和性能非常重要，因此我们将在下一节更深入地解释它的使用。 每个分片的事务日志Elasticsearch事务日志确保数据可以安全地被索引到 Elasticsearch 中，而无需为每个文档执行低级别的 Lucene 提交。提交 Lucene 索引会在 Lucene 级别创建一个新段，该段是fsync()-ed 并导致大量磁盘 I/O 影响性能。 为了接受文档进行索引并使其可搜索而不需要完整的 Lucene 提交，Elasticsearch 将其添加到LuceneIndexWriter并将其附加到事务日志中。每次之后refresh_interval，它都会调用reopen()Lucene 索引，这将使数据无需提交即可搜索。这是 Lucene 近实时 API 的一部分。当IndexWriter由于事务日志的自动刷新或显式刷新操作而最终提交时，先前的事务日志将被丢弃，而新的事务日志将取而代之。 如果需要恢复，Lucene中写入磁盘的段将首先恢复，然后重放事务日志，以防止丢失尚未完全提交到磁盘的操作。 Lucene 索引文件Lucene 在记录Lucene 索引目录中的文件方面做得很好，为了方便起见，在此处复制（Lucene 中的链接文档还详细介绍了这些文件自 Lucene 2.1 以来所经历的更改，因此请检查出来）： Name Extension Brief Description Segments File segments_N Stores information about a commit point Lock File write.lock The Write lock prevents multiple IndexWriters from writing to the same file. Segment Info .si Stores metadata about a segment Compound File .cfs, .cfe An optional “virtual” file consisting of all the other index files for systems that frequently run out of file handles. Fields .fnm Stores information about the fields Field Index .fdx Contains pointers to field data Field Data .fdt The stored fields for documents Term Dictionary .tim The term dictionary, stores term info Term Index .tip The index into the Term Dictionary Frequencies .doc Contains the list of docs which contain each term along with frequency Positions .pos Stores position information about where a term occurs in the index Payloads .pay Stores additional per-position metadata information such as character offsets and user payloads Norms .nvd, .nvm Encodes length and boost factors for docs and fields Per-Docukment Values .dvd, .dvm Encodes additional scoring factors or other per-document information. Term Vector Index .tvx Stores offset into the document data file Term Vector Documents .tvd Contains information about each document that has term vectors Term Vector Fields .tvf The field level info about term vectors Live Documents .liv Info about what files are live 通常，您还会segments.gen在 Lucene 索引目录中看到一个文件，该文件是一个帮助文件，其中包含有关当前/最新segments_N文件的信息，用于可能无法通过目录列表返回足够信息来确定最新一代段文件的文件系统. 在较旧的 Lucene 版本中，您还可以找到带有.del后缀的文件。它们与 Live Documents ( ) 文件的用途相同.liv——换句话说，它们是删除列表。如果您想知道所有这些关于实时文档和删除列表的讨论是关于什么的，您可能想阅读自下而上文章中关于在我们的 Elasticsearch中构建索引的部分。 修复有问题的碎片由于 Elasticsearch 分片包含 Lucene 索引，因此我们可以使用 Lucene 出色的CheckIndex 工具，它使我们能够扫描和修复有问题的段，通常数据丢失最少。我们通常会建议 Elasticsearch 用户简单地重新索引数据，但如果由于某种原因无法重新索引并且数据非常重要，那么即使需要相当多的手动工作和时间，也可以采用这种方法，取决于分片的数量及其大小。 LuceneCheckIndex工具包含在默认的 Elasticsearch 发行版中，无需额外下载。 12345# 更改它以反映您的分片路径，格式为# &#123;path.data&#125;/&#123;cluster_name&#125;/nodes/&#123;node_id&#125;/indices/&#123;index_name&#125;/&#123;shard_id&#125;/index/$ export SHARD_PATH = data / elasticsearch / nodes / 0 / indices / foo / 0 / index / $java - cp lib / elasticsearch- *。jar : lib /*:lib/sigar/* -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex $SHARD_PATH -fix如果 CheckIndex 检测到问题并且它的修复建议看起来很合理，您可以通过添加命令行参数 告诉 CheckIndex 应用修复程序。 存储快照您可能想知道所有这些文件如何转换为快照存储库使用的存储。不要再想了：获取这个集群，将其作为my-snapshot基于文件系统的网关进行快照，然后检查存储库中的文件，我们会找到这些文件（为简洁起见，省略了一些文件）： 1234567891011121314151617181920212223242526272829$ tree -h snapshotssnapshots├── [ 31] index├── [ 102] indices│ └── [ 136] foo│ ├── [1.2K] 0│ │ ├── [ 350] __0│ │ ├── [1.8K] __1...│ │ ├── [ 350] __w│ │ ├── [ 380] __x│ │ └── [8.2K] snapshot-my-snapshot│ └── [ 249] snapshot-my-snapshot├── [ 79] metadata-my-snapshot└── [ 171] snapshot-my-snapshot$ tree - h 快照快照├── [ 31 ] 索引├── [ 102 ] 索引│ └── [ 136 ] foo │ ├── [ 1.2K ] 0 │ │ ├── [ 350 ] __0 │ │ ├── [ 1.8K ] __1 ... │ │ ├── [ 350 ] __w │ │ ├── [ 380 ] __x │ │ └── [ 8.2K ] 快照-我的-快照│ └── [ 249 ] 快照-我的-快照├── [ 79 ] 元数据-我的-快照└── [ 171 ] 快照-我的-快照 在根目录下，我们有一个index文件，其中包含有关此存储库中所有快照的信息，并且每个快照都有一个关联的文件snapshot-和一个metadata-文件。根目录中的snapshot-文件包含有关快照状态的信息，它包含哪些索引等等。metadata-根目录中 的文件包含快照时的集群元数据。 compress: true设置时， 文件metadata-使用snapshot-LZF 进行压缩，它侧重于压缩和解压缩速度，这使得它非常适合 Elasticsearch。数据与标题一起存储：ZV + 1 byte indicating whether the data is compressed。在标头之后会有一个或多个压缩的 64K 块，格式为：2 byte block length + 2 byte uncompressed size + compressed data. 使用此信息，您可以使用任何与 LibLZF兼容的解压缩器。如果您想了解有关 LZF 的更多信息，请查看此格式的精彩描述。 在索引级别有另一个文件，indices/{index_name}/snapshot-{snapshot_name}其中包含索引元数据，例如快照时索引的设置和映射。 在分片级别，您会发现两种文件：重命名的 Lucene 索引文件和分片快照文件：indices/{index_name}/{shard_id}/snapshot-{snapshot_name}. 该文件包含有关快照中使用了分片目录中的哪些文件的信息，以及从快照中的逻辑文件名到它们在恢复时应存储为磁盘上的具体文件名的映射。它还包含可用于检测和防止数据损坏的所有相关文件的校验和、Lucene 版本控制和大小信息。 您可能想知道为什么这些文件被重命名，而不是仅仅保留它们的原始文件名，这可能更容易直接在磁盘上使用。原因很简单：可以对索引进行快照、删除并在再次创建快照之前重新创建它。在这种情况下，几个文件最终将具有相同的名称，但内容不同。 参考文献及资料1、官网，链接：https://www.elastic.co/cn/ 2、https://www.shenyanchao.cn/blog/2018/12/04/lucene-index-files/ 3、https://www.elastic.co/cn/blog/found-dive-into-elasticsearch-storage 4、https://elasticsearch.cn/article/6178]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch测试数据快速导入]]></title>
    <url>%2F2022%2F05%2F19%2F2022-05-19-Elasticsearch%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E5%BF%AB%E9%80%9F%E5%AF%BC%E5%85%A5%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境 第二部分 导入数据案例 参考文献及资料 背景测试elasticsearch需要一些案例数据导入，然后进行相关工程测试和验证。elasticsearch官方提供了一个公开数据集：莎士比亚作品对白文本数据。 使用这个数据进行测试。 第一部分 环境本次实战的环境信息如下： 操作系统：Ubuntu 16.04 LTS JDK：1.8.0 elasticsearch：7.6.0 第二部分 导入数据案例测试环境地址：192.168.52.142是elasticsearch服务的IP地址，9200是elasticsearch服务的端口号。导入的shell脚本如下： 1234567891011121314151617181920212223242526272829303132#!/bin/bashES_HOST=$1ES_PORT=$2echo "elasticsearch server is $ES_HOST:$ES_PORT"echo "downloading data from github"wget --no-check-certificate https://download.elastic.co/demos/kibana/gettingstarted/7.x/shakespeare.jsonecho "downloading data success"echo "creating index ..."curl -X PUT \http://$&#123;ES_HOST&#125;:$&#123;ES_PORT&#125;/shakespeare \-H 'Accept: application/json' \-H 'Content-Type: application/json' \-H 'cache-control: no-cache' \-d '&#123; "mappings": &#123; "properties": &#123; "speaker": &#123;"type": "keyword"&#125;, "play_name": &#123;"type": "keyword"&#125;, "line_id": &#123;"type": "integer"&#125;, "speech_number": &#123;"type": "integer"&#125; &#125; &#125;'echo ""echo "loading data ..."curl -u elastic -H 'Content-Type: application/x-ndjson' -XPOST '$ES_HOST:$ES_PORT/shakespeare/_bulk?pretty' --data-binary @shakespeare.json &gt; result.logecho "finish" 保存为执行脚本：create_shakespeare_index 12chmod a+x create_shakespeare_index.sh \&amp;&amp; ./create_shakespeare_index.sh 192.168.52.142 9200 执行完毕后，会创建名为shakespeare的索引，并带有111,396个文档，案例数据如下： 12345678910111213141516&#123; "_index": "shakespeare", "_type": "_doc", "_id": "111395", "_version": 1, "_score": 0, "_source": &#123; "type": "line", "line_id": 111396, "play_name": "A Winters Tale", "speech_number": 38, "line_number": "", "speaker": "LEONTES", "text_entry": "Exeunt" &#125;&#125; 注意：由于特殊网络限制，大陆下载较慢可以先下载文件，然后再导入: https://download.elastic.co/demos/kibana/gettingstarted/7.x/shakespeare.json 参考文献及资料1、官网，链接：https://www.elastic.co/cn/]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python项目打包实践]]></title>
    <url>%2F2022%2F05%2F19%2F2022-05-19-Python%E9%A1%B9%E7%9B%AE%E6%89%93%E5%8C%85%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[[TOC] 背景第一部分1.6 总结参考文献及资料1、Logging包，]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python研发单元测试实践]]></title>
    <url>%2F2022%2F05%2F17%2F2022-05-17-Python%E7%A0%94%E5%8F%91%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[[TOC] 背景python 标准库的 unittest 已经满足了所有功能，但是为啥很多人还是不喜欢写单测呢？因为不够简单，程序员是最嫌麻烦的人，但凡有点费事都会避开。按照 unittest 的写法，必须得创建一个测试类，创建个单独的测试文件，各种都是面向对象的写法，太重。 第一部分1.6 总结参考文献及资料1、Logging包，链接：https://docs.python.org/3.9/library/logging.html 2、日志介绍，链接：https://rmcomplexity.com/article/2020/12/01/introduction-to-python-logging.html 3、日志介绍，链接：https://coralogix.com/blog/python-logging-best-practices-tips/]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[logstash、Flume、filebeat文件解析如何实现exactly once语义]]></title>
    <url>%2F2022%2F05%2F13%2F2022-05-13-logstash%E3%80%81Flume%E3%80%81filebeat%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0exactly%20once%E8%AF%AD%E4%B9%89%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料]]></content>
      <categories>
        <category>logstash Flume filebeat</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python中日志框架（Logging库）]]></title>
    <url>%2F2022%2F05%2F05%2F2022-05-05-Python%E4%B8%AD%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6%EF%BC%88Logging%E5%BA%93%EF%BC%89%2F</url>
    <content type="text"><![CDATA[[TOC] 背景https://murphypei.github.io/blog/2019/09/python-logging Python版本为Python 3.8.8 第一部分 内置Logging包1.1 日志级别 等级 数值 CRITICAL 严重；导致应用无法运行的重大错误； 50 ERROR 错误 40 WARNING 警告 30 INFO 信息 20 DEBUG 调试 10 NOTSET 通知 0 当然logging包也提供用户自定义日志级别。日志级别通过数值量化比较。例如： 123# 新增格式：# logging.addLevelName(level, levelName)logging.addLevelName(25, 'POINT') 代码中定义了新的日志级别：POINT，级别值25，在DEBUG和INFO之间。 1.2 日志流参考官网文档：https://docs.python.org/3/howto/logging.html#logging-basic-tutorial 详细流程如下图，其中涉及角色有： Logger：日志对象。定义后供业务程序调用。判断可用性开关参数，日志级别。不满足条件结束。 LogRecord ：日志记录器。多于满足过滤器条件，流程结束。否则继续给将日志传到相应的处理器（Handler）处理。 Handler ：处理器。将日志记录器产生的日志记录发送至合适的目的地。提供各类处理器配置。 Filter ：过滤器。提供了更好的粒度控制,可以决定输出哪些日志记录。 Formatter：格式化器。指明了最终输出中日志记录的格式。 官网中将日志Flow分为Logger Flow和Handler Flow，具体处理如下： 判断 Logger 对象对于设置的级别是否可用，如果可用，则往下执行，否则，流程结束。 创建 LogRecord 对象，如果注册到 Logger对象中的 Filter 对象过滤后返回 False，则不记录日志，流程结束，否则，则向下执行。 LogRecord 对象将 Handler 对象传入当前的 Logger 对象，（图中的子流程）如果 Handler 对象的日志级别大于设置的日志级别，再判断注册到 Handler 对象中的 Filter 对象过滤后是否返回 True 而放行输出日志信息，否则不放行，流程结束。 如果传入的 Handler 大于 Logger 中设置的级别，也即 Handler 有效，则往下执行，否则，流程结束。 判断这个 Logger 对象是否还有父 Logger 对象，如果没有（代表当前 Logger 对象是最顶层的 Logger 对象 root Logger），流程结束。否则将 Logger 对象设置为它的父 Logger 对象，重复上面的 3、4 两步，输出父类 Logger 对象中的日志输出，直到是 root Logger 为止。 上述流程我们后面会结合具体配置文件详细说明。 1.3 基本使用1.3.1 例子123456789101112131415161718import logginglogging.basicConfig(filename="test.log", filemode="a", format="%(asctime)s %(name)s:%(levelname)s:%(message)s", datefmt="%d-%M-%Y %H:%M:%S", level=logging.INFO)logging.debug('debug')logging.info('info')logging.warning('warning')logging.error('error')logging.critical('critical')# 输出test.log（文件若不存在，自动创建），追加写入。#05-38-2022 20:38:32 root:INFO:info#05-38-2022 20:38:32 root:WARNING:warning#05-38-2022 20:38:32 root:ERROR:error#05-38-2022 20:38:32 root:CRITICAL:critical basicConfig 定义了日志的各类配置参数，具体如下： 参数名 参数说明 filename 日志输出到文件的文件名 filemode 文件模式，r[+]、w[+]、a[+] format 日志输出的格式 datefmt 日志附带日期时间的格式 style 格式占位符，默认为 “%” 和 “{}” level 设置日志输出级别 stream 定义输出流，用来初始化 StreamHandler 对象，不能 filename 参数一起使用，否则会ValueError 异常 handles 定义处理器，用来创建 Handler 对象，不能和 filename 、stream 参数一起使用，否则也会抛出 ValueError 异常 1.3.2 日志记录每条日志都是一个LogRecord 对象，record对象对象的全部属性如下表。例子中，通过format参数定义日志输出格式，例如：format=&quot;%(asctime)s %(name)s:%(levelname)s:%(message)s&quot;。其中message参数是用户提供，其他参数有用户自定义选取和组织。 参数名 格式 描述 args You shouldn’t need to format this yourself. The tuple of arguments merged into msg to produce message, or a dict whose values are used for the merge (when there is only one argument, and it is a dictionary). asctime %(asctime)s 将日志的时间构造成可读的形式，默认情况下是精确到毫秒，如 2018-10-13 23:24:57,832，可以额外指定 datefmt 参数来指定该变量的格式 created %(created)f Time when the LogRecord was created (as returned by time.time()). exc_info You shouldn’t need to format this yourself. Exception tuple (à la sys.exc_info) or, if no exception has occurred, None. filename %(filename)s 不包含路径的文件名 funcName %(funcName)s 日志记录所在的函数名 levelname %(levelname)s 日志的级别名称 (&#39;DEBUG&#39;, &#39;INFO&#39;, &#39;WARNING&#39;, &#39;ERROR&#39;,&#39;CRITICAL&#39;). levelno %(levelno)s Numeric logging level for the message (DEBUG, INFO, WARNING, ERROR,CRITICAL). lineno %(lineno)d 日志记录所在的行号 module %(module)s Module (name portion of filename). msecs %(msecs)d Millisecond portion of the time when the LogRecord was created. message %(message)s 具体的日志信息, computed as msg % args. This is set whenFormatter.format() is invoked. msg You shouldn’t need to format this yourself. The format string passed in the original logging call. Merged with args to produce message, or an arbitrary object (see Using arbitrary objects as messages). name %(name)s 日志对象的名称 pathname %(pathname)s 进行日志调用的源文件的完整路径名 process %(process)d 当前进程ID processName %(processName)s 当前进程名称 relativeCreated %(relativeCreated)d Time in milliseconds when the LogRecord was created, relative to the time the logging module was loaded. stack_info You shouldn’t need to format this yourself. Stack frame information (where available) from the bottom of the stack in the current thread, up to and including the stack frame of the logging call which resulted in the creation of this record. thread %(thread)d 当前线程ID threadName %(threadName)s 当前线程名称 如果对于系统自带的参数集合不满足需求。logging包从Python 3.2起提供了Factory函数getLogRecordFactory() 和setLogRecordFactory()支持用户自定义record属性。例如对于分布式部署的环境，日志需要有程序部署节点和主机名和ip信息，需要用户自己新增定义。例如下面的案例： 123456789101112import loggingimport sockethistoryFactory = logging.getLogRecordFactory()def record_factory(*args, **kwargs): record = historyFactory(*args, **kwargs) record.hostname = socket.gethostname() record.hostip = socket.gethostbyname(socket.gethostname()) return recordlogging.setLogRecordFactory(record_factory) 这样就可以配置format的时候就可以引用已经定义的参数hostname和hostip。上面的函数相当于在record中新增两个参数的定义。 12format: "%(asctime)s - %(hostname)s - %(hostip)s - %(levelname)s - %(name)s(%(lineno)d) - %(message)s"# 输出：2022-05-08 15:24:54,163 - localhost-PC - 192.168.152.1 - INFO - TestLogger(48) - info 另外还可以继承logging.Formatter，例如（代码片段放在__ini__.py中，Python的模块名为：logTest）： 1234567import socketclass CustomFormatter(logging.Formatter): def format(self, record): record.hostname = socket.gethostname() record.hostip = socket.gethostbyname(socket.gethostname()) return super().format(record) 使用yaml配置文件时，配置如下： 1234formatters: custom: format: "%(asctime)s - %(hostname)s - %(hostip)s - %(levelname)s - %(name)s(%(lineno)d) - %(message)s" (): logTest.CustomFormatter 另外也可以通过自定义日志过滤器 filter 方法来实现。filter实际上既可以过滤日志，也可以添加字段参数和改写record，具有灵活的表达能力。我们在后文介绍。 1.3.3 日志切割生产环境程序常驻运行，为了避免日志文件过大，需要有生命周期管理。通常采用日志切割配置，将日志按照规则进行切割，并按照生命周期策略清理历史文件。 logging包中提供两种方式切割：按照时间和按照大小。 按大小：logging.handlers.RotatingFileHandler类； 123456789101112131415161718import loggingfrom logging.handlers import RotatingFileHandler# logging.basicConfig()logger = logging.getLogger('test')logger.setLevel(logging.INFO)formatter = logging.Formatter('"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s"')# 定义一个RotatingFileHandler，最多备份5个日志文件，每个日志文件最大0.01MrotatingHandler = RotatingFileHandler('test.log', maxBytes=0.01*1024*1024, backupCount=5)rotatingHandler.setFormatter(formatter)logger.addHandler(rotatingHandler)# 下面是测试案例if "__name__" == "__main__": while(True): logger.info("info")# 会有5个日志文件：test.log test.log.1 test.log.2 test.log.3 test.log.4 test.log.5 按时间：logging.handlers.TimedRotatingFileHandler类； 其中参数：TimedRotatingFileHandler(filename [,when [,interval [,backupCount]]]) filename 输出文件名前缀； when 时间单位，字典如下： 123456“S”: Seconds “M”: Minutes“H”: Hours“D”: Days“W”: Week day (0=Monday)“midnight”: Roll over at midnight（半夜） interval 等待多少个单位when的时间后，Logger会自动重建文件。 backupCount 是保留日志个数，默认的0是不会自动删除掉日志。 123456789101112131415161718import loggingfrom logging.handlers import TimedRotatingFileHandlerlogger = logging.getLogger('test')logger.setLevel(logging.INFO)formatter = logging.Formatter('"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s"')# 定义一个TimedRotatingFileHandler，最多备份5个日志文件，1分钟切割一次timedRotatingFileHandler = TimedRotatingFileHandler('Test.log', "M", 1, 5)timedRotatingFileHandler.suffix = "%Y%m%d-%H%M.log"timedRotatingFileHandler.setFormatter(formatter)logger.addHandler(timedRotatingFileHandler)# 下面是测试案例if __name__ == "__main__": while(True): logger.info("info")# 输出日志：Test.log.20220505-2309.log，文件名为：filename+suffix 1.3.4 handler类logging包提供大量的handler方法类，其中StreamHandler、FileHandler、NullHandler源码在__init__.py文件中。其他handler在handlers.py中。全部说明如下： StreamHandler 实例发送消息到流（类似文件对象）。可以是sys.stderr、sys.stdout或者文件。 FileHandler 实例将消息发送到硬盘文件。 BaseRotatingHandler 是轮换日志文件的处理程序的基类。它并不应该直接实例化。而应该使用 RotatingFileHandler 或 TimedRotatingFileHandler 代替它。 RotatingFileHandler 实例将消息发送到硬盘文件，支持最大日志文件大小和日志文件轮换。 TimedRotatingFileHandler 实例将消息发送到硬盘文件，以特定的时间间隔轮换日志文件。 SocketHandler 实例将消息发送到 TCP/IP 套接字。从 3.4 开始，也支持 Unix 域套接字。 DatagramHandler 实例将消息发送到 UDP 套接字。从 3.4 开始，也支持 Unix 域套接字。 SMTPHandler 实例将消息发送到指定的电子邮件地址。 SysLogHandler 实例将消息发送到 Unix syslog 守护程序，可能在远程计算机上。 NTEventLogHandler 实例将消息发送到 Windows NT/2000/XP 事件日志。 MemoryHandler 实例将消息发送到内存中的缓冲区，只要满足特定条件，缓冲区就会刷新。 HTTPHandler 实例使用 GET 或 POST 方法将消息发送到 HTTP 服务器。 WatchedFileHandler 实例会监视他们要写入日志的文件。如果文件发生更改，则会关闭该文件并使用文件名重新打开。此处理程序仅在类 Unix 系统上有用； Windows 不支持依赖的基础机制。 QueueHandler 实例将消息发送到队列，例如在 queue 或 multiprocessing 模块中实现的队列。 NullHandler 实例对错误消息不执行任何操作。它们由想要使用日志记录的库开发人员使用，但是想要避免如果库用户没有配置日志记录，则显示 “无法找到记录器XXX的消息处理器” 消息的情况。有关更多信息，请参阅 配置库的日志记录 。 另外用户也可以自己实现logging.Handler方法，定义自己的Handler方法。然后在配置中引用使用即可。 1.3.5 日志开关在特殊场景下，如果不需要输出日志。logging也提供响应的参数开关。例如对于定义的logger可以进行关闭，如下： 12logger = logging.getLogger("FileLogger")logger.disabled = True 这样就关闭了。如果是配置文件中，可以将handlers的list置为空，即可。 1234FileLogger: handlers: [] level: DEBUG propagate: no 如果需要禁用某个级别以下所有日志输出，可以使用下面的参数： 12logging.disable(logging.INFO)# 禁用INFO级别以下的所有日志 注：这个参数会限制所有的logger的日志输出级别。 1.3.5 日志过滤器业务代码产生的日志通常并不是都要作为日志输出的。所以在打印日志的时候我们对每条日志紧要程度进行了量化划分等级，即INFO、DEBUG 等。但是一些特殊的需求，这种粗粒度的等级划分并不能满足需求。能否提供可编程自定义的过滤函数filter(record)？ logging并没有像Handler一样，为用户提供丰富的filter工具类。建议项目组可以像java中日志框架 logback一样提供部分常用的filter工具类。 logging没有现成可用的filter工具，就需要用户自己继承实现（logging.Filter）。例如下面的案例： 12345class NoStringFilter(logging.Filter): def filter(self, record): return not record.getMessage().startswith('user')logger.addFilter(NoStringFilter()) 定义 filter 方法，日志记录 record 作为唯一参数，返回值为 0 或 False 表示日志记录将被抛弃，1 或 True则表示记录别放过。 上面的案例实现了过滤message中以user开头的日志消息。对于yaml文件配置方式，我们在后文介绍。 另外从Python3.2起，用户自定义filter也可以不继承logging.Filter，只需要定义一个实现函数，logger.addFilter方法引入即可。例如我们在打印日志的时候，需要对于日志中一些敏感信息进行脱敏处理，例如密码信息。 下面的代码片段放在__ini__.py中，Python的模块名为：logTest。 123456789class CustomFilter(logging.Filter): def filter(self, record): if 'password' in record.msg: record.msg = record.msg.replace("password", "*****") return True if 'pwd' in record.msg: record.msg = record.msg.replace("pwd", "*****") return True return True 对于msg中的字符串进行关键字替换脱密，对于含有password和pwd的字段。 在yaml配置文件中配置如下，在handlers中console应用该过滤器。 123456789filters: filter_pwd: (): logTest.CustomFilterhandlers: console: class: logging.StreamHandler level: INFO formatter: custom filters: [filter_pwd] 这样对于日志中还有相关关键字的日志将被过滤。 12logger.error("user：admin and password")# user：admin and ***** 1.4 最佳实践1.4.1 yaml配置文件对于生产环境Python项目运行，最佳实践当然是日志相关参数需要配置化，而不是耦合在业务代码中。所以首先是日志配置参数配置文件化。通常有几种方式：ini格式、yaml格式、JSON 格式、Python文件等，建议使用yaml格式。 例如下面的配置文件（logConfig.yaml）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970version: 1disable_existing_loggers: Trueincremental: Falseformatters: standard: format: "%(asctime)s - %(levelname)s - %(name)s(%(lineno)d) - %(message)s" error: format: "%(asctime)s-%(levelname)s &lt;PID %(process)d:%(processName)s&gt; %(name)s.%(funcName)s(): %(message)s" custom: format: "%(asctime)s - %(hostname)s - %(hostip)s - %(levelname)s - %(name)s(%(lineno)d) - %(message)s" (): logTest.CustomFormatterfilters: require_debug_false: (): logTest.NoParsingFilter filter_pwd: (): logTest.CustomFilterhandlers: console: class: logging.StreamHandler level: INFO formatter: custom filters: [filter_pwd] file: class: logging.FileHandler filename: ./logs/logging.log level: DEBUG formatter: standard TimedRotatingfile: class: logging.handlers.TimedRotatingFileHandler level: DEBUG formatter: standard filters: [require_debug_false] when: M backupCount: 5 filename: ./logs/timedRotating.log encoding: utf8 rotatingFileHandler: class: logging.handlers.RotatingFileHandler level: DEBUG formatter: standard filename: ./logs/RotatingFile.log maxBytes: 100000 backupCount: 5 encoding: utf8loggers: FileLogger: handlers: [] level: DEBUG #propagate: no SampleLogger: level: DEBUG handlers: [console,rotatingFileHandler] #propagate: no TestLogger: level: DEBUG handlers: [console] #propagate: noroot: level: DEBUG handlers: [] 配置文件中参数说明如下： version，版本参数，目前限定为1（或者1.0）； disable_existing_loggers ,参数值默认为True。 关于这个参数需要重点讲解一下，因为网上各类文章解释均有出入，笔者做了实验并查看了代码，解释如下。首先从字面意思来看，是否禁止已经存在的loggers。那么怎么理解已经存在的loggers？有下面的例子，我们有两个配置文件中分别定义了日志配置（Config.yaml和logConfig.yaml）： 1234567with open('Config.yaml', 'r') as f: config = yaml.safe_load(f.read()) logging.config.dictConfig(config)with open('logConfig.yaml', 'r') as f: config = yaml.safe_load(f.read()) logging.config.dictConfig(config) 两个配置文件中均定义了名称为TestLogger的loggers（但是配置定义不同，例如Handler参数不同）。但是配置文件Config.yaml和logConfig.yaml有加载先后顺序，那么哪个loggers是有效的呢？ 这时候配置文件logConfig.yaml中disable_existing_loggers参数值为True，那么旧的配置失效，否则新的配置失效。 incremental，中文是增加的意思。是否将此配置文件解释为现有配置的增量, 默认为False。该参数True是有不明错误，暂未研究参数用途。 formatters ，日志格式化器。可以定义多个日志格式类型，供handler配置使用。 filters，日志过滤器。配置 handlers，日志处理器。例如配置文件中我们定义了：console、file、TimedRotatingfile、RotatingFileHandler。供后续loggers配置使用。举个例子，其他handler不在展开讲了。 1234567891011121314151617181920# handler名称标签TimedRotatingfile: # handler实现类，也可以配置用户自己实现的Handler方法 class: logging.handlers.TimedRotatingFileHandler # 日志过滤级别 level: DEBUG # 日志格式化方式而error，配置文件中配置使用 formatter: error # 过滤器选择，配置文件中需要已配置 filters: [require_debug_false] # 日志切割时间，M为分钟切割 when: M # 日志切割保存的文件数量 backupCount: 5 # 日志文件名称，注意这里的绝对路径 filename: ./logs/timedRotating.log # 日志输出编码，utf8支持中文 encoding: utf8 # 注意配置文件方式不支持自定义suffix值，目前只支持在代码中basicConfig配置 # suffix: "%Y-%m-%d_%H-%M" loggers，日志对象。配置文件中可以定义多个loggers。程序中使用下面方式调用： 1logger = logging.getLogger("FileLogger") 另外logger有一个重要的概念：父子logger。每个用户自定义的logger默认都是root的子logger，这时候，子logger的日志会发送给父级的Logger。当其中的参数propagate为no时候，则不再发送给父。例如下面的配置。 1234FileLogger: handlers: [TimedRotatingfile] level: DEBUG propagate: no 另外除了默认的root作为父logger，用户也可以自己定义，例如： 1logger = logging.getLogger("FileLogger.SampleLogger") 这时候FileLogger.SampleLogger的父logger为FileLogger，并且默认FileLogger的父logger是root（如果有配置和定义）。即关系为：FileLogger.SampleLogger----&gt;FileLogger---&gt;root。 那么对于FileLogger.SampleLogger.TestLogger，是否有多重父子传递呢？经过测试验证是不支持的，也就是说最多就2层。 root，终极父logger，接受所有子logger的日志发送，属于兜底处理。例如下面的配置： 1234root: level: DEBUG handlers: [] # handlers: [console] root属于可选配置项。另外handlers配置为空，并且所有的子logger的handlers配置也为空，这时候会有默认输出，输出格式较为简单只有%(message)s信息。 如果一个日志对象没有指定具体的logger，那么就是root，例如： 12print(logging.getLogger() == logging.root)# True 查看一下root的配置： 123456print(logging.root.handlers)print(logging.lastResort)print(logging.root.filters)# []# &lt;_StderrHandler &lt;stderr&gt; (WARNING)&gt;# [] 所以root默认的Handler是StderrHandler，并且日志级别是warning，输出到stderr（默认显示的字体是红色）。如果是shell脚本运行可以配置日志重定向： 1$ python log.py 2&gt; /dev/null 业务代码中如下加载配置： 123456789101112import logging.configimport yamlwith open('logConfig.yaml', 'rb') as f: config = yaml.safe_load(f.read()) logging.config.dictConfig(config)logger = logging.getLogger("FileLogger")logger1 = logging.getLogger("SampleLogger")logger.debug("debug")logger1.debug("debug") 1.4.2 其他配置文件对于yaml文件读取，使用了yaml包方式先将配置文件解释成字典对象，然后再通过下面的方法解析配置信息。 1logging.config.dictConfig() 其实logging提供原生接口直接读取配置文件的解析方法（如下）。 1logging.config.fileConfig() 方式该方法目前只支持configparser类型的文件，即下面的ini格式的配置文件（截取片段）： 12345[loggers]keys=root,sampleLogger[handlers]keys=consoleHandler fileConfig()是一个较老的接口，不支持配置Filter。官方建议后续尽量使用dictConfig()接口，后续新功能都将增加到dictConfig()接口。 对于json文件，logConfig.json。 12345678910111213141516171819202122232425262728&#123; "version":1, "disable_existing_loggers":false, "formatters":&#123; "simple":&#123; "format":"%(asctime)s - %(name)s - %(levelname)s - %(message)s" &#125; &#125;, "handlers":&#123; "console":&#123; "class":"logging.StreamHandler", "level":"DEBUG", "formatter":"simple", "stream":"ext://sys.stdout" &#125; &#125;, "loggers":&#123; "my_module":&#123; "level":"ERROR", "handlers":["console"], "propagate":"no" &#125; &#125;, "root":&#123; "level":"INFO", "handlers":["console"] &#125;&#125; 文件解析代码案例： 123456789import logging.configimport jsonwith open('logConfig.json', 'r') as f: config = json.load(f) logging.config.dictConfig(config)logger = logging.getLogger("FileLogger")logger.debug("debug") 1.5 线程安全和进程不安全1.5.1 线程安全logging 库是线程安全的。 123456789101112131415161718192021222324252627282930313233343536373839404142class Handler(Filterer): def __init__(self, level=NOTSET): # ...... self.createLock() def createLock(self): """ Acquire a thread lock for serializing access to the underlying I/O. """ self.lock = threading.RLock() _register_at_fork_reinit_lock(self) def acquire(self): """ Acquire the I/O thread lock. """ if self.lock: self.lock.acquire() def release(self): """ Release the I/O thread lock. """ if self.lock: self.lock.release() def handle(self, record): """ Conditionally emit the specified logging record. Emission depends on filters which may have been added to the handler. Wrap the actual emission of the record with acquisition/release of the I/O thread lock. Returns whether the filter passed the record for emission. """ rv = self.filter(record) if rv: self.acquire() try: self.emit(record) finally: self.release() return rv 1.5.2 进程安全1.6 总结参考文献及资料1、Logging包，链接：https://docs.python.org/3.9/library/logging.html 2、日志介绍，链接：https://rmcomplexity.com/article/2020/12/01/introduction-to-python-logging.html 3、日志介绍，链接：https://coralogix.com/blog/python-logging-best-practices-tips/]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[impala使用总结]]></title>
    <url>%2F2022%2F05%2F05%2F2022-05-11-impala%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料 官网：http://impala.apache.org/ 使用手册：http://impala.apache.org/docs/build/html/index.html Sql：http://impala.apache.org/docs/build/html/topics/impala_langref_sql.html 窗口函数：http://impala.apache.org/docs/build/html/topics/impala_functions.html 基本操作：http://impala.apache.org/docs/build/html/topics/impala_tutorial.html impala-shell：http://impala.apache.org/docs/build/html/topics/impala_impala_shell.html]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JDBC使用小结]]></title>
    <url>%2F2022%2F05%2F05%2F2022-05-05-JDBC%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 scope标签作用 第二部分 打包 参考文献及资料 背景Java 数据库连接( JDBC ) 是Java编程语言的)应用程序编程接口(API) ，它定义了客户端如何访问数据库。它是一种基于 Java 的数据访问技术，用于 Java 数据库连接。它是Oracle Corporation的Java 标准版平台的一部分。它提供了查询和更新数据库中数据的方法，面向关系数据库。JDBC 到ODBC桥允许连接到Java 虚拟机(JVM) 主机环境中的任何 ODBC 可访问的数据源。 Sun Microsystems于 1997 年 2 月 19 日将 JDBC 作为Java 开发工具包(JDK) 1.1 的一部分发布。 [1] 从那时起，它就成为Java 平台标准版(Java SE) 的一部分。 第一部分 实践1.1 1.2 各类DBMS数据库链接实践1.2.1 Oracle连接驱动名：oracle.jdbc.driver.OracleDriver Maven坐标： 12345&lt;dependency&gt; &lt;groupId&gt;com.oracle&lt;/groupId&gt; &lt;artifactId&gt;ojdbc7&lt;/artifactId&gt; &lt;version&gt;12.1.0.2&lt;/version&gt;&lt;/dependency&gt; 代码案例： 12345Class.forName("oracle.jdbc.dirver.OracleDriver").newInstance();String url="jdbc:oracle:thin:@localhost:1521:orcl";String user="user";String pwd="password";java.sql.Connection conn=DriverManager.getConnection(url,user,pwd); 1.2.2 Mysql按照版本的不同，Mysql有两种驱动： Mysql 6：com.mysql.cj.jdbc.Driver Mysql 5：com.mysql.jdbc.Driver Maven 坐标： 12345678910111213&lt;!-- Mysql 6 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;6.0.6&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Mysql 5 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.49&lt;/version&gt;&lt;/dependency&gt; 代码案例： 12345678//Mysql 6Class.forName("com.mysql.cj.jdbc.Driver").new Instance(); //Mysql 5Class.forName("com.mysql.jdbc.Driver").new Instance();String URL="jdbc:mysql://localhost:3306/db_name?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false"; String Username="username"; String Password="password"; java.sql.Connection con=DriverManager.getConnection(URL,Username,Password); 1.2.3 Sql Server连接驱动名：com.microsoft.jdbc.sqlserver.SQLServerDriver Maven: 12345&lt;dependency&gt; &lt;groupId&gt;com.microsoft.sqlserver&lt;/groupId&gt; &lt;artifactId&gt;sqljdbc4&lt;/artifactId&gt; &lt;version&gt;4.0&lt;/version&gt;&lt;/dependency&gt; 案例： 12345Class.forName("com.microsoft.jdbc.sqlserver.SQLServerDriver").newInstance();String url="jdbc:microsoft:sqlserver://localhost:1433;DatabaseName=dbName";String user="user";String pwd="password";java.sql.Connection conn=DriverManager.getConnection(url,user,pwd); 第二部分 原理https://facingissuesonit.com/2018/07/22/jdbc-history-and-features-evaluations/ 参考文献及资料1、JDBC维基百科，链接：https://en.wikipedia.org/wiki/Java_Database_Connectivity 2、Java JDBC Tutorial，链接：https://www.javatpoint.com/java-jdbc 3、Open Database Connectivity，链接：https://en.wikipedia.org/wiki/Open_Database_Connectivity]]></content>
      <categories>
        <category>Maven</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Maven使用总结]]></title>
    <url>%2F2022%2F05%2F05%2F2022-05-05-Maven%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 scope标签作用 第二部分 打包 参考文献及资料 背景https://segmentfault.com/a/1190000021609439 https://www.cnblogs.com/lenve/p/12047793.html Maven 安装部署构建项目的主要环节 clean validate compile test package verify install site deploy 清理（clean）：删除以前的编译结果，为重新编译做好准备 编译（compile）：将Java 源程序编译为字节码文件 测试（test）：针对项目中的关键点进行测试，确保项目在迭代开发过程中关键点的正确性 报告（）：在每一次测试后以标准的格式记录和展示测试结果 打包（package）：将一个包含诸多文件的工程封装为一个压缩文件用于安装或部署。Java 工程对应 jar 包，Web工程对应 war 包。 安装（install）：在 Maven 环境下特指将打包的结果——jar 包或 war 包安装到本地仓库中。 部署（deploy）：将打包的结果部署到远程仓库或将 war 包部署到服务器上运行。 Maven核心概念Maven 能够实现自动化构建是和它的内部原理分不开的，这里我们从 Maven 的九个核心概念入手， 看看 Maven 是如何实现自动化构建的 POM 约定的目录结构 坐标 依赖管理 仓库管理 生命周期 插件和目标 继承 聚合 Maven 的核心程序中仅仅定义了抽象的生命周期，而具体的操作则是由 Maven 的插件来完成的。可是 Maven 的插件并不包含在 Maven 的核心程序中，在首次使用时需要联网下载。 下载得到的插件会被保存到本地仓库中。本地仓库默认的位置是：~.m2repository。 5.1. Maven约定的工程目录： Java开发领域普遍认同的一个观点：约定&gt;配置&gt;编码（能用配置解决的问题就不编码，能基于约定的就不配置） 5.2. POMProject Object Model：项目对象模型。将 Java 工程的相关信息封装为对象作为便于操作和管理的模型。 Maven 工程的核心配置。 5.3. 坐标 Maven 的坐标 使用如下三个向量在 Maven 的仓库中唯一的确定一个 Maven 工程。 groupid：公司或组织的域名倒序+当前项目名称 artifactId：当前项目的模块名称 version：当前模块的版本 123&lt;groupId&gt;net.lazyegg.maven&lt;/groupId&gt;&lt;artifactId&gt;Hello&lt;/artifactId&gt;&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; 如何通过坐标到仓库中查找 jar 包？ 将 gav 三个向量连起来 1net.lazyegg.maven+Hello+0.0.1-SNAPSHOT 以连起来的字符串作为目录结构到仓库中查找 net/lazyegg/maven/Hello/0.0.1-SNAPSHOT/Hello-0.0.1-SNAPSHOT.jar ※ 注意：我们自己的 Maven 工程必须执行安装操作才会进入仓库。安装的命令是：mvn install 5.4. 依赖Maven 中最关键的部分，我们使用 Maven 最主要的就是使用它的依赖管理功能。要理解和掌握 Maven 的依赖管理，我们只需要解决以下几个问题： ① 依赖的目的是什么当 A jar 包用到了 B jar 包中的某些类时，A 就对 B 产生了依赖，这是概念上的描述。那么如何在项目中以依赖的方式引入一个我们需要的 jar 包呢？ 答案非常简单，就是使用 dependency 标签指定被依赖 jar 包的坐标就可以了。 123456&lt;dependency&gt; &lt;groupId&gt;net.lazyegg.maven&lt;/groupId&gt; &lt;artifactId&gt;Hello&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; ② 依赖的范围有时依赖信息中除了目标 jar 包的坐标还有一个 scope 设置，这就是依赖的范围。依赖的范围有几个可选值，常用的有：compile、test、provided 三个，当然还有不常用的 runtime、system.. compile：默认范围，编译测试运行都有效 provided：在编译和测试时有效 runtime：在测试和运行时有效 test：只在测试时有效 system：在编译和测试时有效，与本机系统关联，可移植性差 常用依赖范围有效性总结 compile test provided 主程序 √ × √ 测试程序 √ √ √ 参与部署 √ × × ③ 依赖的传递性A 依赖 B，B 依赖 C，A 能否使用 C 呢？那要看 B 依赖 C 的范围是不是 compile，如果是则可用，否则不可用。 ④ 依赖的排除如果我们在当前工程中引入了一个依赖是 A，而 A 又依赖了 B，那么 Maven 会自动将 A 依赖的 B 引入当 前工程，但是个别情况下 B 有可能是一个不稳定版，或对当前工程有不良影响。这时我们可以在引入 A 的时候将 B 排除。 123456789101112&lt;dependency&gt; &lt;groupId&gt;net.lazyegg.maven&lt;/groupId&gt; &lt;artifactId&gt;Hello&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; ⑤ 统一管理所依赖 jar 包的版本，对同一个框架的一组 jar 包最好使用相同的版本。为了方便升级框架，可以将 jar 包的版本信息统一提取出来 统一声明版本号 1234&lt;properties&gt; &lt;starfish.spring.version&gt;4.1.1.RELEASE&lt;/starfish.spring.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&lt;/properties&gt; 引用前面声明的版本号 123456&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;starfish.spring.version&#125;&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; ⑥ 依赖的原则：解决 jar 包冲突 路径最短者优先 路径相同时先声明者优先 项目版本冲突时候的那种蛋疼的感觉，只有疼过的才知道，所以，我们来看看疼过的人是怎么解决的，推荐一个IDEA插件，Maven Helper，比自带的好用，一目了然 5.5. 仓库 分类 本地仓库：为当前本机电脑上的所有 Maven 工程服务 远程仓库 私服：架设在当前局域网环境下，为当前局域网范围内的所有 Maven 工程服务 中央仓库：架设在 Internet 上，为全世界所有 Maven 工程服务 中央仓库的镜像：架设在各个大洲，为中央仓库分担流量。减轻中央仓库的压力，同时更快的响应用户请求，比如阿里的镜像 仓库中的文件 Maven 的插件 我们自己开发的项目的模块 第三方框架或工具的 jar 包 ※ 不管是什么样的 jar 包，在仓库中都是按照坐标生成目录结构，所以可以通过统一的方式查询或依赖，查询地址：http://mvnrepository.com/ 5.6. 生命周期5.6.1. 什么是 Maven 的生命周期？Maven 生命周期定义了各个构建环节的执行顺序，有了这个清单，Maven 就可以自动化的执行构建命令了。 Maven 有三套相互独立的生命周期，分别是： Clean Lifecycle 在进行真正的构建之前进行一些清理工作 Default Lifecycle 构建的核心部分，编译，测试，打包，安装，部署等等 Site Lifecycle 生成项目报告，站点，发布站点 它们是相互独立的，你可以仅仅调用 clean 来清理工作目录，仅仅调用 site 来生成站点。当然你也可以直接运行 mvn clean install site 运行所有这三套生命周期。 每套生命周期都由一组阶段(Phase)组成，我们平时在命令行输入的命令总会对应于一个特定的阶段。比 如，运行 mvn clean，这个 clean 是 Clean 生命周期的一个阶段。有 Clean 生命周期，也有 clean 阶段。 5.6.2. Clean 生命周期Clean 生命周期一共包含了三个阶段： pre-clean 执行一些需要在 clean 之前完成的工作 clean 移除所有上一次构建生成的文件 post-clean 执行一些需要在 clean 之后立刻完成的工作 5.6.3. Site 生命周期 pre-site 执行一些需要在生成站点文档之前完成的工作 site 生成项目的站点文档 post-site 执行一些需要在生成站点文档之后完成的工作，并且为部署做准备 site-deploy 将生成的站点文档部署到特定的服务器上 这里经常用到的是 site 阶段和 site-deploy 阶段，用以生成和发布 Maven 站点，这可是 Maven 相当强大 的功能，Manager 比较喜欢，文档及统计数据自动生成，很好看。 5.6.4. Default 生命周期Default 生命周期是 Maven 生命周期中最重要的一个，绝大部分工作都发生在这个生命周期中（列出一些重要阶段） validate：验证工程是否正确，所有需要的资源是否可用。 compile：编译项目的源代码。 test：使用合适的单元测试框架来测试已编译的源代码。这些测试不需要已打包和布署。 package：把已编译的代码打包成可发布的格式，比如 jar、war 等。 integration-test：如有需要，将包处理和发布到一个能够进行集成测试的环境。 verify：运行所有检查，验证包是否有效且达到质量标准。 install：把包安装到maven本地仓库，可以被其他工程作为依赖来使用。 deploy：在集成或者发布环境下执行，将最终版本的包拷贝到远程的repository，使得其他的开发者或者工程可以共享 5.6.5. 生命周期与自动化构建运行任何一个阶段的时候，它前面的所有阶段都会被运行，例如我们运行 mvn install 的时候，代码会被编译，测试，打包。这就是 Maven 为什么能够自动执行构建过程的各个环节的原因。此外，Maven 的插件机制是完全依赖 Maven 的生命周期的，因此理解生命周期至关重要。 5.7. 插件和目标 Maven 的核心仅仅定义了抽象的生命周期，具体的任务都是交由插件完成的 每个插件都能实现多个功能，每个功能就是一个插件目标 Maven 的生命周期与插件目标相互绑定，以完成某个具体的构建任务 例如：compile 就是插件 maven-compiler-plugin 的一个目标；pre-clean 是插件 maven-clean-plugin 的一个目标 5.8. 继承 为什么需要继承机制？ 由于非 compile 范围的依赖信息是不能在“依赖链”中传递的，所以有需要的工程只能单独配置 创建父工程 创建父工程和创建一般的 Java 工程操作一致，唯一需要注意的是：打包方式处要设置为 pom 在子工程中引用父工程 ，从当前目录到父项目的 pom.xml 文件的相对路径 1234567 &lt;parent&gt; &lt;groupId&gt;com.starfish.maven&lt;/groupId&gt; &lt;artifactId&gt;Parent&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;!-- 以当前文件为基准的父工程pom.xml文件的相对路径 --&gt; &lt;relativePath&gt;../Parent/pom.xml&lt;/relativePath&gt;&lt;/parent&gt; 此时如果子工程的 groupId 和 version 如果和父工程重复则可以删除。 在父工程中管理依赖 将 Parent 项目中的 dependencies 标签，用 dependencyManagement 标签括起来 12345678910&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.9&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 在子项目中重新指定需要的依赖，删除范围和版本号 1234&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt;&lt;/dependency&gt; 5.9. 聚合 为什么要使用聚合？ 将多个工程拆分为模块后，需要手动逐个安装到仓库后依赖才能够生效。修改源码后也需要逐个手动进 行 clean 操作。而使用了聚合之后就可以批量进行 Maven 工程的安装、清理工作。 如何配置聚合？ 在总的聚合工程中使用 modules/module 标签组合，指定模块工程的相对路径即可 1234567&lt;!-- 配置聚合 --&gt;&lt;modules&gt; &lt;!-- 指定各个子工程的相对路径 --&gt; &lt;module&gt;starfish-learn-grpc&lt;/module&gt; &lt;module&gt;starfish-learn-kafka&lt;/module&gt; &lt;module&gt;starfish-web-demo&lt;/module&gt;&lt;/modules&gt; 参考文献及资料1、Maven官网，链接：https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html 2、Maven依赖访问，链接：https://www.baeldung.com/maven-dependency-scopes 2、案例，链接：https://stackoverflow.com/questions/7070570/can-a-program-depend-on-a-library-during-compilation-but-not-runtime/59964828#59964828]]></content>
      <categories>
        <category>Maven</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Maven中scope标签使用总结]]></title>
    <url>%2F2022%2F04%2F28%2F2022-04-28-Maven%E4%B8%ADscope%E5%8F%82%E6%95%B0%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 scope标签作用 第二部分 打包 参考文献及资料 背景我们在使用Maven管理包依赖的时候，经常看到下面的配置（一个Flink Java项目的pom文件片段）： 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-java-bridge_2.11&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 其中&lt;scope&gt;provided&lt;/scope&gt;标签参数是什么含义呢？ 我们在编码过程通常有：程序测试、程序运行、程序编译等阶段，不同阶段依赖的包并不完全相同。即依赖包是有依赖作用范围的。最直观的例子是我们经常使用的测试依赖包junit，我们只在测试阶段会使用。所以限定依赖包的生效范围为：test，参考下面的配置方式。 123456&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 接下来我们将详细展开讲解。 第一部分 scope标签作用1.1 六种依赖范围通过背景介绍，我们知道Maven中将使用 scope 标签参数来配置依赖包的依赖范围。scope 主要是用在 pom.xml 文件中的依赖定义部分。常见的可选值有compile, provided, runtime, test, system，一共6个。 compile这是默认参数，如果用户没有指定时，scope标签值为compile。编译依赖项在项目的编译、运行、测试所有阶段均有效。是一个比较强的依赖。打包的时候通常包含进去。 provided这很像compile，但表示您希望 JDK 或容器在运行时提供依赖项。例如，在为 Java 企业版构建 Web 应用程序时，您可以将 Servlet API 和相关 Java EE API 的依赖设置为范围provided，因为 Web 容器提供了这些类。具有此范围的依赖项被添加到用于编译和测试的类路径中，而不是运行时类路径中。它不是传递的。 provided意味着打包的时候可以不用包进去，别的设施(Web Container)会提供。事实上该依赖理论上可以参与编译，测试，运行等周期。相当于compile，但是在打包阶段做了exclude的动作。 runtime classpaths 这个范围表示编译时不需要依赖，但测试和运行有效。Maven在运行时和测试类路径中包含具有此范围的依赖项，但不包括编译类路径。 runntime表示被依赖项目无需参与项目的编译，不过后期的测试和运行周期需要其参与。与compile相比，跳过编译而已，说实话在终端的项目（非开源，企业内部系统）中，和compile区别不是很大。比较常见的如JSR×××的实现，对应的API jar是compile的，具体实现是runtime的，compile只需要知道接口就足够了。oracle jdbc驱动架包就是一个很好的例子，一般scope为runntime。另外runntime的依赖通常和optional搭配使用，optional为true。我可以用A实现，也可以用B实现。 范围是为了runtime防止程序员在代码中向实现库添加直接依赖项，而不是使用抽象或外观。 换句话说，它强制使用接口。 具体例子： 1）您的团队正在使用 SLF4J 而不是 Log4j。您希望您的程序员使用 SLF4J API，而不是 Log4j。Log4j 仅供 SLF4J 在内部使用。解决方案： 将 SLF4J 定义为常规编译时依赖项 将 log4j-core 和 log4j-api 定义为运行时依赖项。 2) 您的应用程序正在使用 JDBC 访问 MySQL。您希望您的程序员针对标准 JDBC 抽象进行编码，而不是直接针对 MySQL 驱动程序实现。 mysql-connector-java将（MySQL JDBC 驱动程序）定义为运行时依赖项。 运行时依赖项在编译期间被隐藏（如果您的代码对它们具有“直接”依赖关系，则会引发编译时错误），但在执行期间和创建可部署工件（WAR 文件、SHADED jar 文件等）时包含在内。 1234567&lt;!-- From Log4j 2.x API to Log4j 2.x Core --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.17.1&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 123456&lt;!-- For DataBase connection --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; test这个范围表示应用程序的正常使用不需要依赖，只在测试编译和执行阶段可用。这个范围不是传递的。通常，此范围用于测试库，例如 JUnit 和 Mockito。如果这些库用于单元测试 (src/test/java) 但不在模型代码 (src/main/java) 中，它也用于非测试库，例如 Apache Commons IO。 scope为test表示依赖项目仅仅参与测试相关的工作，包括测试代码的编译，执行。比较典型的如junit。 system范围类似于provided，除了你必须提供明确包含它的 JAR 之外。工件始终可用，不会在存储库中查找。 从参与度来说，也provided相同，不过被依赖项不会从maven仓库抓，而是从本地文件系统拿，一定需要配合systemPath属性使用。 1234567&lt;dependency&gt; &lt;groupId&gt;javax.sql&lt;/groupId&gt; &lt;artifactId&gt;jdbc-stdext&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;java.home&#125;/lib/rt.jar&lt;/systemPath&gt;&lt;/dependency&gt; import（only available in Maven 2.0.9 or later） 此范围仅受该部分中类型的依赖项支持。它表示依赖项将被指定 POM部分中的有效依赖项列表替换。由于它们被替换，范围为 的依赖项实际上并不参与限制依赖项的传递性。pom`&lt;dependencyManagement&gt;import` testscope为test表示依赖项目仅仅参与测试相关的工作，包括测试代码的编译，执行。比较典型的如junit。 runntimerunntime表示被依赖项目无需参与项目的编译，不过后期的测试和运行周期需要其参与。与compile相比，跳过编译而已，说实话在终端的项目（非开源，企业内部系统）中，和compile区别不是很大。比较常见的如JSR×××的实现，对应的API jar是compile的，具体实现是runtime的，compile只需要知道接口就足够了。oracle jdbc驱动架包就是一个很好的例子，一般scope为runntime。另外runntime的依赖通常和optional搭配使用，optional为true。我可以用A实现，也可以用B实现。 providedprovided意味着打包的时候可以不用包进去，别的设施(Web Container)会提供。事实上该依赖理论上可以参与编译，测试，运行等周期。相当于compile，但是在打包阶段做了exclude的动作。 system从参与度来说，也provided相同，不过被依赖项不会从maven仓库抓，而是从本地文件系统拿，一定需要配合systemPath属性使用。 1.2 小结 scope取值 编译 测试 运行 依赖传递 例子 compile ✓ ✓ ✓ 是 spring-core、log4j provided ✓ ✓ 否 servlet-api runtime ✓ 是 JDBC-driver test ✓ 否 junit system ✓ ✓ 是 非Maven仓库且本地 1.3 依赖传递Maven中依赖包本身具有依赖链关系。例如：项目A—(依赖)–&gt;B.jar—(依赖)–&gt;C.jar，当前A项目的pom文件中引用的B.jar，并且B.jar依赖包的scope的标签属性已知。那么C.jar在项目A中的依赖怎么确定呢？ 初始项目scope值 compile provided runtime test compile compile(*) - runtime - provided provided - provided - runtime runtime - runtime - test test - test - scope的依赖传递A–&gt;B–&gt;C。当前项目为A，A依赖于B，B依赖于C。知道B在A项目中的scope，那么怎么知道C在A中的scope呢？答案是：当C是test或者provided时，C直接被丢弃，A不依赖C；否则A依赖C，C的scope继承于B的scope。 每个范围（除了import）以不同的方式影响传递依赖关系，如下表所示。如果将依赖项设置为左列中的范围，则该依赖项的传递依赖项具有跨顶行的范围会导致主项目中的依赖项具有交叉点列出的范围。如果没有列出范围，则意味着省略了依赖项。 第二部分 打包2.1 打包范围在Java项目完成开发后，我们最终会把项目打包成jar后部署在生产运行。对于项目中依赖的jar包可能在项目运行的classpath（部署服务器环境）中了。特别常见就是对于大数开发（MapReduce、Spark、Flink）中，很多依赖包已经在客户端lib目录中了，无需重复将依赖包重复打入应用包中，甚至造成包冲突。 对于不用打包至应用包的依赖包我们配置scope标签为provided（即目标服务器已经提供了）。 compile 在开发项目的classpath（编译环境）和打包时候都有效。 provided 表示该依赖包已经由目标服务器环境（hadoop）、容器（如tomcat的libs目录）和JDK提供。只在编译的classpath中加载和使用，打包的时候不会包含在项目目标包中。 runtime 一般是运行和测试环境使用，编译时候不用加入classpath，打包时候会打包到目标包中。一般是通过动态加载或接口反射加载的情况比较多。也就是说程序只使用了接口，具体的时候可能有多个，运行时通过配置文件或jar包扫描动态加载的情况。典型的包括：JDBC驱动等。 test 单元测试场景使用。在编译环境加入classpath，但打包时不会加入。 system 不会打包至项目目标包。如果需要本地的依赖包，需要将该包注册制本地镜像库。然后再使用compile打包至项目目标包。 2.2 小结对于上面的分析总结成表格如下： scope标签值 打包情况（是否打入项目包） compile 是。打包环境有效，依赖包打包至项目包 provided 否。打包环境无效，不会打包至项目包 runtime 是。打包环境有效，依赖包打包至项目包 test 否。打包环境无效，不会打包至项目包 system 否。打包环境无效，不会打包至项目包 第三部分 总结https://segmentfault.com/a/1190000019266080 参考文献及资料1、Maven官网，链接：https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html 2、Maven依赖访问，链接：https://www.baeldung.com/maven-dependency-scopes 2、案例，链接：https://stackoverflow.com/questions/7070570/can-a-program-depend-on-a-library-during-compilation-but-not-runtime/59964828#59964828]]></content>
      <categories>
        <category>Maven</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink CDC实现实时数仓]]></title>
    <url>%2F2022%2F04%2F26%2F2022-04-26-%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%EF%BC%88Flink%20CDC%E5%AE%9E%E7%8E%B0%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景Flink 1.11 引入了 Flink SQL CDC，由阿里巴巴技术专家伍翀 (云邪）个人兴趣项目孵化而来。从最开始支持Mysql到现在支持各类数据库，具体见项目说明。 关于Flink CDC的原理介绍可以参考Flink中文社区文章介绍。本位以Mysql为例详细介绍实际使用，供大家参考。 实验环境： os:ubuntu 16.4 第一部分 环境准备目前对于Mysql版本支持如下： Connector Database Driver mysql-cdc MySQL: 5.6, 5.7, 8.0.xRDS MySQL: 5.6, 5.7, 8.0.xPolarDB MySQL: 5.6, 5.7, 8.0.xAurora MySQL: 5.6, 5.7, 8.0.xMariaDB: 10.xPolarDB X: 2.0.1 JDBC Driver: 8.0.2 1.1 Mysql开启binlog日志检查Mysql数据库是否开启binlog： 1show variables like '%log_bin%' 如果回显如下，说明已经开启： 变量名 变量值 log_bin ON log_bin_basename /var/lib/mysql/mysql-bin log_bin_index /var/lib/mysql/mysql-bin.index log_bin_trust_function_creators OFF log_bin_use_v1_row_events OFF sql_log_bin ON 如果没有开启，修改my.cnf配置文件增加下面的参数配置（重启生效）： 123# 开启binlog日志，binlog格式必须设置成：ROW模式log_bin=/var/lib/mysql/mysql-binbinlog-format=ROW 进一步还可以通过下面的命令查看binlog日志文件: 1SHOW BINARY LOGS; 回显如下： 1234Log_name, File_sizemysql-bin.000003 13979mysql-bin.000004 177mysql-bin.000005 872 还使用show binlog events命令，查看binlog日志事件： 12345Log_name, Pos, Event_type, Server_id, End_log_pos, Info'mysql-bin.000003', '742', 'Anonymous_Gtid', '223344', '807', 'SET @@SESSION.GTID_NEXT= \'ANONYMOUS\''# 字段说明：log_name：指定要查询的binlog文件名(不指定就是第一个binlog文件) 1.2 为CDC创建专用用户Flink CDC 使用 Debezium 同步Mysql，所以用户权限需要满足Debezium组件要求。 Create the MySQL user（ex：FlinkCdc）: 1mysql&gt; CREATE USER 'FlinkCdc'@'localhost' IDENTIFIED BY 'password'; Grant the required permissions to the user: 1mysql&gt; GRANT SELECT, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'FlinkCdc' IDENTIFIED BY 'password'; 注: The RELOAD permissions is not required any more when scan.incremental.snapshot.enabled is enabled (enabled by default). Keyword Description SELECT SELECT查询权限。只被用在初始化阶段。 RELOAD 执行 FLUSH 语句清除重新加载内部缓存。只被用在初始化阶段。 SHOW DATABASES 执行 SHOW DATABASE 语句。只被用在初始化阶段。 REPLICATION SLAVE 读取MySQL binlog。 REPLICATION CLIENT 执行SHOW MASTER STATUS、SHOW SLAVE STATUS、SHOW BINARY LOGS等语句。 生效权限: 1mysql&gt; FLUSH PRIVILEGES; 1.3 注意事项1.3.1 service id原因：MySQL binlog 数据同步的原理是，CDC source 会伪装成 MySQL 集群的一个 slave（使用指定的 server id 作为唯一 id），然后从 MySQL 拉取 binlog 数据。如果一个 MySQL 集群中有多个 slave 有同样的 id，就会导致拉取数据错乱的问题。解决方法：默认会随机生成一个 server id，容易有碰撞的风险。所以建议使用动态参数（table hint）在 query 中覆盖 server id。如下所示：SELECT FROM bill_info /+ OPTIONS(‘server-id’=’123456’) */ ; 每个作业需显式配置不同的SERVER ID每个同步数据库数据的客户端，都会有一个唯一ID，即SERVER ID。MySQL SERVER会根据该ID来维护网络连接以及Binlog位点。因此如果有大量不同的SERVER ID的客户端一起连接MySQL SERVER，可能导致MySQL SERVER的CPU陡增，影响线上业务稳定性。此外，多个作业共享相同的SERVER ID，会导致Binlog位点错乱，多读或少读数据。因此建议您通过动态Hints，在每个CDC作业都配置上不同的SERVER ID，例如SELECT FROM source_table /+ OPTIONS(‘server-id’=’123456’) */ ;。动态Hints详情请参见动态Hints 1.3.2 session 超时第二部分 实践案例第三部分 snapshot.mode Debezium 支持五种模式: initial ：默认模式，在没有找到 offset 时(记录在 Kafka topic 的 connect-offsets 中，Kafka connect 框架维护)，做一次 snapshot——遍历有 SELECT 权限的表，收集列名，并且将每个表的所有行 select 出来写入 Kafka； when_needed: 跟 initial 类似，只是增加了一种情况，当记录的 offset 对应的 binlog 位置已经在 MySQL 服务端被 purge 了时，就重新做一个 snapshot。 never: 不做 snapshot，也就是不拿所有表的列名，也不导出表数据到 Kafka，这个模式下，要求从最开头消费 binlog，以获取完整的 DDL 信息，MySQL 服务端的 binlog 不能被 purge 过，否则由于 DML binlog 里只有 database name、table name、column type 却没有 column name，Debezium 会报错 Encountered change event for table some_db.some_table whose schema isn&#39;t known to this connector； schema_only: 这种情况下会拿所有表的列名信息，但不会导出表数据到 Kafka，而且只从 Debezium 启动那刻起的 binlog 末尾开始消费，所以很适合不关心历史数据，只关心最近变更的场合。 schema_only_recovery: 在 Debezium 的 schema_only 模式出错时，用这个模式恢复，一般不会用到。 https://www.jianshu.com/p/d6ac601438a5 附录 Connector Database Driver mongodb-cdc MongoDB: 3.6, 4.x, 5.0 MongoDB Driver: 4.3.1 mysql-cdc MySQL: 5.6, 5.7, 8.0.xRDS MySQL: 5.6, 5.7, 8.0.xPolarDB MySQL: 5.6, 5.7, 8.0.xAurora MySQL: 5.6, 5.7, 8.0.xMariaDB: 10.xPolarDB X: 2.0.1 JDBC Driver: 8.0.27 oceanbase-cdc OceanBase CE: 3.1.x JDBC Driver: 5.7.4x oracle-cdc Oracle: 11, 12, 19 Oracle Driver: 19.3.0.0 postgres-cdc PostgreSQL: 9.6, 10, 11, 12 JDBC Driver: 42.2.12 sqlserver-cdc Sqlserver: 2012, 2014, 2016, 2017, 2019 JDBC Driver: 7.2.2.jre8 tidb-cdc TiDB: 5.1.x, 5.2.x, 5.3.x, 5.4.x, 6.0.0 JDBC Driver: 8.0.27 参考文献及资料 Flink CDC 2.0 正式发布，详解核心改进，链接：https://flink-learning.org.cn/article/detail/3ebe9f20774991c4d5eeb75a141d9e1e Working with MySQL Binary Logs，链接：https://maxchadwick.xyz/blog/working-with-mysql-binary-logs debezium，链接：https://debezium.io/documentation/reference/1.5/connectors/mysql.html#mysql-creating-user]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink 中的API]]></title>
    <url>%2F2022%2F04%2F26%2F2022-04-28-Flink%20%E4%B8%AD%E7%9A%84API%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景第一部分 Flink 中的 APIFlink 为流式/批式处理应用程序的开发提供了不同级别的抽象。 Flink API 最底层的抽象为有状态实时流处理。其抽象实现是 Process Function，并且 Process Function 被 Flink 框架集成到了 DataStream API 中来为我们使用。它允许用户在应用程序中自由地处理来自单流或多流的事件（数据），并提供具有全局一致性和容错保障的状态。此外，用户可以在此层抽象中注册事件时间（event time）和处理时间（processing time）回调方法，从而允许程序可以实现复杂计算。 Flink API 第二层抽象是 Core APIs。实际上，许多应用程序不需要使用到上述最底层抽象的 API，而是可以使用 Core APIs 进行编程：其中包含 DataStream API（应用于有界/无界数据流场景）和 DataSet API（应用于有界数据集场景）两部分。Core APIs 提供的流式 API（Fluent API）为数据处理提供了通用的模块组件，例如各种形式的用户自定义转换（transformations）、联接（joins）、聚合（aggregations）、窗口（windows）和状态（state）操作等。此层 API 中处理的数据类型在每种编程语言中都有其对应的类。 Process Function 这类底层抽象和 DataStream API 的相互集成使得用户可以选择使用更底层的抽象 API 来实现自己的需求。DataSet API 还额外提供了一些原语，比如循环/迭代（loop/iteration）操作。 Flink API 第三层抽象是 Table API。Table API 是以表（Table）为中心的声明式编程（DSL）API，例如在流式数据场景下，它可以表示一张正在动态改变的表。Table API 遵循（扩展）关系模型：即表拥有 schema（类似于关系型数据库中的 schema），并且 Table API 也提供了类似于关系模型中的操作，比如 select、project、join、group-by 和 aggregate 等。Table API 程序是以声明的方式定义应执行的逻辑操作，而不是确切地指定程序应该执行的代码。尽管 Table API 使用起来很简洁并且可以由各种类型的用户自定义函数扩展功能，但还是比 Core API 的表达能力差。此外，Table API 程序在执行之前还会使用优化器中的优化规则对用户编写的表达式进行优化。 表和 DataStream/DataSet 可以进行无缝切换，Flink 允许用户在编写应用程序时将 Table API 与 DataStream/DataSet API 混合使用。 Flink API 最顶层抽象是 SQL。这层抽象在语义和程序表达式上都类似于 Table API，但是其程序实现都是 SQL 查询表达式。SQL 抽象与 Table API 抽象之间的关联是非常紧密的，并且 SQL 查询语句可以在 Table API 中定义的表上执行。 第二部分参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：https://github.com/ververica/flink-cdc-connectors]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CDC总结]]></title>
    <url>%2F2022%2F04%2F26%2F2022-05-11-CDC%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景第一部分 概述https://www.striim.com/blog/change-data-capture-cdc-what-it-is-and-how-it-works/ 第一部分 实践参考文献及资料1、]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据仓库学习系列-血缘关系]]></title>
    <url>%2F2022%2F04%2F20%2F2022-04-24-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景血缘分析是一种技术手段，用于对数据处理过程的全面追踪，从而找到某个数据对象为起点的所有相关元数据对象以及这些元数据对象之间的关系。元数据对象之间的关系特指表示这些元数据对象的数据流输入输出关系。在元数据管理系统成型后，我们便可以通过血缘分析来对数据仓库中的数据健康、数据分布、集中度、数据热度等进行分析。 二 在企业的数据资产中，通过数据血缘，我们能够解决以下几个场景的问题：1、快速了解到一张业务报表来源于那几个系统、来源于哪几张表。2、快速洞察到这张业务报表经过了几个步骤生成的。3、在数据仓库中，能快速判断一张ODS层的源表数据会流动到下游哪些DW、DM层数据表内。如果改动这张源表，会影响到哪些下游数据、哪些业务报表。4、在生产环境中，如果有一张数据表出现错误，通过数据血缘图，可以快速知晓，数据错误来源于前面哪个环节、可能会影响到后面哪个环节。5、通过分析数据血缘图的关键节点（即图中节点入度、出度较高的节点），我们能更好的总结出哪些业务数据是被经常用到的、是关键性的，更好的编排节点间的调度工作流，优化计算资源。 https://cloud.tencent.com/developer/article/1876878 魔改hive实现：https://www.i4k.xyz/article/qq_44962075/106945827 第一部分 atlas开源项目Apache Atlas 是 Apache 基金会的孵化项目，是 Hadoop 生态圈的数据治理和元数据框架。Atlas 是一套核心基础治理服务的集合，有很好的伸缩性和可扩展性，能够满足企业对 Hadoop 生态系统的多样性需求，并能和企业的数据生态系统集成。它为 Hadoop 集群提供了包括数据分类、集中策略引擎、数据血缘、安全和生命周期管理在内的元数据治理核心能力。 但 atlas 的缺点是：只能对 hadoop 的元数据进行管理（虽然也是连的 Mysql ），对传统数据库的支持力度非常小；同时血缘分析也只支持特定的数据库。 三 Atlas的数据溯源功能介绍Atlas主要针对与Hadoop旗下的数据产品进行数据溯源。Apache Hive等应用在产生元数据变化（比如新增表、新增字段）和数据流动（比如Insert into）的时候，通过钩子的方式主动告知Atlas更新数据血缘图。 四 Atlas是如何实现数据溯源的？ Atlas主要通过Hook方式让Hive将元数据信息通过Apache Kafka传送过来。Atlas使用了JanusGraph[2]做图数据存储引擎。借助JanusGraph，数据血缘关系主要通过图的形式进行存储在Hbase中，每个节点的详细信息存储在Solr中。除了前端UI，Atlas还支持第三方应用通过API、Kafka获取数据血缘相关信息。 元数据管理在数据仓库的实践应用https://blog.51cto.com/wang/4186308 https://atlas.apache.org/#/HookHive https://www.csdn.net/tags/MtTaMg4sMDE5OTgyLWJsb2cO0O0O.html 源码分析：https://www.1024sou.com/article/27372.html https://aws.amazon.com/cn/blogs/china/apache-atlas-data-bloodline/ 其他数据库的自定义方法:https://github.com/picomy/manually-catch-data-for-atlas 另一个产品： https://absaoss.github.io/spline/ 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python语言中函数装饰器总结]]></title>
    <url>%2F2022%2F04%2F18%2F2022-04-18-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Python%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%87%BD%E6%95%B0%E8%A3%85%E9%A5%B0%E5%99%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 包和模块 第二部分 Import 方法 第三部分 命名空间（namespace） 第四部分 Import的过程 第五部分 将模块、包的路径加入检索路径 参考文献及资料 背景https://foofish.net/python-decorator.html 前面章节中，我们已经讲解了 Python 内置的 3 种函数装饰器，分别是 ＠staticmethod、＠classmethod 和 @property，其中 staticmethod()、classmethod() 和 property() 都是 Python 的内置函数。 在Python语言中，一切皆为对象，甚至于函数。函数可以像普通变量一样当做参数传给别的函数，也可以赋值给其他变量。我们看一下下面的例子： 123456789101112def func1(): print("func1")def func2(func): func() print("func2")func2(func1)del func1func3 = func1func3() func1作为参数传给func2 func3指向函数func1，即使func1被删除。 第一部分装饰器本质上是一个 Python 函数或类，它可以让其他函数或类在不需要做任何代码修改的前提下增加额外功能，装饰器的返回值也是一个函数/类对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景，装饰器是解决这类问题的绝佳设计。有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。 参考文章1、http://www.cnblogs.com/russellluo/p/3328683.html# 2、https://github.com/Liuchang0812/slides/tree/master/pycon2015cn]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink内存管理和优化]]></title>
    <url>%2F2022%2F04%2F17%2F2022-04-17-Flink%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%92%8C%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Flink内存管理详解 第二部分 Flink内存优化 第三部分 总结 参考文献及资料 背景Flink计算框架和Spark相同，都是基于JVM虚拟机实现的。基于 JVM 的数据分析引擎都需要面对将大量数据存到内存中，这就不得不面对 JVM 存在的几个问题： Java 对象存储密度低。一个只包含 boolean 属性的对象占用了16个字节内存：对象头占了8个，boolean 属性占了1个，对齐填充占了7个。而实际上只需要一个bit（1/8字节）就够了。 Full GC 会极大地影响性能，尤其是为了处理更大数据而开了很大内存空间的JVM来说，GC 会达到秒级甚至分钟级。 OOM 问题影响稳定性。OutOfMemoryError是分布式计算框架经常会遇到的问题，当JVM中所有对象大小超过分配给JVM的内存大小时，就会发生OutOfMemoryError错误，导致JVM崩溃，分布式框架的健壮性和性能都会受到影响。 第一部分参考文献及资料1、《Deep Dive: Apache Spark Memory Management》介绍视频，链接：https://youtu.be/dPHrykZL8Cg 2、探索Spark Tungsten的秘密，链接：https://github.com/hustnn/TungstenSecret]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[智能运维综述系列（日志处理）]]></title>
    <url>%2F2022%2F03%2F28%2F2022-03-30-%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4%E7%BB%BC%E8%BF%B0%E7%B3%BB%E5%88%97%EF%BC%88%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 第四部分 参考文献及资料 背景信息系统在运行过程中会产生海量的日志数据。按照系统类型分，主要有操作系统日志和应用日志，其中应用日志有：基础设施类平台日志（例如：vmware日志、openstack日志、K8s日志等）、应用服务进程日志。 程序中日志原生作用就是记录程序运行过程中关键步骤的状态数据和参数信息，当信息系统出现故障或异常的时候，运维和开发人员能通过查看系统日志，回溯甚至复现问题，并定位原因。这是传统运维的经典维护场景。 随着信息系统云化和分布式化，系统越加复杂和庞大，产生的日志更是海量级别的。如果仍然通过人工查看日志，没有任何自动化辅助机制，这个工作是繁杂和低效的。所以急需将这个过程抽象成自动化过程，利用人工智能方式进行智能辅助，甚至完全自动化分析定位。这个工作对于信息系统维护是极具价值的。 第一部分 日志模板挖掘日志通常是运行程序产生，当前各类语言通常有自己的日志框架包实现，例如下面Spark源码： 12// core\src\main\scala\org\apache\spark\storage\memory\MemoryStore.scalalogInfo("MemoryStore started with capacity %s".format(Utils.bytesToString(maxMemory))) 运行中生成日志如下： 117/06/09 20:10:41 INFO storage.MemoryStore: MemoryStore started with capacity 17.7 GB 通常日志中有相应的日志模板，运行中补充模板中参数数据。但是在实际日志数据分析中，通常没有这些模板信息，就需要通过通过海量日志数据进行分析，提炼有用的数据信息。 1.1 频繁项方法日志数据本质是文本数据，均是由日志模板填充数据生成的。最朴素的想法就是对日志数据进行分词形成词向量，然后统计单词出现的频率，找出频繁集合其实就是日志模板。例如下面的HDFS服务日志案例： 12345678081109 203615 148 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_38865049064139660 terminating081109 203807 222 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-6952295868487656571 terminating081109 204005 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.73.220:50010 is added to blk_7128370237687728475 size 67108864081109 204015 308 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8229193803249955061 terminating081109 204106 329 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6670958622368987959 terminating081109 204132 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.43.115:50010 is added to blk_3050920587428079149 size 67108864081109 204324 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.203.80:50010 is added to blk_7888946331804732825 size 67108864081109 204453 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.85:50010 is added to blk_2377150260128098806 size 67108864 提取出两个日志模板： 12* INFO dfs.DataNode$PacketResponder: PacketResponder * for block * terminating* INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: * is added to * size * 其中最具代表性的算法是FPTree(Frequent Pattern Tree)算法。该算法从Apriori算法 算法论文：清华/南开/腾讯的 FT-tree 开源实现：https://github.com/WeibinMeng/ft-tree 自然语言处理方法深度学习方法DeepLog 论文(包含模式检测、参数检测、工作流检测三部分)： https://acmccs.github.io/papers/p1285-duA.pdf 开源实现：https://github.com/wuyifan18/DeepLog 另一个开源实现，还实现了另外两种算法LogAnomaly和RobustLog，可切换：https://github.com/donglee-afar/logdeep 开源数据集收集日志 国防科大的日志领域研究综述(日志监测部分比我前面列的老，但还提了基于源码的静态分析和基于虚拟机增强的日志内容改进两个方向，基本都是袁丁教授团队做的)： http://www.jos.org.cn/1000-9825/4936.htm morningpaper 博客关于静态分析的 lprof 论文解析：https://blog.acolyer.org/2015/10/08/lprof-a-non-intrusive-request-flow-profiler-for-distributed-systems/ morningpaper 博客关于日志增强的 log20 论文解析：https://blog.acolyer.org/2017/11/03/log20-fully-automated-optimal-placement-of-log-printing-statements-under-specified-overhead-threshold/ 香港中文大学的日志领域研究综述(比国防科大的新，加入了关于日志压缩、人机交互、语义等新方向)：https://arxiv.org/pdf/2009.07237.pdf 荷兰代尔夫特理工大学的日志领域研究综述(2021 年，统计了不同方向的研究趋势)：https://pdfs.semanticscholar.org/b3c1/e91f3f73ff1d63504fb8d522558baa7334d4.pdf?_ga=2.256964171.1591127296.1641452870-511869175.1640757218 加拿大滑铁卢大学的日志领域研究综述(2022 年，总结了各方向各算法的优劣)：https://arxiv.org/pdf/2110.12489.pdf 香港中文大学团队收集的多篇日志异常检测相关论文和数据集(共87GB)： https://github.com/logpai/loghub 他们也做了各种现有算法的开源实现和自己的 Drain 算法进行横向测试对比，报告见：https://arxiv.org/pdf/1811.03509.pdf 华为开源的 NuLog 项目(采用MLM掩码语言模型，并复现了上一篇论文一样的对比)：https://github.com/nulog/nulog IBM云数据中心团队改进和开源的 Drain3 包，加强了持久化，自定义参数替换等：https://github.com/IBM/Drain3 一个开源流式日志模板挖掘器。 IBM 在 Drain3 基础上，通过公开文档爬虫获取事件 ID 的关键字描述，然后走语义分析相似度，来提取复杂变量类型(即除了常量、变量以外，新定义了sequential、optional 和 single-select 类型)：https://arxiv.org/pdf/2202.07169.pdf 上海交通大学采用日志中的 punct 部分作为日志模式学习的来源，实现了一个 logpunk 系统，在 loghub 下对比，效果居然也好过其他算法：https://www.mdpi.com/2076-3417/11/24/11974/pdf 微软的 UniParser 论文，通过语义分析，识别训练集中某些常量为变量：https://arxiv.org/pdf/2202.06569.pdf 斯里兰卡莫拉图瓦大学/WSO2 公司的 vue4logs-parser 开源实现，直接利用倒排索引搜索相关性来完成模式过滤：https://github.com/IsuruBoyagane15/vue4logs-parser IBM 研究院基于语言模型做的日志异常检测模型，对比了 fasttext 和 BERT 的效果：https://www.researchgate.net/publication/344693315_Using_Language_Models_to_Pre-train_Features_for_Optimizing_Information_Technology_Operations_Management_Tasks 香港中文大学的 LogZip 开源实现： https://github.com/logpai/logzip 清华/阿里的 LogReducer 系统(用 C/C++ 重写了 logzip，并加上对特定数值型参数值的差分、关联和变长压缩优化)，论文：https://www.usenix.org/system/files/fast21-wei.pdf 香港中文大学的 SemParser 论文，尝试用语义分析来命名模式中的参数位：https://arxiv.org/pdf/2112.12636.pdf 中山大学的 SwissLog 论文，和 RobustLog 一样关注模型的鲁棒性问题：https://www.researchgate.net/publication/346867203_SwissLog_Robust_and_Unified_Deep_Learning_Based_Log_Anomaly_Detection_for_Diverse_Faults 清华/南开/百度的 LogClass 开源实现：https://github.com/NetManAIOps/LogClass 北卡顾晓晖团队做日志异常检测的 ELT 系统(拆分为粗粒度的 MAV 和细粒度的 MFG 两层)：http://dance.csc.ncsu.edu/papers/srds11.pdf NEC 美国实验室/北卡做云系统工作流监控的 CloudSeer 系统：https://people.engr.ncsu.edu/gjin2/Classes/591/Spring2017/case-cloudseer.pdf NEC 美国实验室 LogMine 系统： http://www.cs.unm.edu/~mueen/Papers/LogMine.pdf 开源实现：https://github.com/trungdq88/logmine NEC 美国实验室/蚂蚁金服做的 LogLens 系统(在 LogMine 基础上，和 ELK 的 Grok 设计结合；并加上了对 traceid 的判断处理，支持序列异常检测)，论文：http://120.52.51.14/www.cs.ucsb.edu/~bzong/doc/icdcs-18.pdf 香港中文大学/华为的 POP 系统(和 LogMine 思路比较类似，在 Spark 上运行)：http://www.cse.cuhk.edu.hk/lyu/_media/journal/pjhe_tdsc18.pdf 康考迪亚大学发表的 logram 论文(用 n-gram 来做日志解析)：https://petertsehsun.github.io/papers/HetongTSE2020.pdf 加拿大麦克马斯特大学的日志序列异常检测开源实现，加上序列每一步 duration 子序列做神经网络特征：https://github.com/hfyxin/Ts-models-log-data-analysis RedHat公司CTO办公室开源的Log Anomaly Detector项目(基于word2vec和SOM算法)：https://github.com/AICoE/log-anomaly-detector 其他商业公司： Loomsystems(已被 serviceNow 收购，其对参数类型的 meter/gauge/timeless-gauge/histogram/invalid/root-cause 分类值得借鉴)：https://www.loomsystems.com/hubfs/SophieTechnicalOverview.pdf coralogix(有基础的无关顺序的关联模式检测，对 XML/JSON 类型进行对象参数检测)：https://coralogix.com/tutorials/what-is-coralogix-pattern-anomaly/ zebrium(存 newsql，参数名称的自动识别值得借鉴，最后用 GPT-3 生成告警描述也很有趣)：https://www.zebrium.com/blog/using-ml-to-auto-learn-changing-log-structures 参考文献及资料1、CloudRCA: A Root Cause Analysis Frameworkfor Cloud Computing Platforms，链接：https://arxiv.org/pdf/2111.03753 2、logparser项目，链接：https://github.com/logpai/logparser 3、Syslog Processing for Switch Failure Diagnosis and Prediction in Datacenter Networks，链接：https://netman.aiops.org/wp-content/uploads/2015/12/IWQOS_2017_zsl.pdf]]></content>
      <categories>
        <category>aiops</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[智能运维综述系列（异常检测）]]></title>
    <url>%2F2022%2F03%2F28%2F2022-03-30-%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4%E7%BB%BC%E8%BF%B0%E7%B3%BB%E5%88%97%EF%BC%88%E6%80%A7%E8%83%BD%E5%AE%B9%E9%87%8F%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 第四部分 参考文献及资料 背景参考文献及资料1、]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch中数据查询总结]]></title>
    <url>%2F2022%2F03%2F28%2F2022-03-28-Elasticsearch%E4%B8%AD%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Search 接口 第二部分 Scroll 接口 第三部分 Scan 接口 第四部分 总结 参考文献及资料 背景Elasticsearch是为了全文检索出现的，在实际使用中查询数据是客户端和集群交互的主要交互形式。我们先看一下Elasticsearch的查询原理： 第一部分 Search 接口1.1 接口案例我们使用Elasticsearch官方提供 Python 包进行查询交互： 12345678910111213141516171819from elasticsearch import ElasticsearchelasticClient = Elasticsearch('http://192.168.52.142:9200')query_body = &#123; "query":&#123; "bool":&#123; "must":&#123; "match":&#123; "text_entry": "we" &#125; &#125; &#125; &#125;, "from": 1, "size": 1&#125;result = elasticClient.search(index="shakespeare", body=query_body)print(result) 查询结果为: 12345678910111213141516171819202122232425262728293031323334&#123; "took": 0, "timed_out": false, "_shards": &#123; "total": 1, "successful": 1, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": &#123; "value": 3109, "relation": "eq" &#125;, "max_score": 5.6665335, "hits": [ &#123; "_index": "shakespeare", "_type": "_doc", "_id": "35519", "_score": 5.6665335, "_source": &#123; "type": "line", "line_id": 35520, "play_name": "Hamlet", "speech_number": 33, "line_number": "4.5.119", "speaker": "Danes", "text_entry": "We will, we will." &#125; &#125; ] &#125;&#125; 按照顺序说明一下字段参数的含义： took，表示Elasticsearch查询语句花费的时间，单位毫秒； timed_out，表示查询是否超时； _shards，表示查询中查看的分片（shard）信息，其中total表示shard总量，successful成功个数，skipped跳过个数，failed失败个数； hits ，查询的结果； hits.total，是包含与搜索条件匹配的文档总数信息的对象 hits.total.value，表示查询共命中数量（结合hits.total.relation参数看）； hits.total.relation，当值是eq时，hits.total.value的值是准确计数。当值是gte时，hits.total.value的值是不准确的。 hits.max_score，最大分数； hits.hits，查询结果数据存储List（默认为前10个文档，查询中size参数决定）。 第35行，hits.sort 表示结果排序键（如果请求中没有指定，则默认按分数排序）。 现在我们来重点关注11~14行，参数track_total_hits可以控制total值的准确性，可以设为false、true或正整数。当为false时，不再返回total计数，total始终为1；当为正整数时，表示精确的命中文档计数，例如为1000时，返回如下结构 : 1234"total": &#123; "value": 3109, "relation": "eq"&#125; 此时如果命中结果小于或等于1000时，value是精确的计数，relation是eq，如果命中的结果数大于了1000时，此时value等于1000，relation是gte，如果需要精确统计结果数，这个值要设置的很大或者设为true，这个参数的设基更多的出于性能考虑。具体工程应用中，我们可以首先检查relation的值，如果是eq，则表示value的值真实代表了命中的结果数，如果为gte表示value的值是不准确的。 那么track_total_hits这个参数到底如何设置才是最合理的呢？这要结合具体的业务需求和应用场景。可以遵循如下三个原则： 保持默认值：10000，不变，这足以满足一般的业务需求，就算是淘宝、京东这样的大型电商网站，一页展示40个结果，10000个结果可以展示250页，相信没有用户会看250页后的商品，大多数情况下用户基本上都是浏览前10也的商品。如果需要精确知道命中的文档数量，此时应把track_total_hits设置为true,但用户需要清楚的明白，如果命中的文档数量很大，会影响查询性能，而且会消耗大量的内存，甚至存在内存异常的风险。如果你确切知道不需要知道命中的结果数，则把track_total_hits设为false,这会提升查询性能。 一般的分页需求我们可以使用form和size的方式实现，但是这种分页方式在深度分页的场景下应该是要避免使用的。深度分页会随着请求的页次增加，所消耗的内存和时间的增长也是成比例的增加，为了避免深度分页产生的问题，elasticsearch从2.0版本开始，增加了一个限制： 1index.max_result_window =10000 1.2 接口说明用search API 可以获取数据，但是存在下面两个问题： 返回的数据默认只有10条 这个是因为ES默认的返回数据都值设置成了10条，所以无论索引中的文档数有多少，都只反馈最开始的10条 我们可以看到，可以根据设置size数值，来获取返回的文档数，但是另外的问题又来了，如果文档数量大于10000（size的最大值）呢？且这样获取貌似速度上比较慢？ 2.文档数量大于10000，且查询速度不够快 这时候scan API就派上用场了。 第二部分 Scroll 接口elasticsearch的scroll是什么？可以简单理解为mysql的cursor游标，比如你一次请求的数据会很大，可以使用scroll这样的流式接口，scroll会把你的所需要的结果标记起来。但是这scroll的查询还是会对数据进行排序的，这样会影响性能。 如果你只是单纯的想要数据，那么可以使用scan，因为scan会告诉 elasticsearch 不去排序。scan模式会扫描shard分片中的数据,单纯的扫除匹配，而不会像scroll进行排序处理。 https://zhuanlan.zhihu.com/p/392363821 Slice使用方式以及原理。多线程方式提升查询速度。 slice 在 search + scroll 的时候可以用，在聚类结果导出的时候也可以用。 注意用完的scroll应该及时删除。es默认保留500个滚动任务id。多了就不能再使用了！ 注意scroll状态保留的时间。如果你后续接到数据以后的处理时间比较长，或者查询本身时间花费也长，你应该合理的设置一个长的状态保留时间。 第三部分 Scan 接口不用排序，单纯将数据查出来。 如果没有排序的深度分页需求，最好使用 scan scroll的组合。 scan scroll的流式接口用法很是简单,在url里扩充字段 search_type 是scan类型，scroll是3分钟，当次查询的结果会在elasticsearch标记3分钟。这里的size 1000个会在每个shard起到作用。 并不是把所有结果限制为1000个 ！ 如果你的分片数目有10个，那么你最多可以拿到 1000 * 10的数据。 第四部分 总结当需求查询es数据库中大量数据时,用_search就不符合应用场景了，建议使用helpers.scan，helpers.scan返回的数据对象时迭代器，很大节省内存空间，而且查询速度要远远大于search；search在利用from、size参数控制返回数据的条数,使用 from and size 的深度分页，size=10&amp;from=10000 是非常低效的，因为 100,000 排序的结果必须从每个分片上取出并重新排序最后返回 10 条。这个过程需要对每个请求页重复，scroll进行数据分页，也可以返回大数据，但是search返回的数据是以list的形式，如果一次需要返回的数据量比较大的话，则会十分耗费内存，而且数据传输速度也会比较慢 第五部分 附录5.1 参数track_total_hits说明Generally the total hit count can’t be computed accurately without visiting all matches, which is costly for queries that match lots of documents. The track_total_hits parameter allows you to control how the total number of hits should be tracked. Given that it is often enough to have a lower bound of the number of hits, such as “there are at least 10000 hits”, the default is set to 10,000. This means that requests will count the total hit accurately up to 10,000 hits. It’s is a good trade off to speed up searches if you don’t need the accurate number of hits after a certain threshold. When set to true the search response will always track the number of hits that match the query accurately (e.g. total.relation will always be equal to &quot;eq&quot; when track_total_hits is set to true). Otherwise the &quot;total.relation&quot; returned in the &quot;total&quot; object in the search response determines how the &quot;total.value&quot; should be interpreted. A value of &quot;gte&quot; means that the &quot;total.value&quot; is a lower bound of the total hits that match the query and a value of &quot;eq&quot; indicates that &quot;total.value&quot; is the accurate count. 123456789GET twitter/_search&#123; "track_total_hits": true, "query": &#123; "match" : &#123; "message" : "Elasticsearch" &#125; &#125;&#125; Copy as curlView in Console … returns: 12345678910111213&#123; &quot;_shards&quot;: ... &quot;timed_out&quot;: false, &quot;took&quot;: 100, &quot;hits&quot;: &#123; &quot;max_score&quot;: 1.0, &quot;total&quot; : &#123; &quot;value&quot;: 2048, &quot;relation&quot;: &quot;eq&quot; &#125;, &quot;hits&quot;: ... &#125;&#125; The total number of hits that match the query. The count is accurate (e.g. &quot;eq&quot; means equals). It is also possible to set track_total_hits to an integer. For instance the following query will accurately track the total hit count that match the query up to 100 documents: 123456789GET twitter/_search&#123; "track_total_hits": 100, "query": &#123; "match" : &#123; "message" : "Elasticsearch" &#125; &#125;&#125; Copy as curlView in Console The hits.total.relation in the response will indicate if the value returned in hits.total.value is accurate (&quot;eq&quot;) or a lower bound of the total (&quot;gte&quot;). For instance the following response: 12345678910111213&#123; &quot;_shards&quot;: ... &quot;timed_out&quot;: false, &quot;took&quot;: 30, &quot;hits&quot; : &#123; &quot;max_score&quot;: 1.0, &quot;total&quot; : &#123; &quot;value&quot;: 42, &quot;relation&quot;: &quot;eq&quot; &#125;, &quot;hits&quot;: ... &#125;&#125; 42 documents match the query and the count is accurate (&quot;eq&quot;) … indicates that the number of hits returned in the total is accurate. If the total number of hits that match the query is greater than the value set in track_total_hits, the total hits in the response will indicate that the returned value is a lower bound: 1234567891011&#123; &quot;_shards&quot;: ... &quot;hits&quot; : &#123; &quot;max_score&quot;: 1.0, &quot;total&quot; : &#123; &quot;value&quot;: 100, &quot;relation&quot;: &quot;gte&quot; &#125;, &quot;hits&quot;: ... &#125;&#125; There are at least 100 documents that match the query This is a lower bound (&quot;gte&quot;). If you don’t need to track the total number of hits at all you can improve query times by setting this option to false: 123456789GET twitter/_search&#123; "track_total_hits": false, "query": &#123; "match" : &#123; "message" : "Elasticsearch" &#125; &#125;&#125; Copy as curlView in Console … returns: 123456789&#123; &quot;_shards&quot;: ... &quot;timed_out&quot;: false, &quot;took&quot;: 10, &quot;hits&quot; : &#123; &quot;max_score&quot;: 1.0, &quot;hits&quot;: ... &#125;&#125; The total number of hits is unknown. Finally you can force an accurate count by setting &quot;track_total_hits&quot; to true in the request. 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[智能运维综述系列（异常检测）]]></title>
    <url>%2F2022%2F03%2F28%2F2022-03-30-%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4%E7%BB%BC%E8%BF%B0%E7%B3%BB%E5%88%97%EF%BC%88%E5%91%8A%E8%AD%A6%E5%A4%84%E7%90%86%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 第四部分 参考文献及资料 背景参考文献及资料1、]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[智能运维综述系列（异常检测）]]></title>
    <url>%2F2022%2F03%2F28%2F2022-03-30-%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4%E7%BB%BC%E8%BF%B0%E7%B3%BB%E5%88%97%EF%BC%88%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 第四部分 参考文献及资料 背景参考文献及资料1、]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[智能运维综述系列（异常检测）]]></title>
    <url>%2F2022%2F03%2F28%2F2022-03-30-%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4%E7%BB%BC%E8%BF%B0%E7%B3%BB%E5%88%97%EF%BC%88%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 第四部分 参考文献及资料 背景参考文献及资料1、]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[智能运维综述系列（异常检测）]]></title>
    <url>%2F2022%2F03%2F28%2F2022-03-30-%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4%E7%BB%BC%E8%BF%B0%E7%B3%BB%E5%88%97%EF%BC%88%E6%A0%B9%E6%BA%90%E5%AE%9A%E4%BD%8D%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 第四部分 参考文献及资料 背景参考文献及资料1、]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[智能运维综述系列（异常检测）]]></title>
    <url>%2F2022%2F03%2F28%2F2022-03-30-%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4%E7%BB%BC%E8%BF%B0%E7%B3%BB%E5%88%97%EF%BC%88%E8%AE%BF%E9%97%AE%E9%93%BE%E8%B7%AF%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 第四部分 参考文献及资料 背景目前，链路追踪组件有Google的Dapper，Twitter 的Zipkin，以及阿里的Eagleeye （鹰眼）等，它们都是非常优秀的链路追踪开源组件。 第一部分 服务调用链系统服务调用链路是指从用户或是机器发起服务请求到结束，按顺序记录整个请求链路的相关数据，以备后续查询分析、定位系统 bug 或性能优化所用。 目前市面上，几乎所有服务调用链路的实现，理论基础都是基于 Google Dapper 的那篇论文，其中最重要的概念就是 traceId 和 spanId。 traceId 记录整个服务链路的 ID，由首次请求方创建，服务链路中唯一。一系列Span组成的一个树状结构。请求一个微服务系统的API接口，这个API接口，需要调用多个微服务，调用每个微服务都会产生一个新的Span，所有由这个请求产生的Span组成了这个Trace。 spanId 记录当前服务块的 ID，由当前服务方创建。基本工作单元，发送一个远程调度任务 就会产生一个Span，Span是一个64位ID唯一标识的，Trace是用另一个64位ID唯一标识的，Span还有其他数据信息，比如摘要、时间戳事件、Span的ID、以及进度ID。 parentId 记录上一个请求服务的 spanId。 第二部分数据说明： [‘timestamp’, ‘cmdb_id’, ‘span_id’, ‘trace_id’, ‘duration’, ‘type’, ‘status_code’, ‘operation_name’, ‘parent_span’] Trace Tree 一次跟踪树 唯一标识是：trace_id。一颗跟踪树有多个Span构成。 span_id，标识这次调用的Span Span具有父节点（也是Span） cmdb_id 是微服务的名称（或id） timestamp span调用时间 duration，表示Span这次调用的耗时 服务调用关系图： 一共有40个服务（cmdb_id ），通过调用父子关系。 参考文献及资料1、Google 论文，链接：Dapper, a Large-Scale Distributed Systems Tracing Infrastructure]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-GPU开发环境部署总结]]></title>
    <url>%2F2022%2F03%2F20%2F2022-03-20-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-GPU%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 GPU环境 第二部分 Tensorflow-GPU环境部署 第三部分 Pytorch-GPU环境部署 参考文献及资料 背景本文环境：Ubuntu 16.04 GPU：Nvidia GTX 1080 第一部分 GPU环境1.1 种类目前市场上商用GPU主要是N卡（Nvidia生产）和A卡（AMD生产）。本文主要是介绍N卡环境部署。 12345678910111213root@deeplearning:~# lspci -k | grep -A 2 -i "VGA"00:02.0 VGA compatible controller: Intel Corporation HD Graphics 630 (rev 04) DeviceName: Onboard IGD Subsystem: ASUSTeK Computer Inc. Device 872f--01:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1080] (rev a1) Subsystem: eVga.com. Corp. Device 5186 Kernel driver in use: nvidia Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia-- Subsystem: eVga.com. Corp. Device 5186 Kernel driver in use: snd_hda_intel Kernel modules: snd_hda_intel 1.2 安装驱动参看显卡的驱动版本： 1234567891011root@deeplearning:# ubuntu-drivers devices== /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 ==modalias : pci:v000010DEd00001B80sv00003842sd00005186bc03sc00i00vendor : NVIDIA Corporationdriver : xserver-xorg-video-nouveau - distro free builtindriver : nvidia-410 - third-party freedriver : nvidia-418 - third-party freedriver : nvidia-415 - third-party freedriver : nvidia-384 - third-party non-freedriver : nvidia-390 - third-party non-freedriver : nvidia-430 - third-party free recommended 其中建议安装版本是nvidia-430 - third-party free recommended 所以使用下面的命令进行安装： 12345678910sudo apt-get update# 关闭图形界面sudo service lightdm stop# 安装 sudo apt-get install nvidia-430# 开启图形界面 sudo service lightdm start# 重启osreboot 上面的方法失败率较高，建议使用手动下载驱动介质后，手动安装。介质在下面NVIDIA官方网址下载： https://www.nvidia.cn/geforce/drivers/ 按照要求检索后，选择最新版本的介质，例如：NVIDIA-Linux-x86_64-510.54.run（2022-03-22） 123456789# 使用root用户# 先卸载旧的驱动残余$apt-get remove --purge nvidia*# 安装部分依赖$apt-get update$apt-get install dkms build-essential linux-headers-generic# 安装$chmod u+x NVIDIA-Linux-x86_64-510.54.run$./NVIDIA-Linux-x86_64-510.54.run 按照完成后重启os。 使用下面命令查看，注意确认版本： 123456789101112131415161718192021root@deeplearning:# nvidia-smiSun Mar 20 19:17:13 2022 +-----------------------------------------------------------------------------+| NVIDIA-SMI 510.54 Driver Version: 510.54 CUDA Version: 11.6 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||===============================+======================+======================|| 0 NVIDIA GeForce ... Off | 00000000:01:00.0 Off | N/A || 0% 31C P0 34W / 180W | 0MiB / 8192MiB | 1% Default || | | N/A |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Processes: || GPU GI CI PID Type Process name GPU Memory || ID ID Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 1.3 安装cuda CUDA(Compute Unified Device Architecture，统一计算设备架构)，是显卡厂商NVIDIA在2007年推出的并行计算平台和编程模型。CUDA仅能在有NVIDIA显卡的设备上才能执行，并不是所有的NVIDIA显卡都支持CUDA，目前NVIDIA的GeForce、ION、Quadro以及Tesla显卡系列上均可支持。根据显卡本身的性能不同，支持CUDA的版本也不同。 首先检查当前系统的 GPU 型号： 123root@deeplearning:~# lspci | grep -i nvidia01:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1080] (rev a1)01:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1) 去官网（https://developer.nvidia.com/cuda-toolkit）下载介质： 根据系统环境选择配置： 12wget https://developer.download.nvidia.com/compute/cuda/11.6.1/local_installers/cuda_11.6.1_510.47.03_linux.runsudo sh cuda_11.6.1_510.47.03_linux.run 安装前线卸载旧的版本，卸载脚本位置： 1/usr/local/cuda-9.0/bin/uninstall_cuda_9.0.pl 验证安装： 12root@deeplearning:/usr/local/cuda/extras/demo_suite# ./deviceQuery# 回显最后有：Result = PASS 1.4 安装 cuDNN官网地址：https://developer.nvidia.com/cudnn 下载包： 12345678Navigate to your &lt;cudnnpath&gt; directory containing the cuDNN tar file.Unzip the cuDNN package.$ tar -zxvf cudnn-linux-x86_64-8.3.3.40_cuda11.5-archive.tar.xzCopy the following files into the CUDA toolkit directory.$ sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include $ sudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64 $ sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* 第二部分 Tensorflow-GPU环境部署使用下面的命令直接安装： 1pip install tensorflow-gpu 测试： 123import tensorflow as tftf.config.list_physical_devices('GPU')# [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] 第三部分 Pytorch-GPU环境部署1.这里pytorch和cudatoolkit版本对应关系： https://pytorch.org/get-started/previous-versions/ 使用下面的语句安装： 1pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html 测试： 123import torchprint(torch.cuda.is_available())# True 参考文献及资料1、https://pytorch.org/get-started/locally/]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-异常检测算法（VAE）]]></title>
    <url>%2F2022%2F03%2F19%2F2022-03-19-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%EF%BC%88VAE%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景对于学习一个知识，例如画画临摹，如何顶一个度量来检验学习效果呢？现实中我们会让学习者去实际画一幅相同的画作，然后和原画进行对比。其中的哲学原理就是：你学会一门知识后，你就会独立创造。 同理在机器学习中同样借鉴这个哲学思想，其实就是生成模型(generative models)，其中两类代表性的模型就是：GAN（）和VAEs（） https://www.cnblogs.com/asawang/p/10407551.html 年份 事件 相关论文 2013 Diederik P.Kingma和Max Welling首次提出变分自编码器 Kingma, D. P., &amp; Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. 2016 Doersch, C.对变分自编码器更加简化的介绍给读者 Doersch, C. (2016). Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908. 2016 Sønderby, C. K., Raiko, T.,提出Ladder variational autoencoders Sønderby, C. K., Raiko, T., Maaløe, L., Sønderby, S. K., &amp; Winther, O. (2016). Ladder variational autoencoders. In Advances in neural information 第一部分 自编码器(AutoEncoder)降噪 我们先来考虑一下能否用AutoEncoder进行KPI异常检测，以及它有什么缺点。因为AutoEncoder具有降噪的功能，那它理论上也有过滤异常点的能力，因此我们可以考虑是否可以用AutoEncoder对原始输入进行重构，将重构后的结果与原始输入进行对比，在某些点上相差特别大的话，我们可以认为原始输入在这个时间点上是一个异常点。 第二部分 变分自编码器(Variational AutoEncoder)本篇博客。 AutoEncoder主要用于数据的降维和特征的提取。而现在也被扩展用于生成模型。 与其他神经网路（通常关注于输出层）不同的是：AutoEncoder主要关注于中间的隐藏层（Hidden Layer）。 1、AutoEncoder（自编码器） ​ 原始数据映射到原数据，通过压缩来提取数据的特征。如果将激活函数换成线性函数，这是一个PCA模型。 2、Sparse AutoEncoder（稀疏自编码器） 3、Denoising AutoEncoder（降噪自编码器） 4、Stacked AutoEncoder（堆叠自编码器，SAE） 4、Variational AutoEncoder（VAE） 代码实现： https://github.com/Michedev/VAE_anomaly_detection]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark中RDD的宽窄依赖]]></title>
    <url>%2F2022%2F03%2F16%2F2022-03-16-Spark%E4%B8%ADRDD%E7%9A%84%E5%AE%BD%E7%AA%84%E4%BE%9D%E8%B5%96%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景我们知道Spark计算框架中RDD（Resilient Distributed Dataset）是他的核心概念，RDD是一个只读数据模型，计算中只能对RDD进行转换生成新的RDD。新旧RDD之间就有了血缘关系，通常称为父子关系。 本文，我将用新的绘图带大家搞清楚究竟什么是宽依赖（ShuffleDependency），什么是窄依赖（NarrowDependency） https://www.cnblogs.com/upupfeng/p/12344963.html https://www.interhorse.cn/a/2301943088/ rdd之间的依赖关系 narrow dependences ：父rdd 中的数据不被拆分 shuffle dependences ： 父rdd中的数据被拆分 No. If two RDDs have the same partitioner, the join will not cause a shuffle. You can see this in CoGroupedRDD.scala: 1234567891011override def getDependencies: Seq[Dependency[_]] = &#123; rdds.map &#123; rdd: RDD[_ &lt;: Product2[K, _]] =&gt; if (rdd.partitioner == Some(part)) &#123; logDebug("Adding one-to-one dependency with " + rdd) new OneToOneDependency(rdd) &#125; else &#123; logDebug("Adding shuffle dependency with " + rdd) new ShuffleDependency[K, Any, CoGroupCombiner](rdd, part, serializer) &#125; &#125;&#125; Note however, that the lack of a shuffle does not mean that no data will have to be moved between nodes. It’s possible for two RDDs to have the same partitioner (be co-partitioned) yet have the corresponding partitions located on different nodes (not be co-located). This situation is still better than doing a shuffle, but it’s something to keep in mind. Co-location can improve performance, but is hard to guarantee. Co-Located and Co-Partitioned RDDs Co-Located指分布在内存的位置相同。如果两个rdd被相同的job分布到内存，且拥有相同partitioner，就会是co-located，能方便CoGroupedRDD函数，如cogroup和join等。 Co-Partitioned指拥有相同partitioner的rdd。 123456789//下面count执行后，rddA和rddB会是Co-Located。如果把rddA和rddB的注释去掉，则是Co-Partitionedval rddA = a.partitionBy(partitioner)rddA.cache()val rddB = b.partitionBy(partitioner)rddB.cache()val rddC = a.cogroup(b)//rddA.count()//rddB.count()rddC.count() 参考文献及资料1、Understanding Co-partitions and Co-Grouping In Spark，链接：https://amithora.com/understanding-co-partitions-and-co-grouping-in-spark/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Github中黑话总结]]></title>
    <url>%2F2022%2F03%2F15%2F2022-03-15-Github%E4%B8%AD%E9%BB%91%E8%AF%9D%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 参考文献及资料 背景初入Github这个坑，发现交流中有大量缩写，简称码农之间的黑话。收集并长期更新如下： PR: Pull Request的缩写，把自己分支的代码合并到主项目的请求。 LGTM: Looks Good To Me SGTM: Sounds Good To Me.已经通过了 review WIP: Work In Progress. PTAL: Please Take A Look. TBR: To Be Reviewed. TL;DR: Too Long; Didn’t Read. TBD: To Be Done (or Defined/Discussed/Decided/Determined).]]></content>
      <categories>
        <category>github</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何共享开源（github）学习]]></title>
    <url>%2F2022%2F03%2F15%2F2022-03-15-%E5%A6%82%E4%BD%95%E5%85%B1%E4%BA%AB%E5%BC%80%E6%BA%90%EF%BC%88github%EF%BC%89%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境准备 第二部分 具体步骤 参考文献及资料 背景GitHub 上就有一个现成的教程项目 first-contributions，为初学者带来一个简单的方法来学习及参与开源项目。项目地址： https://github.com/firstcontributions/first-contributions 第一部分 环境准备部署git，不再赘述。 第二部分 具体步骤2.1 fork目标库点击Fork按钮去 Fork 这个代码仓库。 这样自己的账户中就有一个新库了：git账户/目标库名 2.2 克隆复制库到本地1$ git clone git@github.com:github账户/first-contributions.git 例如： 1$ git clone git@github.com:xiaoming/first-contributions.git 或者使用https协议的url。 12345678$ git clone git@github.com:xiaoming/first-contributions.gitCloning into 'first-contributions'...remote: Enumerating objects: 92834, done.remote: Counting objects: 100% (104/104), done.remote: Compressing objects: 100% (75/75), done.remote: Total 92834 (delta 53), reused 80 (delta 29), pack-reused 92730Receiving objects: 100% (92834/92834), 78.97 MiB | 23.00 KiB/s, done.Resolving deltas: 100% (53372/53372), done. 由于GTW原因比较慢。 2.3 新建一个代码分支12cd first-contributionsgit checkout -b &lt;新分支的名称&gt; 例如： 123git checkout -b add-xiaoming#回显# Switched to a new branch 'add-xiaoming' 2.4 修改并Commit提交我们只修改主目录下的文件：Contributors.md，编辑文件，按照格式添加自己的账户信息，保存。 命令行方式： 1git add Contributors.md 提交本地库： 1234git commit -m "Add xiaoming to Contributors list"# 回显# [add-xiaoming 3f8a5f2dc] Add xiaoming to Contributors list# 1 file changed, 1 insertion(+) 或者通过github desktop工具操作更简单。 另外也可以创建翻译文件，例如：addtional-material.chs.md 然后提交。 2.5 提交到github主库(克隆的库)1git push origin &lt;分支的名称&gt; 例如： 1git push origin add-xiaoming 或者通过github desktop工具操作更简单。 2.6 提交到目标库审阅这时候github页面会显示: add-xiaoming had recent pushes 1 minute ago 你会看到一个 Compare &amp; pull request 的按钮。点击该按钮。填写相关信息后正式提交。等待项目人审批后正式合并在目标库中。 参考文章1、 first-contributions项目地址：https://github.com/firstcontributions/first-contributions]]></content>
      <categories>
        <category>github</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python系列文章-Python中语法知识积存]]></title>
    <url>%2F2022%2F03%2F14%2F2022-03-15-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Python%E4%B8%AD%E8%AF%AD%E6%B3%95%E7%9F%A5%E8%AF%86%E7%A7%AF%E5%AD%98%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Python中的__all__ 第二部分 第三部分 第四部分 第五部分 参考文献及资料 背景第一部分 Python中的__all__1.1 介绍Python语言没有Java语言中有public，private关键字控制可见性，所以可见性需要制定统一的约定。例如__all__属性，我们先举个栗子： 123456789101112131415161718192021# module1.py 模块__all__ = [ 'A', 'fun1',]class A(): def __init__(self,name,age): self.name=name self.age=ageclass B(): def __init__(self,name,id): self.name=name self.id=iddef fun1(): print 'fun1() is called!'def fun2(): print 'fun2() is called!' 这时候我们使用代码转载module1的时候： 1234from module1 import *A() # 没有问题fun2() # 报错函数未定义，NameError: name 'fun2' is not defined 所以当我们使用 from module1 import *来导入时，此时被导入模块若定义了__all__属性，则只有__all__内指定的属性、方法、类可被导入。若没定义，则导入模块内的所有公有属性，方法和类 。即这个变量是个控制白名单。 当然对于非模糊导入，即精确指定导入是不影响的。例如上面的案例，可以使用下面的语句： 1from module1 import fun2 1.2 总结 Python不提倡用 from xxx import * 这种写法，应该按需导入。 以上是针对模块的导入，对于package的导入需要使用__init__.py文件。__init__.py为空时，通过from package import *无法导入任何name。 __all__ 的形式都是 list类型。 按照PEP8建议的风格，__all__ 应该写在所有 import 语句下面，函数、常量等成员定义的上面。 约定__all__可以这样定义： 12345__all__ = [ 'A', 'fun1',]# 最后多出的逗号在 Python 中是允许的，符合 PEP8 风格 第二部分 Python中__init__.py文件__init__.py该文件的作用就是相当于把自身整个文件夹当作一个包来管理，每当有外部import的时候，就会自动执行里面的函数 __init__.py 在包被导入时会被执行 参考文章]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据仓库学习系列-大数据数仓综述]]></title>
    <url>%2F2022%2F02%2F26%2F2022-02-09-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B0%E4%BB%93%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 数据仓库缘起 第二部分 数据仓库概述 第三部分 数据仓库技术实现 第四部分 常见数据仓库产品 第五部分 数据仓库的架构设计 第六部分 建模方法 第七部分 最佳实践 参考文献及资料 背景第一部分 数据仓库缘起1.1 历史数据的积存历史数据使用频率低，堆积在业务库中，导致性能降低。将冷数据迁移至数据仓库。 1.2 企业数据分析需要历史积存，开展数据分析工作，作为经营管理的决策分析。 各个业务线分别建立数据抽取和数据仓库。管理和维护成本较大。需要统一建设企业级别数据仓库。 统一的数据口径。 第二部分 数据仓库概述数据仓库是一个面向主题、集成的、非易失的且随时间变化 2.1 数据仓库的特点2.1 面向主题为数据分析提供服务。根据主题将元素数据集合在一起。 2.2 集成原始数据来源不同的数据源，需要整合。经过抽取、清洗、转换（ETL）过程。 数据治理 2.3 非易失保存在数据仓库中数据原则上不允许修改，只允许通过工具进行查询和分析。 2.4 时变性数据仓库会定期接受集成新的数据，反应数据的最新变化。 2.2 数据仓库VS数据库数据库是面向事物设计的，属于OLTP（在线事物处理）系统。主要是随机读写，设计避免冗余，设计需要符合范式（数据库3范式）。 数据仓库是面向主题设计，属于OLAP（在线分析处理）系统。主要是批量读写。关注数据的整合，以及分析处理性能。会有意的引入数据冗余，反范式设计。以空间换取时间，提升数据获取的效率。 特征 数据库 数据仓库 面向 事物 数据分析 数据类型 细节、业务 ETL处理后的数据 数据特点 当前的、最新的 历史的、跨时间 目的 日常操作 长期信息需求、决策支持 设计模型 基于ER模型、面向应用 星形、雪花模型、面向主题 操作 读/写 读为主、批量写 数据规模 GB、TB PB 注： 设计实体关系模型（ER Model） 第三部分 数据仓库技术实现3.1 传统数仓技术实现由关系型数据库组成的MPP（大规模并行处理）集群。 业务库是关系型数据库，多个关系型数据库，组成集群。 数据分布在各个节点存储，分库分表。兼容SQL语法。 存在的问题：扩展性有限（分库分表的限制）、热点问题。 3.2 MPP架构将单机数据节点组成集群，提升整体处理性能。 非共享架构，每个节点是独立的资源、独立运行。 每个节点使用专用网络相互连接，彼此协同计算，作为整体提供服务。 设计上优先考虑C（一致性）、其次考虑A（可用性）、尽量做好P（分区容错性）。CAP理论。 架构优点运算精细化 延迟低 吞吐低 架构适用于中等规模的数据仓库数据处理。 架构缺点存储位置不透明。使用hash分布。查询任务会在每个节点执行。 并行计算是，单节点有瓶颈。容错性差（随着集群规模的增加，故障率。世界上也没超大规模的MPP数据库）。 分布式事务的实现会导致扩展性降低。 3.2 大数据数仓技术实现依托大数据技术产生数据仓库。完成海量数据的存储（分布式文件系统）。 移动计算，而不是移动数据。 将SQL转换成大数据计算引擎任务，完成数据分析。 缺点：SQL支持率。事务支持（对事物要求不高）。 3.2.1 分布式架构大数据中常见架构。Hadoop架构。 各节点实现场地自治。可以单独运行局部应用。数据在集群中全局透明共享。 每个节点通过局域网，廉价网络，节点间数据通信开销较大。需要在运算过程中减少数据的移动，即计算去贴近数据。 在CAP理论中，优先考虑P（分区容错性）、然后是A（可用性）、最后考虑C（一致性）。 数据处理较为粗狂，将数据作为文件对待。对于小数据场景下，处理效率低。 3.3 MPP+分布式的架构数据存储在分布式架构中的公共存储，提高分区容错性。 上层架构采用MPP，减少运算的延迟。 两者之间的折中。计算架构没有好坏之分，只有场景适合。 第四部分 常见数据仓库产品4.1 传统数仓 Oracle RAC oracle的集群版本。共享磁盘存储。单个集群只能支持100个节点。 DB2 IBM的场景。 Teradata 商业数据库。性能和易用高。和硬件一起卖。稳定易用。 Greenplum 开源产品。基于Postgre SQL改造。技术更新快。稳定性比TD弱一些。 4.2 大数据数仓 Hive Hadoop的生态。HQL语法SQL。延迟大。底层是MapReduce计算引擎。 Spark SQL 底层计算基于Spark Hbase nosql。存储非结构化数据。高并发读。实时流处理、前端高并发的业务查询。DDL频发变化的业务。 Impala 数据查询引擎。底层兼容hive hbase。快速交互查询服务。数据仓库的查询接口。 HAWQ Greenplum在hadoop上的移植产品。分布式批处理架构+MPP架构。兼容两者的性能。 Tidb new sql数据库。架构是MPP+SMP。底层是nosql存储，但是支持nosql语法。主要兼容的是mysql。同时可用用来作为OLTP和OLAP，但是更侧重于OLTP。 第五部分 数据仓库的架构设计5.1 架构图 数据应用层、ADS层（报表决策、并发查询、搜索检索） 数据集市层。 公共维护模型层、CDM层(数据汇总层 DWS、数据明细层DWD) 明细层存储大量零散表、清洗了，还满足3范式。数据汇总层按照主题进行聚合，宽表进行存储。 操作数据源层（ODS层） 贴源数据 数据积存。 ETL（Sqoop、Kettle等） 数据的接入。 5.2 ETL流程ETL（Extract Transform Load） 将数据通过抽取、交互转换、加载至目的端的过程。 ETL规则的设计和实施占用数据仓库工作量的60%-80%。 数据抽取（Extract ） 抽取的数据：结构化数据、非结构化数据、半结构化数据 结构化数据通常使用JDBC方式、数据库日志方式获取，例如CDC；非|半结构化数据通过监听文件变动获取等。 抽取方式 1、数据抽取方式有全量同步、增量同步两种方式 2、全量同步会将全部数据进行抽取，一般用于初始化数据装载。 3、检测变动，抽取发生变化的数据，一般用于数据更新。 数据的转换（Transform ） 数据转换需要经历数据清洗和数据转换两个阶段。 数据清洗解决的问题：数据的重复去重、二义性、不完整、违反业务或者逻辑规则。 数据转换解决的问题：数据标准化处理、进行字段、数据类型、数据定义的转换。 数据加载（Load） 将最后处理完的数据导入到对应的目标源中。 常见ETL工具结构化： Sqoop 结构数据，jdbc方式，mapreduce计算框架； Kettle 可视化界面。开源免费。 Datastage 商业。 Informatica 商业。 Kafka 消息队列。可以进行ETL处理。 非|半结构化数据 Flume 老牌。 Logstash ELK家族。 5.3 ODS层数据积存层。数据和业务数据库保持一致。 存储的数据只读。业务库的扩充集合。 业务系统对数据完成修改后，只能追加。 id name age 0001 test1 23 0002 test 25 入库后： id name age update_time from update_type 0001 test1 23 2022-02-01 DB01 insert 0002 test 25 2022-02-01 DB01 update 离线数仓中，业务数据定期通过ETL流程导入至ODS层。有全量和增量两种方式。 全量：数据第一次导入 增量：非第一次，根据业务库的变化，导入变化的数据。使用外连接的方式来更新（建议）、或者覆盖，避免冗余。 5.4 数据分析5.4.1 DWD 数据明细层DWD层对ODS层的数据进行清洗、标准化、维度退化（时间、分类、地域等） 数据仍然满足3NF范式，为分析计算做准备。轻度汇总。 注：维度是我们对数据的组织方式。 5.4.2 DWS 数据汇总层数据汇总层对数据明细层的数据，按照分析主题进行计算汇总，存放便于分析的宽表。 存储模型非3NF范式。而是注重数据的聚合，复杂查询、处理性能更优的数仓模型，如维度模型。 数据仓库的核心 面向主题 5.4.3 ADS 数据应用层数据应用层也称为 数据集市 为不同业务场景提供接口。根据不同的场景来存储。 例如报表：存储在Kylin层、并发查询存储在HBase、搜索检索 存储在Elasticsearch 第六部分 建模方法6.1 基本概念OLTP系统建模方法 在线事务处理系统。主要操作是随机读写。 为了保证数据一致性、减少冗余、使用关系模型 关系模型中 使用3NF规则减少冗余。 注：其实现在为了保证前端查询速度，会把表做成宽表，减少join操作。 OLAP 在线联机分析。主要复杂分析查询。关注数据整合，以及分析、处理性能。 安装存储方式的不同分为：ROLAP、MOLAP、HOLAP 6.2 ROLAPROLAP（Relation OLAP） 关系型OLAP。使用关系模型构建，存储系统一般为RDBMS。 经典的数据仓库建模方法有：ER模型、维度模型、data value、Anchor 用的最多的是维度模型。 业务变化多、灵活 维度模型。 维度模型中 表被分为：维度表、事实表。维度是对事实的一种组织。 维度包括：分类、时间、地域等。 维度模型分为：星形模型、雪花模型、星座模型 星形模型：维度只有一层 分析性能最优 雪花模型有多层维度 比较接近三范式 较为灵活。 星座模型：基于多个事实表 事实表之间共享一些维度表。是大型数仓中的常态，是业务增长的结果与模型设计无关。 什么是宽表模型 宽表模型是维度模型的衍生。适合join性能不佳的数据仓库产品。 宽表模型将维度冗余到事实表中，形成宽表，以减少join的操作。 传统数据库中join由于有索引，性能还可以。但是对于大数据数仓，数据大量移动会有大量数据搬运。 面向DWS数据汇总层。 6.3 MOLAPMOLAP（多维 OLAP）；预先聚合计算。使用多维数组的形式保存。加快查询分析。预计算。 将结果进行预计算，并将聚合结果存储在CUBE模型中。空间换时间。 CUBE模型以多维数组的形式，物化在存储系统中，加快后续的查询。 生产CUBE需要大量的时间、空间，维度预处理可能会导致数据膨胀。 CUBE 数据立方体； 常见的产品： Kylin 使用较多。存储在HBase。 Druid 适用于ADS层。 6.4 多维分析HOLAP（混合 OLAP）；底层是关系层、高维多维矩阵型。 OLAP主要操作是复杂查询，多表关联、COUNT、SUM、AVG等聚合函数。 钻取 通过更改维度的层次来变换分析的粒度。 切片 切块 旋转 第七部分 最佳实践7.1 表的分类维度建模中，表： 事实表 存储一个现实存在的业务对象。 事务事实表 随着业务不断产生的数据，一旦产生就不会再变化。例如交易流水、操作日志、出库记录。 周期快照表 随着业务周期性的推进而变化。完成间隔周期内度量的统计。 累积快照表 记录不确定周期的度量统计。只有一条记录，针对此记录不断更新。 维度表 对业务状态 代码的解释表。也称为码表。 通常使用维度对事实表中的数据进行统计、聚合运算。 技术实现： 拉链表： 拉链表记录每条信息的生命周期，用于保留数据的所有历史变更状态； 拉链表将表数据的随机修改方式变为顺序追加。 7.3 ETL同步策略全量同步 数据初始装载； 因为业务、技术原因 直接全量覆盖原有表数据。 增量同步： 传统数据整合方案中，大多采用merge方式（update+insert） 主流的大数据平台不支持update操作。可以采用全外连接+数据覆盖的方式 也可以按天分区，全量保存，控制数据生命周期。 7.4 任务调度解决任务单元间的依赖关系。 自动化完成任务的定时执行。 常见任务：Shell、Java程序、MapReduce、SQL脚本; 任务调度工具： Azkaban（阿兹卡班） 新 Oozie（乌贼） 老牌 7.5 SQL查询引擎Presto 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Data Warehouse</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[大数据调度产品对比（海豚调度和天融信神灯）]]></title>
    <url>%2F2022%2F02%2F25%2F2022-02-25-%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6%E4%BA%A7%E5%93%81%E5%AF%B9%E6%AF%94%EF%BC%88%E6%B5%B7%E8%B1%9A%E8%B0%83%E5%BA%A6%E5%92%8C%E5%A4%A9%E8%9E%8D%E4%BF%A1%E7%A5%9E%E7%81%AF%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景随着数字化转型的趋势，各行业都在建设自身的大数据中台，实现数字化运营。在大数据中台建设中，必然涉及到大量大数据任务、机器学习任务以及各类脚本任务的运行。随着业务量的增长就会自然需要建设一个集中式的任务调度系统。 笔者企业内部大数据调度平台从天融信公司采购，产品名称为神灯（magiclanp）调度系统。在使用过程中，也在关注Apache基金会的开源调度项目—DolphinScheduler（下面简称海豚调度）。 在使用神灯调度系统过程中，我们也参考海豚调度系统，完成了调度系统服务分布式高可用架构的改造。 本文将以介绍神灯调度系统，并与海豚调度进行产品比较。内容主要分为：业务架构、技术架构两个方面展开。 第一部分 业务架构1.1 业务抽象神灯（magiclanp）调度系统，将所有类型的大数据任务（MapReduce、Spark、Flink）均抽象为DAG计算图。计算图的数据结构使用XML文件进行定义。 将大数据计算任务原子化为各类算子，算子通过计算依赖关系组成计算图（pipline）。算子按照功能分为：输入算子、数据处理算子、输出算子以及控制算子。 输入算子 负责各类数据输入对接各类内外部数据源。数据分为文件系统和外部数据库两种类型。 数据处理算子 负责对上游算子输出数据进行ETL处理。 输出算子 负责对处理结果传输至内外部文件系统、数据库存储、消息中间件等进行持久化。 控制算子 负责控制数据处理逻辑，例如条件分支等。 为了增加系统易用性，系统提供用户web可视化方式，支持画布、托拉拽方式进行创建大数据处理任务，系统内部定义为大数据模型。 例如下图的Spark模型案例： 1.2 模型类型平台按照模型的研发方法，模型分为：画布模型和程序模型。另外按照模型使用场景，分为：规则模型和机器学习模型。 1.2.1 画布模型调度平台支持MapReduce、Spark、Flink三种计算框架。每类计算框架编写一套自身的pipline处理JDK，任务提交至Yarn集群运行时，通过解析xml文件和算子jar包，完成任务计算处理。 画布模型对于业务人员较为友好，极大降低了大数据使用技术门槛。但是平台算子集合毕竟是有限的，所以算子组合的模型表达能力是有上限的。对于更为灵活和复杂的大数据处理场景依然需要代码级别的研发。这就是更为灵活的程序模型。 1.2.2 程序模型支持用户灵活化开发Python任务、Spark任务，后续将支持Flink计算框架。按照规范打包后，由平台创建模型后调度运行。 1.2.3 规则模型规则模型主要是为了和机器学习模型区分。模型主要使用SQL处理算子等，对数据按照规则配置，进行数据过滤和筛选。 1.2.4 机器学习模型目前只有Spark计算框架实现了基于Spark ML内置模块，研发了各类机器学习算子。对于更灵活的机器学习任务，目前支持PySpark模式提交至集群运行，即：PySpark On Yarn模式。 1.3 任务调度系统中创建的大数据模型是一个静态概念（即静态计算图），需要针对模型创建相应的调度任务后运行模型。模型的调度方式，支持即时任务、定时任务、流式任务。 即时任务 即刻发起一次大数据模型运行。使用场景通常为模型试运行。 定时任务 按照定时周期，有平台周期性调度运行任务。 流式任务（服务） 针对流处理模型（例如Flink计算框架），任务运行后常驻运行。 1.4 任务编排在实际业务场景中，存在复杂场景不能通过单个模型完成，需要多个模型共同协作完成，模型之间还存在顺序依赖关系。这就需要平台具备各类模型的编排能力。 神灯调度系统基于该场景进行支持。 1.5 数据源支持目前支持的 1.6 权限管理1.6.1 用户管理神灯平台集成了企业内部统一认证系统，就不展开介绍了。 1.6.2 数据源权限管理神灯平台对接了各类数据源，企业内部不同人员对于数据访问就需要权限隔离。目前权限管理颗粒度到表的读、写、删。 还没有对 1.7 监控告警1.8 任务高可用第二部分 技术架构2.1 系统整体架构目前神灯平台还属于传统应用，业务架构图如下： 2.2 系统高可用设计目前只有MapReduce计算框架实现了分布式处理能力。其他任务仍然是单机模式。 2.3 易用性功能2.4 大数据任务安全认证架构2.4.1 Kerberos票据模式架构上，系统所有调度节点均部署定时服务，周期性（11小时）向集群kerboros服务申请票据（有效期24小时）。票据缓存在本地/tmp目录，所有大数据任务提交至Yarn集群的时候，只需要使用票据即可。不需要实时使用Principal和keytab文件。 架构上这样设计主要是减少Kerboros服务的请求压力。目前线上集群实时任务量500+，避免了Kerboros服务瓶颈（虽然没有压测过）。 2.4.2 多集群互信机制对于线上环境多集群场景下，引入了Kerberos的集群互信机制。例如线上有3个集群分别是为：A、B、C集群，架构上选取A集群为主集群。配置A与B互信，A与C互信。这样调度平台每个节点只需要申请A集群的票据即可完成资源认证。 2.5 Python程序技术架构第三部分 总结3.1 功能差异天融信神灯平台除了调度功能，还集成了各种大数据任务无代码创建功能。对于业务人员只需要懂SQL语法就可以上手完成大数据任务的创建到调度运行。 优点：用户面向业务人员也面对开发人员 海豚调度更专注于调度功能。面向用户为大数据研发人员（需要会自己编写大数据任务并打包成可运行的jar包） 3.2 架构上3.3 整体对照表单机部署： https://dolphinscheduler.apache.org/zh-cn/docs/1.3.4/user_doc/standalone-deployment.html 最新架构图： https://www.analysys.cn/developer/apache-dolphinscheduler/ 源码分析: https://www.cnblogs.com/gabry/p/12217966.html 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>大数据调度</category>
      </categories>
      <tags>
        <tag>大数据调度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库学习系列--主题]]></title>
    <url>%2F2022%2F02%2F20%2F2022-02-09-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-%E4%B8%BB%E9%A2%98%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景从上面的引言里面，我们其实可以知道主题在数仓建设里面绝对是很重要的一环，这的确是的。数仓在建设过程中，对数据的组织管理上，不仅仅要进行横向的分层，也需要根据业务情况进行纵向的主题域划分。 看到这里可能就有疑问了，上面明明说的是面向主题，怎么又突然说到主题域了，这里就延伸出主题和主题域的关系了。 下面我就围绕数仓主题、主题域以及两者之间关系、划分方式等，进行更详细的阐述。 二、数仓主题是什么，主题域又是什么呢？1. 数仓主题是什么？数仓主题(Subject) 是在较高层次上将企业信息系统中某一分析对象（重点是分析的对象）的数据进行整合、归类并分析的一种范围，属于一个抽象概念，简单点说每一个主题对应一个宏观分析领域。 下面举例说明一下： 对于一个erp系统而言，“销售分析”就是一个分析领域，这个“销售分析”所涉及到的分析对象有商品、供应商、顾客、仓库等，那么数仓主题就确定为商品主题、供应商主题、顾客主题、仓库主题，“销售分析”就可以作为一个主题域； 如果“产品分析”是一个分析领域，“产品分析”所涉及到的分析对象为商品、地域、时间、类别等，那么数仓的主题可以确定为商品主题、地域主题、时间主题、类别主题，“产品分析”可以作为一个主题域。 2. 数仓主题域是什么呢？主题域通常是联系较为紧密的数据主题的集合，可以根据业务的关注点，将这些数据主题划分到不同的主题域，这种划分个人感觉与Kimball思想更为相似，自下而上的方式，根据业务需求分析视角进行划分。 其实这里市面上，也有一些不同的描述，上面对主题域的描述被归于集合论，还有一种叫做是边界论，这里稍微扩展下： 边界论的论点是 “主题域是对某个主题进行分析后确定的主题的边界“，这点个人感觉和 Inmon 指导思想类似，理清主题之间的边界，由ER模型进行逻辑转化，对某一主题域的分析，需要先确定这个主题的关系边界，然后再进行逻辑建模。 我的话觉得两者并不矛盾，只是所站的视角不同，边界论是先从细微处也就是微观延伸到宏观，而集合论则是从宏观到微观的过程。 三、主题的划分主题的划分和设计是对主题域进一步的分解，细化的过程。主题域下面可以有多个主题，主题还可以划分成更多的子主题，主题和主题之间的建设可能会有交叉现象，而实体则是不可划分的最小单位。 主题域、主题、实体的关系如下图所示： 可以显而易见地看出，主题域是一个更大的概念，主题是略次之，实体最小，这里的实体表示的是实体对象（对应企业中某一宏观分析领域所涉及的分析对象）,我的理解在维度建模的方法论上也可以说实体和维度某些概念是相似的。 四、主题域划分方法在进行数据仓库设计时，一般是先基于一个主题或某部分主题进行优先建设，所以在大多数数据仓库的设计过程中都有一个主题域的选择过程，主题域的确定必须由最终用户和数据仓库的设计人员共同完成。 而在划分主题域时，大家的切入点不同可能会造成一些争论、重构等的现象，考虑的方法有下面一些： 1. 按照所属系统划分：业务系统有几种，就划分几种 2. 按照业务（功能模块/业务线）或业务过程划分比如一个靠销售广告位置的门户网站主题域可能会有广告域，客户域等，而广告域可能就会有广告的库存，销售分析、内部投放分析等主题； 3. 按照部门划分主题域比如公司里面的人力、财务、销售、运营等，运营域中可能会有工资支出分析、活动宣传效果分析等主题。 4. 按照行业案例分析划分主题域在某些行业，比如电信、金融都是最早建设数仓的行业，都有一些规范，比如IBM 公司的 BDWM 九大金融主题模型,Teradata 公司的 FS-LDM 十大金融主题模型，都是行业应用比较广泛的标准，如果是这两个行业就可以参考构建自己的企业数据仓库模型规范。 总而言之，切入的出发点逻辑不一样，就可以存在不同的划分逻辑。在建设过程中可采用迭代方式，不纠结于一次完成所有主题的抽象，可先从明确定义的主题开始，后续逐步归纳总结成自身行业的标准模型。 五、数据域是什么，和主题域之间的关系在很多文档上都有说数据域，反而没有主题域的概念，那数据域到底是什么，又和主题域什么关系呢？ 我自己在网上也搜索了很多，也没查到对两者的来源和区别说明让我满意的，但是我在看《阿里大数据之路》和 阿里的官方相关文档 介绍上，看到了这个词，下面可以看下引用的阿里对数据域的介绍： 数据域是指面向业务分析，将业务过程或者维度进行抽象的集合。为保障整个体系的生命力，数据域需要抽象提炼，并长期维护更新。 在划分数据域时，既能涵盖当前所有的业务需求，又能让新业务在进入时可以被包含进已有的数据域或扩展新的数据域。 数据域的划分工作可以在业务调研之后进行，需要分析各个业务模块中有哪些业务活动。 我个人理解其实主题域和数据域差异不大，在实际过程中可以把主题域和数据域都当做一种域来处理了，不必纠结。 当我我也查到网上，有人总结的一段话，是将两者描述为一种包含关系，姑且可以看下： 主题域：面向业务过程，将业务活动事件进行抽象的集合，如下单、支付、退款都是业务过程，针对公共明细层（DWD）进行主题划分。 数据域：面向业务分析，将业务过程或者维度进行抽象的集合，针对公共汇总层（DWS）进行数据域划分。 六、主题域及主题划分的准则为保证整个数仓体系的生命力，数据域需要抽象提炼，长期维护及更新，但不要轻易变动，在划分数据域时，既能涵盖当前所有的业务需求，又能在新业务接入时无影响的包含进已有的数据域中或者扩展出新的数据域，这是划分的一个准则。 特别说明的是，主题域是无法一次划分完整的，在大多数数据仓库的设计过程中都有一个主题域的选择过程。业务一直发展的，设计之初就想着一次把所有主题全部划分完整，是不太可能，也不太适用的，我们可以遵循上面说的划分主题域的准则，以不断迭代的方式进行。 七、案例介绍1、银行FS-LDM十大金融主体模型在各行业中，科技信息化最先发展的是银行业，所以银行最先完成了数据的积存。也就自然需要建设数据仓库。市场上，Teradata公司提出了FS-LDM模型（Financial Services Logcial Data Model）。模型支持保险、银行和正确，包括十大主题：当事人、产品、协议、事件、财务、机构、地域、营销、渠道。 注：资源项、技术类、产品、当事人、事件、地理位置（地域）、账户、渠道、协议、介质。 当事人：银行服务的任意对象和感兴趣进行分析的各种个人或机构 | 主题域 | 主题 | 案例表 || —— | ——– | —— || | 关系 | || | 客户 | || | 内部机构 | || | 员工 | | 产品（与服务）：银行提供产品和服务，包含期限和定价等信息 协议：银行因提供产品或服务于客户建立的契约关系，如账户、合同、借据等 事件：银行和客户之间资金与非资金活动的信息 财务（与风险）：记录银行内部财务管理信息与风险管理信息 机构： 地域（地理地域）：用于观察和分析的任何区域，如国家、地区、邮件等 营销 渠道： 1. 马蜂窝数仓主题、主题域划分案例以马蜂窝订单交易模型的建设为例，基于业务生产总线的设计是常见的模式，首先调研订单交易的完整过程，定位过程中的关键节点，确认各节点上发生的核心事实信息。 2. 网易云音乐数仓主题、主题域划分案例网易云音乐将一级主题域划分为参与者、服务及产品，版权及协议、公共、事实这5个大的主题域，二级细节分类按照业务过程抽象获得。 八、个人工作中的案例介绍之前在一家互联网医疗公司工作，主题域的划分是按照部门bu进行划分的，这种方式适合较大的集团公司，各个事业部或者业务交叉不大的，不同的bu使用不同的数据域，这种架构它是一种小型的、部门级数据仓库，企业的不同部门有不同的 “主题域”，因而就有不同的独立性数据集市。 实际操作是按照部门划分了独立的数据集市，也就是主题域之后，再利用业务过程抽象出细分的主题。。 九、扩展下独立性数据集市的概念独立型数据集市的实质，是为了满足企业内各部门的分析需求而建立的微型数据仓库。有些企业在实施数据仓库项目时，为了节省投资，尽快见效，针对不同部门的需要，分步建立起这类数据集市，以解决一些较为迫切的问题。 但是，当多个独立的数据集市增长到一定规模后，由于没有统一的数据仓库协调，企业只会又增长出一些新的信息孤岛，仍然不能以整个企业的视角来分析数据，所以就延伸出另外一种企业级的数据仓库架构，后面有时间再单独分析这块。 十、我的一些建议结合我参与过的数仓项目建设经验和踩过的坑，对于数仓主题、主题域划分个人比较推荐按照业务系统划分或者bu部门来划分主题域（一级主题），这样的话边界较为清晰，数据仓库开发过程也不会因为模型主题的归属引发扯皮和不同意见，然后根据各个系统中的业务过程抽象整合出主题（叫二级主题域也可以）。 十一、总结数仓建设是一整套方法论，但方法论不一定是真理，每个公司都有自己的业务场景及需求，方法论或别人的方案不一定适用自己的公司，我们需要学习利用这些方法论，然后结合自己公司实际的业务场景来制定自己的主题及主题域划分规范。 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[运维大数据仓库建设主题设计规划]]></title>
    <url>%2F2022%2F02%2F20%2F2022-02-09-%E8%BF%90%E7%BB%B4%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E8%AE%BE%E4%B8%BB%E9%A2%98%E8%AE%BE%E8%AE%A1%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景运维类大数据仓库中通常有两类数据： 1、设备运行产生的数据。主要有：服务器（服务器、网络设备、安全防护设备等）产生的性能容量数据、日志数据（系统日志、应用日志）； 2、运维活动产生的数据。主要是运维管理或辅助系统产生的数据。有监控系统、变更系统、应急切换系统等运维业务系统产生的数据。 目前挑战：国内很多金融企业在大数据技术应用前并不是很重视数据治理，出现像投入大量资源建设大数据平台，但用的时候又发现报表不准、数据质量不高，导致项目没有达到预期效果的普遍性问题。 第一部分 主题和主题域1.1 理论主题是在较高层次上将企业信息系统中的数据进行综合、归类和分析利用的一个抽象概念，每一个主题基本对应一个宏观的分析领域。在逻辑意义上，它是对应企业中某一宏观分析领域所涉及的分析对象。 简单说，一个主题对应一个分析对象。分析对象就是在决策、分析时重点关注的东西，这个东西其实是非常主观的，在不同的企业，或者企业的不同发展时期，所关注的点会不一样，从而影响有些主题可能存在或者不存在。 数据仓库是面向主题的应用，主要功能是将数据综合、归类并进行分析利用。数据仓库模型设计除横向的分层外，通常还需要根据业务情况纵向划分主题域。主题域是业务对象高度概括的概念层次归类，目的是便于数据的管理和应用。 1.2 案例覆盖当事人、产品、协议、账户、介质、地理位置、资源项、事件、渠道和通用十大标准主题 一是建设以贴源层、聚合层、萃取层为核心的数据分层体系，实现全行数据的规范统一和共享 第二部分 运维大数据数仓主题设计https://codeantenna.com/a/wdCtwvu0CH 主题域 主题 涵盖内容 运维运营 变更 变更流程数据、变更操作数据、变更附件 监控 集中监控、应用监控、基础设施监控 应急 应急预案、应急操作、故障场景 事件 事件数据、问题数据 投产 投产操作日志、版本管理 容量规划 性能数据 服务器产生性能容量数据（内存、CPU） 容量配置 机房容量；动力容量；服务器资源容量；网络容量；私有云容量 系统日志 操作系统日志；网络设备日志（DNS、防火墙、交换机）；私有云日志 应用日志 应用类日志、表清理日志 流量数据 网络流量 安全运营 运维操作 运维登录操作日志 安全防护 客户端日志、VPN日志、反病毒日志、IPS防护日志、DDOS日志、WAF日志 安全知识 漏洞库、病毒码库、安全防护知识库、补丁数据 配置管理 系统配置 系统节点配置、存储配置 网络配置 网络设备配置、专线、网络端口配置、IP地址、VLAN 设备配置 存储设备、服务器设备、机房环境设备、硬件、硬盘、配件 主机配置 主机存储、主机操作系统、主机数据库、主机设备、主机CICS 应用配置 应用版本、应用节点、应用维护信息、表生命周期 参数配置 高可用参数、系统软件 架构管理 应用架构 应用高可用架构数据、应用互访链路 组织结构 机构部门数据、运维人员数据 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/ 2、数据治理强化个人信息保护，链接：https://new.qq.com/omn/20211025/20211025A03NQX00.html]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据仓库学习系列-《Building the Data Warehouse》学习笔记]]></title>
    <url>%2F2022%2F02%2F13%2F2022-02-09-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-%E3%80%8ABuilding%20the%20Data%20Warehouse%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景《数据仓库》（《Building the Data Warehouse》）是数据仓库系统丛书的第一本著作，作者是Inmon。 第一章 决策支持系统的发展参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Apache Kudu综述]]></title>
    <url>%2F2022%2F01%2F20%2F2022-03-01-Apache%20Kudu%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景第一部分现在提起大数据存储，我们能想到的技术有很多，比如HDFS，以及在HDFS上的列式存储技术Apache Parquet，Apache ORC，还有以KV形式存储半结构化数据的Apache HBase和Apache Cassandra等等。既然有了如此多的存储技术，Cloudera公司为什么要开发出一款全新的存储引擎Kudu呢？ 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[题目]]></title>
    <url>%2F2022%2F01%2F20%2F2022-02-10-%E9%A2%98%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景1、cookie session jwt oauth2 httpshttps://www.cnblogs.com/holaJava/p/13189125.html 这几种方式本质上都是一种授权鉴权的机制。 cookie：Cookie是服务器发送到客户端并保存在本地的一小块数据，它会在客户端下次向同一服务器再发起请求时被携带并发送到服务器上。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie 使基于无状态的 HTTP 协议记录稳定的状态信息成为了可能。cookie中的数据以{key：value}的形式存在。 对于Cookie来说，Cookie的同源只关注域名，是忽略协议和端口的。所以一般情况下，https://localhost:80和http://localhost:8080的Cookie是共享的。单个 Cookie 保存的数据不能超过 4K。 session：Session 代表着服务器和客户端一次会话的过程。Session 对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当客户端关闭会话，或者 Session 超时失效时会话结束。 jwt（JSON Web Token）JWT只是一种token的协议 简单说，JWT是一个身份认证协议(protocol)，他的优点是简单和对URL安全。另一个优势是可使服务器免除session存取的压力，同时在集群机或多子站上面，也免除了session需要同步的烦恼。 什么是JWT？JSON Web Token(JWT),是一个开放安全的行业标准,用于多个系统之间传递安全可靠的信息. oauth2第三方授权就是，委托第三方来对既定的用户进行鉴定，鉴定成功之后，下发信任凭证，信任凭证和用户挂钩，同时可以使用此凭证来去第三方平台，获得该用户开放的部分信息。直白的说，就是将用户授权的工作交给第三方来做，而自己只维护信任凭证，并且获取用户信息。 首先OAuth只是一个授权协议，不是一个实现或是一个中间件。 2、设计元数据系统元数据获取层将涉及的各子系统的元数据经过元数据桥 接器导入到元模型中，元数据服务接口可以通过数据访问接 口返回元数据中的数据内容，并生成其他数据系统需要的数 据字典或提供其他应用的访问接口。元数据应用层提供元数 据浏览、查询、分析的用户界面，提供与 ETL 系统、数据质 量管理系统的数据交换机制。对各层的说明如下： (1)元数据源层。元数据源层包括银行数据仓库涉及的数 据仓库产品、数据挖掘工具、建立数据仓库过程中所需的数 据信息(如 ERWin 文件、Excel 文件)等。 (2)元数据获取层。实现元数据源中各个系统的元数据抽 取。元数据桥接器通过符合双方约定规范的接口或各个产品提供的特定接口实现元数据的抽取，并把抽取出的元数据存 入元数据存储部分中的元数据库。 (3)元数据存储层。实现元数据的存储，存储的元数据包 括业务元数据和技术元数据，元数据按模型主题组织。存储 库的逻辑模型设计须兼顾效率和模型的可扩展性与灵活性。 (4)元数据管理层。由元数据管理和系统管理 2 个部分构 成。元数据管理实现元数据的更新管理、同步管理、版本管 理等功能。系统管理实现用户管理、权限管理、日志管理、 备份与恢复等功能。一些元数据管理部分的功能需要人工或 半人工操作。 (5)元数据服务接口层。包括元数据对外的访问接口，包 括 ETL、DQM、OA 系统或其他系统的服务接口，这些系统 通过元数据服务接口部分访问元数据存储部分的元数据。该 部分为其他用户或系统使用元数据提供了扩展方式。 (6)元数据应用层。提供元数据管理、技术、业务用户的 访问。该部分实现元数据查询、元数据浏览、元数据分析等 基本功能模块。 3、mysql索引介绍MYSQL数据库属于传统的关系型数据库，底层的数据存储基于操作系统文件系统实现，物理上存储在硬盘。数据的写入按照存储块进行存储，大量的表数据分别存储在磁盘的各个磁道上。 当用户进行表数据条件查询的时候，就需要顺序读取磁盘上的数据。由于磁盘I/O有延迟，达到大数据量将会非常耗时。所以需要有元数据来存储表中行数据和硬盘物理地址的映射关系，即数据库表索引。这样查询时就不会出现全表遍历，而是按图索骥。 Mysql默认存储引擎是InnoDB引擎，该引擎下，每个表的索引是一个B+tree数据结构。 DDL(Data Definition Language)数据定义语言： 适用范围：对数据库中的某些对象(例如，database,table)进行管理，如Create,Alter和Drop. DDL(数据定义语言,Data Definition Language) 建库、建表、设置约束等：create\drop\alter 4、mvc概念M-model V-view C-controller MVC是一种前后端代码组织结构思想设计模式。 MVC的主要流程是：客户端发送请求到服务器，由控制器(servlet)接收请求，调用对应的模型层处理数据，模型处理数据之后，再将结果返回给控制器，控制器根据返回的结果调用(渲染)对应的视图响应结果。MVC的主要意义在于，让视图和模型解耦。 5、ETL流程ETL是数据处理中流程概念，其中E表示数据抽取、T表示数据转换、L表示数据加载。所以ETL流程指的是在数据处理中对原始数据依次进行数据抽取、数据转换、数据加载的过程。其中： 数据抽取：通常原始数据源（业务数据库等）数据存储形式较为繁杂。数据入库前需要对业务库数据进行筛选表、字段等，对于不同类型的存储类型（结构数据、半结构数据、非结构数据）选取不同的解析方式等。 数据转换：这里的转换有数据转换（）和数据清洗（不完整的数据、错误的数据、重复的数据）。 数据加载：数据的加载，指的处理后的数据落盘到数据仓库DW层，或者作为其他数据处理的数据源。例如通过ETL处理后的数据作为机器学习模型的训练和验证数据输入。 https://www.cnblogs.com/yjd_hycf_space/p/7772722.html 6、算法题 7、概率题 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Apache Tez介绍]]></title>
    <url>%2F2022%2F01%2F20%2F2022-02-26-Apache%20Tez%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景https://tez.apache.org/ 第一部分参考文献及资料1、Apache Tez官网，链接：https://tez.apache.org/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库的范式总结]]></title>
    <url>%2F2022%2F01%2F20%2F2022-02-13-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E8%8C%83%E5%BC%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景传统的关系型数据库中数据通常是通过库（表的集合）、表（字段的集合）、字段（数据的集合）进行组织的。在使用过程中，人们逐渐对各种最佳实践进行总结归纳，就形成了技术规范。 其中影响最大的就是：数据库3范式（简称3NF，其中NF是英文Normal Form的缩写，是英国人 E.F.Codd（关系数据库理论的奠基人）在上个世纪70年代提出关系数据库模型后总结出来的）。 我们先放出定义： 第一范式（1NF）：要求数据库表的每一列都是不可分割的原子数据项； 第二范式（2NF）：在1NF的基础上，非码属性必须完全依赖于候选码（在1NF基础上消除非主属性对主码的部分函数依赖）； 第三范式（3NF）：在2NF基础上，任何非主属性不依赖于其它非主属性（在2NF基础上消除传递依赖）； 定义通常是归纳抽象的我们下文将详细讲解。范式是关系数据库理论的基础，也是我们在设计数据库结构过程中所要遵循的规则和指导方法。 在这之后又出现了巴斯-科德范式（BCNF）、 第四范式(4NF）和 第五范式（5NF，又称完美范式）。 巴斯-科德范式（BCNF） 第四范式(4NF） 第五范式（5NF） 在设计关系数据库时，遵从不同的规范要求，设计出合理的关系型数据库，各种范式呈递次规范，越高的范式数据库冗余越小。 第一部分 第一范式（1NF）第二部分 第二范式（2NF）第三部分 第三范式（3NF）参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[算法系列-日志挖掘中的FP-Tree算法]]></title>
    <url>%2F2022%2F01%2F20%2F2022-04-01-%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97-%E6%97%A5%E5%BF%97%E6%8C%96%E6%8E%98%E4%B8%AD%E7%9A%84FT-Tree%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 第四部分 参考文献及资料 背景https://blog.csdn.net/hunhun1122/article/details/79699791 https://blog.csdn.net/peiwang245/article/details/79434853?spm=1001.2101.3001.6650.5&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-5.topblog&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-5.topblog&amp;utm_relevant_index=10 FP-Growth(Frequent Pattern Tree，频繁模式树)算法是韩家炜老师提出的关联分析算法，巧妙的将树型结构引入算法中，它采取如下分治策略：提供频繁项集的数据库压缩到一棵频繁模式树（FP-Tree），但仍保留项集关联信息；该算法和Apriori算法最大的不同有两点：第一，不产生候选集。第二，只需要两次遍历数据库，大大提高了效率。 流程如下：1：先扫描一遍数据集，得到频繁项为1的项目集，定义最小支持度（项目出现最少次数），删除那些小于最小支持度的项目，然后将原始数据集中的条目按项目集中降序进行排列。2：第二次扫描，创建项头表（从上往下降序），以及FP树。3：对于每个项目（可以按照从下往上的顺序）找到其条件模式基（CPB，conditional patten base）,递归调用树结构，删除小于最小支持度的项。如果最终呈现单一路径的树结构，则直接列举所有组合；非单一路径的则继续调用树结构，直到形成单一路径即可。 问题抽象： 频繁模式(Frequent pattern) 设 $I=\left{a_{1}, a_{2}, \ldots, a_{m}\right}$ 是项目集合，在数据库中存储相关事务 $D B=\left\langle T_{1}, T_{2}, \ldots, T_{n}\right\rangle$, 其中 $T_{i}(i \in[1 \ldots n])$，$T_{i}$为非空集合，集合中元素由项目集合$I$中元素（元素可以重复）构成。例如下面案例： 1234567I = &#123;'o', 'e', 'd', 'g', 'h', 'p', 'l', 'j', 'k', 'i', 'a', 'b', 'f', 's', 'c', 'n', 'm'&#125;T1 = &#123;f,a,c,d,g,i,m,p&#125;T2 = &#123;a,b,c,f,l,m,o&#125;T3 = &#123;b,f,h,j,o&#125;T4 = &#123;b,c,k,s,p&#125;T5 = &#123;a,f,c,e,l,p,m,n&#125; n项集 集合中每条数据含有的n个项目。例如集合$I$就是1项集。通常即为Ln。 支持度（support ） 模式集合A的发生频率。 频繁模式（frequent pattern） 对于指定的支持度 $\xi$ ，支持度大于等于 $\xi$ 值的模式集合全体。 由此对于理论抽象，我们有朴素但重要的引理： 引理1：设A、B是模式集合，并且A是B的真子集，如果A的支持度为a，则B的支持度b&lt;=a。 第一部分 Apriori算法在FP-Tree算法前，关联规则挖掘的经典算法是Apriori算法。找出所有的频繁项集（支持度必须大于等于给定的最小支持度阈值），在这个过程中连接步和剪枝步互相融合，最终得到最大频繁项集L k 算法基本实现流程描述如下： 连接步 的目的是找到K项集。对给定的最小支持度阈值，分别对1项候选集C 1 ，剔除小于该阈值的项集得到1项频繁集L 1 ；下一步由L 1 自身连接产生2项候选集C 2 ，保留C 2 中满足约束条件的项集得到2项频繁集，记为L 2 ；再下一步由L 2 与L 3 连接产生3项候选集C 3 ，保留C 2 中满足约束条件的项集得到3项频繁集，记为L 3 ……这样循环下去，得到最大频繁项集L k 剪枝步： 剪枝步紧接着连接步，在产生候选项C k 的过程中起到减小搜索空间的目的。由于C k 是L k-1 与L 1 连接产生的，根据Apriori的性质频繁项集的所有非空子集也必须是频繁项集，所以不满足该性质的项集不会存在于C k 中，该过程就是剪枝。 2）由频繁项集产生强关联规则：由过程1）可知未超过预定的最小支持度阈值的项集已被剔除，如果剩下这些规则又满足了预定的最小置信度阈值，那么就挖掘出了强关联规则。 Apriori算法的缺点是对于候选项集里面的每一项都要扫描一次数据，从而需要多次扫描数据，I/O操作多，效率低。为了提高效率，提出了一些基于Apriori的算法，比如FPGrowth算法。 并行处理https://www.cnblogs.com/mstk/archive/2017/07/16/7190179.html 参考文献及资料[1] Han, J., Pei, J., &amp; Yin, Y. (2000). Mining frequent patterns without candidate generation. ACM sigmod record, 29(2), 1-12.]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch 8.0正式版发布介绍]]></title>
    <url>%2F2022%2F01%2F20%2F2022-02-13-Elasticsearch%208.0%E6%AD%A3%E5%BC%8F%E7%89%88%E5%8F%91%E5%B8%83%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景2022年2月11日Elasticsearch官方正式发布了Elasticsearch 8.0.0版本。官网博客也对新版本进行了功能介绍。本文作为中文材料进行总结分享供大家参考。 第一部分 新版本特性总结官网的介绍博客标题为：《Elastic 8.0: A new era of speed, scale, relevance, and simplicity》。简单翻译就是新版速度、规模、相关性和简洁性都进入了一个新的时代。 主要更新特性有： 增强了向量搜索功能（Improve search relevance with native vector search）； Open up a new world of analysis with the power of NLP Search with speed, search at scale Simple things should be simple 第二部分 新特性介绍2.1 增强了向量搜索功能了解Elasticsearch历史的同学应该知道Elasticsearch开始是用来检索菜谱的！Elasticsearch 的设计宗旨始终是提供快速且强大的全文本搜索能力。本质上Elasticsearch检索是基于文本相似度实现（实际上是Lucene实现的），将文本表示为数字向量的一种常见方式是为词汇表中的每个词语分配一个维度。然后便会基于词汇表中每个字词的出现次数得出这段文本的向量。这种表示文本的方法通常称为“词袋”，因为只是简单地数一数词语的出现次数，而不会考虑句子语义。 Elastic 8.0 带来了一整套原生矢量搜索功能，使客户和员工能够使用自己的文字和语言搜索和接收高度相关的结果。 在过去的两年里，我们一直在努力使 Elasticsearch 成为进行矢量搜索的好地方。回到过去，随着Elasticsearch 7.0的发布，我们为高维向量引入了字段类型。在Elasticsearch 7.3和Elasticsearch 7.4中，我们引入了对向量相似度函数的支持。这些早期版本展示了将矢量搜索技术引入 Elasticsearch 生态系统的前景。我们很高兴看到我们的客户和社区急切地将它们用于各种用例。 今天，借助 Elasticsearch 8.0，我们将自然语言处理 (NLP) 模型的原生支持直接引入 Elasticsearch，从而使向量搜索的实施更加实用。此外，Elasticsearch 8.0 包括对近似最近邻(ANN) 搜索的原生支持——这使得基于向量的查询与基于向量的文档语料库的速度和规模进行比较成为可能。 2、宏观看 Elastic 8.0 新特性可以简记为： 一个创新（NLP）； 一个增强（向量检索）； 两个简化（安全和 AWS 集成）； 一个不变（为更大规模数据提供更快速度检索的初心不变）。 2.1 一个创新：新增了对自然语言处理模型 NLP 的原生支持。 如上两图红色、绿色对比，7.X 版本的 NLP 机器学习模型需要借助第三方组件，8.0 版本 Elastic 自己全搞定。 细分创新 1：无需额外组件或编码即可实现：“命名实体识别”、“情感分析”、“文本分类”等 NLP 操作。 细分创新 2：用户可以直接在 Elasticsearch 中使用 PyTorch 机器学习模型（例如 BERT），并使用这些模型（自定义模型或 Hugging Face 社区模型）进行推理。 2.2 一个增强：增强了向量搜索功能。 带来了一整套原生矢量搜索功能。 NLP 的引入使得向量搜索更加实用。 ANN 的支持使得大规模数据下高速查询成为可能。 2.3 两个简化遵循 Elastic 的 slogan：“Simple things should be simple”简单的事情应该简单化。 2.3.1 简化1：安全配置再简化。 默认启用安全性防护。 自动生成令牌和证书。 2.3.2 简化2：集成了两个 AWS 服务。 集成了一个新的 AWS Lambda 应用程序。 集成了一个新的 Amazon Simple Storage Service (Amazon S3) Storage Lens。 2.4 一个不变为更大规模数据提供更快速度检索的初心不变。 3、微观看 Elastic 8.0 新特性3.1 REST API 引入了几项重大更改没有了High-level REST API、没有了 Low-level REST API，简单好用还得 REST API。 这点，java 开发同学要着重关注 API 的兼容性问题。 兼容性推荐： https://www.elastic.co/guide/en/elasticsearch/reference/8.0/rest-api-compatibility.html 3.2 安全简化了啥？首次启动 Elasticsearch 时，会自动进行以下安全配置： 为传输层和 HTTP 层生成 TLS 证书和密钥。 TLS 配置设置被写入elasticsearch.yml。 为 elastic 用户生成密码。 为 Kibana 生成一个注册令牌。 原来 Elasticsearch 安全复杂配置的日子一去不复返了！ 3.3 系统索引得到更好保护要访问系统索引，您现在必须将 allow_restricted_indices权限设置为 true。 该 superuser角色也不再授予对系统索引的写访问权限。 因此，默认情况下，内置 elastic 超级用户也无法更改系统索引。 拥有 elastic用户就拥有集群全部为所欲为的日子一去不复返了！ 3.4 新的 kNN 搜索 API新的 kNN 搜索 API 允许我们在更大的数据集上以更快的速度运行近似 kNN 搜索。 3.5 更新了倒排索引的内部数据结构，节省了磁盘存储空间。这种变化将使 keyword 字段类型、 match_only_text 字段类型以及在较小程度上的 text字段类型受益。 该更新使得转化为 message 字段索引大小（映射为match_only_text）减少了 14.4%，磁盘占用空间总体减少了 3.5%。 3.6 更快地索引geo_point,geo_shape和 range 字段优化了多维点的索引速度，这些字段类型的索引速度提高了 10-15%。 3.7 PyTorch 模型支持自然语言处理 (NLP)可以上传在 Elasticsearch 之外训练的 PyTorch模型，并在摄取时使用它们进行推理。 第三方模型支持为 Elastic Stack 带来了现代自然语言处理 (NLP)和搜索用例，例如： 填充蒙版 Fill-mask 命名实体识别 Named entity recognition (NER) 文本分类 Text classification 文本嵌入 Text embedding 零样本分类 Zero-shot classification 4、Elastic 8.0 部署体验 如上验证，Elasticsearch 8.0 默认启动 TLS 安全、Https 安全。自动生成是很方便了。 Kibana 端仍需要手动配置，Kibana 端更多安全配置推荐阅读： https://www.elastic.co/guide/en/kibana/current/configuring-tls.html 猜测 8.X 版本会改进 Kibana 端配置，现在仍然较复杂。 5、Elastic 版本更迭历史图鉴 发布日期 版本号 时间间隔 2010-05-14 V0.7 2014-02-14 V1.0 1372 天 2015-10-28 V2.0 621 天 2016-10-26 V5.0 364 天 2017-11-14 V6.0 384 天 2019-04-10 V7.0 512 天 2022-02-11 V8.0 1038 天 6、小结 Elastic 版本更迭速度一直快到超出大家的想象。没有最快、只有更快！ 2010年——2022年，Elastic 团队由1人扩展为 2800人 作为 Elastic 从业者要能跟上它的速度，了解并提前使用新特性，更为重要的是待验证充分后将新特性应用到企业环境，以提升开发、运维效率是为王道。 不建议线上环节立即升级 8.0 版本，建议先把 8.0 环境搭建起来，测试数据导入进来，反复测试验证且确保没有问题后，再迁移线上环境。 今天凌晨的直播中 Elastic 创始人又提到了“index everything”，所以拥抱 Elastic，仍然大有可为！ 用当下最流行的话：你永远可以相信快速变化的 Elastic！ 图片来自：2022 年 Elastic 全球社区大会 参考https://www.elastic.co/guide/en/elasticsearch/reference/current/release-highlights.htm https://www.elastic.co/cn/blog/whats-new-elastic-8-0-0 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据湖系列-数据湖综述]]></title>
    <url>%2F2022%2F01%2F20%2F2022-02-26-%E6%95%B0%E6%8D%AE%E6%B9%96%E7%B3%BB%E5%88%97-%E6%95%B0%E6%8D%AE%E6%B9%96%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景数据湖（data Lake），这个术语最早由James Dixon为了与数据集市对比而提出，当时他是Pentaho的首席技术官。 在维基百科中，数据湖是这样定义的。 数据湖，是一类存储数据自然/原始格式的系统或存储，通常是对象块或者文件，包括原始系统所产生的原始数据拷贝以及为了各类任务而产生的转换数据，包括来自于关系型数据库中的结构化数据（行和列）、半结构化数据（如CSV、日志、XML、JSON）、非结构化数据（如email、文档、PDF等）和二进制数据（如图像、音频、视频）。 大部分介绍材料都在传达一个含义：数据湖并没有一个确认的定义。 第一部分Delta Lake 、Iceberg和Hudi三个定位类似的开源项目从数据库方法论中汲取灵感，将事务等能力带到了大数据领域，并抽象成统一的中间格式供不同引擎适配对接。 其本质上并不定义数据存储方式，而是定义数据、元数据的组织方式，向上提供统一的“表”的语义。“表”的底层数据存储仍然使用 Parquet、ORC 等格式。 第三部分 湖仓一体(Lake House)总之，一个后期成本高，一个前期成本高，对于既想修湖、又想建仓的用户来说，仿佛玩了一个金钱游戏。 比如，让“数仓”在进行数据分析的时候，可以直接访问数据湖里的数据（Amazon Redshift Spectrum是这么干的）。再比如，让数据湖在架构设计上，就“原生”支持数仓能力（DeltaLake是这么干）。 湖里的“新鲜”数据可以流到仓里，甚至可以直接被数仓使用，而仓里的“不新鲜”数据，也可以流到湖里，低成本长久保存，供未来的数据挖掘使用。 数据积累得越多，移动起来就越困难，这就是所谓的“数据重力”。 所以，Lake House不仅要把湖、仓打通，还要克服“数据重力”，让数据在这些服务之间按需来回移动：入湖、出湖、环湖…… 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据仓库介绍]]></title>
    <url>%2F2022%2F01%2F20%2F2022-02-09-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-%E6%A8%A1%E6%9D%BF%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景结合我参与过的数仓项目建设经验和踩过的坑，对于数仓主题、主题域划分个人比较推荐按照业务系统划分或者bu部门来划分主题域（一级主题），这样的话边界较为清晰，数据仓库开发过程也不会因为模型主题的归属引发扯皮和不同意见，然后根据各个系统中的业务过程抽象整合出主题（叫二级主题域也可以）。 数仓建设是一整套方法论，但方法论不一定是真理，每个公司都有自己的业务场景及需求，方法论或别人的方案不一定适用自己的公司，我们需要学习利用这些方法论，然后结合自己公司实际的业务场景来制定自己的主题及主题域划分规范。 http://www.woshipm.com/pd/4845057.html 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Yarn集群资源调度策略总结]]></title>
    <url>%2F2022%2F01%2F20%2F2022-01-20-Yarn%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景对于分布式大数据资源管理器，必然要运行大量的大数据处理任务。在面对多租户，任务服务水平优先级差异、集群资源有限的复杂业务场景下，就需要处理好：多租户资源的隔离、任务资源的分配优先级、最大发挥集群资源使用率。 通常业务级大数据平台 在YARN中，资源调度器（Scheduler）是ResourceManager中的重要组件，主要负责对整个集群（CPU，内存）的资源进行分配和调度，分配以资源Container的形式分发到各个应用程序中（如MapReduce作业），应用程序与资源所在节点的NodeManager协作利用Container完成具体的任务（如Reduce Task） https://zhuanlan.zhihu.com/p/349882099 第一部分 FIFO Scheduler第二部分 Capacity Scheduler第三部分 Fair Scheduler如果是使用yarn的公平调度抢占式资源模式，会强制干掉不属于当前用户队列的进程以空出资源。 参考文献及资料1、Hadoop: Capacity Scheduler，链接：https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据湖系列-Iceberg实践总结]]></title>
    <url>%2F2022%2F01%2F20%2F2022-02-26-%E6%95%B0%E6%8D%AE%E6%B9%96%E7%B3%BB%E5%88%97-Iceberg%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景数据湖技术实现上，有开源三剑客（Hudi，Delta Lake，Iceberg），本篇文章主要介绍其中一员：Iceberg。Iceberg官网产品介绍: Iceberg is a high-performance format for huge analytic tables. Iceberg brings the reliability and simplicity of SQL tables to big data, while making it possible for engines like Spark, Trino, Flink, Presto, and Hive to safely work with the same tables, at the same time. Iceberg是一个用于海量数据分析表的高性能格式。也就是说Iceberg本质是一种表格格式。表是一个具象的概念，应用层面的概念，我们天天说的表是简单的行和列的组合。而表格式是数据库系统实现层面一个抽象的概念，它定义了一个表中包含哪些字段，表下面文件的组织形式、表索引信息、统计信息以及上层查询引擎读取、写入表中文件的接口。 我们可以简单理解为他是基于计算层（flink、spark）和存储层（orc、parqurt）的一个中间层，我们可以把它定义成一种“数据组织格式”，Iceberg将其称之为“表格式”也是表达类似的含义。他与底层的存储格式（比如ORC、Parquet之类的列式存储格式）最大的区别是，它并不定义数据存储方式，而是定义了数据、元数据的组织方式，向上提供统一的“表”的语义。它构建在数据存储格式之上，其底层的数据存储仍然使用Parquet、ORC等进行存储。在hive建立一个iceberg格式的表。用flink或者spark写入iceberg，然后再通过其他方式来读取这个表，比如spark、flink、presto等。 Iceberg的架构和实现并未绑定于某一特定引擎，它实现了通用的数据组织格式，利用此格式可以方便地与不同引擎（如Flink、Hive、Spark）对接。 https://www.cnblogs.com/swordfall/p/14548574.html https://iceberg.apache.org/docs/latest/flink/ 第一部分 预备知识：表格格式传统的大数据仓库我们通常是基于Hadoop生态中Hive进行建设，技术上Hive底层的存储是基于HDFS分布式文件系统。HDFS的文件存储有多种方式：Text、Json、Parquet、ORC等。而对于实时查询要求较高的场景我们将数据存储在HBase中，而HBase的文件个是HFile。 第二部分 Iceberg的表格格式2. Iceberg优势 增量读取处理能力：Iceberg支持通过流式方式读取增量数据，支持Structed Streaming以及Flink table Source； 支持事务（ACID），上游数据写入即可见，不影响当前数据处理任务，简化ETL；提供upsert和merge into能力，可以极大地缩小数据入库延迟； 可扩展的元数据，快照隔离以及对于文件列表的所有修改都是原子操作； 同时支持流批处理、支持多种存储格式和灵活的文件组织：提供了基于流式的增量计算模型和基于批处理的全量表计算模型。批处理和流任务可以使用相同的存储模型，数据不再孤立；Iceberg支持隐藏分区和分区进化，方便业务进行数据分区策略更新。支持Parquet、Avro以及ORC等存储格式。 支持多种计算引擎，优秀的内核抽象使之不绑定特定的计算引擎，目前Iceberg支持的计算引擎有Spark、Flink、Presto以及Hive。 第三部分 创建Iceberg表Apache Iceberg支持多种方式创建Iceberg表，其中包括使用catalog方式（所谓Catalog就是一系列创建、删除、加载表的操作API（A Catalog API for table create, drop, and load operations））或实现org.apache.iceberg.Tables接口。 Apache Iceberg支持Apache Flink的DataStream Api和Table Api写记录进iceberg表。当前，我们只集成Iceberg和apache flink 1.11.x。 当然，Apache Iceberg 表元数据存储地方是可插拔的，所以我们完全可以自定义元数据存储的方式 3.0 计算引擎3.0.1 Flink需要下载两个依赖包： 12# https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-flink-runtime/0.12.1/iceberg-flink-runtime-0.12.1.jar# https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-hive-3.1.2_2.11/1.12.7/flink-sql-connector-hive-3.1.2_2.11-1.12.7.jar 启动flink sql client，可以创建hadoop catalog如下： 1# ./bin/sql-client.sh embedded -j lib/flink-runtime_2.12-1.12.7.jar shell 启动flink sql client，带hive connector jar包，可以创建hadoop catalog和hive catalog如下： 1234# ./bin/sql-client.sh embedded \ -j lib/flink-runtime_2.12-1.12.7.jar \ -j lib/flink-runtime_2.12-1.12.7.jar \ shell 3.0.2 Sparkhttps://jishuin.proginn.com/p/763bfbd55830 3.0.3 Hivehttps://www.cxybb.com/article/qq_31866793/116169683 3.1 catalogFlink1.11支持通过flink sql创建catalogs。catalog是Iceberg对表进行管理（create、drop、rename等）的一个组件。目前Iceberg主要支持HiveCatalog和HadoopCatalog两种Catalog。其中HiveCatalog将当前表metadata文件路径存储在hive Metastore，这个表metadata文件是所有读写Iceberg表的入口，所以每次读写Iceberg表都需要先从hive Metastore中取出对应的表metadata文件路径，然后再解析这个Metadata文件进行接下来的操作。而HadoopCatalog将当前表metadata文件路径记录在一个文件目录下，因此不需要连接hive Metastore。 Catalog元数据本身既可以存储到Hadoop HDFS文件系统也可以存储在Hive Metastore（HMS）。 3.1.1 Hive catalogHive catalog 是通过连接 Hive 的 MetaStore，把 Iceberg 的表存储到其中，它的实现类为 org.apache.iceberg.hive.HiveCatalog 创建一个名为hive_catalog的 iceberg catalog ，用来从 hive metastore 中加载表。 12345678CREATE CATALOG hive_catalog WITH ( 'type'='iceberg', 'catalog-type'='hive', 'uri'='thrift://localhost:9083', 'clients'='5', 'property-version'='1', 'warehouse'='hdfs://nn:8020/warehouse/path'); 参数说明： type: 只能使用iceberg,用于 iceberg 表格式。(必须) catalog-type: Iceberg 当前支持hive或hadoopcatalog 类型。(必须) uri: Hive metastore 的 thrift URI。 (必须) clients: Hive metastore 客户端池大小，默认值为 2。 (可选) property-version: 版本号来描述属性版本。此属性可用于在属性格式发生更改时进行向后兼容。当前的属性版本是 1。(可选) warehouse: Hive 仓库位置, 如果既不将 hive-conf-dir 设置为指定包含 hive-site.xml 配置文件的位置，也不将正确的 hive-site.xml 添加到类路径，则用户应指定此路径。 hive-conf-dir: 包含 Hive-site.xml 配置文件的目录的路径，该配置文件将用于提供自定义的 Hive 配置值。 如果在创建 iceberg catalog 时同时设置 hive-conf-dir 和 warehouse，那么将使用 warehouse 值覆盖 &lt; hive-conf-dir &gt;/hive-site.xml (或者 classpath 中的 hive 配置文件)中的 hive.metastore.warehouse.dir 的值。 3.1.2 Hadoop catalogIceberg 还支持 HDFS 中基于目录的 catalog ，可以使用’catalog-type’=’hadoop’进行配置： 123456CREATE CATALOG hadoop_catalog WITH ( 'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='hdfs://nn:8020/warehouse/path', 'property-version'='1'); warehouse：hdfs目录存储元数据文件和数据文件。（必须） 我们可以执行sql命令USE CATALOG hive_catalog来设置当前的catalog。 3.1.3 Hadoop catalogFlink也支持通过指定catalog-impl属性来加载自定义的Iceberg catalog接口。当catalog-impl设置了，catalog-type的值可以忽略，这里有个例子： 12345CREATE CATALOG my_catalog WITH ( 'type'='iceberg', 'catalog-impl'='com.my.custom.CatalogImpl', 'my-additional-catalog-config'='my-value'); 3.1.4 Hadoop catalog在启动SQL客户端之前，Catalogs可以通过在sql-client-defaults.yaml文件中注册。这里有个例子： 12345catalogs: - name: my_catalog type: iceberg catalog-type: hadoop warehouse: hdfs://nn:8020/warehouse/path 3.2 文件表的组织形式1. HiveCatalog1234hadoop@xxx:~$ hdfs dfs -ls /libis/hive-2.3.6/hive_iceberg.db/action_logsFound 2 itemsdrwxrwxrwx - hadoop supergroup 0 2020-06-08 12:20 /libis/hive-2.3.6/hive_iceberg.db/action_logs/datadrwxrwxrwx - hadoop supergroup 0 2020-06-08 12:20 /libis/hive-2.3.6/hive_iceberg.db/action_logs/metadata 其中data目录下存储数据文件，metadata目录下存储元数据文件。 2. metadata目录;) 123456hadoop@xxx:~$ hdfs dfs -ls /libis/hive-2.3.6/hive_iceberg.db/action_logs/metadataFound 4 items-rw-r--r-- 1 hadoop supergroup 1448 2020-06-08 11:31 /libis/hive-2.3.6/hive_iceberg.db/action_logs/metadata/00000-e7c1e6ce-8eb9-4faf-a176-bd94dec3c0e4.metadata.json-rw-r--r-- 1 hadoop supergroup 2217 2020-06-08 12:20 /libis/hive-2.3.6/hive_iceberg.db/action_logs/metadata/00001-62ade8ab-c1cf-40d3-bc21-fd5027bc3a55.metadata.json-rw-r--r-- 1 hadoop supergroup 5040 2020-06-08 12:20 /libis/hive-2.3.6/hive_iceberg.db/action_logs/metadata/bb641961-162a-49a8-b567-885430d4e799-m0.avro-rw-r--r-- 1 hadoop supergroup 2567 2020-06-08 12:20 /libis/hive-2.3.6/hive_iceberg.db/action_logs/metadata/snap-6771375506965563160-1-bb641961-162a-49a8-b567-885430d4e799.avro ;) 其中00001-62ade8ab-c1cf-40d3-bc21-fd5027bc3a55.metadata.json中存储表的shcema、partition spec以及当前snapshot manifests文件路径。snap-6771375506965563160-1-bb641961-162a-49a8-b567-885430d4e799.avro存储manifest文件路径。bb641961-162a-49a8-b567-885430d4e799-m0.avro记录本次提交的文件以及文件级别元数据。 3. data目录123hadoop@xxx:~$ hdfs dfs -ls /libis/hive-2.3.6/hive_iceberg.db/action_logs/data/event_time_hour=2020-06-04-19/action=clickFound 1 items-rw-r--r-- 1 hadoop supergroup 1425 2020-06-08 12:20 /libis/hive-2.3.6/hive_iceberg.db/action_logs/data/event_time_hour=2020-06-04-19/action=click/00015-47-a9f5ce8f-ee6f-4748-9f49-0f94761859bc-00000.parquet 4. HadoopCatalogHadoopcatalog与Hivecatalog的data目录完全相同，metadata目录下文件稍有不同，HadoopCatalog管理的metadata目录如下所示： ;) 123456789hadoop@xxx:~$ hdfs dfs -ls /libis/hive-2.3.6/hadoop_iceberg/action_logs/metadataFound 5 items-rw-r--r-- 1 hadoop supergroup 5064 2020-06-08 17:24 /libis/hive-2.3.6/hadoop_iceberg/action_logs/metadata/b222d277-2692-4e35-9327-3716dec9f070-m0.avro-rw-r--r-- 1 hadoop supergroup 2591 2020-06-08 17:24 /libis/hive-2.3.6/hadoop_iceberg/action_logs/metadata/snap-3124052841098464551-1-b222d277-2692-4e35-9327-3716dec9f070.avro-rw-r--r-- 1 hadoop supergroup 1476 2020-06-08 17:23 /libis/hive-2.3.6/hadoop_iceberg/action_logs/metadata/v1.metadata.json-rw-r--r-- 1 hadoop supergroup 2261 2020-06-08 17:24 /libis/hive-2.3.6/hadoop_iceberg/action_logs/metadata/v2.metadata.json-rw-r--r-- 1 hadoop supergroup 1 2020-06-08 17:24 /libis/hive-2.3.6/hadoop_iceberg/action_logs/metadata/version-hint.text 其中文件version-hint.text中存储当前iceberg表的最新snapshot_id，如下所示：hadoop@xxx:~$ hdfs dfs -cat /libis/hive-2.3.6/hadoop_iceberg/action_logs/metadata/version-hint.text 2 ;) 说明该表的最新snapshot_id是2，即对应的snapshot元数据文件是v2.metadata.json，解析v2.metadata.json可以获取到该表当前最新snapshot对应的scheme、partition spec、父snapshot以及该snapshot对应的manifestList文件路径等，因此version-hint.text是HadoopCatalog获取当前snapshot版本的入口。 HiveCatalog的metadata目录下并没有version-hint.text文件，那它获取当前snapshot版本的入口在哪里呢？它的入口在Metastore中的schema里面，可以在HiveCatalog建表schema中的TBPROPERTIES中有个key是“metadata_location”，对应的value就是当前最新的snapshot文件。因此，有两点需要说明： HiveCatalog创建的表，每次提交写入文件生成新的snapshot后都需要更新Metastore中的metadata_location字段。 HiveCatalog和HadoopCatalog不能混用。即使用HiveCatalog创建的表，再使用HadoopCatalog是不能正常加载的，反之亦然。 3.3 技术选择上面说到Iceberg目前支持两种Catalog，而且两种Catalog相互不兼容。那这里有两个问题： 社区是出于什么考虑实现两种不兼容的Catalog？ 因为两者不兼容，必须选择其一作为系统唯一的Catalog，那是选择HiveCatalog还是HadoopCatalog，为什么？ 先回答第一个问题。社区是出于什么考虑实现两种不兼容的Catalog？ 在回答这个问题之前，首先回顾一下上一篇文章中介绍到的基于HadoopCatalog，Iceberg实现数据写入提交的ACID机制，最终的结论是使用了乐观锁机制和HDFS rename的原子性一起保障写入提交的ACID。如果某些文件系统比如S3不支持rename的原子性呢？那就需要另外一种机制保障写入提交的ACID，HiveCatalog就是另一种不依赖文件系统支持，但是可以提供ACID支持的方案，它在每次提交的时候都更新MySQL中同一行记录，这样的更新MySQL本身是可以保证ACID的。这就是社区为什么会支持两种不兼容Catalog的本质原因。 再来回答第二个问题。HadoopCatalog依赖于HDFS提供的rename原子性语义，而HiveCatalog不依赖于任何文件系统的rename原子性语义支持，因此基于HiveCatalog的表不仅可以支持HDFS，而且可以支持s3、oss等其他文件系统。但是HadoopCatalog可以认为只支持HDFS表，比较难以迁移到其他文件系统。但是HadoopCatalog写入提交的过程只依赖HDFS，不和Metastore/MySQL交互，而HiveCatalog每次提交都需要和Metastore/MySQL交互，可以认为是强依赖于Metastore，如果Metastore有异常，基于HiveCatalog的Iceberg表的写入和查询会有问题。相反，HadoopCatalog并不依赖于Metastore，即使Metastore有异常，也不影响Iceberg表的写入和查询。 考虑到我们目前主要还是依赖HDFS，同时不想强依赖于Metastore，所以我们选择HadoopCatalog作为我们系统唯一的Catalog。即使有一天，想要把HDFS上的表迁移到S3上去，也是可以办到的，大家想想，无论是HadoopCatalog还是HiveCatalog，数据文件和元数据文件本身都是相同的，只是标记当前最新的snapshot的入口不一样，那只需要简单的手动变换一下入口就可以实现Catalog的切换，切换到HiveCatalog上之后，就可以摆脱HDFS的依赖，问题并不大。 第四部分 Benchmark测试4.1 测试集介绍Star Schema Benchmark(SSB)Star-Schema-Benchmark 测试是一个轻量级的数仓场景下的性能测试集。SSB基于 TPC-HStar-Schema-Benchmark 测试。提供了一个简化版的星形模型数据集，主要用于测试在星形模型下，多表关联查询的性能表现。 123$ git clone git@github.com:vadimtk/ssb-dbgen.git$ cd ssb-dbgen$ make 考虑到测试环境资源，使用-s 10级别。 注：使用-s 100dbgen 将生成 6 亿行数据(67GB), 如果使用-s 1000它会生成 60 亿行数据(这需要很多时间)) 12345$ ./dbgen -s 10 -T c$ ./dbgen -s 10 -T l$ ./dbgen -s 10 -T p$ ./dbgen -s 10 -T s$ ./dbgen -s 10 -T d 一共生成4张维度表（customer，part，dwdate，supplier）和一张事实表（lineorder） 123456root@deeplearning:/data/benchmark/ssb-dbgen# ll -h|grep tbl-rw-r--r-- 1 root root 32M 2月 27 14:14 customer.tbl-rw-r--r-- 1 root root 270K 2月 27 14:19 date.tbl-rw-r--r-- 1 root root 6.5G 2月 27 14:17 lineorder.tbl-rw-r--r-- 1 root root 77M 2月 27 14:19 part.tbl-rw-r--r-- 1 root root 1.9M 2月 27 14:19 supplier.tbl 4.2 环境准备参考文献及资料1、Star Schema Benchmark，链接：https://clickhouse.com/docs/zh/getting-started/example-datasets/star-schema/]]></content>
      <categories>
        <category>Iceberg</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hive表的索引]]></title>
    <url>%2F2022%2F01%2F20%2F2022-02-10-Hive%E8%A1%A8%E7%9A%84%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景简介Hive从0.7.0版本开始加入了索引， 在 0.7.0 版中添加了 Hive 索引，在 0.8.0 版中添加了位图索引。 索引已在 3.0 版(HIVE-18448)中被“删除”。 https://issues.apache.org/jira/browse/HIVE-18448 目的是提高Hive表指定列的查询速度。没有索引的时候，Hive在执行查询时需要加载整个表或者整个分区，然后处理所有的数据，但当在指定列上存在索引，再通过指定列查询时，那么只会加载和处理部分文件。此外，同传统关系型数据库一样，增加索引在提升查询速度的同时，会额外消耗资源去创建索引和需要更多的磁盘空间存储索引。 机制和原理Hive的索引其实是一张索引表（Hive的物理表），在表里面存储索引列的值，该值对应的HDFS的文件路径，该值在数据文件中的偏移量。 当Hive通过索引列执行查询时，首先通过一个MR Job去查询索引表，根据索引列的过滤条件，查询出该索引列值对应的HDFS文件目录及偏移量，并且把这些数据输出到HDFS的一个文件中，然后再根据这个文件中去筛选原文件，作为查询Job的输入。 优点 可以避免全表扫描和资源浪费 可以加快含有group by的语句的查询速度 创建索引1234create index test_index on table test(id)as 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'with deferred rebuildin table test; 生成索引数据刚创建完的Hive索引表是没有数据的，需要生成索引数据 1alter index test_index on test rebuild; 使用索引123SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;SET hive.optimize.index.filter=true;SET hive.optimize.index.filter.compact.minsize=0; 删除索引1drop index test_index on test; 缺点 使用过程繁琐 需用额外Job扫描索引表 不会自动刷新，如果表有数据变动，索引表需要手动刷新 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据湖及开源三剑客]]></title>
    <url>%2F2022%2F01%2F20%2F2022-02-08-%E6%95%B0%E6%8D%AE%E6%B9%96%E7%B3%BB%E5%88%97-%E6%95%B0%E6%8D%AE%E6%B9%96%E5%8F%8A%E5%BC%80%E6%BA%90%E4%B8%89%E5%89%91%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景Delta Lake 、Iceberg和Hudi三个定位类似的开源项目从数据库方法论中汲取灵感，将事务等能力带到了大数据领域，并抽象成统一的中间格式供不同引擎适配对接。 其本质上并不定义数据存储方式，而是定义数据、元数据的组织方式，向上提供统一的“表”的语义。“表”的底层数据存储仍然使用 Parquet、ORC 等格式。 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[运维大数据平台建设总结]]></title>
    <url>%2F2022%2F01%2F20%2F2022-01-20-%E8%BF%90%E7%BB%B4%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E5%BB%BA%E8%AE%BE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景https://www.infoq.cn/article/jqr7zoiucoqi0skgiagb https://www.6aiq.com/article/1635205226188 我们认为数据中台是企业级大数据通过系统化的方式实现统一、标准、安全、共享的数据组织，以服务化的方式赋能前台数据应用，提高数据的使用效率。 数据中台与数据平台最本质的区别在于数据中台是具备业务属性的，输入的是原始数据，输出的是指标。数据中台包含了业务对数据的组织方法论，体现在主题域，业务过程的划分，数据模型的设计，指标、维度、度量的管理，如果我们想确定一个数据是指标还是维度，就必须理解业务。大数据平台提供的是与业务属性无关的工具集合，是数据的加工能力，至于加工的什么数据，平台并不关心。 https://www.secrss.com/articles/15465 第一部分- 监控数据：监控事件报警数据、监控性能/KPI指标数据两类，特点是实时、代理、海量、时序为主。 - 日志数据：机器运行日志、系统日志、应用日志，特点是海量、实时、非结构化、格式不统一、有业务相关数据。 - 性能数据：APM、NPM、BPM，或应用主动上报的性能数据，特点是海量、实时、贴近业务与用户体验、链路关系、格式不统一。 - 配置数据：围绕CMDB的配置CI、关系、架构数据，特点是CMDB方案较成熟，关系与架构数据复杂但自发现能力困难。 - 流程数据：围绕ITSM，以及其他运维场景工具（监管控析、安全、CMP等）记录的数据，特点是关键流程基于ITSM、实时性不够、大量琐碎工作来源于各类工具。 - 应用运行数据：记录在业务系统数据库中的系统运行数据，特点是与系统相关、贴近业务与用户体验、依赖研发支持、格式不统一。 其中技术平台指支撑运维海量数据的“采、存、算、管、用”的技术架构 鉴于运维数据有着来源多、标准化、实时、海量、非结构化、格式不统一等特点，仅从“技术平台+应用场景”两个角度看运维数据平台，很容易将运维数据相关项目建成一个个数据孤岛式的数据应用场景，无法发挥数据价值。 大数据领域的数据治理主要包括元数据、主数据、数据标准、数据质量、数据模型、数据安全、数据生命周期7部分内容 数据安全在大数据平台建设初期，安全也许并不是被重点关注的一环。大数据平台的定位主要是服务数据开发人员，提高数据开发效率，提供便捷的开发流程，有效支持数仓建设。大数据平台的用户都是公司内部人员。数据本身的安全性已经由公司层面的网络及物理机房的隔离来得到保证。那么数据平台建设过程中，需要考虑哪些安全性方面的问题？ 环境隔离，数据开发人员应当只需关注自己相关业务域的数据，也应该只能访问这一部分数据。从数据的角度，减小了被接触面，降低了被误操作的可能。从数据开发人员的角度，只能访问自己业务域的数据，在数据开发的过程中，可以减少干扰项，提高效率。 数据脱敏，有些敏感数据即使是公司内部的数据开发人员，也需要限制其直接访问的权限。 明晰权责，各业务域数据都有相应的负责人，对自己的数据负责。同时，所有数据访问与操作都有审计信息记录，对数据的转化与流动有据可查。 最后，大数据平台的目标是赋能数据开发人员，提高数据开发效率，而安全管理必然会降低数据平台的便利性。如何平衡安全和便利性的关系，尤为重要。 有赞大数据平台安全建设是在大数据平台本身的发展以及数仓元数据建设的过程中不断演进的。概括起来可以分为三个阶段。 https://tech.youzan.com/bigdatasafety/ 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka如何实现Exactly-once语义]]></title>
    <url>%2F2022%2F01%2F20%2F2022-02-26-Kafka%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0Exactly-once%E8%AF%AD%E4%B9%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于运维未来发展的展望]]></title>
    <url>%2F2022%2F01%2F20%2F2022-01-29-%E5%85%B3%E4%BA%8E%E8%BF%90%E7%BB%B4%E6%9C%AA%E6%9D%A5%E5%8F%91%E5%B1%95%E7%9A%84%E5%B1%95%E6%9C%9B%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景https://www.6aiq.com/article/1635205226188 我们认为数据中台是企业级大数据通过系统化的方式实现统一、标准、安全、共享的数据组织，以服务化的方式赋能前台数据应用，提高数据的使用效率。 数据中台与数据平台最本质的区别在于数据中台是具备业务属性的，输入的是原始数据，输出的是指标。数据中台包含了业务对数据的组织方法论，体现在主题域，业务过程的划分，数据模型的设计，指标、维度、度量的管理，如果我们想确定一个数据是指标还是维度，就必须理解业务。大数据平台提供的是与业务属性无关的工具集合，是数据的加工能力，至于加工的什么数据，平台并不关心。 https://www.secrss.com/articles/15465 第一部分- 监控数据：监控事件报警数据、监控性能/KPI指标数据两类，特点是实时、代理、海量、时序为主。 - 日志数据：机器运行日志、系统日志、应用日志，特点是海量、实时、非结构化、格式不统一、有业务相关数据。 - 性能数据：APM、NPM、BPM，或应用主动上报的性能数据，特点是海量、实时、贴近业务与用户体验、链路关系、格式不统一。 - 配置数据：围绕CMDB的配置CI、关系、架构数据，特点是CMDB方案较成熟，关系与架构数据复杂但自发现能力困难。 - 流程数据：围绕ITSM，以及其他运维场景工具（监管控析、安全、CMP等）记录的数据，特点是关键流程基于ITSM、实时性不够、大量琐碎工作来源于各类工具。 - 应用运行数据：记录在业务系统数据库中的系统运行数据，特点是与系统相关、贴近业务与用户体验、依赖研发支持、格式不统一。 其中技术平台指支撑运维海量数据的“采、存、算、管、用”的技术架构 鉴于运维数据有着来源多、标准化、实时、海量、非结构化、格式不统一等特点，仅从“技术平台+应用场景”两个角度看运维数据平台，很容易将运维数据相关项目建成一个个数据孤岛式的数据应用场景，无法发挥数据价值。 大数据领域的数据治理主要包括元数据、主数据、数据标准、数据质量、数据模型、数据安全、数据生命周期7部分内容 数据安全在大数据平台建设初期，安全也许并不是被重点关注的一环。大数据平台的定位主要是服务数据开发人员，提高数据开发效率，提供便捷的开发流程，有效支持数仓建设。大数据平台的用户都是公司内部人员。数据本身的安全性已经由公司层面的网络及物理机房的隔离来得到保证。那么数据平台建设过程中，需要考虑哪些安全性方面的问题？ 环境隔离，数据开发人员应当只需关注自己相关业务域的数据，也应该只能访问这一部分数据。从数据的角度，减小了被接触面，降低了被误操作的可能。从数据开发人员的角度，只能访问自己业务域的数据，在数据开发的过程中，可以减少干扰项，提高效率。 数据脱敏，有些敏感数据即使是公司内部的数据开发人员，也需要限制其直接访问的权限。 明晰权责，各业务域数据都有相应的负责人，对自己的数据负责。同时，所有数据访问与操作都有审计信息记录，对数据的转化与流动有据可查。 最后，大数据平台的目标是赋能数据开发人员，提高数据开发效率，而安全管理必然会降低数据平台的便利性。如何平衡安全和便利性的关系，尤为重要。 有赞大数据平台安全建设是在大数据平台本身的发展以及数仓元数据建设的过程中不断演进的。概括起来可以分为三个阶段。 https://tech.youzan.com/bigdatasafety/ 参考文献及资料1、数据治理对运维数据体系的思考与启发，链接：http://blog.itpub.net/69994525/viewspace-2762789/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[大数据工程师技术自测题库]]></title>
    <url>%2F2022%2F01%2F19%2F2022-01-19-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%B8%88%E6%8A%80%E6%9C%AF%E8%87%AA%E6%B5%8B%E9%A2%98%E5%BA%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景第一部分https://book.itheima.net/study/1269935677353533441/1270254018572066817 参考文献及资料1、RuoYi-Cloud项目文档，链接：https://blog.csdn.net/ifenggege/article/details/107968518]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop HDFS高可用机制]]></title>
    <url>%2F2022%2F01%2F16%2F2022-01-18-Hadoop%20HDFS%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景先从一个生产线上问题说起。生产线上我们使用的华为封装的Hadoop大数据平台，使用开源版本Flink 1.11作为Flink客户端提交任务到平台Yarn集群上运行。 但是提交后报错： 1cused by： java.lang.NoSuchMethodException: org.aphache.hadoop.hdfs.server.namedode.ha.AdaptiveFailoverProxyProvider.&lt;int&gt;.... 即在提交过程中Flink客户端找不到下面这个java class类导致的： 1org.aphache.hadoop.hdfs.server.namedode.ha.AdaptiveFailoverProxyProvider 通过官网（如下），这个类是用来寻找active状态的HDFS NameNode服务的。通常对于开源集群默认类是： 1org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider 然而华为集群自定义了这个类（参考客户端中配置文件），所以开源Flink包中并没有这个类，所以会报错找不到类。 重新调整客户端中的配置为开源默认参数（或者集群控制台中修改生效后，重新下载和部署客户端），既能提交任务。 dfs.client.failover.proxy.provider.[nameservice ID] - the Java class that HDFS clients use to contact the Active NameNode Configure the name of the Java class which will be used by the DFS Client to determine which NameNode is the current Active, and therefore which NameNode is currently serving client requests. The two implementations which currently ship with Hadoop are the ConfiguredFailoverProxyProvider and the RequestHedgingProxyProvider (which, for the first call, concurrently invokes all namenodes to determine the active one, and on subsequent requests, invokes the active namenode until a fail-over happens), so use one of these unless you are using a custom proxy provider. For example: 123&lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt; 本文将介绍背后的原因。 第一部分 HDFS服务高可用原理1.1 HDFS服务Hadoop 2.0.0 之前，集群只有一个Namenode服务，这显然是个单点问题。对于线上生产环境部署NameNode服务的服务器机器发生宕机，会导致整个HDFS服务不可用。即使应急快，NameNode节点从磁盘中将元数据加载至内存也是一个极端缓慢的过程（大数据场景）。显然这对于业务连续性要求是不能容忍的。另外日常运维中，对于NameNode的变更需要计划内停机，在变更过程中，集群同样是不能对外提供服务的。 所以在Hadoop 2.0 +之后的版本引入了HA高可用机制。HA using Quorum Journal Manager (QJM)是当前主流HDFS HA方案，由Cloudera工程师Todd Lipcon在2012年发起，随Hadoop 2.0.3发布，此后一直被视为HDFS HA默认方案。具体架构图如下。 注：我们在架构图（参考网络）中看到FailoverController了。 在现有框架下，HDFS一共有下面几类服务： DataNode（DN）；数据节点； 除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。 NameNode（NN）；管理节点； Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。 ZKFailoverController（FailoverControllerActive\Standby）；服务角色：主备切换控制器； JournalNode（JN）；共享存储； 共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务，主要有JournalNode 。 Zookeeper集群（ZK）；提供主备选举支持（通常和其他HA服务共用）； ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换。 1.2 HA机制原理1.2.1 机制当Active NN故障时，Zookeeper创建的临时节点ActiveStandbyElectorLock将要被删除，其他NN节点注册的Watcher 来监听到该变化，NN节点的ZKFailoverController 会马上再次进入到创建/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点的流程，如果创建成功，这个本来处于 Standby 状态的 NameNode 就选举为主 NameNode 并随后开始切换为 Active 状态。 新当选的Active NN将确保从QJM(Quorum Journal Manager)同步完所有的元数据文件EditLog文件，然后切换为主节点，并向外提供服务。 1.2.2 手动切换Hadoop客户端提供haadmin命令对NN节点可以实时手动切换，具体命令清单有： 123456hdfs haadmin -checkHealth &lt;serviceId&gt;hdfs haadmin -failover [--forcefence] [--forceactive] &lt;serviceId&gt; &lt;serviceId&gt;hdfs haadmin -getServiceState &lt;serviceId&gt;hdfs haadmin -help &lt;command&gt;hdfs haadmin -transitionToActive &lt;serviceId&gt; [--forceactive]hdfs haadmin -transitionToStandby &lt;serviceId&gt; 其中serviceId可以通过配置文件hdfs.xml中找到service id，配置文件内容如下: 1234567&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;cluster&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.ha.namenodes.cluster&lt;/name&gt;&lt;value&gt;14,15&lt;/value&gt; 所以集群的NN节点有两个，名称为：14、15（比较特别，这是华为大数据产品）。 那么就可以通过下面的命令来查看NN的状态： 1234hdfs haadmin -getServiceState 14# activehdfs haadmin -getServiceState 15# standy 通过下面命令把active服务调整为standy： 12hdfs haadmin -transitionToStandby 14 --forcemanual# 会回显风险提示确认是否继续 第二部分 客户端交互便于客户端确定哪个nn是主节点。对于第一次调用，它同时调用所有名称节点以确定活动的名称节点,之后便直接调用主节点（active nn）,可以理解帮助客户端获取主节点的代理。 ConfiguredFailoverProxyProvider 和RequestHedgingProxyProvider 选其一即可。 HDFS jar包冲突列表 Jar包名称 描述 处理方案 hadoop-plugins-*.jar HDFS可以直接使用开源同版本的hadoop包运行样例代码，但是MRS 3.x之后的版本默认的主备倒换类是dfs.client.failover.proxy.provider.hacluster=org.apache.hadoop.hdfs.server.namenode.ha.AdaptiveFailoverProxyProvider默认HDFS的LZC压缩格式类io.compression.codec.lzc.class=com.huawei.hadoop.datasight.io.compress.lzc.ZCodec 方式一：参考样例代码里面的pom.xml文件，增加配置：&lt;properties&gt; &lt;hadoop.ext.version&gt;8.0.2-302002&lt;/hadoop.ext.version&gt; &lt;/properties&gt; ... &lt;dependency&gt; &lt;groupId&gt;com.huawei.mrs&lt;/groupId&gt; &lt;artifactId&gt;hadoop-plugins&lt;/artifactId&gt; &lt;version&gt;${hadoop.ext.version}&lt;/version&gt; &lt;/dependency&gt;方式二：将hdfs-site.xml配置文件里面的参数dfs.client.failover.proxy.provider.hacluster改为开源一致的值：org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider不使用LZC压缩格式。 参考文献及资料1、RuoYi-Cloud项目文档，链接：https://blog.csdn.net/ifenggege/article/details/107968518]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于PyFlink实现在线机器学习]]></title>
    <url>%2F2021%2F11%2F16%2F2022-01-15-%E5%9F%BA%E4%BA%8EPyFlink%E5%AE%9E%E7%8E%B0%E5%9C%A8%E7%BA%BF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境准备 第二部分 原理解析 第三部分 案例运行 第四部分 线上架构设计 参考文献及资料 背景目前Flink逐步成为企业级大数据平台的使用最广泛的实时计算框架，特别在构建TB级别的实时数仓场景。Flink经历大量企业级线上环境的业务考验。 那么Flink的强大实时计算能力能否赋能给机器学习场景呢？这就是PyFlink项目的目的。我们知道机器学习主要是基于Python语言生态圈。但是Python（Cpython）语言是一个单核语言（即全局解析锁GIL），限制了单机处理性能。但是有了Flink赋能，Python机器学习可以分布式实时处理，就能大大提升机器学习的处理能力。 本文将介绍PyFlink项目的实现原理、环境部署、机器学习案例运行，最后对实际线上架构部署提出架构建议。 第一部分 环境准备1.1 Hadoop Yarn集群准备测试环境我们使用CDH开源集群，Flink的运行方式采用Flink on Yarn模式。单机测试的时候还有下面几种模式： Local-SingleJVM 模式，开发测试使用，所有角色TM、JM 都在同一个 JVM 里面，线程模拟； Local-SingleNode 模式，开发测试使用，运行在单机，进程模拟，伪分布式。 Cluster 模式，Flink自带集群模式，即Standalone集群； k8s模式，k8s云模式部署； 1.1 Flink准备Flink 版本采用最新（2022年1月15日）稳定版：Apache Flink 1.14.2。 1.2 Python环境准备Python环境我们使用Aconda环境部署，Python内核版本为Python3.7.6。目前官网要求版本为： 3.6 或 3.7+，否则会出错。 另外需要部署安装PyFlink包。从flink1.10开始，PyFlink安装无需编译源码. 安装命令如下（注意对应的Flink版本）： 1root@deeplearning:~# pip install apache-flink==1.14.2 由于墙的原因建议指定国内源（使用清华大学源：https://pypi.tuna.tsinghua.edu.cn/simple/），否则会很慢： 1root@deeplearning:~# pip install apache-flink==1.14.2 -i https://pypi.tuna.tsinghua.edu.cn/simple/ 1.3 其他环境 集群（每个节点）的JAVA_HOME要求1.8或者1.11。 第二部分 原理解析2.1 运行原理Flink项目组并没有使用Python语言重新实现Flink引擎，而是基于最小化设计原则（以最小的成本实现既定目标），在Flink Java核心外面套上一层Python API，重用现有的Java核心引擎。整个设计实现类似PySpark。 从Flink版本演进上看，主要提供能力有： Flink 1.8.x，开始对Python支持； 存在的问题：支持Datase/Stream 两个独立实现的API，底层使用JPython实现。 Flink 1.9.x，对Table的支持； Flink 1.10.x，增加Python UDFs的支持，在 Table API/SQL 中注册并使用自定义函数； Flink 1.11.x，Pandas UDF 和用户自定义的 Metrics； 2.1.1 通讯选型Flink赋能Python最核心的问题是：Java（Flink为Java研发）和Python进程如何实现通讯。在Flink 1.8.x版本的时候使用Jython来实现，性能上比cpython要快。但是机器学习等生态包支持上，远不如cpython。所以从生态融合上看选择Cpython才是正确的方向。 Java和Python通信有两种解法。 第一种，第三方实现，即实现一个统一的大数据处理管道（pipline），并支持多语言开发，通信由管道平台统一处理。这就是Google开源的Apache Beam（如下图）。 通常流程是：用户使用Apache Beam Python SDK编写数据处理管道，选择runner为FlinkRunner，最后将代码提交至Flink集群运行。 第二种，由Flink本身在不改变源Java内核的前提下，外层套上一层薄薄的API，然后Python进程通过和这层API通信实现和Java虚拟机内部的通信。这就是大名鼎鼎的Py4J，即Py4J 作为 Java VM 和 Python 解释器之间通讯的桥梁（如下图）。 第一种解法需要依赖Apache Beam SDK能力和生态，为了通用性，必然要牺牲灵活性，参考Beam的外部I/O的支持清单。 最终Flink选择了第二种解法。其实熟悉Spark和PySpark项目的同学应该知道Spark赋能Python也同样选择Py4J的架构（经历过线上实践考验的）。 Py4J库分为Java和Python两部分，基本原理是： Java侧，通过py4j.GatewayServer，启动一个GatewayServer，监听一个tcp socket（记做server_socket），用于接收Python侧的请求。 Python侧，启动一个Geteway。通过Socket访问JVM中对象或者调用方法。 Python侧在创建JavaGateway对象时，可以选择同时创建一个CallbackServer，它会在Python侧监听一个tcp socket（记做callback_socket），用来给Java回调Python代码提供一条渠道。 Python 这边创建一个 table 对象的时候，它也会在相应的 Java 这边创建一个相同 table 对象。如果创建一个 TableEnvironment 对象，在 Java 部分也会创建一个 TableEnvironment 对象。调用 table 对象上的方法，那么也会映射到 Java 这边，所以是一个一一映射的关系。 Py4J提供了一套文本协议用来在tcp socket间传递命令。 基于这样的设计，如果用户使用 Python Table API 写出了一个作业（没有 Python UDF 的时候），那么这个作业的性能和用 Java 写出来的作业性能是一样的。因为底层的架构都是同一套 Java 的架构。但是需要考虑socket通信的消耗。 2.1.2 PyFlink UDFhttps://www.modb.pro/db/128622 https://developer.aliyun.com/article/738962 2.2 提交任务参数清单： -py,--python Python script with the program entry. The dependent resources can be configured with the --pyFiles option. 说明：Python程序脚本作为入口程序。例如：-py /home/flink-1.14.2/examples/python/table/word_count.py，指定入口程序脚本本地路径。 -pym,--pyModule Python module with the program entry point. This option must be used in conjunction with --pyFiles. 说明：这个参数和--pyFiles参数结合使用。例如这个参数案例：-pym table_api_demo -pyfs file:///path/to/table_api_demo.py；其中pyfs参数指定了入口文件的路径（也可分布式文件系统），pym指定了入口程序脚本名（使用pyfs参数中文件）。 -pyfs,--pyFiles Attach custom files for job. The standard resource file suffixes such as .py/.egg/.zip/.whl or directory are all supported. These files will be added to the PYTHONPATH of both the local client and the remote python UDF worker. Files suffixed with .zip will be extracted and added to PYTHONPATH. Comma (‘,’) could be used as the separator to specify multiple files (e.g., –pyFiles file:///tmp/myresource.zip,hdfs:///$namenode_address/myresource2.zip). 说明：该参数主要用于上传运行依赖的客户化脚本文件。文件类型支持：.py/.egg/.zip/.whl，甚至目录（没有验证过）。这些文件最后会追加在客户端和远端的Python检索路径中（PYTHONPATH）中。多个文件（目录）需要使用英文逗号间隔。对于zip类文件，解压后，路径追加Python检索路径。 -pyarch,--pyArchives Add python archive files for job. The archive files will be extracted to the working directory of python UDF worker. For each archive file, a target directory be specified. If the target directory name is specified, the archive file will be extracted to a directory with the specified name. Otherwise, the archive file will be extracted to a directory with the same name of the archive file. The files uploaded via this option are accessible via relative path. ‘#’ could be used as the separator of the archive file path and the target directory name. Comma (‘,’) could be used as the separator to specify multiple archive files. This option can be used to upload the virtual environment, the data files used in Python UDF (e.g., –pyArchives file:///tmp/py37.zip,file:///tmp/data.zip#data –pyExecutable py37.zip/py37/bin/python). The data files could be accessed in Python UDF, e.g.: f = open(‘data/data.txt’, ‘r’). 说明：参数指定需要上传的压缩包文件（通常是Python SDK编译环境文件系统，也可以数据类的依赖包）。上传后压缩文件会在目的节点侧进行文件解压。解压的目标文件目录名可以通过#符号来指定（例如：file:///tmp/data.zip#data，解压后目录名即为data，如果压缩包中有文件data.txt，那么python程序就可用通过下面的路径进行访问：f = open(‘data/data.txt’, ‘r’)）。多个压缩文件使用英文逗号间隔。 -pyclientexec,--pyClientExecutable The path of the Python interpreter used to launch the Python process when submitting the Python jobs via \”flink run\” or compiling the Java/Scala jobs containing Python UDFs. (e.g., –pyArchives file:///tmp/py37.zip –pyClientExecutable py37.zip/py37/python) 说明：参数指定了flink 客户端侧python的编译环境。例如：参数–pyArchives file:///tmp/py37.zip 指定了python的SDK包，解压后路径为：py37.zip/py37/python，所以通过这个参数可以指定客户端侧的python解析环境路径：–pyClientExecutable py37.zip/py37/python -pyexec,--pyExecutable Specify the path of the python interpreter used to execute the python UDF worker (e.g.: –pyExecutable /usr/local/bin/python3). The python UDF worker depends on Python 3.6+, Apache Beam (version == 2.27.0), Pip (version &gt;= 7.1.0) and SetupTools (version &gt;= 37.0.0). Please ensure that the specified environment meets the above requirements. 说明：类似-pyclientexec，-pyexec参数用来指定执行器的python编译环境，不再赘述。需要注意的是执行器侧python环境需要依赖包版本。 -pyreq,--pyRequirements Specify the requirements.txt file which defines the third-party dependencies. These dependencies will be installed and added to the PYTHONPATH of the python UDF worker. A directory which contains the installation packages of these dependencies could be specified optionally. Use ‘#’ as the separator if the optional parameter exists (e.g., –pyRequirements file:///tmp/requirements.txt#file:///tmp/cached_dir). 说明：参数用来指定第三方依赖包。 第三部分 案例运行3.1 Hello World案例运行我们首先运行一个Flink项目自带的简单案例：Hello World。下面是提交命令： 1234567# run.sh 脚本内容(其中路径自行调整)export HADOOP_CLASSPATH=`hadoop classpath`/home/flink-1.14.2/bin/flink run -m yarn-cluster \-pyarch /home/pyflink/python3.7.6.zip \-pyclientexec python3.7.6.zip/anaconda3/bin/python \-pyexec python3.7.6.zip/anaconda3/bin/python3 \-py /home/flink-1.14.2/examples/python/table/word_count.py 这样Yarn上就会运行一个Flink任务。 注：Pyflink其中依赖包pyarrow包需要GLIBC 2.14，所以需要注意集群节点操作系统的版本需要大于这个最低要求。 3.2 在线机器学习3.2.1 背景第四部分 线上架构设计https://enjoyment.cool/2019/12/05/Apache-Flink-%E8%AF%B4%E9%81%93%E7%B3%BB%E5%88%97-%E5%A6%82%E4%BD%95%E5%9C%A8PyFlink-1-10%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89Python-UDF/ https://github.com/uncleguanghui/pyflink_learn/blob/master/examples/README.md https://flink-learning.org.cn/article/detail/65bcdf72a377d468b5436c3e76a63437?spm=a2csy.flink.0.0.10473bdcqCaEq6 https://cloud.tencent.com/developer/article/1651257 https://zhuanlan.zhihu.com/p/114717285 参考文献及资料1、PyFlink项目文档，链接：https://nightlies.apache.org/flink/flink-docs-release-1.14/api/python/ 2、The Flink Ecosystem: A Quick Start to PyFlink，链接：https://alibaba-cloud.medium.com/the-flink-ecosystem-a-quick-start-to-pyflink-6ad09560bf50 3、讨论，链接：https://docs.google.com/document/d/1ybYt-0xWRMa1Yf5VsuqGRtOfJBz4p74ZmDxZYg3j_h8/edit#heading=h.p6h40rdmqvbx]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列-贝叶斯深度学习综述]]></title>
    <url>%2F2021%2F11%2F16%2F2022-03-12-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景第一部分 概率和频率第二部分 贝叶斯公式2.1第三部分 贝叶斯深度学习&amp;贝叶斯网络第四部分 贝叶斯深度学习第五部分 工程实现贝叶斯深度学习框架珠算 BoTorch (Bayesian Optimization in PyTorch) 珠算 https://github.com/thu-ml/zhusuan Edward TensorFlow Probability 参考文献及资料Eric J. Ma - An Attempt At Demystifying Bayesian Deep LearningDeep Bayesian Neural Networks. – Stefano CosentinoEdward – A library for probabilistic modeling, inference, and criticism.Deep Learning Is Not Good Enough, We Need Bayesian Deep Learning for Safe AI贝叶斯网络 – 百度百科]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark内存管理和优化]]></title>
    <url>%2F2021%2F11%2F16%2F2021-12-24-Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%92%8C%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark内存优化 第三部分 总结 参考文献及资料 背景当前企业各类业务应用场景，对于大数据处理的速度追求是越来越”卷”了，特别是互联网行业。回头看，MapReduce这种基于磁盘的大数据计算框架早已过时。一方面，内存IO的速度和磁盘不是一个级别，另外内存硬件的成本逐渐降低。所以基于内存的计算框架逐渐成为主流。Spark计算框架就是在这个背景下出现（其实本质都是实现Google关于大数据的三篇论文）。虽然现在Flink已然形成流式霸权，但大部分企业（传统行业）中，Spark仍然是大数据生产环境上，最主要的分布式计算引擎。 但是在开发和维护Spark任务的时候经常出现内存资源问题。本文将详细介绍Spark的内存管理原理和各个版本的演进。内容主要基于Spark 2.3.0版本。需要读者具备Java JVM基础、Spark RDD、Shuffle等Spark基础知识。前面主要将理论和原理，后面介绍案例。 本文图片使用visio画图，需要原文件的同学可以私信我提供。限于经验等各方面原因，难免有所疏漏或者有失偏颇。如有问题，欢迎联系一起讨论。 第一部分 Spark 内存管理详解1.1 Spark任务运行基础Spark在运行时，会启动Executor和Driver两种JVM进程。其中Driver为管理容器，负责创建Spark Context、提交Spark作业（job），最后将作业转换成计算任务（Task），并协调、调度、下发给任务集群的Executor容器。Executor容器负责具体的计算任务，执行完成后将结果返回给Driver，并提供对RDD的存储服务。参考下图（对于Spark on Yarn运行模式时，Cluster Manager对应为ResourceManager，Worker Node 对应NodeManager）。 Spark2.3.0版本前，Driver进程的内存管理即为典型JVM内存管理。主要区别是对于Executor，Spark内存管理引入了堆外内存的概念。 Spark 2.3.0版本开始，Driver进程也引入了堆外内存的概念，如下图。 我们以Spark on Yarn运行模式来描述Execution的内存管理。当我们将任务提交给Yarn集群，Yarn集群首先分配和AM（Application Master）容器（即Driver）资源，然后向RM（Resource Manager）申请Executior容器资源。RM处理内存资源请求并分配Executior容器。 1.2 Spark内存管理演进Spark目前版本演进中支持两种内存管理模式：静态内存管理器（Static Memory Manager）和统一内存管理器（Unified Memory Manager），主要版本演进如下： Spark 1.0+版本，静态内存管理器; Spark 1.6 版本，Executor容器引入堆外（off-Head）内存机制； Spark 1.6+ 版本，引入统一内存管理器； Spark 2.3+版本，Driver容器也引入堆外内存机制（Spark on Yarn 和Spark on K8s）； 注：在官网Spark2.3.0版本特性说明上没找到，但查证为该版本新增特性（目前大量相关Spark的中文介绍材料还没有更新，即Driver没有堆外内存。建议大家可以留意一下官网）； Spark 3.0+ 版本，去除对静态内存管理器的支持； 从Spark 源码中看：org/apache/spark/memory，通过MemoryManager接口实现管理Storage 内存和 Execution 内存、同一个Executor中的任务调用接口申请或释放内存。而MemoryManager有两个实现 StaticMemoryManager和UnifiedMemoryManager，即分别是静态内存管理器（Static Memory Manager）和统一内存管理器（Unified Memory Manager）。 Spark 1.6+ 版本中，可以通过参数spark.memory.useLegacyMode配置使用哪种模式，默认开启Unified Memory Manager。 12# 下面的配置开启静态内存管理器（Static Memory Manager），默认值为falsespark.memory.useLegacyMode = true 后文，将详细介绍两种资源分配的方式。 1.2.1 静态管理模式所谓静态管理（Static Memory Manager）模式，即存储内存（Storage Memory）、执行内存（Execution Memory）以及其他内存（Other Memory）资源的大小，在Spark应用程序运行期间是固定值。用户需要在任务运行前进行配置，任务一旦运行中将无法动态调整。 1.2.1.1 堆内内存下图是静态内存管理模式下的内存资源划分： 预留内存（Reserved Memory）： 其中Storage内存和Execution内存均有预留区域，用于防止OOM的风险。 Storage内存（存储内存）： 计算公式（其中 systemMaxMemory取决于当前 JVM 堆内内存的大小）： 12可用的Storage内存 = systemMaxMemory*spark.storage.memoryFraction*spark.storage.safetyFraction# spark.storage.memoryFraction 默认为0.6 用于存储 RDD的缓存（cache）数据 和 广播（Broadcast）。这部分存储对象更多是为将来计算重用的数据。 Execution内存（执行内存）： 计算公式： 12可用的Execution内存 = systemMaxMemory*spark.shuffle.memoryFraction*spark.shuffle.safetyFraction# spark.shuffle.memoryFraction 默认为0.2 用于执行Shuffle时占用的内存，主要用于存放 Shuffles、Joins、Sort、aggregations等计算过程中的临时数据。待完成操作后，就会释放资源，寿命较短。 其他内存（Otrher Memory）： Spark内部元数据和用户自定义数据类型。 1.2.1.2 堆外内存Spark任务本质还是运行在JVM虚机上的Java进程，所以Executor的内存管理仍然是基于JVM的内存管理。而堆内存受到 JVM统一管理，GC基于一定算法逻辑，当spark任务需要申请和释放内存的时候，并不自由灵活。具体流程如下： 申请内存： Spark在代码中new一个对象实例; JVM从堆内内存分配空间，创建对象并返回对象引用; Spark保存该对象的引用，记录该对象占用的内存; 释放内存： Spark记录该对象释放的内存，删除该对象的引用; 等待JVM的垃圾回收机制释放该对象占用的堆内内存; 所以，对于堆内内存的申请和释放实际是由 JVM 来管理的。因此，在统计堆内内存具体使用量时，考虑性能等各方面原因，Spark 目前采用的是抽样统计的方式来计算已使用的内存。Spark不能准确记录实际可用的堆内内存，当数据量较大时，如果不能及时溢出(Spill)数据到磁盘，也就无法避免内存溢出OOM。 所以Spark 从1.6 版本开始引入了Off-heap memory(SPARK-11389)。该内存资源不属于JVM内存，而是调用 Java 的 unsafe 相关 API 直接向操作系统申请内存，直接在服务器节点的内存中开辟空间，存储经过序列化的二进制数据。由于这种方式不需要 JVM 内存管理，所以可以避免频繁的 GC，但是缺点是用户必须自行编写内存申请和释放的逻辑。 默认情况下，堆外内存是禁用的。可以通过参数spark.memory.offHeap.enabled 参数启用它 ，并通过spark.memory.offHeap.size 参数设置内存大小，单位为字节 。 与堆内内存相比，堆外内存模型比较简单，只有Storage memory和Execution memory，其分布如下图所示： 1.2.1.2 总结Static Memory Manager机制实现起来比较简单，但是使用中需要用户熟悉Spark的存储机制，并具有丰富的资源预设评估能力。否则很容易导致Storage memory和Execution memory空间资源使用冰火两重天，饿的饿死，饱的饱死。 于是Spark社区考虑引入新的内存管理模式，参考提案文件：Unified Memory Manager提案。 1.2.2 统一管理模式在统一管理模式（Unified Memory Manager ）机制下，Storage memory 和Execution memory 共享一个内存区域，两者可以相互占用空闲区域，不再有严格的资源限制边界。 1.2.2.1 堆内(On-Heap)内存 注：上图显示spark.memory.fraction=0.75 。根据SPARK-15796，从 Spark 2.0 版本开始减少到 0.6。 主要变化是预留内存： Reserved预留内存 这部分内存主要用户Spark内部对象存储。大小不可配置，写死在代码中： 12// 代码文件：org.apache.spark.memory.UnifiedMemoryManagerprivate val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024 注：对于测试环境，可以通过参数调小该资源。参数值为：spark.testing.reservedMemory。 堆内内存的大小为spark任务提交是参数executor-memory或配置参数spark.executor.memory决定。 1.2.2.2 堆外(Off-Heap)内存1.2.2.2.1 Exector堆外内存 对于Spark on Yarn（Spark 2.3.0）模式，通过下面的参数修改任务的对外内存大小： 123456spark.memory.enable.offheap.enable = true# 默认开启spark.yarn.executor.memoryOverhead=1024# 单位M# 默认值为executorMemory * 0.10, with minimum of 384m 例如下面的启动日志，默认申请了384MB的对外内存： 1YarnAllocator:54 - Will request 2 executor container(s), each with 1 core(s) and 1408 MB memory (including 384 MB of overhead) 1.2.2.2.2 Driver堆外内存Spark 2.3.0版本对于Driver也引入了堆外内存的机制。堆外内存的大小为：max(driverMemory * 0.10,384MB)。主要注意的是只支持对于Spark任务基于Yarn和K8s集群调度运行的场景。 官网参数说明如下： spark.driver.memoryOverhead driverMemory * 0.10, with minimum of 384 The amount of off-heap memory to be allocated per driver in cluster mode, in MiB unless otherwise specified. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the container size (typically 6-10%). This option is currently supported on YARN and Kubernetes. 可以在任务提交日志中看下面下面的日志： 1234# driverMemory = 1G, max(driverMemory * 0.10,384MB) = 384MBClient:54 - Will allocate AM container, with 1408 MB memory including 384 MB overhead# driverMemory = 4G, max(driverMemory * 0.10,384MB) = 409MBClient:54 - Will allocate AM container, with 4505 MB memory including 409 MB overhead 1.2.2.3 计算案例我们在测试集群上运行一个Spark任务，并计算和验证上面的理论，Yarn集群资源限制： 参数yarn.scheduler.maximum-allocation-mb限制单个容器最大分配内存大小，高于该值请求无法生效。 同样对于单个Node Manager也有资源限制，由参数yarn.nodemanager.resource.memory-mb控制。即单个Node Manager可提供给Yarn集群的物理内存资源。 1234# nodemanager能够申请的最大内存，默认值为30Gyarn.nodemanager.resource.memory-mb: 30G# 调度时一个container能够申请的最大资源，默认值为4Gyarn.scheduler.maximum-allocation-mb: 4G 对于统一内存管理模式，运行脚本如下： 123456789101112[root@quickstart spark-2.3.0]# cat run_example.sh export YARN_CONF_DIR=/etc/hadoop/conf/home/spark-2.3.0/bin/spark-submit \--conf "spark.executorEnv.JAVA_HOME=/home/openjdk" \--conf "spark.yarn.appMasterEnv.JAVA_HOME=/home/openjdk" \--master yarn \--deploy-mode cluster \--num-executors 2 \--driver-memory 1G \--executor-memory 1G \--class org.apache.spark.examples.SparkPi \/home/spark-2.3.0/examples/jars/spark-examples_2.11-2.3.0.jar 任务运行后我们查看Spark UI，发现Storage Memory的大小为384.1MB。 接下来我们使用之前理论进行计算： 12345678910111213141516171819202122232425# Spark任务提交参数spark.executor.memory=1gspark.memory.fraction=0.6spark.memory.storageFraction=0.5# 堆内存Java Heap Memory = 1 GB = 1 * 1024 MB = 1024 MB# 保留内存Reserved Memory = 300 MB# Usable Memory = (Java Heap Memory — Reserved Memory)= 1024 MB - 300 MB = 724 MB#Other Memory = Usable Memory * (1.0 — spark.memory.fraction) = 724 MB * (1.0 - 0.6) = 724 MB * 0.4 = 289.6 MBSpark Memory = Usable Memory * spark.memory.fraction = 724 MB * 0.6 = 434.4 MBSpark Storage Memory = Spark Memory * spark.memory.storageFraction = 434.4 MB * 0.5 = 217.2 MBSpark Execution Memory = Spark Memory * (1.0 - spark.memory.storageFraction) = 434.4 MB * ( 1 - 0.5) = 434.4 MB 这个计算结果（Spark Storage Memory=217.2 MB）和UI显示的Storage Memory（384.1MB）有很大差距的。事实上UI中的Storage Memory = Spark Storage Memory+Spark Execution Memory，即434.4 MB。 但是这个结果和UI仍然有差异。我们查看Spark的源码发现可用内存（Usable Memory）的计算方法并不是堆栈内存，而是Runtime.getRuntime.maxMemory。 1234567891011//org.apache.spark.memory.UnifiedMemoryManagerprivate def getMaxMemory(conf: SparkConf): Long = &#123; val systemMemory = conf.getLong("spark.testing.memory", Runtime.getRuntime.maxMemory) val reservedMemory = conf.getLong("spark.testing.reservedMemory", if (conf.contains("spark.testing")) 0 else RESERVED_SYSTEM_MEMORY_BYTES) val usableMemory = systemMemory - reservedMemory val memoryFraction = conf.getDouble("spark.memory.fraction", 0.6) //获取最大的内存值 (usableMemory * memoryFraction).toLong&#125; 在Java 虚拟机中（JDK8）堆栈内存被划分为新生代（NewGen）和老年代（OldGen ），而新生代又被划分为：Eden、From Survivor、To Survivor。即有下面公式： 12ExecutorMemory = Eden + 2 * Survivor + OldGen = 1GsystemMemory = Runtime.getRuntime.maxMemory=ExecutorMemory - Survivor 所以UI显示的大小会略小于上面计算的结果，属于正常。 1.2.3 动态占用模式统一内存管理模式最大新颖点，就是动态占用机制的引入。在实际生产线上Spark任务运行时，数据计算是动态变化的，无法在任务运行前合理的分配好资源，动态占用模式正是解决该需求场景的。 动态占用机制的规则如下： Spark任序提交时，根据spark.memory.storageFraction 参数设置Storage内存和Execution内存 （初始化）。 运行时，如果Storage内存和Execution内存的空间均不够（标准是：存储空间不够放下一个完整的块（Block）），会根据LRU缓存策略，将数据存储到磁盘。若一方空间不足，而对方空间空余时，可借用对方的空间。 Storage占用对方的内存，会将占用的部分转移到硬盘上，然后“归还”借来的空间。 Execution占用了对方的内存时，却无法立刻“归还”借来的空间，只能等待释放。由于Shuffle过程生成的数据（本质是文件）会在后面使用，而Cache中的数据不一定会在后面使用，因此回收内存可能会导致性能严重下降。 1.2.3 对比最后表格对比一下堆内堆外内存，Auto的自动但管理不精细。 内存类别 管理方式 对比 on-heap 由JVM管理 受到JVM GC管理，容易OOM off-heap 手动管（spark） 用户编写内存申请和释放的逻辑 第二部分 Spark内存优化讲完枯燥的理论后，我们要使用这些理论指导日常的研发调优、生产问题的分析定位。即理论指导实践，解决问题才是目的。 通常我们说Spark是基于内存计算的，但是并不是说所有的数据对象都是缓存在内存中。如果数据量较大而内存资源不足的场景下，Spark也会把数据缓存在磁盘中。但是为了提升任务运行效率，需要尽量避免缓存数据溢出（Spill）到磁盘。 那么如何判断Spark任务的内存资源管理是合适的？通过上文的理论介绍，Spark任务的内存资源主要使用分配有：存储（Storage）内存和执行（Executor）内存，所以我们分别讨论。通常通过Spark UI界面，来查看Spark任务资源实时使用情况。 2.1 存储（Storage）内存优化Storage子菜单界面中，如果发现缓存cache中数据开始溢出到磁盘上(Size on Disk)。这时候说明预留给 Storage内存资源不足。例如下图，Size on Disk数值为4.7GB，表示任务中有4.7GB的数据因为资源不足溢出到磁盘进行缓存。 这时候可以尝试先通过适当增加Storage内存资源解决。涉及参数有： spark.memory.fraction，这是资源比例参数（存储（Storage）内存+执行（Executor）内存资源整体），默认是0.6。可以适当调大该值，这样 Execution 和 Storage 的整体可用内存资源变大。 spark.memory.storageFraction，同样是资源比例参数（Storage 占 Storage+Executor 内存总和的比例）。我们知道在统一内存管理机制下，Storage 和 Executor 之间内存资源可以动态伸缩借用。但是spark.memory.storageFraction越大，任务运行过程中，Storage 能用的内存就会越多。可以适当调大该值。反之，如果任务更吃 Executor 内存，适当把这个值调小。 2.2 执行（Executor）内存Stages子菜单界面中，点击最新完成的stage，页面显示stage的运行情况汇总。发现shuffle spill (disk)的数值较大，Executor内存资源不足。例如下图，任务中shuffle的时候，2个Executor平均使用300M的磁盘空间。 这时候通过调整参数，适当增加Executor内存资源解决。 2.3 其他内存问题2.3.1 Driver内存报错： 1Job aborted due to stage failure: Total size of serialized results of 334502 tasks (1024.0 MB) is bigger than spark.driver.maxResultSize (1024.0 MB) 处理： 对于collect和一些操作，driver会接收各task执行后的数据，spark.driver.maxResultSize参数控制接收数据大小，建议先检查代码，避免或减少take，collect操作，如果不成功再考虑增大该参数。尽量不要使用collect操作即可。 第三部分 总结Spark引入堆外内存，其实是Spark钨丝计划（Spark Tungsten）的一部分。该计划的细节可以参考databricks公司的官网博客：Project Tungsten: Bringing Apache Spark Closer to Bare Metal 。文章总结了目前Spark的瓶颈主要在CPU和内存。以前磁盘和网络I/O随着高速网络和廉价SSD的使用，已经不再是性能瓶颈。 钨丝计划（Project Tungsten）包含三个方面： 内存管理（Memory Management）和二进制处理（Binary Processing）：利用应用的语义（application semantics）来更明确地管理内存，同时消除JVM对象模型和垃圾回收开销。 缓存友好的计算（Cache-aware Computation）：使用算法和数据结构来实现内存分级结构（MemoryHierarchy）。 代码生成（Code Generation，CG）：使用代码生成来利用新型编译器和CPU。 其中本文介绍Spark内存优化就是钨丝计划的内存管理部分。 计划安排： Project Tungsten (Spark 1.5 Phase 1)，链接：https://issues.apache.org/jira/browse/SPARK-7075 Project Tungsten (Phase 2)，链接：https://issues.apache.org/jira/browse/SPARK-9697 参考文献及资料1、《Deep Dive: Apache Spark Memory Management》介绍视频，链接：https://youtu.be/dPHrykZL8Cg 2、探索Spark Tungsten的秘密，链接：https://github.com/hustnn/TungstenSecret]]></content>
      <categories>
        <category>Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Yarn高可用机制]]></title>
    <url>%2F2021%2F11%2F16%2F2022-01-18-Hadoop%20Yarn%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景ResourceManager High Availability 第一部分1234567&lt;property&gt; &lt;description&gt;When HA is enabled, the class to be used by Clients, AMs and NMs to failover to the Active RM. It should extend org.apache.hadoop.yarn.client.RMFailoverProxyProvider&lt;/description&gt; &lt;name&gt;yarn.client.failover-proxy-provider&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; 1234&lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt; https://jxy.me/2015/04/09/hadoop-ha-active-nn/ 参考文献及资料1、RuoYi-Cloud项目文档，链接：https://blog.csdn.net/ifenggege/article/details/107968518]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark钨丝计划（Spark Tungsten）]]></title>
    <url>%2F2021%2F11%2F16%2F2022-03-05-Spark%E9%92%A8%E4%B8%9D%E8%AE%A1%E5%88%92%EF%BC%88Spark%20Tungsten%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景第三部分 钨丝计划（Spark Tungsten）第五部分 总结参考文献及资料1、《Deep Dive: Apache Spark Memory Management》介绍视频，链接：https://youtu.be/dPHrykZL8Cg]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark对外内存实践]]></title>
    <url>%2F2021%2F11%2F16%2F2022-01-15-Spark%E5%AF%B9%E5%A4%96%E5%86%85%E5%AD%98%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景第一部分https://www.waitingforcode.com/apache-spark/apache-spark-off-heap-memory/read 参考文献及资料1、RuoYi-Cloud项目文档，链接：https://blog.csdn.net/ifenggege/article/details/107968518]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark分布式存储系统BlockManager总结]]></title>
    <url>%2F2021%2F11%2F16%2F2022-01-03-Spark%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9FBlockManager%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景BlockManager 是一个嵌入在 spark 中的 key-value型分布式存储系统，是为 spark 量身打造的。 BlockManager 在一个 spark 应用中作为一个本地缓存运行在所有的节点上， 包括所有 driver 和 executor上。BlockManager 对本地和远程提供一致的 get 和set 数据块接口，BlockManager 本身使用不同的存储方式来存储这些数据， 包括 memory, disk, off-heap。 Spark中RDD是一个逻辑概念，实际的数据是通过BlockManager组件实现物理存储的。其中RDD是一个分布式对象，即分区的概念。其中每个分区（partition）在BlockManager中对应一个Block对象。 对于RDD的存取是以block为单位进行的，本质上partition和block是等价的，只是看待的角度不同。在Spark storage模块中中存取数据的最小单位是block，所有的操作都是以block为单位进行的。 http://spark.coolplayer.net/?p=209 参考文献及资料1、RuoYi-Cloud项目文档，链接：https://blog.csdn.net/ifenggege/article/details/107968518]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark中类型推断]]></title>
    <url>%2F2021%2F11%2F16%2F2022-01-13-Spark%E4%B8%AD%E7%B1%BB%E5%9E%8B%E6%8E%A8%E6%96%AD%20-%20%E5%89%AF%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景请注意，会自动推断分区列的数据类型。目前，支持数字数据类型和字符串类型。有些用户可能不想自动推断分区列的数据类型。对于这些用例，自动类型推断可以由spark.sql.sources.partitionColumnTypeInference.enabled配置，默认值为true。当禁用类型推断时，字符串类型将用于分区列。 第一部分https://www.cnblogs.com/yurunmiao/p/4934053.html 参考文献及资料1、RuoYi-Cloud项目文档，链接：https://blog.csdn.net/ifenggege/article/details/107968518]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark中Shuffle详解]]></title>
    <url>%2F2021%2F11%2F16%2F2022-01-03-Spark%E4%B8%ADShuffle%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景Shuffle英文中含义是：“洗牌”，这个含义和分布式大数据计算框架有什么联系呢？我们先从大数据计算的”Hello World”案例讲解。 给定一个英文文本文件（存储在分布式文件系统中），然后计算每个单词的词频。这是我们运行大数据计算框架最常用的案例。通常我们将每个单词作为键（key），出现的频率为1，这样就有一个键值对（例如：{“the”:1}）。最后我们将相同键值进行合并统计即可。但是在分布式处理场景下，如何统计分布在不同节点的键值对呢？唯一的办法就是需要把相同的键值对收集到同一个节点进行汇总计算求和。 这个计算过程中（参考下图），收集各个节点键值对，并分发到同个节点汇总的过程，就是Shuffle。相当于对数据进行从“洗牌”。 注：相同的键值会被分发到相同的reduce计算节点。 在分布式计算框架中，Shuffle的过程代价比较大，因为数据的重分配，意味着数据在磁盘、内存中的I/O以及节点通信中的网络I/O。所以Shuffle阶段的设计优劣是决定一个分布式计算框架优劣的关键因素。 正是由于Shuffle的计算几乎需要消耗所有类型的硬件资源，比如CPU、内存、磁盘与网络，在绝大多数的Spark作业中，Shuffle往往是作业执行性能的瓶颈。 第一部分 Shuffle概述1.1 MapReduce shuffle上文我们案例其实是MapReduce计算框架。在MapReduce计算框架中Shuffle阶段是链接Map和Reduce之间的桥梁。Spark是基于MapReduce思想实现的计算框架，同样也存在shuffle流程。 在正式介绍Spark shuffle前，我们先引用介绍MapReduce shuffle过程来作为入门和比较。下图简要展示了MapReduce框架中Shuffle流程（图中黑框部分）。 MapReduce中，根据处理特点，将Shuffle分为两个子阶段：Map端和Reduce端。 Map端 环形内存缓存区：每个split数据交由一个map任务处理，map的处理结果不会直接写到硬盘上，会先输送到环形内存缓存区中，默认的大小是100M（可通过配置修改），当缓冲区的内容达到80%后会开始溢出，此时缓存区的溢出内容会被写到磁盘上，形成一个个spill file，注意这个文件没有固定大小。 在内存中经过分区、排序后溢出到磁盘：分区主要功能是用来指定 map 的输出结果交给哪个 reduce 任务处理，默认是通过 map 输出结果的 key 值取hashcode 对代码中配置的 redue task数量取模运算，值一样的分到一个区，也就是一个 reduce 任务对应一个分区的数据。这样做的好处就是可以避免有的 reduce 任务分配到大量的数据，而有的 reduce 任务只分配到少量甚至没有数据，平均 reduce 的处理能力。并且在每一个分区（partition）中，都会有一个 sort by key 排序，如果此时设置了 Combiner，将排序后的结果进行 Combine 操作，相当于 map 阶段的本地 reduce，这样做的目的是让尽可能少的数据写入到磁盘。 合并溢出文件：随着 map 任务的执行，不断溢出文件，直到输出最后一个记录，可能会产生大量的溢出文件，这时需要对这些大量的溢出文件进行合并，在合并文件的过程中会不断的进行排序跟 Combine 操作，这样做有两个好处：减少每次写入磁盘的数据量&amp;减少下一步 reduce 阶段网络传输的数据量。最后合并成了一个分区且排序的大文件，此时可以再进行配置压缩处理，可以减少不同节点间的网络传输量。合并完成后着手将数据拷贝给相对应的reduce 处理，那么要怎么找到分区数据对应的那个 reduce 任务呢？简单来说就是 JobTracker 中保存了整个集群中的宏观信息，只要 reduce 任务向 JobTracker 获取对应的 map 输出位置就可以了。具体请参考上方的MapReduce工作原理。 Reduce端 reduce 会接收到不同 map 任务传来的有序数据，如果 reduce 接收到的数据较小，则会存在内存缓冲区中，直到数据量达到该缓存区的一定比例时对数据进行合并后溢写到磁盘上。随着溢写的文件越来越多，后台的线程会将他们合并成一个更大的有序的文件，可以为后面合并节省时间。这其实跟 map端的操作一样，都是反复的进行排序、合并，这也是 Hadoop 的灵魂所在，但是如果在 map 已经压缩过，在合并排序之前要先进行解压缩。合并的过程会产生很多中间文件，但是最后一个合并的结果是不需要写到磁盘上，而是可以直接输入到 reduce 函数中计算，每个 reduce 对应一个输出结果文件。 Spark根据RDD的宽依赖划分stage，stage中又包含了task。每个stage中的task依赖上游stage中task的输出，上游task落盘称为shuffle写，下游task读称为shuffle读，上游task相当于MR的map阶段，下游task相当于MR的reduce阶段。不同stage间task的读写构成了spark的shuffle流程。 下游task（reduce端）会去上游task（map端）所在节点读取自己需要的分区数据。整个过程涉及到序列化、磁盘IO等操作。 shuffle write - Data move from Executor(s) to another Executor(s) - is used when data needs to move between executors (e.g. due to JOIN, groupBy, etc) 第二部分 Shuffle的框架2.1 Shuffle框架的演进Spark Shuffle历史节点Spark 0.8及以前 Hash Based ShuffleSpark 0.8.1 为Hash Based Shuffle引入File Consolidation机制Spark 0.9 引入ExternalAppendOnlyMapSpark 1.1 引入Sort Based Shuffle，但默认仍为Hash Based ShuffleSpark 1.2 默认的Shuffle方式改为Sort Based ShuffleSpark 1.4 引入Tungsten-Sort Based ShuffleSpark 1.6 Tungsten-sort并入Sort Based ShuffleSpark 2.0 Hash Based Shuffle退出历史舞台 https://blog.csdn.net/ifenggege/article/details/107968518 2.2 Shuffle框架内核第三部分 基于Hash的Shuffle第四部分 基于Sort的Shuffle第四部分 基于Tungsten的Shufflehttps://tech.meituan.com/2016/05/12/spark-tuning-pro.html https://0x0fff.com/spark-architecture-shuffle/ 第五部分 常见问题 磁盘临时文件空间不足 报错： java.io.IOException: No space left on device 处理： 在shuffle过程中，中间文件都放在/tmp目录，当shuffle文件达到磁盘空间上限，就报错。解决方法可以增大executor个数，分担压力，如果仍不可以的话就联系平台同学配置spark-defaults.conf中设置spark.local.dir（默认是/tmp）为磁盘空间足够的目录即可解决。在yarn模式则配置LOCAL_DIRS。 第六部分 附录Shuffle 中的重要参数： spark.local.dir: Shuffle 缓存目录 spark.shuffle.file.buffer： shuffle write 阶段 buffer 缓冲大小，将数据写到磁盘文件之前，会先写入 buffer 缓冲中，待缓冲写满之后，才会溢写到磁盘。默认值为 32K。 spark.reducer.maxSizeInFlight： shuffle read 阶段 buffer 缓冲区大小。默认值为 48M。 spark.shuffle.io.maxRetries： Shuffle read 阶段拉取数据失败时的最大重试次数。默认值 3。 spark.shuffle.io.retryWait： Shuffle read 阶段拉取数据失败重试时的等待时间。默认值 5s。 spark.shuffle.sort.bypassMergeThreshold： 使用 bypassMergeSortShuffleWriter 机制，RDD 分区数的限制阈值。默认值为 200。 spark.memory.fraction &amp; spark.memory.storageFraction： 调整 Shuffle 相关内存所占的比例 spark.memory.fraction： 缺省值 0.6。存储内存和执行内存占（heap 内存 - 300M）的百分比 spark.memory.storageFraction： 缺省值 0.5 存储内存与 （存储内存与执行内存之和）的百分比 spark.shuffle.manager： 通过反射方式生成的 SortShuffleManager 的实例。默认为 SortShuffleManager。 Spark 1.5 以后，有三个可选项：hash、sort 和 tungsten-sort。 spark.shuffle.consolidateFiles： spark.shuffle.mapOutput.minSizeForBroadcast：默认值 512K spark.shuffle.mapOutput.dispatcher.numThreads: 默认值为 8，map 端输出派发线程池中的线程数 参考文献及资料1、RuoYi-Cloud项目文档，链接：https://blog.csdn.net/ifenggege/article/details/107968518]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python语言温习总结]]></title>
    <url>%2F2021%2F11%2F16%2F2021-12-18-Python%E8%AF%AD%E8%A8%80%E6%B8%A9%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 基础环境准备 第二部分 开发环境项目部署 第三部分 生产部署 第四部分 总结 参考文献及资料 背景第一部分 Python中的类1.1 类的创建1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-class Employee: employeeCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.employeeCount += 1 def displayCount(self): print("Total Employee %d" % Employee.employeeCount) def displayEmployee(self): print("name: ",self.name,"Salary: ",self.salary)if __name__ == '__main__': employee = Employee("xiaogming", 1000) employee.displayCount() employee.displayEmployee() employee1 = Employee("xiaohong", 1000) employee1.displayCount() employee1.displayEmployee() 注： employeeCount 变量是一个类变量，它的值将在这个类的所有实例之间共享。例如上面的例子中，第一次调用值为1，第二次创建类实例的时候输出是2。 def __init__(self, name, salary)，这是类的初始化方法或称为类的构造函数。当创建这个类的实例的时候就会调用该方法。 self 代表类的实例。代表当前类实例和地址。注意这个不是Python的关键字。 Python中实例的创建并不想Java中需要new关键字，类的实例化类似函数调用。 1.2 类的继承继承语法： 12class 派生类名(基类名) ... 123456class Boss(Employee): def __init__(self, age, name, salary): super().__init__(name, salary) self.age = age def displayAge(self): print(self.age) 参考文献及资料1、RuoYi-Cloud项目文档，链接：]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列-什么是概率、随机变量、统计？]]></title>
    <url>%2F2021%2F11%2F16%2F2022-03-13-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-%E4%BB%80%E4%B9%88%E6%98%AF%E6%A6%82%E7%8E%87%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 概率定义形式化到公理化 第二部分 随机变量 第三部分 概率和统计 第四部分 参考文献及资料 背景在日常生活中，人们经常会有各种关于概率的论断。例如：明天下雨的概率是1/4，明天股市上涨的概率是2/3，抛硬币出现正面的概率是1/2，等等。特别是在机器学习中，我们使用概率来度量不确定性。那么什么是概率呢？ 最早概率起源于赌博游戏。在1654年，赌徒梅勒（同时也是一位业余数学家）向当时的“数学神童”帕斯卡提出了他在赌场遇到的几个不解问题。后帕斯卡与费马在日常交流的通信中，对这些问题进行了详细讨论，但二人都不愿意发表研究成果。费马与帕斯卡的通信中关于分赌注问题的讨论被公认为是概率论诞生的标志。这些问题被统一称为赌博问题。例如下面问题之一： 有两个赌徒聚众赌博。约定谁先赢 d 局就算赢了，当赌徒 A 赢 a 局 (a &lt; d )，而赌徒 B 赢 b 局(b &lt; d) 时。这时候突发事件，赌博被中止，问需要怎样分配赌资？ 后来，惠根斯也用自己的方法解答了赌徒问题，并写成了《论赌博中的计算》一书，这是概率论最早的论著。他们的解法中引出了数学期望（mathematical expectation）这一概念，并由此奠定了古典概率的基础。因此可以说早期概率论的真正创立者是帕斯卡、费尔马和惠更斯。 第一部分 概率定义形式化到公理化1.1 古典定义在古典定义中，主要针对试验进行讨论，并且实验满足两个条件： 试验结果是一个有限集合； 每个试验结果的可能性相等； 这就是古典试验。对于古典试验中事件 $A$ ，它的概率定义为：$\mathbb{P}(A)=n/m$ ，其中n为该试验中所有可能出现的基本结果的总数目。m表示事件 $A$ 包含的试验基本结果数。现实中掷骰子就是最典型的案例。 古典定义是一个过于简单的定义模型，无法解释更为复杂的不确定现象。另外定义中出现了关键词”可能性相等”，逻辑上甚至有自己定义自己的逻辑问题。 1.2 频率、统计定义随着经验的积累，人们逐渐认识到，在做大量重复试验时，随着试验次数的增加，一个事件出现的频率，总在一个固定值的附近。用这个固定值用来定义事件的概率，这就是频率定义。 后来有了微积分工具后，提出建立在频率理论基础上的统计概率。即计算一个事件的概率值，唯一的方式是通过对该事件进行成千上万次独立试验，例如第n次事件A发生的相对频率为 $f_n(A)$ ，随着次数的增加，相对频率会逐渐收敛于一个固定值，这个极限值定义为事件的概率值。$$\mathbb{P}(A)=\lim {n \rightarrow \infty} f{n}(A)$$事实上，这个思想就是概率论中第一个极限定理：伯努利大数定理。 但是并不是所有的事件都是可以重复试验的。例如我们评估2022年俄罗斯出兵乌克兰的概率有多大，显然无法通过统计定义进行重复试验然后计算极限，而且生活中有很多这样的例子。所以这并不是一个严谨的定义。 1.3 贝叶斯概率统计定义认为概率是重复试验下频率的极限值。但是很多随机事件是无法重复试验的。这时候就有了贝叶斯派对概率的解释，认为概率是主观对事件发生的信念强度。这种思想本质上将概率主观化，认为概率是主观根据经验和外界数据对事件发生可能性度量。底层是人脑对数据的处理结果。事件的随机性不过是观察者掌握信息不完备所造成的，观察者所掌握的信息多寡将影响观察者对于事件的认知。 例如：2022年上海疫情爆发期间，意味着每个人被感染的概率较大，这就是一个先验判断。如果这个人抗原检测两道杠，那么确认感染的可能性就更大了，这是后验判断。 根据这个思想进而有了著名的贝叶斯公式：$$P(A \mid B)=\frac{P(A) P(B \mid A)}{P(B)}$$其中 $A$ 、$B$ 为随机事件（ $P(B)$ 不为零）。 $P(A \mid B)$ 是指在事件 $B$ 发生的情况下事件 $A$ 发生的概率，即$A$的后验概率（条件概率）。而 $P(A)$ 为先验概率。 贝叶斯的思想类似于人类大脑对应信息的处理过程，不断迭代使用后验概率修正更新先验概率，即贝叶斯推理（Bayesian inference）。所以被广泛应用于统计和机器学习中。 1.4 概率论公理化20世纪初，随着测度论的研究发展，为概率公理体系的建立奠定了基础。1933年苏联数学家柯尔莫哥洛夫（Kolmogorov）在他的《概率论基础》一书中，首次使用测度论将概率的定义以公理化语言进行定义，才将概率论正式纳入数学学科的研究范围，称为严谨的数学分支。 概率定义：设 $\Omega$ 为样本空间，$\mathcal{F}$ 为样本空间 $\Omega$ 上的 $\sigma$ 代数，那么定义在 $\mathcal{F}$ 上的函数 $\mathbb{P}$ 称为概率测度（即概率），如果满足下面的条件： 非负性：对于任何$A \in \mathcal{F}$, $\mathbb{P}(A) \ge 0$ ; 规范性：$\mathbb{P}(\Omega)=0$ ; 可列可加性：对于 $\mathcal{F}$ 中互斥的可列个事件${A_i,i\ge 1}$,有： $$\mathbb{P}\left(\bigcup_{i\ge 1} A_{i}\right)=\sum_{i\ge 1} \mathbb{P}\left(A_{i}\right)$$ 上面定义有个测度论中的重要概念： $\sigma$ 代数。 $\sigma$ 代数：$X$ 为集合， $\mathcal{P}(X)$ 代表 $X$ 的幂集（子集全体），假设有集合 $\mathcal{F} \subseteq \mathcal{P}(X)$ 。若 $\mathcal{F}$ 满足下列条件 $X \in \mathcal{F}$ $A \in \mathcal{F} \Rightarrow A^{c} \in \mathcal{F}$ $A_{n} \in \mathcal{F}, \forall n \in \mathbb{N} \Rightarrow \bigcup_{n=1}^{\infty} A_{n} \in \mathcal{F}$. 则称集合 $\mathcal{F}$ 是 $X$ 的 $\sigma$-代数。在测度论里 $(X, \mathcal{F})$ 称为一个可测空间。集合 $\mathcal{F}$ 中的元素，也就是 $X$ 的某子集，称为可测集合。这里的 $\sigma$ 代数的概念限定主要解决的问题是：不可测。那是因为并不是样本空间的任意子集都是可测的。 定义中将概率定义为特殊的测度函数，没有形式化解释什么是概率对象。而是描述满足公理条件的测度就定义为概率。 第二部分 随机变量有些读者可能对上面的概率定义很陌生。那是因为目前国内高校中概率论课程（甚至部分数学系）中介绍的概率定义并不是这样的。通常是基于随机变量的概念定义的。 随机变量定义：给定样本空间 $\Omega$，其上的实值函数 $X: \Omega \rightarrow \mathbb{R}$ 称 $X$ 为（实值）随机变量。 这不是一个严谨的定义，注意点有： 随机变量是定义在样本空间 $\Omega$ 的 $\sigma$ 代数上的可测函数。大部分概率论书籍不会介绍测度论知识，所以忽略了这个可测条件，后文会讲解原因。 随机变量是一个确定性函数，值并不是随机的（这个名称有一定的误导性）。 我们先看一下例子。连续掷两次硬币（正面记为$H$，反面记为$T$），容易得到样本空间为: $\Omega={ HH, HT, TH, TT }$ 。定义一个样本空间上的函数 $X$ :出现正面的数量，显然 $X$ 是一个随机变量。函数取值表如下： 随机事件 X（出现正面的数量）随机变量值 概率值 ${HH}$ 2 $\mathbb{P}({HH})$ =1/4 ${HT}$ 1 $\mathbb{P}({HT})$ =1/4 ${TH}$ 1 $\mathbb{P}({TH})$ =1/4 ${TT}$ 0 $\mathbb{P}({TT})$ =1/4 那么我们为什么需要引入随机变量这个概念了？还是上面的例子，如果我们要计算出现正面的数量为1的概率。那么首先我们要选出符合条件的随机事件全体集合，然后计算，也就是：$$\mathbb{P}(正面数量为1的事件集合) = \mathbb{P}({HT}\cup{TH})=\mathbb{P}({HT})+\mathbb{P}({TH})=1/4+1/4=1/2$$在数学上，这种列举计算是不方便的。通常数学上我们把这种集合重新编码成数值，以方便后续的计算。所以我们引入了随机变量映射函数，将样本空间和数值对应起来，即信息编码。当然编码方式有多种，尽量结合计算场景定义合适的随机变量。 如果我们引入了随机变量，计算流程变为：$$\mathbb{P}(X\leq1) = \mathbb{P}({HT}\cup{TH})=\mathbb{P}({HT})+\mathbb{P}({TH})=1/4+1/4=1/2$$由于数值映射对应后，我们可以快速找到小于等于1的事件集合。计算推导过程更为数字化，而不是集合形式化。事实上严格的表达应该是（$\mathbb{P}$ 是个集合函数）：$$\mathbb{P}(X\leq1) = \mathbb{P}({x\in \Omega|X(x)\leq1})=\mathbb{P}({x\in \Omega|X(x)=1 \ or\ X(x)=2 })$$其中集合 $ {x\in \Omega|X(x)\leq1}={HT}\cup{TH}$ 。 2.1 离散型随机变量样本空间本质是一个集合，例如上面的案例样本空间集合的势（集合元素多少的度量）是4，这是有限集合。当样本空间是无限集合的时候呢？ 数学上无限有两种情况：可数集合（或可列集合）和不可数集合。对于样本空间可数时，下面的 $\mathbb{P}(X\leq x)$ 仍然是可以在最样本空间中写成多个可数个集合的并集。这样仍然是可测集。 这一类随机变量我们称为离散型随机变量。常见的离散型随机变量有：0-1分布（也叫两点分布或伯努利分布）、二项分布、几何分布、泊松分布等。 2.2 连续型随机变量而当样本空间是不可数集合时，$\mathbb{P}(X\leq x)$ 集合在样本空间中，可能是不可测集合（即不属于$\sigma$ 代数，参考上文的定义思考）。那么如何解决这个危机呢？ 这时候我们加强随机变量函数的条件，要求是连续函数。在测度论中，我们有鲁津定理描述连续函数和可测函数的关系，并且有下面的推论： 推论： 对于任意开集 $G$，由连续函数反射开集的性质 $f^{-1}(G) $ 也为开集。 另外我们有个特殊的 $\sigma$ 代数：Borel 代数： Borel代数定义：一个拓扑空间的开集全体所生成的$\sigma$代数就是borel集。显然也是可测的。 所以对于连续随机变量$X$ ，$X^{-1}((-\infin,x))$ 显然也是$R$​一个开集，borel 可测集。这样我们就解决了不可测集的危机。而这一类随机变量，我们称为连续性随机变量。常见的连续性随机变量有：均匀分布、指数分布、正态分布等。 事实上，对于随机变量函数只需要可测条件即可。实数轴上任何开区间（乃至任何开集）的原像都是可测的。更一般地，对于任何开集，通过余集、可列并集、可列交集所生成的集合（Borel集合），其原像也都是可测的。本质上，可测函数是一种几乎连续的函数，因为它的不连续部分为零测集（即测度为零的集合）。 但是实际应用中，遇到的大多函数通常都是连续函数。所以通俗概率论书籍中也不会讨论测度论（降低门槛），所以就缩小了讨论范围为连续型随机变量，而不是可测型随机变量。 第三部分 概率和统计3.1 方法论区别关于概率论和统计学，下图很形象的解释了两者的区别。 概率论是统计学的理论基础。概率论解决的问题是：已知数据的概率分布，然后理论应用于实际数据，观测和研究数据性质。概率论的方法论属于演绎，即给定数据的概率模型，演绎出关于数据的丰富多彩的推论。 统计学解决的问题是：已知部分数据（采样数据），然后推导、预测整体数据的概率分布。统计学的方法论属于归纳，通过采样数据，推断整体分布规律。 在现实世界中，大部分问题都是统计问题。由样本来推断总体的方法就叫统计推断法。 3.2 机器学习与统计所以对于机器学习，其实我们更多是在使用统计方法。例如深度神经网络，已知采样的数据，然后构建深度网络结构（实际是非线性函数集合），然后通过性能函数挑选出最佳函数（即模型）。所以有一种观点是机器学习只是统计学的外延，披了一层华丽的外衣。 我们以监督学习为例，给定一个数据集：$(X,Y)={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，这里 $x_i \in R^n$ 和 $y_i \in R$ 分别是输入和输出。 3.2.1 判别模型学习系统基于数据集构建拟合决策函数 $y=f(x)$ （非概率模型），对新的输入$x_{new}$，函数$f$ 给出预测输出 $y_{new}$。 常见模型有：神经网络、SVM支持向量机、k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、Boosting方法和条件随机场等。 3.2.2 生成模型而生成模型是数据集学习构建联合概率分布 $P(x, y)$ 和先验概率分布 $P(x)$ ，然后通过贝叶斯公式获得条件概率分布 $P(y \mid x)$ 作为预测的模型，即得到生成模型:$$P(y \mid x)=\frac{P(x, y)}{P(x)}$$生成方法强调的是 : 通过联合概率分布 $P(x, y)$ ，继而得到后验概率分布 $P(y \mid x)$ 。 常见方法有：朴素贝叶斯、LDA、隐马尔可夫模型（HMM）、混合高斯模型。 事实上，判别模型（概率模型）中决策函数写成下面的形式： $y=f(x)=argmax_{y_i}{P(y_i \mid x)}$，其中 $P(y \mid x)$为后验概率。所以对于分类问题，判别和生成模型是统一的， 所以说从统计学角度，机器学习本质目的是获得数据的后验概率：$P(y \mid x)$ 。 第四部分 总结本文只要介绍概率论定义从约定俗成的形式化形式化定义到公理化定义的过程。最后解释了随机变量的本质和测度论的关系。 参考文献及资料1、维基百科词条：Probability，https://en.wikipedia.org/wiki/Probability 2、维基百科词条：Random variable，https://en.wikipedia.org/wiki/Random_variable 3、A Brief Introduction to Probability &amp; Statistics，链接：https://betterexplained.com/articles/a-brief-introduction-to-probability-statistics/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python语言PEP8规范（翻译）]]></title>
    <url>%2F2021%2F11%2F16%2F2021-12-18-Python%E8%AF%AD%E8%A8%80PEP8%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 基础环境准备 第二部分 开发环境项目部署 第三部分 生产部署 第四部分 总结 参考文献及资料 背景Contents Introduction A Foolish Consistency is the Hobgoblin of Little Minds Code Lay-out Indentation Tabs or Spaces? Maximum Line Length Should a Line Break Before or After a Binary Operator? Blank Lines Source File Encoding Imports Module Level Dunder Names String Quotes Whitespace in Expressions and Statements Pet Peeves Other Recommendations When to Use Trailing Commas Comments Block Comments Inline Comments Documentation Strings Naming Conventions Overriding Principle Descriptive: Naming Styles Prescriptive: Naming Conventions Names to Avoid ASCII Compatibility Package and Module Names Class Names Type Variable Names Exception Names Global Variable Names Function and Variable Names Function and Method Arguments Method Names and Instance Variables Constants Designing for Inheritance Public and Internal Interfaces Programming Recommendations Function Annotations Variable Annotations References Copyright 简介This document gives coding conventions for the Python code comprising the standard library in the main Python distribution. Please see the companion informational PEP describing style guidelines for the C code in the C implementation of Python [1]. This document and PEP 257 (Docstring Conventions) were adapted from Guido’s original Python Style Guide essay, with some additions from Barry’s style guide [2]. This style guide evolves over time as additional conventions are identified and past conventions are rendered obsolete by changes in the language itself. Many projects have their own coding style guidelines. In the event of any conflicts, such project-specific guides take precedence for that project. A Foolish Consistency is the Hobgoblin of Little MindsOne of Guido’s key insights is that code is read much more often than it is written. The guidelines provided here are intended to improve the readability of code and make it consistent across the wide spectrum of Python code. As PEP 20 says, “Readability counts”. A style guide is about consistency. Consistency with this style guide is important. Consistency within a project is more important. Consistency within one module or function is the most important. However, know when to be inconsistent – sometimes style guide recommendations just aren’t applicable. When in doubt, use your best judgment. Look at other examples and decide what looks best. And don’t hesitate to ask! In particular: do not break backwards compatibility just to comply with this PEP! Some other good reasons to ignore a particular guideline: When applying the guideline would make the code less readable, even for someone who is used to reading code that follows this PEP. To be consistent with surrounding code that also breaks it (maybe for historic reasons) – although this is also an opportunity to clean up someone else’s mess (in true XP style). Because the code in question predates the introduction of the guideline and there is no other reason to be modifying that code. When the code needs to remain compatible with older versions of Python that don’t support the feature recommended by the style guide. 代码布局缩进Use 4 spaces per indentation level. 每级缩进使用4个空格。 注：使用空格（space）来表示缩进，避免使用制表符（tab） Continuation lines should align wrapped elements either vertically using Python’s implicit line joining inside parentheses, brackets and braces, or using a hanging indent [6]. When using a hanging indent the following should be considered; there should be no arguments on the first line and further indentation should be used to clearly distinguish itself as a continuation line: 123456789101112131415161718192021222324252627# Correct:# Aligned with opening delimiter.foo = long_function_name(var_one, var_two, var_three, var_four)# Add 4 spaces (an extra level of indentation) to distinguish arguments from the rest.def long_function_name( var_one, var_two, var_three, var_four): print(var_one)# Hanging indents should add a level.foo = long_function_name( var_one, var_two, var_three, var_four)# Wrong:# Arguments on first line forbidden when not using vertical alignment.foo = long_function_name(var_one, var_two, var_three, var_four)# Further indentation required as indentation is not distinguishable.def long_function_name( var_one, var_two, var_three, var_four): print(var_one) The 4-space rule is optional for continuation lines. Optional: 1234# Hanging indents *may* be indented to other than 4 spaces.foo = long_function_name( var_one, var_two, var_three, var_four) When the conditional part of an if-statement is long enough to require that it be written across multiple lines, it’s worth noting that the combination of a two character keyword (i.e. if), plus a single space, plus an opening parenthesis creates a natural 4-space indent for the subsequent lines of the multiline conditional. This can produce a visual conflict with the indented suite of code nested inside the if-statement, which would also naturally be indented to 4 spaces. This PEP takes no explicit position on how (or whether) to further visually distinguish such conditional lines from the nested suite inside the if-statement. Acceptable options in this situation include, but are not limited to: 12345678910111213141516# No extra indentation.if (this_is_one_thing and that_is_another_thing): do_something()# Add a comment, which will provide some distinction in editors# supporting syntax highlighting.if (this_is_one_thing and that_is_another_thing): # Since both conditions are true, we can frobnicate. do_something()# Add some extra indentation on the conditional continuation line.if (this_is_one_thing and that_is_another_thing): do_something() (Also see the discussion of whether to break before or after binary operators below.) The closing brace/bracket/parenthesis on multiline constructs may either line up under the first non-whitespace character of the last line of list, as in: 12345678my_list = [ 1, 2, 3, 4, 5, 6, ]result = some_function_that_takes_arguments( 'a', 'b', 'c', 'd', 'e', 'f', ) or it may be lined up under the first character of the line that starts the multiline construct, as in: 12345678my_list = [ 1, 2, 3, 4, 5, 6,]result = some_function_that_takes_arguments( 'a', 'b', 'c', 'd', 'e', 'f',) Tabs or Spaces?Spaces are the preferred indentation method. Tabs should be used solely to remain consistent with code that is already indented with tabs. Python disallows mixing tabs and spaces for indentation. Maximum Line LengthLimit all lines to a maximum of 79 characters. For flowing long blocks of text with fewer structural restrictions (docstrings or comments), the line length should be limited to 72 characters. Limiting the required editor window width makes it possible to have several files open side by side, and works well when using code review tools that present the two versions in adjacent columns. The default wrapping in most tools disrupts the visual structure of the code, making it more difficult to understand. The limits are chosen to avoid wrapping in editors with the window width set to 80, even if the tool places a marker glyph in the final column when wrapping lines. Some web based tools may not offer dynamic line wrapping at all. Some teams strongly prefer a longer line length. For code maintained exclusively or primarily by a team that can reach agreement on this issue, it is okay to increase the line length limit up to 99 characters, provided that comments and docstrings are still wrapped at 72 characters. The Python standard library is conservative and requires limiting lines to 79 characters (and docstrings/comments to 72). The preferred way of wrapping long lines is by using Python’s implied line continuation inside parentheses, brackets and braces. Long lines can be broken over multiple lines by wrapping expressions in parentheses. These should be used in preference to using a backslash for line continuation. Backslashes may still be appropriate at times. For example, long, multiple with-statements cannot use implicit continuation, so backslashes are acceptable: 123with open('/path/to/some/file/you/want/to/read') as file_1, \ open('/path/to/some/file/being/written', 'w') as file_2: file_2.write(file_1.read()) (See the previous discussion on multiline if-statements for further thoughts on the indentation of such multiline with-statements.) Another such case is with assert statements. Make sure to indent the continued line appropriately. Should a Line Break Before or After a Binary Operator?For decades the recommended style was to break after binary operators. But this can hurt readability in two ways: the operators tend to get scattered across different columns on the screen, and each operator is moved away from its operand and onto the previous line. Here, the eye has to do extra work to tell which items are added and which are subtracted: 1234567# Wrong:# operators sit far away from their operandsincome = (gross_wages + taxable_interest + (dividends - qualified_dividends) - ira_deduction - student_loan_interest) To solve this readability problem, mathematicians and their publishers follow the opposite convention. Donald Knuth explains the traditional rule in his Computers and Typesetting series: “Although formulas within a paragraph always break after binary operations and relations, displayed formulas always break before binary operations” [3]. Following the tradition from mathematics usually results in more readable code: 1234567# Correct:# easy to match operators with operandsincome = (gross_wages + taxable_interest + (dividends - qualified_dividends) - ira_deduction - student_loan_interest) In Python code, it is permissible to break before or after a binary operator, as long as the convention is consistent locally. For new code Knuth’s style is suggested. Blank LinesSurround top-level function and class definitions with two blank lines. Method definitions inside a class are surrounded by a single blank line. Extra blank lines may be used (sparingly) to separate groups of related functions. Blank lines may be omitted between a bunch of related one-liners (e.g. a set of dummy implementations). Use blank lines in functions, sparingly, to indicate logical sections. Python accepts the control-L (i.e. ^L) form feed character as whitespace; Many tools treat these characters as page separators, so you may use them to separate pages of related sections of your file. Note, some editors and web-based code viewers may not recognize control-L as a form feed and will show another glyph in its place. Source File EncodingCode in the core Python distribution should always use UTF-8, and should not have an encoding declaration. In the standard library, non-UTF-8 encodings should be used only for test purposes. Use non-ASCII characters sparingly, preferably only to denote places and human names. If using non-ASCII characters as data, avoid noisy Unicode characters like z̯̯͡a̧͎̺l̡͓̫g̹̲o̡̼̘ and byte order marks. All identifiers in the Python standard library MUST use ASCII-only identifiers, and SHOULD use English words wherever feasible (in many cases, abbreviations and technical terms are used which aren’t English). Open source projects with a global audience are encouraged to adopt a similar policy. Imports Imports should usually be on separate lines: 123# Correct:import osimport sys 12# Wrong:import sys, os It’s okay to say this though: 12# Correct:from subprocess import Popen, PIPE Imports are always put at the top of the file, just after any module comments and docstrings, and before module globals and constants. Imports should be grouped in the following order: Standard library imports. Related third party imports. Local application/library specific imports. You should put a blank line between each group of imports. Absolute imports are recommended, as they are usually more readable and tend to be better behaved (or at least give better error messages) if the import system is incorrectly configured (such as when a directory inside a package ends up on sys.path): 123import mypkg.siblingfrom mypkg import siblingfrom mypkg.sibling import example However, explicit relative imports are an acceptable alternative to absolute imports, especially when dealing with complex package layouts where using absolute imports would be unnecessarily verbose: 12from . import siblingfrom .sibling import example Standard library code should avoid complex package layouts and always use absolute imports. When importing a class from a class-containing module, it’s usually okay to spell this: 12from myclass import MyClassfrom foo.bar.yourclass import YourClass If this spelling causes local name clashes, then spell them explicitly: 12import myclassimport foo.bar.yourclass and use “myclass.MyClass” and “foo.bar.yourclass.YourClass”. Wildcard imports (from &lt;module&gt; import *) should be avoided, as they make it unclear which names are present in the namespace, confusing both readers and many automated tools. There is one defensible use case for a wildcard import, which is to republish an internal interface as part of a public API (for example, overwriting a pure Python implementation of an interface with the definitions from an optional accelerator module and exactly which definitions will be overwritten isn’t known in advance). When republishing names this way, the guidelines below regarding public and internal interfaces still apply. Module Level Dunder NamesModule level “dunders” (i.e. names with two leading and two trailing underscores) such as __all__, __author__, __version__, etc. should be placed after the module docstring but before any import statements except from __future__ imports. Python mandates that future-imports must appear in the module before any other code except docstrings: 12345678910111213"""This is the example module.This module does stuff."""from __future__ import barry_as_FLUFL__all__ = ['a', 'b', 'c']__version__ = '0.1'__author__ = 'Cardinal Biggles'import osimport sys String QuotesIn Python, single-quoted strings and double-quoted strings are the same. This PEP does not make a recommendation for this. Pick a rule and stick to it. When a string contains single or double quote characters, however, use the other one to avoid backslashes in the string. It improves readability. For triple-quoted strings, always use double quote characters to be consistent with the docstring convention in PEP 257. Whitespace in Expressions and StatementsPet PeevesAvoid extraneous whitespace in the following situations: Immediately inside parentheses, brackets or braces: 12# Correct:spam(ham[1], &#123;eggs: 2&#125;) 12# Wrong:spam( ham[ 1 ], &#123; eggs: 2 &#125; ) Between a trailing comma and a following close parenthesis: 12# Correct:foo = (0,) 12# Wrong:bar = (0, ) Immediately before a comma, semicolon, or colon: 12# Correct:if x == 4: print x, y; x, y = y, x 12# Wrong:if x == 4 : print x , y ; x , y = y , x However, in a slice the colon acts like a binary operator, and should have equal amounts on either side (treating it as the operator with the lowest priority). In an extended slice, both colons must have the same amount of spacing applied. Exception: when a slice parameter is omitted, the space is omitted: 123456# Correct:ham[1:9], ham[1:9:3], ham[:9:3], ham[1::3], ham[1:9:]ham[lower:upper], ham[lower:upper:], ham[lower::step]ham[lower+offset : upper+offset]ham[: upper_fn(x) : step_fn(x)], ham[:: step_fn(x)]ham[lower + offset : upper + offset] 12345# Wrong:ham[lower + offset:upper + offset]ham[1: 9], ham[1 :9], ham[1:9 :3]ham[lower : : upper]ham[ : upper] Immediately before the open parenthesis that starts the argument list of a function call: 12# Correct:spam(1) 12# Wrong:spam (1) Immediately before the open parenthesis that starts an indexing or slicing: 12# Correct:dct['key'] = lst[index] 12# Wrong:dct ['key'] = lst [index] More than one space around an assignment (or other) operator to align it with another: 1234# Correct:x = 1y = 2long_variable = 3 1234# Wrong:x = 1y = 2long_variable = 3 Other Recommendations Avoid trailing whitespace anywhere. Because it’s usually invisible, it can be confusing: e.g. a backslash followed by a space and a newline does not count as a line continuation marker. Some editors don’t preserve it and many projects (like CPython itself) have pre-commit hooks that reject it. Always surround these binary operators with a single space on either side: assignment (=), augmented assignment (+=, -= etc.), comparisons (==, &lt;, &gt;, !=, &lt;&gt;, &lt;=, &gt;=, in, not in, is, is not), Booleans (and, or, not). If operators with different priorities are used, consider adding whitespace around the operators with the lowest priority(ies). Use your own judgment; however, never use more than one space, and always have the same amount of whitespace on both sides of a binary operator: 123456# Correct:i = i + 1submitted += 1x = x*2 - 1hypot2 = x*x + y*yc = (a+b) * (a-b) 123456# Wrong:i=i+1submitted +=1x = x * 2 - 1hypot2 = x * x + y * yc = (a + b) * (a - b) Function annotations should use the normal rules for colons and always have spaces around the -&gt; arrow if present. (See Function Annotations below for more about function annotations.): 123# Correct:def munge(input: AnyStr): ...def munge() -&gt; PosInt: ... 123# Wrong:def munge(input:AnyStr): ...def munge()-&gt;PosInt: ... Don’t use spaces around the = sign when used to indicate a keyword argument, or when used to indicate a default value for an unannotated function parameter: 123# Correct:def complex(real, imag=0.0): return magic(r=real, i=imag) 123# Wrong:def complex(real, imag = 0.0): return magic(r = real, i = imag) When combining an argument annotation with a default value, however, do use spaces around the = sign: 123# Correct:def munge(sep: AnyStr = None): ...def munge(input: AnyStr, sep: AnyStr = None, limit=1000): ... 123# Wrong:def munge(input: AnyStr=None): ...def munge(input: AnyStr, limit = 1000): ... Compound statements (multiple statements on the same line) are generally discouraged: 123456# Correct:if foo == 'blah': do_blah_thing()do_one()do_two()do_three() Rather not: 123# Wrong:if foo == 'blah': do_blah_thing()do_one(); do_two(); do_three() While sometimes it’s okay to put an if/for/while with a small body on the same line, never do this for multi-clause statements. Also avoid folding such long lines! Rather not: 1234# Wrong:if foo == 'blah': do_blah_thing()for x in lst: total += xwhile t &lt; 10: t = delay() Definitely not: 1234567891011# Wrong:if foo == 'blah': do_blah_thing()else: do_non_blah_thing()try: something()finally: cleanup()do_one(); do_two(); do_three(long, argument, list, like, this)if foo == 'blah': one(); two(); three() When to Use Trailing CommasTrailing commas are usually optional, except they are mandatory when making a tuple of one element. For clarity, it is recommended to surround the latter in (technically redundant) parentheses: 1234# Correct:FILES = ('setup.cfg',)# Wrong:FILES = 'setup.cfg', When trailing commas are redundant, they are often helpful when a version control system is used, when a list of values, arguments or imported items is expected to be extended over time. The pattern is to put each value (etc.) on a line by itself, always adding a trailing comma, and add the close parenthesis/bracket/brace on the next line. However it does not make sense to have a trailing comma on the same line as the closing delimiter (except in the above case of singleton tuples): 1234567891011# Correct:FILES = [ 'setup.cfg', 'tox.ini', ]initialize(FILES, error=True, )# Wrong:FILES = ['setup.cfg', 'tox.ini',]initialize(FILES, error=True,) CommentsComments that contradict the code are worse than no comments. Always make a priority of keeping the comments up-to-date when the code changes! Comments should be complete sentences. The first word should be capitalized, unless it is an identifier that begins with a lower case letter (never alter the case of identifiers!). Block comments generally consist of one or more paragraphs built out of complete sentences, with each sentence ending in a period. You should use two spaces after a sentence-ending period in multi- sentence comments, except after the final sentence. Ensure that your comments are clear and easily understandable to other speakers of the language you are writing in. Python coders from non-English speaking countries: please write your comments in English, unless you are 120% sure that the code will never be read by people who don’t speak your language. Block CommentsBlock comments generally apply to some (or all) code that follows them, and are indented to the same level as that code. Each line of a block comment starts with a # and a single space (unless it is indented text inside the comment). Paragraphs inside a block comment are separated by a line containing a single #. Inline CommentsUse inline comments sparingly. An inline comment is a comment on the same line as a statement. Inline comments should be separated by at least two spaces from the statement. They should start with a # and a single space. Inline comments are unnecessary and in fact distracting if they state the obvious. Don’t do this: 1x = x + 1 # Increment x But sometimes, this is useful: 1x = x + 1 # Compensate for border Documentation StringsConventions for writing good documentation strings (a.k.a. “docstrings”) are immortalized in PEP 257. Write docstrings for all public modules, functions, classes, and methods. Docstrings are not necessary for non-public methods, but you should have a comment that describes what the method does. This comment should appear after the def line. PEP 257 describes good docstring conventions. Note that most importantly, the &quot;&quot;&quot; that ends a multiline docstring should be on a line by itself: 1234"""Return a foobangOptional plotz says to frobnicate the bizbaz first.""" For one liner docstrings, please keep the closing &quot;&quot;&quot; on the same line: 1"""Return an ex-parrot.""" Naming ConventionsThe naming conventions of Python’s library are a bit of a mess, so we’ll never get this completely consistent – nevertheless, here are the currently recommended naming standards. New modules and packages (including third party frameworks) should be written to these standards, but where an existing library has a different style, internal consistency is preferred. Overriding PrincipleNames that are visible to the user as public parts of the API should follow conventions that reflect usage rather than implementation. Descriptive: Naming StylesThere are a lot of different naming styles. It helps to be able to recognize what naming style is being used, independently from what they are used for. The following naming styles are commonly distinguished: b (single lowercase letter) B (single uppercase letter) lowercase lower_case_with_underscores UPPERCASE UPPER_CASE_WITH_UNDERSCORES CapitalizedWords (or CapWords, or CamelCase – so named because of the bumpy look of its letters [4]). This is also sometimes known as StudlyCaps. Note: When using acronyms in CapWords, capitalize all the letters of the acronym. Thus HTTPServerError is better than HttpServerError. mixedCase (differs from CapitalizedWords by initial lowercase character!) Capitalized_Words_With_Underscores (ugly!) There’s also the style of using a short unique prefix to group related names together. This is not used much in Python, but it is mentioned for completeness. For example, the os.stat() function returns a tuple whose items traditionally have names like st_mode, st_size, st_mtime and so on. (This is done to emphasize the correspondence with the fields of the POSIX system call struct, which helps programmers familiar with that.) The X11 library uses a leading X for all its public functions. In Python, this style is generally deemed unnecessary because attribute and method names are prefixed with an object, and function names are prefixed with a module name. In addition, the following special forms using leading or trailing underscores are recognized (these can generally be combined with any case convention): _single_leading_underscore: weak “internal use” indicator. E.g. from M import * does not import objects whose names start with an underscore. single_trailing_underscore_: used by convention to avoid conflicts with Python keyword, e.g. 1tkinter.Toplevel(master, class_='ClassName') __double_leading_underscore: when naming a class attribute, invokes name mangling (inside class FooBar, __boo becomes _FooBar__boo; see below). __double_leading_and_trailing_underscore__: “magic” objects or attributes that live in user-controlled namespaces. E.g. __init__, __import__ or __file__. Never invent such names; only use them as documented. Prescriptive: Naming ConventionsNames to AvoidNever use the characters ‘l’ (lowercase letter el), ‘O’ (uppercase letter oh), or ‘I’ (uppercase letter eye) as single character variable names. In some fonts, these characters are indistinguishable from the numerals one and zero. When tempted to use ‘l’, use ‘L’ instead. ASCII CompatibilityIdentifiers used in the standard library must be ASCII compatible as described in the policy section of PEP 3131. Package and Module NamesModules should have short, all-lowercase names. Underscores can be used in the module name if it improves readability. Python packages should also have short, all-lowercase names, although the use of underscores is discouraged. When an extension module written in C or C++ has an accompanying Python module that provides a higher level (e.g. more object oriented) interface, the C/C++ module has a leading underscore (e.g. _socket). Class NamesClass names should normally use the CapWords convention. The naming convention for functions may be used instead in cases where the interface is documented and used primarily as a callable. Note that there is a separate convention for builtin names: most builtin names are single words (or two words run together), with the CapWords convention used only for exception names and builtin constants. Type Variable NamesNames of type variables introduced in PEP 484 should normally use CapWords preferring short names: T, AnyStr, Num. It is recommended to add suffixes _co or _contra to the variables used to declare covariant or contravariant behavior correspondingly: 1234from typing import TypeVarVT_co = TypeVar('VT_co', covariant=True)KT_contra = TypeVar('KT_contra', contravariant=True) Exception NamesBecause exceptions should be classes, the class naming convention applies here. However, you should use the suffix “Error” on your exception names (if the exception actually is an error). Global Variable Names(Let’s hope that these variables are meant for use inside one module only.) The conventions are about the same as those for functions. Modules that are designed for use via from M import * should use the __all__ mechanism to prevent exporting globals, or use the older convention of prefixing such globals with an underscore (which you might want to do to indicate these globals are “module non-public”). Function and Variable NamesFunction names should be lowercase, with words separated by underscores as necessary to improve readability. Variable names follow the same convention as function names. mixedCase is allowed only in contexts where that’s already the prevailing style (e.g. threading.py), to retain backwards compatibility. Function and Method ArgumentsAlways use self for the first argument to instance methods. Always use cls for the first argument to class methods. If a function argument’s name clashes with a reserved keyword, it is generally better to append a single trailing underscore rather than use an abbreviation or spelling corruption. Thus class_ is better than clss. (Perhaps better is to avoid such clashes by using a synonym.) Method Names and Instance VariablesUse the function naming rules: lowercase with words separated by underscores as necessary to improve readability. Use one leading underscore only for non-public methods and instance variables. To avoid name clashes with subclasses, use two leading underscores to invoke Python’s name mangling rules. Python mangles these names with the class name: if class Foo has an attribute named __a, it cannot be accessed by Foo.__a. (An insistent user could still gain access by calling Foo._Foo__a.) Generally, double leading underscores should be used only to avoid name conflicts with attributes in classes designed to be subclassed. Note: there is some controversy about the use of __names (see below). ConstantsConstants are usually defined on a module level and written in all capital letters with underscores separating words. Examples include MAX_OVERFLOW and TOTAL. Designing for InheritanceAlways decide whether a class’s methods and instance variables (collectively: “attributes”) should be public or non-public. If in doubt, choose non-public; it’s easier to make it public later than to make a public attribute non-public. Public attributes are those that you expect unrelated clients of your class to use, with your commitment to avoid backwards incompatible changes. Non-public attributes are those that are not intended to be used by third parties; you make no guarantees that non-public attributes won’t change or even be removed. We don’t use the term “private” here, since no attribute is really private in Python (without a generally unnecessary amount of work). Another category of attributes are those that are part of the “subclass API” (often called “protected” in other languages). Some classes are designed to be inherited from, either to extend or modify aspects of the class’s behavior. When designing such a class, take care to make explicit decisions about which attributes are public, which are part of the subclass API, and which are truly only to be used by your base class. With this in mind, here are the Pythonic guidelines: Public attributes should have no leading underscores. If your public attribute name collides with a reserved keyword, append a single trailing underscore to your attribute name. This is preferable to an abbreviation or corrupted spelling. (However, notwithstanding this rule, ‘cls’ is the preferred spelling for any variable or argument which is known to be a class, especially the first argument to a class method.) Note 1: See the argument name recommendation above for class methods. For simple public data attributes, it is best to expose just the attribute name, without complicated accessor/mutator methods. Keep in mind that Python provides an easy path to future enhancement, should you find that a simple data attribute needs to grow functional behavior. In that case, use properties to hide functional implementation behind simple data attribute access syntax. Note 1: Try to keep the functional behavior side-effect free, although side-effects such as caching are generally fine. Note 2: Avoid using properties for computationally expensive operations; the attribute notation makes the caller believe that access is (relatively) cheap. If your class is intended to be subclassed, and you have attributes that you do not want subclasses to use, consider naming them with double leading underscores and no trailing underscores. This invokes Python’s name mangling algorithm, where the name of the class is mangled into the attribute name. This helps avoid attribute name collisions should subclasses inadvertently contain attributes with the same name. Note 1: Note that only the simple class name is used in the mangled name, so if a subclass chooses both the same class name and attribute name, you can still get name collisions. Note 2: Name mangling can make certain uses, such as debugging and __getattr__(), less convenient. However the name mangling algorithm is well documented and easy to perform manually. Note 3: Not everyone likes name mangling. Try to balance the need to avoid accidental name clashes with potential use by advanced callers. Public and Internal InterfacesAny backwards compatibility guarantees apply only to public interfaces. Accordingly, it is important that users be able to clearly distinguish between public and internal interfaces. Documented interfaces are considered public, unless the documentation explicitly declares them to be provisional or internal interfaces exempt from the usual backwards compatibility guarantees. All undocumented interfaces should be assumed to be internal. To better support introspection, modules should explicitly declare the names in their public API using the __all__ attribute. Setting __all__ to an empty list indicates that the module has no public API. Even with __all__ set appropriately, internal interfaces (packages, modules, classes, functions, attributes or other names) should still be prefixed with a single leading underscore. An interface is also considered internal if any containing namespace (package, module or class) is considered internal. Imported names should always be considered an implementation detail. Other modules must not rely on indirect access to such imported names unless they are an explicitly documented part of the containing module’s API, such as os.path or a package’s __init__ module that exposes functionality from submodules. Programming Recommendations Code should be written in a way that does not disadvantage other implementations of Python (PyPy, Jython, IronPython, Cython, Psyco, and such). For example, do not rely on CPython’s efficient implementation of in-place string concatenation for statements in the form a += b or a = a + b. This optimization is fragile even in CPython (it only works for some types) and isn’t present at all in implementations that don’t use refcounting. In performance sensitive parts of the library, the &#39;&#39;.join() form should be used instead. This will ensure that concatenation occurs in linear time across various implementations. Comparisons to singletons like None should always be done with is or is not, never the equality operators. Also, beware of writing if x when you really mean if x is not None – e.g. when testing whether a variable or argument that defaults to None was set to some other value. The other value might have a type (such as a container) that could be false in a boolean context! Use is not operator rather than not ... is. While both expressions are functionally identical, the former is more readable and preferred: 12# Correct:if foo is not None: 12# Wrong:if not foo is None: When implementing ordering operations with rich comparisons, it is best to implement all six operations (__eq__, __ne__, __lt__, __le__, __gt__, __ge__) rather than relying on other code to only exercise a particular comparison. To minimize the effort involved, the functools.total_ordering() decorator provides a tool to generate missing comparison methods. PEP 207 indicates that reflexivity rules are assumed by Python. Thus, the interpreter may swap y &gt; x with x &lt; y, y &gt;= x with x &lt;= y, and may swap the arguments of x == y and x != y. The sort() and min() operations are guaranteed to use the &lt; operator and the max() function uses the &gt; operator. However, it is best to implement all six operations so that confusion doesn’t arise in other contexts. Always use a def statement instead of an assignment statement that binds a lambda expression directly to an identifier: 12# Correct:def f(x): return 2*x 12# Wrong:f = lambda x: 2*x The first form means that the name of the resulting function object is specifically ‘f’ instead of the generic ‘‘. This is more useful for tracebacks and string representations in general. The use of the assignment statement eliminates the sole benefit a lambda expression can offer over an explicit def statement (i.e. that it can be embedded inside a larger expression) Derive exceptions from Exception rather than BaseException. Direct inheritance from BaseException is reserved for exceptions where catching them is almost always the wrong thing to do. Design exception hierarchies based on the distinctions that code catching the exceptions is likely to need, rather than the locations where the exceptions are raised. Aim to answer the question “What went wrong?” programmatically, rather than only stating that “A problem occurred” (see PEP 3151 for an example of this lesson being learned for the builtin exception hierarchy) Class naming conventions apply here, although you should add the suffix “Error” to your exception classes if the exception is an error. Non-error exceptions that are used for non-local flow control or other forms of signaling need no special suffix. Use exception chaining appropriately. raise X from Y should be used to indicate explicit replacement without losing the original traceback. When deliberately replacing an inner exception (using raise X from None), ensure that relevant details are transferred to the new exception (such as preserving the attribute name when converting KeyError to AttributeError, or embedding the text of the original exception in the new exception message). When catching exceptions, mention specific exceptions whenever possible instead of using a bare except: clause: 1234try: import platform_specific_moduleexcept ImportError: platform_specific_module = None A bare except: clause will catch SystemExit and KeyboardInterrupt exceptions, making it harder to interrupt a program with Control-C, and can disguise other problems. If you want to catch all exceptions that signal program errors, use except Exception: (bare except is equivalent to except BaseException:). A good rule of thumb is to limit use of bare ‘except’ clauses to two cases: If the exception handler will be printing out or logging the traceback; at least the user will be aware that an error has occurred. If the code needs to do some cleanup work, but then lets the exception propagate upwards with raise. try...finally can be a better way to handle this case. When catching operating system errors, prefer the explicit exception hierarchy introduced in Python 3.3 over introspection of errno values. Additionally, for all try/except clauses, limit the try clause to the absolute minimum amount of code necessary. Again, this avoids masking bugs: 1234567# Correct:try: value = collection[key]except KeyError: return key_not_found(key)else: return handle_value(value) 1234567# Wrong:try: # Too broad! return handle_value(collection[key])except KeyError: # Will also catch KeyError raised by handle_value() return key_not_found(key) When a resource is local to a particular section of code, use a with statement to ensure it is cleaned up promptly and reliably after use. A try/finally statement is also acceptable. Context managers should be invoked through separate functions or methods whenever they do something other than acquire and release resources: 123# Correct:with conn.begin_transaction(): do_stuff_in_transaction(conn) 123# Wrong:with conn: do_stuff_in_transaction(conn) The latter example doesn’t provide any information to indicate that the __enter__ and __exit__ methods are doing something other than closing the connection after a transaction. Being explicit is important in this case. Be consistent in return statements. Either all return statements in a function should return an expression, or none of them should. If any return statement returns an expression, any return statements where no value is returned should explicitly state this as return None, and an explicit return statement should be present at the end of the function (if reachable): 123456789101112# Correct:def foo(x): if x &gt;= 0: return math.sqrt(x) else: return Nonedef bar(x): if x &lt; 0: return None return math.sqrt(x) 12345678910# Wrong:def foo(x): if x &gt;= 0: return math.sqrt(x)def bar(x): if x &lt; 0: return return math.sqrt(x) Use &#39;&#39;.startswith() and &#39;&#39;.endswith() instead of string slicing to check for prefixes or suffixes. startswith() and endswith() are cleaner and less error prone: 12# Correct:if foo.startswith('bar'): 12# Wrong:if foo[:3] == 'bar': Object type comparisons should always use isinstance() instead of comparing types directly: 12# Correct:if isinstance(obj, int): 12# Wrong:if type(obj) is type(1): For sequences, (strings, lists, tuples), use the fact that empty sequences are false: 123# Correct:if not seq:if seq: 123# Wrong:if len(seq):if not len(seq): Don’t write string literals that rely on significant trailing whitespace. Such trailing whitespace is visually indistinguishable and some editors (or more recently, reindent.py) will trim them. Don’t compare boolean values to True or False using ==: 12# Correct:if greeting: 12# Wrong:if greeting == True: Worse: 12# Wrong:if greeting is True: Use of the flow control statements return/break/continue within the finally suite of a try...finally, where the flow control statement would jump outside the finally suite, is discouraged. This is because such statements will implicitly cancel any active exception that is propagating through the finally suite: 123456# Wrong:def foo(): try: 1 / 0 finally: return 42 Function AnnotationsWith the acceptance of PEP 484, the style rules for function annotations have changed. Function annotations should use PEP 484 syntax (There are some formatting recommendations for annotations in the previous section). The experimentation with annotation styles that was recommended previously in this PEP is no longer encouraged. However, outside the stdlib, experiments within the rules of PEP 484 are now encouraged. For example, marking up a large third party library or application with PEP 484 style type annotations, reviewing how easy it was to add those annotations, and observing whether their presence increases code understandability. The Python standard library should be conservative in adopting such annotations, but their use is allowed for new code and for big refactorings. For code that wants to make a different use of function annotations it is recommended to put a comment of the form: 1# type: ignore near the top of the file; this tells type checkers to ignore all annotations. (More fine-grained ways of disabling complaints from type checkers can be found in PEP 484.) Like linters, type checkers are optional, separate tools. Python interpreters by default should not issue any messages due to type checking and should not alter their behavior based on annotations. Users who don’t want to use type checkers are free to ignore them. However, it is expected that users of third party library packages may want to run type checkers over those packages. For this purpose PEP 484 recommends the use of stub files: .pyi files that are read by the type checker in preference of the corresponding .py files. Stub files can be distributed with a library, or separately (with the library author’s permission) through the typeshed repo [5]. Variable AnnotationsPEP 526 introduced variable annotations. The style recommendations for them are similar to those on function annotations described above: Annotations for module level variables, class and instance variables, and local variables should have a single space after the colon. There should be no space before the colon. If an assignment has a right hand side, then the equality sign should have exactly one space on both sides: 1234567# Correct:code: intclass Point: coords: Tuple[int, int] label: str = '&lt;unknown&gt;' 1234567# Wrong:code:int # No space after coloncode : int # Space before colonclass Test: result: int=0 # No spaces around equality sign Although the PEP 526 is accepted for Python 3.6, the variable annotation syntax is the preferred syntax for stub files on all versions of Python (see PEP 484 for details). Footnotes [6] Hanging indentation is a type-setting style where all the lines in a paragraph are indented except the first line. In the context of Python, the term is used to describe a style where the opening parenthesis of a parenthesized statement is the last non-whitespace character of the line, with subsequent lines being indented until the closing parenthesis. 参考文献 [1] PEP 7, Style Guide for C Code, van Rossum [2] Barry’s GNU Mailman style guide http://barry.warsaw.us/software/STYLEGUIDE.txt [3] Donald Knuth’s The TeXBook, pages 195 and 196. [4] http://www.wikipedia.com/wiki/CamelCase [5] Typeshed repo https://github.com/python/typeshed CopyrightThis document has been placed in the public domain. Source: https://github.com/python/peps/blob/master/pep-0008.txt]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[若依spring-cloud项目源码阅读解析]]></title>
    <url>%2F2021%2F11%2F16%2F2021-11-16-%E8%8B%A5%E4%BE%9Dspring-cloud%E9%A1%B9%E7%9B%AE%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 基础环境准备 第二部分 开发环境项目部署 第三部分 生产部署 第四部分 总结 参考文献及资料 背景第一部分 权限系统设计https://blog.csdn.net/starzhou/article/details/121232710 第二部分第三部分 服务调用Feign参考文献及资料1、RuoYi-Cloud项目文档，链接：http://doc.ruoyi.vip/ruoyi-cloud/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库架构设计模式总结]]></title>
    <url>%2F2021%2F11%2F16%2F2022-03-05-%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E4%B8%AD%E7%9A%84%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景数据库构架设计中主要有Shared Everything、Shared Disk、Share Memory和Shared Nothing等。 第一部分 数据库架构设计设计模式1.1 Shared EverythingShared Everything指单个主机独立支配CPU、内存、磁盘等硬件资源，其优势是架构简单，搭建方便。但该种架构的缺陷是数据并行处理能力较差，扩展性较低。Shared Everything的典型代表的产品为SQLserver。 1.2 Shared Disk在Shared Disk架构中，CPU和内存对于各个处理单元私有，但各节点共享磁盘系统。该种架构的典型代表为DB2 pureScale和Oracle Rac。这种共享架构具备一定的扩展能力，可通过节点的增加来提升数据并行处理能力。但当存储器接口使用饱和时，磁盘IO成为了系统资源瓶颈，节点的扩充并不能提升系统性能。 Oracle的Cluster就是RAC，其实这不是真正的Cluster产品，属于变相实现的，而Oracle的RAC是通过共享存储实现的，也就是共享磁盘，只不过多数采用中高档存储服务器； 1.3 Shared MemoryShared Memory指多个节点共享内存，各CPU间通过内部通讯网络（Interconnection network）进行通讯。但与Shared Disk类似，但当节点数量过高时，内存竞争（Memory contention）将成为该系统的瓶颈，单纯地堆砌节点数量并不能提升整体数据处理性能。 Shared memory 体系结构的cpu之间通过主存进行通讯，具有很高的效率；但当更多的cpu被添加到主机上时，内存竞争contetion就成为瓶颈，cpu越多，瓶颈越 厉害。Shared disk也存在同样问题，因为磁盘系统由 Interconnection Network 连接在一起。 1.4 Shared NothingShared Nothing的核心思想是各个数据库单元中不存在共享资源，数据处理单元对于各节点完全私有化。早在1986年加州大学伯克利分校的论文中，Michael Stonebraker从当时的数仓原型中对比了Shared Disk，Shared Memory，Shared Nothing架构，并论证了Shared Nothing在数据并行处理中的优势。各单元通过通信协议层交互，处理后的数据会逐步向上层汇总或通过通信层流转于节点间。Teradata公司在1982年申请了YNET技术专利，为无共享的大规模数据并行处理（Massive Parallel Processing）提供了先决基础。在TD数仓架构中，各节点单元通过MPL（Message Passing Layer）的BYNET物理层实现。BYNET是一个双冗余、全双工的网络，以松耦合方式将多个数据处理节点与处理引擎（Parsing Engine）高速连接起来。G行的多元化大数据平台中的TD集群正是采取该架构设计。 share nothing 指的是 在一个MPP分布式系统中，每个处理器有它自己的内存，操作系统，硬盘存储，多个segment节点可独立工作合作完成一个操作。关键看各节点有无独立的处理能力，是否需要master节点协助，greenplum分布式架构是典型的share nothing Shared-Nothing架构：是一种松散耦合（Loosely Coupled）的架构，又可称为Share nothing，代表 系统Teradata，DB2 DPF，阿里Oceabase，腾讯TDSQL， TiDB，Google Spanner ；Shared-Nothing架 构在搭建成本低，表观扩展性等也都非常高。 第二部分 Shared Everthting:一般是针对单个主机，完全透明共享CPU/MEMORY/IO，并行处理能力是最差的，典型的代表SQLServer Shared Disk：各个处理单元使用自己的私有 CPU和Memory，共享磁盘系统。典型的代表Oracle Rac， 它是数据共享，可通过增加节点来提高并行处理的能力，扩展能力较好。其类似于SMP（对称多处理）模式，但是当存储器接口达到饱和的时候，增加节点并不能获得更高的性能 。 Shared Nothing：各个处理单元都有自己私有的CPU/内存/硬盘等，不存在共享资源，类似于MPP（大规模并行处理）模式，各处理单元之间通过协议通信，并行处理和扩展能力更好。典型代表DB2 DPF和Hadoop ，各节点相互独立，各自处理自己的数据，处理后的结果可能向上层汇总或在节点间流转。我们常说的 Sharding 其实就是Share Nothing架构，它是把某个表从物理存储上被水平分割，并分配给多台服务器（或多个实例），每台服务器可以独立工作，具备共同的schema，比如MySQL Proxy和Google的各种架构，只需增加服务器数就可以增加处理能力和容量。 大数据时代，数据已日益成为决策分析、价值挖掘的关键。G行GaussDB、Teradata，Greenplum三个Shared Nothing架构的数仓平台，每日通过40000余个批量任务，对超过60个集市进行抽取（Extract）、加载（Load）、转换（Transform），支持着全行的BI应用及监管报送任务。 参考文献及资料1、《Deep Dive: Apache Spark Memory Management》介绍视频，链接：https://youtu.be/dPHrykZL8Cg]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud系列文章（Feign服务调用）]]></title>
    <url>%2F2021%2F11%2F11%2F2021-11-15-SpringCloud%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88Feign%E6%9C%8D%E5%8A%A1%E8%B0%83%E7%94%A8%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景Feign是Netflix公司开源的轻量级rest客户端，使用Feign可以非常方便的实现Http 客户端。Spring Cloud引入Feign并且集成了Ribbon实现客户端负载均衡调用。 Feign是一种声明式、模板化的HTTP客户端。在Spring Cloud中使用Feign, 我们可以做到使用HTTP请求远程服务时能与调用本地方法一样的编码体验,开发者完全感知不到这是远程方法,更感知不到这是个HTTP请求。 第一部分 Feign 开发环境1.1 pom.xml文件不注明版本，会引入和SpringBoot相匹配的版本。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 1.2 启动类配置注解@EnableFeignClients1234567@SpringBootApplication@EnableFeignClients // Feign注解public class TestApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(TestApplication.class, args); &#125;&#125; 参考文献及资料1、Spring官网，链接：https://spring.io/projects/spring-cloud-gateway]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[信息论和位进制]]></title>
    <url>%2F2021%2F09%2F27%2F2021-10-01-%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%92%8C%E4%BD%8D%E8%BF%9B%E5%88%B6%2F</url>
    <content type="text"><![CDATA[目录 术语说明 背景 第一部分 常见深度学习计算框架在Hadoop集群运行 第一部分 Spark on Yarn 第二部分 Pyspark Application原理 第三部分 业务侧调优 第四部分 总结 参考文献及资料 背景人类为什么选择10进制位进制作为日常计数，而计算机科学中为什么选择2进制作为位进制。 第一部分 信息论基础第二部分 位进制基础数学这门学科起源于计数， 第三部分 两者的关系参考文献1、Tensorflow on YARN Native Service - 可能目前最好的跑分布式Tensorflow训练的选择 https://www.jianshu.com/p/288b484d838b]]></content>
      <categories>
        <category>Consul</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow on Yarn技术方案调研总结]]></title>
    <url>%2F2021%2F09%2F27%2F2021-09-27-Tensorflow%20on%20Yarn%E6%8A%80%E6%9C%AF%E6%96%B9%E6%A1%88%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 术语说明 背景 第一部分 常见深度学习计算框架在Hadoop集群运行 第一部分 Spark on Yarn 第二部分 Pyspark Application原理 第三部分 业务侧调优 第四部分 总结 参考文献及资料 背景企业级大数据平台通常将大量历史数据存储在Hadoop集群中。随着机器学习和深度学习的兴起，利用集群中的数据训练机器学习模型成了自然业务需求。通常有两种架构方式实现： 建设单独的深度学习计算集群； 基于现有的Hadoop集群建设实现； 1、双集群架构在现有Hadoop/Spark大数据集群基础上，新增新的机器学习集群。由Hadoop集群提供历史数据存储服务，机器学习训练集群提供模型训练计算服务。架构数据流参考下图。 优点：这种架构将计算和存储功能解耦。由Hadoop集群提供数据的存储和ETL服务，而机器学习计算特点为迭代式计算，需要高性能计算节点（比如配置GPU）。各自发挥自己的计算专长。 缺点：集群之间存在大量数据的网络传输，会增加系统之间的IO传输，另外对于模型的应用会产生模型端到端的学习应用上线的延迟。 2、混合部署考虑到双集群架构的缺点，架构上将两个功能集群混合部署，即将机器学习、深度学习任务跑在Hadoop/Spark集群上。随着Tensorflow 1.0版本开始对HDFS的支持，使得技术上有了可行性。在同一个集群上运行深度学习（机器学习）可以显著提高数据/计算资源共享的效率。而考虑到数据和计算资源贴近的原则，我们需要将常见机器学习或深度学习计算框架跑在Hadoop集群上。 第一部分 分布式机器学习1.1 分布式机器学习1.2 分布式TensorFlowhttps://zhuanlan.zhihu.com/p/56991108 1.3 开源框架梳理目前有不少大厂开源自己的方案，主要有： 框架名称 公司 项目地址 TensorFlowOnSpark Yahoo！ https://github.com/yahoo/TensorFlowOnSpark TensorFlowOnYARN Intel https://github.com/Intel-bigdata/TensorFlowOnYARN spark-deep-learning Databricks https://github.com/databricks/spark-deep-learning XLearning 360 https://github.com/Qihoo360/XLearning Kubeflow Google https://github.com/kubeflow/kubeflow TonY linkedin https://github.com/linkedin/TonY tf-yarn Criteo https://github.com/criteo/tf-yarn 上面的框架对于一些关键特性的支持，进行了比较： 关键特性主要有： 是否支持Docker Tensorflow不同版本对cuda/cudnn支持的差异性、以及Python版本的多样性，需要对依赖进行隔离，Docker是一个较好的技术选型。 是否支持GPU的隔离 生产环境下的Tensorflow训练任务一定是多任务的，需要架构提供对GPU资源使用的隔离。 是否对Tensorflow原生代码的支持。支持用户对原生Tensorflow代码的支持。 是否支持HDFS文件系统的读写。Tensorflow1.0后支持使用libhdfs访问HDFS，主要考虑访问的便携性，特别是生产集群都是安全集群（kerberorized HDFS） 是否支持DNS。作业进程能否访问DNS类功能组件，这样用户可以通过域名方式访问Tensorboard/notebook。 各计算框架支持情况如下： 框架名称 Docker 深度学习框架 语言 GPU HDFS DNS TensorFlowOnSpark TensorFlow 是 是 TensorFlowOnYARN TensorFlow 是 spark-deep-learning 是 XLearning 是 是 Kubeflow 是 是 是 TonY 是 TensorFlow、Pytorch、MXNet and Horovod java tf-yarn TensorFlow、Pytorch Python 是 上面的开源项目都是基于Yarn或者Spark组件实现的。事实上Hadoop（Yarn）在2.x版本后支持Docker on Yarn的资源调度。这就为Tensorflow运行在Yarn提供了天然的运行支持环境，我们称为：YARN Native Service。 支持Docker Yarn中有三个ContainerExecutor：DefaultContainerExecutor、LinuxContainerExecutor和WindowsSecureContainerExecutor。DefaultContainerExecutor适用于非安全集群、LinuxContainerExecutor适用于安全集群，WindowsSecureContainerExecutor用于在windows安全集群上。 随着Docker的普及，Yarn开始新增了一种新的ContainerExecutor，称为DockerContainerExecutor，允许用户将任务作为Docker容器运行。但是存在一个架构问题，在Yarn中每个NodeManager 只可以使用一个ContainerExecutor，即所有任务都将使用节点配置中指定的ContainerExecutor。一旦将群集配置为使用DockerContainerExecutor，用户将无法启动常规MapReduce、Spark等作业。所以这个实验性架构已经被弃用。为了解决这个架构缺陷，Yarn在LinuxContainerExecutor中添加了对Container Runtimes的支持， 支持GPU的资源调度和隔离；在Hadoop 3.0里面加入的可扩展类型的多资源调度 (multiple resource scheduling), 与Hadoop 3.1里面的GPU隔离可以很轻松的支持这一点。 不需要调整用户原生代码；下面文档里面提到的提交脚本 submit_tf_job.py 可以配合YARN DNS，自动生成TF_CONFIG 环境变量来支持分布式Tensorflow训练，不需要改变用户代码。 支持HDFS的读写；由于YARN原生支持HDFS delegation token来访问Kerberorized HDFS, 下面文档里面提到了怎样可以方便地在运行时把所需要的配置文件mount到Docker container里面以支持访问Kerberorized HDFS. 第二部分 tf-yarn项目介绍tf-yarn项目是Criteo公司（广告公司）开源的Python库，用来在Hadoop/Yarn上训练Pytorch 和TensorFlow模型。支持单机和分布式机器学习训练，支持CPU和GPU两种模式的计算资源。 仅需几行代码即可在YARN上训练TensorFlow模型 https://medium.com/criteo-labs/train-tensorflow-models-on-yarn-in-just-a-few-lines-of-code-ba0f354f38e3 第三部分 TonY项目介绍Open Sourcing TonY: Native Support of TensorFlow on Hadoop https://engineering.linkedin.com/blog/2018/09/open-sourcing-tony--native-support-of-tensorflow-on-hadoop 第四部分 TensorFlowOnSpark项目介绍https://developer.yahoo.com/blogs/157196317141/ 第五部分 TensorFlowOnYARN项目介绍第六部分 TensorFlowOnSpark项目介绍Yahoo的开源框架TensorFlowOnSpark（TFoS）支持在Spark和Hadoop集群上执行分布式Tensorflow。 报错讨论：https://github.com/yahoo/TensorFlowOnSpark/issues/33 第七部分 spark-deep-learning第八部分 XLearning基于Hadoop分布式集群YARN模式下的TensorFlowOnSpark平台搭建 学习笔记TF065:TensorFlowOnSpark 第九部分 Kubeflow项目介绍yahoo https://www.tumblr.com/register/follow/yahoohadoop 文章： https://runitao.github.io/distributed-tensorflow.html https://blog.csdn.net/ChaosJ/article/details/104786247 https://blog.csdn.net/karamos/article/details/80130751 https://www.infoq.cn/article/k4ulimanccimgfuqk_cd 第十部分 总结参考文献及资料1、Tensorflow on YARN Native Service - 可能目前最好的跑分布式Tensorflow训练的选择 https://www.jianshu.com/p/288b484d838b 2、Open Sourcing TensorFlowOnSpark: Distributed Deep Learning on Big-Data Clusters 3、https://github.com/yahoo/TensorFlowOnSpark/wiki/GetStarted_YARN 4、TensorFlowOnSpark 源码解析 链接：https://www.jianshu.com/p/72d153c284cd 5、Tensorflow on Spark爬坑指南 链接：https://www.jianshu.com/p/72cb5816a0f7 6、使用TensorFlowOnSpark进行深度学习 链接：https://kitwaicloud.github.io/tensorflow_on_spark/tensorflow_on_spark.html 7、学习笔记TF065: TensorFlowOnSpark 链接：https://cloud.tencent.com/developer/article/1006361 8、分布式机器学习：算法、理论与实践，链接：https://item.jd.com/12444377.html]]></content>
      <categories>
        <category>Tensorflow，Yarn，python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis集群部署和使用总结]]></title>
    <url>%2F2021%2F09%2F27%2F2021-10-08-Redis%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 术语说明 背景 第一部分 常见深度学习计算框架在Hadoop集群运行 第一部分 Spark on Yarn 第二部分 Pyspark Application原理 第三部分 业务侧调优 第四部分 总结 参考文献及资料 背景https://www.cnblogs.com/jian0110/p/14002555.html https://www.jianshu.com/p/0efaa1a271e0 参考文献1、Tensorflow on YARN Native Service - 可能目前最好的跑分布式Tensorflow训练的选择 https://www.jianshu.com/p/288b484d838b]]></content>
      <categories>
        <category>Consul</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Consul集群搭建]]></title>
    <url>%2F2021%2F09%2F27%2F2021-10-01-Consul%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[目录 术语说明 背景 第一部分 常见深度学习计算框架在Hadoop集群运行 第一部分 Spark on Yarn 第二部分 Pyspark Application原理 第三部分 业务侧调优 第四部分 总结 参考文献及资料 背景参考文献1、Tensorflow on YARN Native Service - 可能目前最好的跑分布式Tensorflow训练的选择 https://www.jianshu.com/p/288b484d838b]]></content>
      <categories>
        <category>Consul</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kubernetes常用命令汇总]]></title>
    <url>%2F2021%2F08%2F02%2F2021-08-14-kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 第一部分 kubectl get - 获取资源列表1.1 获取集群所有pods信息1kubectl get pods 1.2 查看当前所有service1kubectl get services 第二部分 kubectl describe - 查看资源详细信息第三部分 kubectl logs - 查看pod日志第四部分 kubectl delete - 删除一个资源第五部分 kubectl proxy - 启动代理参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud系列文章（nacos服务注册总结）]]></title>
    <url>%2F2021%2F07%2F31%2F2021-08-10-SpringCloud%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88nacos%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E6%80%BB%E7%BB%93%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发、测试环境项目部署 第二部分 项目打包 第三部分 生产环境部署 第四部分 总结 参考文献及资料 背景而 Nacos 作为微服务核心的服务注册与发现中心，让大家在 Eureka 和 Consule 之外有了新的选择，开箱即用，上手简洁，暂时也没发现有太大的坑。 第一部分 Nacos的部署Nacos支持三种部署模式 1、单机模式：可用于测试和单机使用，生产环境切忌使用单机模式（满足不了高可用） 2、集群模式：可用于生产环境，确保高可用 3、多集群模式：可用于多数据中心场景 第二部分 多环境实践开发环境、测试环境、预发环境、正式环境 DataId方案设置启动参数spring.profiles.active，设置不同的环境dev、test； Namespace方案参考文献及资料1、vue项目文档，链接：https://www.cnblogs.com/crazymakercircle/p/14231815.html]]></content>
      <categories>
        <category>Vue</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Vue学习笔记]]></title>
    <url>%2F2021%2F07%2F31%2F2021-07-31-Vue%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发、测试环境项目部署 第二部分 项目打包 第三部分 生产环境部署 第四部分 总结 参考文献及资料 背景第一部分 第一个案例12345678910111213141516171819202122&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="app"&gt;&#123;&#123;message&#125;&#125;&lt;/div&gt;&lt;script src="https://cdn.jsdelivr.net/npm/vue@2.6.14"&gt;&lt;/script&gt;&lt;script&gt; var vm = new Vue(&#123; el: "#app", data: &#123; message: "Hello" &#125; &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 第二部分 组件2.1 v-bind绑定 1234567891011121314151617&lt;div id="app"&gt; &lt;span v-bind:title="messageTest"&gt; 鼠标悬停 &lt;/span&gt;&lt;/div&gt;&lt;script src="https://cdn.jsdelivr.net/npm/vue@2.6.14"&gt;&lt;/script&gt;&lt;script&gt; var vm = new Vue(&#123; el: "#app", data: &#123; //messageTest: "页面加载与"+ new Date().toLocaleString() messageTest: "test" &#125; &#125;);&lt;/script&gt; 2.2 判断循环（v-if、v-else、v-else-if）12345678910111213&lt;div id="app"&gt; &lt;h1 v-if="ok"&gt;Yes&lt;/h1&gt; &lt;h1 v-else&gt;No&lt;/h1&gt;&lt;/div&gt;&lt;script src="https://cdn.jsdelivr.net/npm/vue@2.6.14"&gt;&lt;/script&gt;&lt;script&gt; var vm = new Vue(&#123; el: "#app", data: &#123; ok: true &#125; &#125;);&lt;/script&gt; 表达式： 1234567891011121314&lt;div id="app"&gt; &lt;h1 v-if="type==='A'"&gt;A&lt;/h1&gt; &lt;h1 v-else-if="type==='B'"&gt;B&lt;/h1&gt; &lt;h1 v-else&gt;C&lt;/h1&gt;&lt;/div&gt;&lt;script src="https://cdn.jsdelivr.net/npm/vue@2.6.14"&gt;&lt;/script&gt;&lt;script&gt; var vm = new Vue(&#123; el: "#app", data: &#123; type: "A" &#125; &#125;);&lt;/script&gt; 2.3 Vue.component123456789101112131415161718&lt;div id=&quot;app&quot;&gt; &lt;rongxiang v-for=&quot;item in items&quot; v-bind:mid=&quot;item&quot;&gt;&lt;/rongxiang&gt;&lt;/div&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue@2.6.14&quot;&gt;&lt;/script&gt;&lt;script&gt; Vue.component(&quot;rongxiang&quot;,&#123; props: [&quot;mid&quot;], template: &apos;&lt;li&gt;&#123;&#123;mid&#125;&#125;&lt;/li&gt;&apos; &#125;) var vm = new Vue(&#123; el: &quot;#app&quot;, data: &#123;items: [&quot;java&quot;,&quot;python&quot;,&quot;lua&quot;,&quot;scala&quot;]&#125;, &#125;);&lt;/script&gt; 第三部分 通信Axios参考文献及资料1、vue项目文档，链接：https://vuejs.org/]]></content>
      <categories>
        <category>Vue</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统中的su命令]]></title>
    <url>%2F2021%2F06%2F14%2F2021-06-14-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84su%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 su命令 第二部分 su 和 su -区别 参考文献及资料 背景Linux操作系统是多用户多任务操作系统。多用户多任务就是可以在操作系统上建立多个用户，多个用户可以在同一时间内登录并执行各自不同的任务，互不影响。不同用户具有不同的权限，每个用户是在权限允许的范围内完成不同的任务，Linux正是通过这种权限的划分与管理，实现了多用户多任务的运行机制。 在日常运维中，su命令是最简单的用户切换命令，通过该命令可以实现任何用户身份的切换（包括从普通用户切换为 root 用户、从 root 用户切换为普通用户以及普通用户之间的切换）。普通用户之间切换以及普通用户切换至 root 用户，需要目标用户密钥，只有正确输入密钥，才能实现切换；从 root 用户切换至其他用户，无需知晓对方密钥，直接可切换成功。 第一部分 su命令su 命令的基本格式如下： 1root@VM-0-5-ubuntu:~# su [选项] 用户名 参数选项： -：当前用户不仅切换为指定用户的身份，同时所用的工作环境也切换为此用户的环境（包括 PATH 变量、MAIL 变量等）。使用 - 选项可省略用户名，默认会切换为 root 用户。 -l：同-的使用类似，也就是在切换用户身份的同时，完整切换工作环境，但后面需要添加欲切换的使用者账号。 -p：表示切换为指定用户的身份，但不改变当前的工作环境（不使用切换用户的配置文件）。 -m：和 -p 一样； -c 命令：仅切换用户执行一次命令，执行后自动切换回来，该选项后通常会带有要执行的命令。 第二部分 su 和 su -区别在实际运维使用中，经常踩的坑就是 su 和 su -的区别了。运维人员通常认为两者是相同，或者不知道 su -。 事实上，有-和没有 -是完全不同的，-选项表示在切换用户身份的同时，连当前使用的环境变量也切换成指定用户的。环境变量是用来定义操作系统环境的，因此如果系统环境没有随用户身份切换，很多命令无法正确执行。 初学者可以这样理解它们之间的区别，即有 - 选项，切换用户身份更彻底；反之，只切换了一部分。在不使用 su -的情况下，虽然用户身份成功切换，但环境变量依旧用的是原用户的，切换并不完整。 参考文献及资料1、Su Command in Linux (Switch User)，链接：https://linuxize.com/post/su-command-in-linux/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka安全用户管理API总结]]></title>
    <url>%2F2021%2F06%2F14%2F2021-07-11-Kafka%E5%AE%89%E5%85%A8%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86API%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 su命令 第二部分 su 和 su -区别 参考文献及资料 背景https://github.com/strimzi/strimzi-kafka-oauth/blob/1a4bddca665f01d2728860c91cea64b7dd2c5785/oauth-keycloak-authorizer/src/main/java/io/strimzi/kafka/oauth/server/authorizer/KeycloakRBACAuthorizer.java 参考文献及资料1、Su Command in Linux (Switch User)，链接：https://linuxize.com/post/su-command-in-linux/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink Table API & SQL编程总结]]></title>
    <url>%2F2021%2F06%2F14%2F2021-06-23-Flink%20Table%20API%20%26%20SQL%E7%BC%96%E7%A8%8B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 su命令 第二部分 su 和 su -区别 参考文献及资料 背景Apache Flink提供了两种顶层的关系型API，分别为Table API和SQL，Flink通过Table API&amp;SQL实现了批流统一。其中Table API是用于Scala和Java的语言集成查询API，它允许以非常直观的方式组合关系运算符（例如select，where和join）的查询。Flink SQL基于Apache Calcite 实现了标准的SQL，用户可以使用标准的SQL处理数据集。Table API和SQL与Flink的DataStream和DataSet API紧密集成在一起，用户可以实现相互转化，比如可以将DataStream或者DataSet注册为table进行操作数据。值得注意的是，Table API and SQL目前尚未完全完善，还在积极的开发中，所以并不是所有的算子操作都可以通过其实现。 依赖从Flink1.9开始，Flink为Table &amp; SQL API提供了两种planner,分别为Blink planner和old planner，其中old planner是在Flink1.9之前的版本使用。主要区别如下： 尖叫提示：对于生产环境，目前推荐使用old planner. flink-table-common: 通用模块，包含 Flink Planner 和 Blink Planner 一些共用的代码 flink-table-api-java: java语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用) flink-table-api-scala: scala语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用) flink-table-api-java-bridge: java语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用) flink-table-api-scala-bridge: scala语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用) flink-table-planner:planner 和runtime. planner为Flink1,9之前的old planner(推荐使用) flink-table-planner-blink: 新的Blink planner. flink-table-runtime-blink: 新的Blink runtime. flink-table-uber: 将上述的API模块及old planner打成一个jar包，形如flink-table-*.jar，位与/lib目录下 flink-table-uber-blink:将上述的API模块及Blink 模块打成一个jar包，形如fflink-table-blink-*.jar，位与/lib目录下 Blink planner &amp; old plannerBlink planner和old planner有许多不同的特点，具体列举如下： Blink planner将批处理作业看做是流处理作业的特例。所以，不支持Table 与DataSet之间的转换，批处理的作业也不会被转成DataSet程序，而是被转为DataStream程序。 Blink planner不支持 BatchTableSource，使用的是有界的StreamTableSource。 Blink planner仅支持新的 Catalog，不支持ExternalCatalog (已过时)。 对于FilterableTableSource的实现，两种Planner是不同的。old planner会谓词下推到PlannerExpression(未来会被移除)，而Blink planner 会谓词下推到 Expression(表示一个产生计算结果的逻辑树)。 仅仅Blink planner支持key-value形式的配置，即通过Configuration进行参数设置。 关于PlannerConfig的实现，两种planner有所不同。 Blink planner 会将多个sink优化成一个DAG(仅支持TableEnvironment，StreamTableEnvironment不支持)，old planner总是将每一个sink优化成一个新的DAG，每一个DAG都是相互独立的。 old planner不支持catalog统计，Blink planner支持catalog统计。 第一部分 Flink Table &amp; SQL1.1 程序的pom依赖根据使用的语言不同，可以选择下面的依赖，包括scala版和java版，如下： 1234567891011121314&lt;!-- java版 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-java-bridge_2.11&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;!-- scala版 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 除此之外，如果需要在本地的IDE中运行Table API &amp; SQL的程序，则需要添加下面的pom依赖： 1234567891011121314&lt;!-- Flink 1.9之前的old planner --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_2.11&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;!-- 新的Blink planner --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner-blink_2.11&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 另外，如果需要实现自定义的格式(比如和kafka交互)或者用户自定义函数，需要添加如下依赖： 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; Table API &amp; SQL的编程模板所有的Table API&amp;SQL的程序(无论是批处理还是流处理)都有着相同的形式，下面将给出通用的编程结构形式： 1234567891011121314// 创建一个TableEnvironment对象，指定planner、处理模式(batch、streaming)TableEnvironment tableEnv = ...; // 创建一个表tableEnv.connect(...).createTemporaryTable("table1");// 注册一个外部的表tableEnv.connect(...).createTemporaryTable("outputTable");// 通过Table API的查询创建一个Table 对象Table tapiResult = tableEnv.from("table1").select(...);// 通过SQL查询的查询创建一个Table 对象Table sqlResult = tableEnv.sqlQuery("SELECT ... FROM table1 ... ");// 将结果写入TableSinktapiResult.insertInto("outputTable");// 执行tableEnv.execute("java_job"); 注意：Table API &amp; SQL的查询可以相互集成，另外还可以在DataStream或者DataSet中使用Table API &amp; SQL的API，实现DataStreams、 DataSet与Table之间的相互转换。 创建TableEnvironmentTableEnvironment是Table API &amp; SQL程序的一个入口，主要包括如下的功能： 在内部的catalog中注册Table 注册catalog 加载可插拔模块 执行SQL查询 注册用户定义函数 DataStream 、DataSet与Table之间的相互转换 持有对ExecutionEnvironment 、StreamExecutionEnvironment的引用 一个Table必定属于一个具体的TableEnvironment，不可以将不同TableEnvironment的表放在一起使用(比如join，union等操作)。 TableEnvironment是通过调用 BatchTableEnvironment.create() 或者StreamTableEnvironment.create()的静态方法进行创建的。另外，默认两个planner的jar包都存在与classpath下，所有需要明确指定使用的planner。 1234567891011121314151617181920212223242526272829303132333435363738394041// **********************// FLINK 流处理查询// **********************import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.java.StreamTableEnvironment;EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment();StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings);//或者TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);// ******************// FLINK 批处理查询// ******************import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.table.api.java.BatchTableEnvironment;ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment();BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv);// **********************// BLINK 流处理查询// **********************import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.java.StreamTableEnvironment;StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);// 或者 TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);// ******************// BLINK 批处理查询// ******************import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.TableEnvironment;EnvironmentSettings bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings); 在catalog中创建表临时表与永久表表可以分为临时表和永久表两种，其中永久表需要一个catalog(比如Hive的Metastore)俩维护表的元数据信息，一旦永久表被创建，只要连接到该catalog就可以访问该表，只有显示删除永久表，该表才可以被删除。临时表的生命周期是Flink Session，这些表不能够被其他的Flink Session访问，这些表不属于任何的catalog或者数据库，如果与临时表相对应的数据库被删除了，该临时表也不会被删除。 创建表虚表(Virtual Tables)一个Table对象相当于SQL中的视图(虚表)，它封装了一个逻辑执行计划，可以通过一个catalog创建，具体如下： 123456// 获取一个TableEnvironmentTableEnvironment tableEnv = ...; // table对象，查询的结果集Table projTable = tableEnv.from("X").select(...);// 注册一个表，名称为 "projectedTable"tableEnv.createTemporaryView("projectedTable", projTable); 外部数据源表(Connector Tables)可以把外部的数据源注册成表，比如可以读取MySQL数据库数据、Kafka数据等 123456tableEnvironment .connect(...) .withFormat(...) .withSchema(...) .inAppendMode() .createTemporaryTable("MyTable") 扩展创建表的标识属性表的注册总是包含三部分标识属性：catalog、数据库、表名。用户可以在内部设置一个catalog和一个数据库作为当前的catalog和数据库，所以对于catalog和数据库这两个标识属性是可选的，即如果不指定，默认使用的是“current catalog”和 “current database”。 1234567891011121314151617181920212223TableEnvironment tEnv = ...;tEnv.useCatalog("custom_catalog");//设置catalogtEnv.useDatabase("custom_database");//设置数据库Table table = ...;// 注册一个名为exampleView的视图，catalog名为custom_catalog// 数据库的名为custom_databasetableEnv.createTemporaryView("exampleView", table);// 注册一个名为exampleView的视图，catalog的名为custom_catalog// 数据库的名为other_databasetableEnv.createTemporaryView("other_database.exampleView", table); // 注册一个名为'View'的视图，catalog的名称为custom_catalog// 数据库的名为custom_database，'View'是保留关键字，需要使用``(反引号)tableEnv.createTemporaryView("`View`", table);// 注册一个名为example.View的视图，catalog的名为custom_catalog，// 数据库名为custom_databasetableEnv.createTemporaryView("`example.View`", table);// 注册一个名为'exampleView'的视图， catalog的名为'other_catalog'// 数据库名为other_database' tableEnv.createTemporaryView("other_catalog.other_database.exampleView", table); 查询表Table APITable API是一个集成Scala与Java语言的查询API，与SQL相比，它的查询不是一个标准的SQL语句，而是由一步一步的操作组成的。如下展示了一个使用Table API实现一个简单的聚合查询。 1234567891011// 获取TableEnvironmentTableEnvironment tableEnv = ...;//注册Orders表// 查询注册的表Table orders = tableEnv.from("Orders");// 计算操作Table revenue = orders .filter("cCountry === 'FRANCE'") .groupBy("cID, cName") .select("cID, cName, revenue.sum AS revSum"); SQLFlink SQL依赖于Apache Calcite，其实现了标准的SQL语法，如下案例： 12345678910111213141516171819202122// 获取TableEnvironmentTableEnvironment tableEnv = ...;//注册Orders表// 计算逻辑同上面的Table APITable revenue = tableEnv.sqlQuery( "SELECT cID, cName, SUM(revenue) AS revSum " + "FROM Orders " + "WHERE cCountry = 'FRANCE' " + "GROUP BY cID, cName" );// 注册"RevenueFrance"外部输出表// 计算结果插入"RevenueFrance"表tableEnv.sqlUpdate( "INSERT INTO RevenueFrance " + "SELECT cID, cName, SUM(revenue) AS revSum " + "FROM Orders " + "WHERE cCountry = 'FRANCE' " + "GROUP BY cID, cName" ); 输出表一个表通过将其写入到TableSink，然后进行输出。TableSink是一个通用的支持多种文件格式(CSV、Parquet, Avro)和多种外部存储系统(JDBC, Apache HBase, Apache Cassandra, Elasticsearch)以及多种消息对列(Apache Kafka, RabbitMQ)的接口。 批处理的表只能被写入到 BatchTableSink,流处理的表需要指明AppendStreamTableSink、RetractStreamTableSink或者 UpsertStreamTableSink 123456789101112131415161718// 获取TableEnvironmentTableEnvironment tableEnv = ...;// 创建输出表final Schema schema = new Schema() .field("a", DataTypes.INT()) .field("b", DataTypes.STRING()) .field("c", DataTypes.LONG());tableEnv.connect(new FileSystem("/path/to/file")) .withFormat(new Csv().fieldDelimiter('|').deriveSchema()) .withSchema(schema) .createTemporaryTable("CsvSinkTable");// 计算结果表Table result = ...// 输出结果表到注册的TableSinkresult.insertInto("CsvSinkTable"); Table API &amp; SQL底层的转换与执行上文提到了Flink提供了两种planner，分别为old planner和Blink planner，对于不同的planner而言，Table API &amp; SQL底层的执行与转换是有所不同的。 Old planner根据是流处理作业还是批处理作业，Table API &amp;SQL会被转换成DataStream或者DataSet程序。一个查询在内部表示为一个逻辑查询计划，会被转换为两个阶段: 1.逻辑查询计划优化 2.转换成DataStream或者DataSet程序 上面的两个阶段只有下面的操作被执行时才会被执行： 当一个表被输出到TableSink时，比如调用了Table.insertInto()方法 当执行更新查询时，比如调用TableEnvironment.sqlUpdate()方法 当一个表被转换为DataStream或者DataSet时 一旦执行上述两个阶段，Table API &amp; SQL的操作会被看做是普通的DataStream或者DataSet程序，所以当StreamExecutionEnvironment.execute()或者ExecutionEnvironment.execute() 被调用时，会执行转换后的程序。 Blink planner无论是批处理作业还是流处理作业，如果使用的是Blink planner，底层都会被转换为DataStream程序。在一个查询在内部表示为一个逻辑查询计划，会被转换成两个阶段： 1.逻辑查询计划优化 2.转换成DataStream程序 对于TableEnvironment and StreamTableEnvironment而言，一个查询的转换是不同的 首先对于TableEnvironment，当TableEnvironment.execute()方法执行时，Table API &amp; SQL的查询才会被转换，因为TableEnvironment会将多个sink优化为一个DAG。 对于StreamTableEnvironment，转换发生的时间与old planner相同。 与DataStream &amp; DataSet API集成对于Old planner与Blink planner而言，只要是流处理的操作，都可以与DataStream API集成，仅仅只有Old planner才可以与DataSet API集成，由于Blink planner的批处理作业会被转换成DataStream程序，所以不能够与DataSet API集成。值得注意的是，下面提到的table与DataSet之间的转换仅适用于Old planner。 Table API &amp; SQL的查询很容易与DataStream或者DataSet程序集成，并可以将Table API &amp; SQL的查询嵌入DataStream或者DataSet程序中。DataStream或者DataSet可以转换成表，反之，表也可以被转换成DataStream或者DataSet。 从DataStream或者DataSet中注册临时表(视图)尖叫提示：只能将DataStream或者DataSet转换为临时表(视图) 下面演示DataStream的转换，对于DataSet的转换类似。 1234567// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...// 将DataStream注册为一个名为myTable的视图，其中字段分别为"f0", "f1"tableEnv.createTemporaryView("myTable", stream);// 将DataStream注册为一个名为myTable2的视图,其中字段分别为"myLong", "myString"tableEnv.createTemporaryView("myTable2", stream, "myLong, myString"); 将DataStream或者DataSet转化为Table对象可以直接将DataStream或者DataSet转换为Table对象，之后可以使用Table API进行查询操作。下面演示DataStream的转换，对于DataSet的转换类似。 1234567// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...// 将DataStream转换为Table对象，默认的字段为"f0", "f1"Table table1 = tableEnv.fromDataStream(stream);// 将DataStream转换为Table对象，默认的字段为"myLong", "myString"Table table2 = tableEnv.fromDataStream(stream, "myLong, myString"); 将表转换为DataStream或者DataSet当将Table转为DataStream或者DataSet时，需要指定DataStream或者DataSet的数据类型。通常最方便的数据类型是row类型，Flink提供了很多的数据类型供用户选择，具体包括Row、POJO、样例类、Tuple和原子类型。 将表转换为DataStream一个流处理查询的结果是动态变化的，所以将表转为DataStream时需要指定一个更新模式，共有两种模式：Append Mode和Retract Mode。 Append Mode 如果动态表仅只有Insert操作，即之前输出的结果不会被更新，则使用该模式。如果更新或删除操作使用追加模式会失败报错 Retract Mode 始终可以使用此模式。返回值是boolean类型。它用true或false来标记数据的插入和撤回，返回true代表数据插入，false代表数据的撤回。 1234567891011121314151617// 获取StreamTableEnvironment. StreamTableEnvironment tableEnv = ...; // 包含两个字段的表(String name, Integer age)Table table = ...// 将表转为DataStream，使用Append Mode追加模式，数据类型为RowDataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);// 将表转为DataStream，使用Append Mode追加模式，数据类型为定义好的TypeInformationTupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = new TupleTypeInfo&lt;&gt;( Types.STRING(), Types.INT());DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toAppendStream(table, tupleType);// 将表转为DataStream，使用的模式为Retract Mode撤回模式，类型为Row// 对于转换后的DataStream&lt;Tuple2&lt;Boolean, X&gt;&gt;，X表示流的数据类型，// boolean值表示数据改变的类型，其中INSERT返回true，DELETE返回的是falseDataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = tableEnv.toRetractStream(table, Row.class); 将表转换为DataSet123456789101112// 获取BatchTableEnvironmentBatchTableEnvironment tableEnv = BatchTableEnvironment.create(env);// 包含两个字段的表(String name, Integer age)Table table = ...// 将表转为DataSet数据类型为RowDataSet&lt;Row&gt; dsRow = tableEnv.toDataSet(table, Row.class);// 将表转为DataSet，通过TypeInformation定义Tuple2&lt;String, Integer&gt;数据类型TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = new TupleTypeInfo&lt;&gt;( Types.STRING(), Types.INT());DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toDataSet(table, tupleType); 表的Schema与数据类型之间的映射表的Schema与数据类型之间的映射有两种方式：分别是基于字段下标位置的映射和基于字段名称的映射。 基于字段下标位置的映射该方式是按照字段的顺序进行一一映射，使用方式如下： 123456789// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...// 将DataStream转为表，默认的字段名为"f0"和"f1"Table table = tableEnv.fromDataStream(stream);// 将DataStream转为表，选取tuple的第一个元素，指定一个名为"myLong"的字段名Table table = tableEnv.fromDataStream(stream, "myLong");// 将DataStream转为表，为tuple的第一个元素指定名为"myLong"，为第二个元素指定myInt的字段名Table table = tableEnv.fromDataStream(stream, "myLong, myInt"); 基于字段名称的映射基于字段名称的映射方式支持任意的数据类型包括POJO类型，可以很灵活地定义表Schema映射，所有的字段被映射成一个具体的字段名称，同时也可以使用”as”为字段起一个别名。其中Tuple元素的第一个元素为f0,第二个元素为f1，以此类推。 1234567891011// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...// 将DataStream转为表，默认的字段名为"f0"和"f1"Table table = tableEnv.fromDataStream(stream);// 将DataStream转为表，选择tuple的第二个元素，指定一个名为"f1"的字段名Table table = tableEnv.fromDataStream(stream, "f1");// 将DataStream转为表，交换字段的顺序Table table = tableEnv.fromDataStream(stream, "f1, f0");// 将DataStream转为表，交换字段的顺序，并为f1起别名为"myInt"，为f0起别名为"myLongTable table = tableEnv.fromDataStream(stream, "f1 as myInt, f0 as myLong"); 原子类型Flink将Integer, Double, String或者普通的类型称之为原子类型，一个数据类型为原子类型的DataStream或者DataSet可以被转成单个字段属性的表，这个字段的类型与DataStream或者DataSet的数据类型一致，这个字段的名称可以进行指定。 12345678//获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; // 数据类型为原子类型LongDataStream&lt;Long&gt; stream = ...// 将DataStream转为表，默认的字段名为"f0"Table table = tableEnv.fromDataStream(stream);// 将DataStream转为表，指定字段名为myLong"Table table = tableEnv.fromDataStream(stream, "myLong"); Tuple类型Tuple类型的DataStream或者DataSet都可以转为表，可以重新设定表的字段名(即根据tuple元素的位置进行一一映射，转为表之后，每个元素都有一个别名)，如果不为字段指定名称，则使用默认的名称(java语言默认的是f0,f1,scala默认的是_1),用户也可以重新排列字段的顺序，并为每个字段起一个别名。 1234567891011121314// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; //Tuple2&lt;Long, String&gt;类型的DataStreamDataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...// 将DataStream转为表，默认的字段名为 "f0", "f1"Table table = tableEnv.fromDataStream(stream);// 将DataStream转为表，指定字段名为 "myLong", "myString"(按照Tuple元素的顺序位置)Table table = tableEnv.fromDataStream(stream, "myLong, myString");// 将DataStream转为表，指定字段名为 "f0", "f1"，并且交换顺序Table table = tableEnv.fromDataStream(stream, "f1, f0");// 将DataStream转为表，只选择Tuple的第二个元素，指定字段名为"f1"Table table = tableEnv.fromDataStream(stream, "f1");// 将DataStream转为表，为Tuple的第二个元素指定别名为myString，为第一个元素指定字段名为myLongTable table = tableEnv.fromDataStream(stream, "f1 as 'myString', f0 as 'myLong'"); POJO类型当将POJO类型的DataStream或者DataSet转为表时，如果不指定表名，则默认使用的是POJO字段本身的名称，原始字段名称的映射需要指定原始字段的名称，可以为其起一个别名，也可以调换字段的顺序，也可以只选择部分的字段。 123456789101112// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; //数据类型为Person的POJO类型，字段包括"name"和"age"DataStream&lt;Person&gt; stream = ...// 将DataStream转为表，默认的字段名称为"age", "name"Table table = tableEnv.fromDataStream(stream);// 将DataStream转为表，为"age"字段指定别名myAge, 为"name"字段指定别名myNameTable table = tableEnv.fromDataStream(stream, "age as myAge, name as myName");// 将DataStream转为表，只选择一个name字段Table table = tableEnv.fromDataStream(stream, "name");// 将DataStream转为表，只选择一个name字段，并起一个别名myNameTable table = tableEnv.fromDataStream(stream, "name as myName"); Row类型Row类型的DataStream或者DataSet转为表的过程中，可以根据字段的位置或者字段名称进行映射，同时也可以为字段起一个别名，或者只选择部分字段。 1234567891011121314// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; // Row类型的DataStream，通过RowTypeInfo指定两个字段"name"和"age"DataStream&lt;Row&gt; stream = ...// 将DataStream转为表，默认的字段名为原始字段名"name"和"age"Table table = tableEnv.fromDataStream(stream);// 将DataStream转为表，根据位置映射，为第一个字段指定myName别名，为第二个字段指定myAge别名Table table = tableEnv.fromDataStream(stream, "myName, myAge");// 将DataStream转为表，根据字段名映射，为name字段起别名myName，为age字段起别名myAgeTable table = tableEnv.fromDataStream(stream, "name as myName, age as myAge");// 将DataStream转为表，根据字段名映射，只选择name字段Table table = tableEnv.fromDataStream(stream, "name");// 将DataStream转为表，根据字段名映射，只选择name字段，并起一个别名"myName"Table table = tableEnv.fromDataStream(stream, "name as myName"); 查询优化Old plannerApache Flink利用Apache Calcite来优化和转换查询。当前执行的优化包括投影和过滤器下推，去相关子查询以及其他类型的查询重写。Old Planner目前不支持优化JOIN的顺序，而是按照查询中定义的顺序执行它们。 通过提供一个CalciteConfig对象，可以调整在不同阶段应用的优化规则集。这可通过调用CalciteConfig.createBuilder()方法来进行创建，并通过调用tableEnv.getConfig.setPlannerConfig(calciteConfig)方法将该对象传递给TableEnvironment。 Blink plannerApache Flink利用并扩展了Apache Calcite来执行复杂的查询优化。这包括一系列基于规则和基于成本的优化(cost_based)，例如： 基于Apache Calcite的去相关子查询 投影裁剪 分区裁剪 过滤器谓词下推 过滤器下推 子计划重复数据删除以避免重复计算 特殊的子查询重写，包括两个部分： 将IN和EXISTS转换为左半联接( left semi-join) 将NOT IN和NOT EXISTS转换为left anti-join 调整join的顺序，需要启用 table.optimizer.join-reorder-enabled 注意： IN / EXISTS / NOT IN / NOT EXISTS当前仅在子查询重写的结合条件下受支持。 查询优化器不仅基于计划，而且还可以基于数据源的统计信息以及每个操作的细粒度开销(例如io，cpu，网络和内存）,从而做出更加明智且合理的优化决策。 高级用户可以通过CalciteConfig对象提供自定义优化规则，通过调用tableEnv.getConfig.setPlannerConfig(calciteConfig)，将参数传递给TableEnvironment。 查看执行计划SQL语言支持通过explain来查看某条SQL的执行计划，Flink Table API也可以通过调用explain()方法来查看具体的执行计划。该方法返回一个字符串用来描述三个部分计划，分别为： 关系查询的抽象语法树，即未优化的逻辑查询计划， 优化的逻辑查询计划 实际执行计划 123456789101112StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.fromElements(new Tuple2&lt;&gt;(1, "hello"));DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.fromElements(new Tuple2&lt;&gt;(1, "hello"));Table table1 = tEnv.fromDataStream(stream1, "count, word");Table table2 = tEnv.fromDataStream(stream2, "count, word");Table table = table1 .where("LIKE(word, 'F%')") .unionAll(table2);// 查看执行计划String explanation = tEnv.explain(table);System.out.println(explanation); 执行计划的结果为： 123456789101112131415161718192021222324252627282930== 抽象语法树 ==LogicalUnion(all=[true]) LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')]) FlinkLogicalDataStreamScan(id=[1], fields=[count, word]) FlinkLogicalDataStreamScan(id=[2], fields=[count, word])== 优化的逻辑执行计划 ==DataStreamUnion(all=[true], union all=[count, word]) DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')]) DataStreamScan(id=[1], fields=[count, word]) DataStreamScan(id=[2], fields=[count, word])== 物理执行计划 ==Stage 1 : Data Source content : collect elements with CollectionInputFormatStage 2 : Data Source content : collect elements with CollectionInputFormat Stage 3 : Operator content : from: (count, word) ship_strategy : REBALANCE Stage 4 : Operator content : where: (LIKE(word, _UTF-16LE'F%')), select: (count, word) ship_strategy : FORWARD Stage 5 : Operator content : from: (count, word) ship_strategy : REBALANCE 小结本文主要介绍了Flink TableAPI &amp;SQL，首先介绍了Flink Table API &amp;SQL的基本概念 ，然后介绍了构建Flink Table API &amp; SQL程序所需要的依赖，接着介绍了Flink的两种planner，还介绍了如何注册表以及DataStream、DataSet与表的相互转换，最后介绍了Flink的两种planner对应的查询优化并给出了一个查看执行计划的案例。 参考文献及资料1、Su Command in Linux (Switch User)，链接：https://linuxize.com/post/su-command-in-linux/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[RestFul接口规范总结]]></title>
    <url>%2F2021%2F06%2F14%2F2021-06-24-RestFul%E6%8E%A5%E5%8F%A3%E8%A7%84%E8%8C%83%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 su命令 第二部分 su 和 su -区别 参考文献及资料 背景一、协议API与用户的通信协议，使用HTTP协议。有互联网交互需要使用HTTPS协议。 二、域名应该尽量将API部署在专用域名之下。 12&gt; http://api.example.com&gt; 或者放在放在主域名下。 12&gt; http://example.org/api&gt; 域名背后建议接口高可用部署。 三、版本将API的版本号放入URL，例如： 12&gt; http://api.example.com/v1&gt; 四、路径路径又称”终点”（endpoint），表示API的具体网址。在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的”集合”（collection），所以API中的名词也应该使用复数。 举例来说，有一个API提供动物园（zoo）的信息，还包括各种动物和雇员的信息，则它的路径应该设计成下面这样。 https://api.example.com/v1/zoos https://api.example.com/v1/animals https://api.example.com/v1/employees 五、HTTP动词对于资源的具体操作类型，由HTTP动词表示。常用的HTTP动词有下面五个（括号里是对应的SQL命令）。 GET（SELECT）：从服务器取出资源（一项或多项）。 POST（CREATE）：在服务器新建一个资源。 PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。 PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。 DELETE（DELETE）：从服务器删除资源。 HEAD：获取资源的元数据。 OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。 下面是一些例子。 GET /zoos：列出所有动物园 POST /zoos：新建一个动物园 GET /zoos/ID：获取某个指定动物园的信息 PUT /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息） PATCH /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息） DELETE /zoos/ID：删除某个动物园 GET /zoos/ID/animals：列出某个指定动物园的所有动物 DELETE /zoos/ID/animals/ID：删除某个指定动物园的指定动物 六、过滤信息如果记录数量很多，服务器不可能都将它们返回给用户。API应该提供参数，过滤返回结果。 下面是一些常见的参数。 ?limit=10：指定返回记录的数量 ?offset=10：指定返回记录的开始位置。 ?page=2&amp;per_page=100：指定第几页，以及每页的记录数。 ?sortby=name&amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。 ?animal_type_id=1：指定筛选条件 参数的设计允许存在冗余，即允许API路径和URL参数偶尔有重复。比如，GET /zoo/ID/animals 与 GET /animals?zoo_id=ID 的含义是相同的。 七、状态码服务器向用户返回的状态码和提示信息。 200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。 201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。 202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务） 204 NO CONTENT - [DELETE]：用户删除数据成功。 400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。 401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。 403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。 404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。 406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。 410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。 422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。 500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。 八、错误处理（Error handling）如果状态码是4xx，就应该向用户返回出错信息。一般来说，返回的信息中将error作为键名，出错信息作为键值即可。 1234&gt; &#123;&gt; error: "Invalid API key"&gt; &#125;&gt; 九、返回结果针对不同操作，服务器向用户返回的结果应该符合以下规范。 GET /collection：返回资源对象的列表（数组） GET /collection/resource：返回单个资源对象 POST /collection：返回新生成的资源对象 PUT /collection/resource：返回完整的资源对象 PATCH /collection/resource：返回完整的资源对象 DELETE /collection/resource：返回一个空文档 十、Hypermedia APIRESTful API最好做到Hypermedia，即返回结果中提供链接，连向其他API方法，使得用户不查文档，也知道下一步应该做什么。 比如，当用户向api.example.com的根目录发出请求，会得到这样一个文档。 1234567&gt; &#123;"link": &#123;&gt; "rel": "collection https://www.example.com/zoos",&gt; "href": "https://api.example.com/zoos",&gt; "title": "List of zoos",&gt; "type": "application/vnd.yourformat+json"&gt; &#125;&#125;&gt; 上面代码表示，文档中有一个link属性，用户读取这个属性就知道下一步该调用什么API了。rel表示这个API与当前网址的关系（collection关系，并给出该collection的网址），href表示API的路径，title表示API的标题，type表示返回类型。 Hypermedia API的设计被称为HATEOAS。例如Github的API就是这种设计，访问api.github.com会得到一个所有可用API的网址列表。 123456&gt; &#123;&gt; "current_user_url": "https://api.github.com/user",&gt; "authorizations_url": "https://api.github.com/authorizations",&gt; // ...&gt; &#125;&gt; 从上面可以看到，如果想获取当前用户的信息，应该去访问api.github.com/user，然后就得到了下面结果。 12345&gt; &#123;&gt; "message": "Requires authentication",&gt; "documentation_url": "https://developer.github.com/v3"&gt; &#125;&gt; 上面代码表示，服务器给出了提示信息，以及文档的网址。 十一、其他 服务器返回的数据格式，应该使用JSON，避免使用XML URI结尾不应包含（/） 正斜杠分隔符（/）必须用来指示层级关系 应使用连字符（ - ）来提高URI的可读性 不得在URI中使用下划线（_） URI路径中全都使用小写字母 参考文献及资料1、Su Command in Linux (Switch User)，链接：https://linuxize.com/post/su-command-in-linux/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink DataStream AP总结]]></title>
    <url>%2F2021%2F06%2F14%2F2021-07-11-Flink%20DataStream%20AP%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 su命令 第二部分 su 和 su -区别 参考文献及资料 背景第三部分 算子用户通过算子能将一个或多个 DataStream 转换成新的 DataStream，在应用程序中可以将多个数据转换算子合并成一个复杂的数据流拓扑。 这部分内容将描述 Flink DataStream API 中基本的数据转换API，数据转换后各种数据分区方式，以及算子的链接策略。 参考文献及资料1、Su Command in Linux (Switch User)，链接：https://linuxize.com/post/su-command-in-linux/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink链接Kafka总结]]></title>
    <url>%2F2021%2F06%2F14%2F2021-07-11-Flink%E9%93%BE%E6%8E%A5Kafka%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 su命令 第二部分 su 和 su -区别 参考文献及资料 背景Flink提供了Kafka connector用于消费/生产Apache Kafka topic的数据。Flink的Kafka consumer集成了checkpoint机制以提供精确一次的处理语义。在具体的实现过程中，Flink不依赖于Kafka内置的消费组位移管理，而是在内部自行记录和维护consumer的位移。 第一部分 依赖准备用户在使用时需要根据Kafka版本来选择相应的connector，如下表所示： Maven依赖 支持的最低Flink版本 Kafka客户端类名 说明 flink-connector-kafka-0.8_2.10 1.0.0 FlinkKafkaConsumer08、FlinkKafkaProducer08 使用的是Kafka老版本low-level consumer，即SimpleConsumer. Flink在内部会提交位移到Zookeeper flink-connector-kafka-0.9_2.10 1.0.0 FlinkKafkaConsumer09、FlinkKafkaProducer09 使用Kafka新版本consumer flink-connector-kafka-0.10_2.10 1.2.0 FlinkKafkaConsumer010、FlinkKafkaProducer010 支持使用Kafka 0.10.0.0版本新引入的内置时间戳信息 maven依赖配置： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.10_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 第二部分 消费者消费者的构造Flink kafka connector使用的consumer取决于用户使用的是老版本consumer还是新版本consumer，新旧两个版本对应的connector类名是不同的，分别是：FlinkKafkaConsumer09（或FlinkKafkaConsumer010）以及FlinkKafkaConsumer08。它们都支持同时消费多个topic。 该Connector的构造函数包含以下几个字段： 待消费的topic列表 key/value解序列化器，用于将字节数组形式的Kafka消息解序列化回对象 Kafka consumer的属性对象，常用的consumer属性包括：bootstrap.servers（新版本consumer专用）、zookeeper.connect（旧版本consumer专用）和group.id 123456Properties properties = new Properties();properties.setProperty("bootstrap.servers", "localhost:9092");// only required for Kafka 0.8properties.setProperty("zookeeper.connect", "localhost:2181");properties.setProperty("group.id", "test");DataStream&lt;String&gt; stream = env.addSource(new FlinkKafkaConsumer08&lt;&gt;("topic", new SimpleStringSchema(), properties)); 序列化Flink Kafka Consumer 需要知道如何将 Kafka 中的二进制数据转换为 Java 或者 Scala 对象。KafkaDeserializationSchema 允许用户指定这样的 schema，每条 Kafka 中的消息会调用 T deserialize(ConsumerRecord&lt;byte[], byte[]&gt; record) 反序列化。 为了方便使用，Flink 提供了以下几种 schemas： TypeInformationSerializationSchema（和 TypeInformationKeyValueSerializationSchema) 基于 Flink 的 TypeInformation 创建 schema。 如果该数据的读和写都发生在 Flink 中，那么这将是非常有用的。此 schema 是其他通用序列化方法的高性能 Flink 替代方案。 JsonDeserializationSchema（和 JSONKeyValueDeserializationSchema）将序列化的 JSON 转化为 ObjectNode 对象，可以使用 objectNode.get(&quot;field&quot;).as(Int/String/...)() 来访问某个字段。 KeyValue objectNode 包含一个含所有字段的 key 和 values 字段，以及一个可选的”metadata”字段，可以访问到消息的 offset、partition、topic 等信息。 AvroDeserializationSchema 使用静态提供的 schema 读取 Avro 格式的序列化数据。 它能够从 Avro 生成的类（AvroDeserializationSchema.forSpecific(...)）中推断出 schema，或者可以与 GenericRecords 一起使用手动提供的 schema（用 AvroDeserializationSchema.forGeneric(...)）。此反序列化 schema 要求序列化记录不能包含嵌入式架构！ 此模式还有一个版本，可以在 Confluent Schema Registry 中查找编写器的 schema（用于编写记录的 schema）。 使用这些反序列化 schema 记录将读取从 schema 注册表检索到的 schema 转换为静态提供的 schema（或者通过 ConfluentRegistryAvroDeserializationSchema.forGeneric(...) 或 ConfluentRegistryAvroDeserializationSchema.forSpecific(...)）。 要使用此反序列化 schema 必须添加以下依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-avro&lt;/artifactId&gt; &lt;version&gt;1.13.0&lt;/version&gt;&lt;/dependency&gt; 当遇到因一些原因而无法反序列化的损坏消息时，反序列化 schema 会返回 null，以允许 Flink Kafka 消费者悄悄地跳过损坏的消息。请注意，由于 consumer 的容错能力（请参阅下面的部分以获取更多详细信息），在损坏的消息上失败作业将使 consumer 尝试再次反序列化消息。因此，如果反序列化仍然失败，则 consumer 将在该损坏的消息上进入不间断重启和失败的循环。 容错性伴随着启用 Flink 的 checkpointing 后，Flink Kafka Consumer 将使用 topic 中的记录，并以一致的方式定期检查其所有 Kafka offset 和其他算子的状态。如果 Job 失败，Flink 会将流式程序恢复到最新 checkpoint 的状态，并从存储在 checkpoint 中的 offset 开始重新消费 Kafka 中的消息。 因此，设置 checkpoint 的间隔定义了程序在发生故障时最多需要返回多少。 为了使 Kafka Consumer 支持容错，需要在 执行环境 中启用拓扑的 checkpointing。 如果未启用 checkpoint，那么 Kafka consumer 将定期向 Zookeeper 提交 offset。 第二部分 生产者https://ci.apache.org/projects/flink/flink-docs-release-1.13/zh/docs/connectors/datastream/kafka/ 参考文献及资料1、Su Command in Linux (Switch User)，链接：https://linuxize.com/post/su-command-in-linux/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenResty简介及原理]]></title>
    <url>%2F2021%2F06%2F14%2F2021-07-17-OpenResty%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 su命令 第二部分 su 和 su -区别 参考文献及资料 背景Nginx有很多的特性和好处，但是在Nginx上开发成了一个难题,Nginx模块需要用C开发，而且必须符合一系列复杂的规则，最重要的用C开发模块必须要熟悉Nginx的源代码，使得开发者对其望而生畏。为了开发人员方便，所以有了一种整合了Nginx和lua的框架，那就是OpenResty，它帮我们实现了可以用lua的规范开发，随着系统架构的不断升级、优化，OpenResty在被广泛的应用。 2.OpenResty概念OpenResty(又称：ngx_openresty)是一个基于Nginx的可伸缩的Web平台，由中国人章亦春发起，提供了很多高质量的第三方模块。 OpenResty是一个强大的Web应用服务器，Web开发人员可以使用lua脚本语言调动Nginx支持的各种C以及Lua模块，更主要的是在性能方面，OpenResty可以快速的构造出足以胜任10K以上的并发连接响应的超高性能的Web应用系统。 二.OpenResty的运行原理 Nginx 采用的是 master-worker 模型，一个 master 进程管理多个 worker 进程，基本的事件处理都是放在 woker 中，master 负责一些全局初始化，以及对 worker 的管理。 OpenResty中，每个worker进程使用一个LuaVM，当请求被分配到worker时，将在这个LuaVM中创建一个coroutine协程，协程之间数据隔离，每个协程都具有独立的全局变量。 协程和多线程下的线程类似：有自己的堆栈，自己的局部变量，有自己的指令指针，但是和其他协程程序共享全局变量等信息。线程和协程的主要不同在于：多处理器的情况下，概念上来说多线程是同时运行多个线程，而协程是通过代码来完成协程的切换，任何时刻只有一个协程程序在运行。并且这个在运行的协程只有明确被要求挂起时才会被挂起OpenResty处理请求流程Nginx会把一个请求分成不同阶段，第三方模块可以根据自己的行为，挂在到不同阶段中以达到自身目的。OpenResty采用了同样的特性，不同阶段有着不同的处理行为。 三.OpenResty的优势首先我们选择使用OpenResty，其是由Nginx核心加很多第三方模块组成，其最大的亮点是默认集成了Lua开发环境，使得Nginx可以作为一个Web Server使用。借助于Nginx的事件驱动模型和非阻塞IO，可以实现高性能的Web应用程序。OpenResty提供了大量组件如Mysql、Redis、Memcached等等，使在Nginx上开发Web应用更方便更简单。目前在京东如实时价格、秒杀、动态服务、单品页、列表页等都在使用Nginx+Lua架构，其他公司如淘宝、去哪儿网等。 四.OpenResty的nginx架构的特点 Nginx采用多进程模式，对于每个worker进程都是独立的，因此不需要加锁，所以节省了锁带来的性能开销。采用独立的进程的好处在于worker进程之间相互不会影响，当一个进程退出后，其他进程依然工作，以保证服务不会终端。 Nginx采用异步非堵塞的方式去处理请求，异步非堵塞就是当一个线程调用出现阻塞而等待时，其他线程可以去处理其他任务。 多阶段处理基于 Nginx 使用的多模块设计思想，Nginx 将HTTP请求的处理过程划分为多个阶段。这样可以使一个HTTP请求的处理过程由很多模块参与处理，每个模块只专注于一个独立而简单的功能处理，可以使性能更好、更稳定，同时拥有更好的扩展性。 OpenResty在HTTP处理阶段基础上分别在Rewrite/Access阶段、Content阶段、Log阶段注册了自己的handler，加上系统初始阶段master的两个阶段，共11个阶段为Lua脚本提供处理介入的能力。下图描述了OpenResty可以使用的主要阶段： OpenResty将我们编写的Lua代码挂载到不同阶段进行处理，每个阶段分工明确，代码独立。 init_by_lua*：Master进程加载 Nginx 配置文件时运行，一般用来注册全局变量或者预加载Lua模块。 init_worker_by_lua*：每个worker进程启动时执行，通常用于定时拉取配置/数据或者进行后端服务的健康检查。 set_by_lua*：变量初始化。 rewrite_by_lua*:可以实现复杂的转发、重定向逻辑。 access_by_lua*:IP准入、接口权限等情况集中处理。 content_by_lua*:内容处理器，接收请求处理并输出响应。 header_filter_by_lua*:响应头部或者cookie处理。 body_filter_by_lua*:对响应数据进行过滤，如截断或者替换。 log_by_lua*:会话完成后，本地异步完成日志记录。 五.Lua及其ngx_lua简介1.Lua Lua 是一个小巧的脚本语言。作者是巴西人。该语言的设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能 Lua的特点：Lua脚本可以很容易的被C/C++代码调用，也可以反过来调用C/C++的函数，这使得Lua在应用程序中可以被广泛应用。不仅仅作为扩展脚本，也可以作为普通的配置文件，代替XML,Ini等文件格式，并且更容易理解和维护。Lua由标准C编写而成，代码简洁优美，几乎在所有操作系统和平台上都可以编译，运行。一个完整的Lua解释器不过200k，在目前所有脚本引擎中，Lua的速度是最快的。这一切都决定了Lua是作为嵌入式脚本的最佳选择。 2.ngx_lua ngx_lua是将Lua嵌入Nginx，让Nginx执行Lua脚本，并且高并发、非阻塞的处理各种请求。Lua内建协程，可以很好的将异步回调转换成顺序调用的形式。ngx_lua在Lua中进行的IO操作都会委托给Nginx的事件模型，从而实现非阻塞调用。开发者可以采用串行的方式编写程序，ngx_lua会自动的在进行阻塞的IO操作中终端，保存上下文，然后将IO操作委托给Nginx事件处理机制，在IO操作完成后，ngx_lua会恢复上下文，程序继续执行，这些操作都是对用户程序透明的。 每个Nginx的worker进程持有一个Lua解释器或LuaJIT实例，被这个worker处理的所有请求共享这个实例。每个请求的context上下文会被Lua轻量级的协程分隔，从而保证各个请求时独立的。 3.ngx_lua模块的原理每个工作进程worker创建一个Lua虚拟机（LuaVM）,工作进程worker内部协议共享VM。每个Nginx I/O原语封装后注入Lua虚拟机，并允许Lua代码直接访问。每个外部请求都由一个Lua协程处理，协程之间数据隔离Lua代码调用I/O操作等异步时，会挂起当前协程，而不阻塞工作机进程。I/O等异步操作完成时，还原相关协程相关协议的上下文，并继续运行。 https://blog.csdn.net/even160941/article/details/97308725 参考文献及资料1、OpenResty 作者章亦春访谈实录，链接：https://www.cnblogs.com/zampo/p/4269147.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java中restful请求实现总结]]></title>
    <url>%2F2021%2F06%2F14%2F2021-07-17-Java%E4%B8%ADrestful%E8%AF%B7%E6%B1%82%E5%AE%9E%E7%8E%B0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 su命令 第二部分 su 和 su -区别 参考文献及资料 背景第一部分 restful请求第二部分 HttpURLConnection方式第三部分 HttpClient第四部分 Spring参考文献及资料1、Su Command in Linux (Switch User)，链接：https://linuxize.com/post/su-command-in-linux/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Nginx使用说明]]></title>
    <url>%2F2021%2F05%2F23%2F2021-05-23-Nginx%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 回本溯源 第二部分 HDFS大量小文件的危害 第三部分 小文件治理方案总结 第四部分 总结 参考文献及资料 背景多进程 12345root 55976 1 0 Jul03 ? 00:00:00 nginx: master process nginx -p /usr/local/orange -c /usr/local/orange/conf/nginx.confnobody 55977 55976 0 Jul03 ? 00:00:01 nginx: worker process nobody 55978 55976 0 Jul03 ? 00:00:00 nginx: worker process nobody 55979 55976 0 Jul03 ? 00:00:00 nginx: worker process nobody 55980 55976 0 Jul03 ? 00:00:00 nginx: worker process 参考文献及资料[1] HDFS NameNode内存全景，链接：https://tech.meituan.com/2016/08/26/namenode.html]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Lua语言系列--语言基础]]></title>
    <url>%2F2021%2F05%2F23%2F2021-05-27-Lua%E8%AF%AD%E8%A8%80%E7%B3%BB%E5%88%97--%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Lua语言入门 第二部分 数值 第三部分 字符串 第四部分 表 第五部分 函数 第六部分 输入和输出 第七部分 知识补充 参考文献及资料 背景第一部分 Lua语言入门Lua是解释型语言。 1.1 程序段1.2 语法规范Lua语言中标识符（名称）由任意字母、数值和下划线组成的字符串，但是不能以数值开头。 Lua中关键字（保留字）： 逻辑运算关键字：and、 or、not 基本类型：function、table、nil 控制类：for、 while、do 、break、in、return、until、goto、repeat 逻辑变量：true、false if控制类：if、then 、else、elseif 变量作用域：local Lua语言对于大小写敏感。 Lua语言使用连字符--作为单行注释。多行注释为： 123--[[print("多行注释")--]] 1.3 全局变量Lua语言中，全局变量无需声明，可以直接使用。没有初始化的全局变量初始值为nil。 1.4 类型和值Lua语言属于动态语言。 lua语言中有8种基本类型。 nil（空） boolean（布尔） number（数值） string（字符串） userdata（用户数据） function（函数） thread（线程） table（表） 可以使用函数type来返回变量数据类型。注意type函数返回的是一个字符串。 1.6 练习第二部分 数值第三部分 字符串第四部分 表第五部分 函数第六部分 输入和输出第七部分 知识补充参考文献及资料[1]]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Yarn资源调度的]]></title>
    <url>%2F2021%2F05%2F15%2F2021-05-15-%E6%8F%AD%E5%BC%80HDFS%E5%AD%98%E5%82%A8%E7%9A%84%E9%9D%A2%E7%BA%B1%20-%20%E5%89%AF%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 回本溯源 第二部分 HDFS大量小文件的危害 第三部分 小文件治理方案总结 第四部分 总结 参考文献及资料 背景yarn.resourcemanager.store.class : 有三种StateStore，分别是基于zookeeper, HDFS, leveldb, HA高可用集群必须用ZKRMStateStore 存储 yarn.resourcemanager.store.class ZooKeeper org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore FileSystem org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore LevelDB org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore By default the number of completed applications stored in state store is 10000. https://maprdocs.mapr.com/51/ReferenceGuide/Default-YARN-Parameters.html Try to move/delete some completed applications hadoop fs -mv /var/mapr/cluster/yarn/rm/system/FSRMStateRoot/RMAppRoot/* /path_to_local_dir hadoop conf | grep yarn.resourcemanager.max-completed-applications https://www.programmersought.com/article/36321434084/ https://issues.apache.org/jira/browse/YARN-7150 https://my.oschina.net/dabird/blog/3089265 https://cloud.tencent.com/developer/article/1491079 https://my.oschina.net/dabird/blog/4273830 参考文献及资料[1] HDFS NameNode内存全景，链接：https://tech.meituan.com/2016/08/26/namenode.html]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[揭开HDFS存储的面纱]]></title>
    <url>%2F2021%2F05%2F15%2F2021-05-15-%E6%8F%AD%E5%BC%80HDFS%E5%AD%98%E5%82%A8%E7%9A%84%E9%9D%A2%E7%BA%B1%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 回本溯源 第二部分 HDFS大量小文件的危害 第三部分 小文件治理方案总结 第四部分 总结 参考文献及资料 背景https://blog.csdn.net/m0_37613244/article/details/109920466?utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control HDFS Namenode本地目录的存储结构和Datanode数据块存储目录结构，也就是hdfs-site.xml中配置的dfs.namenode.name.dir和dfs.namenode.data.dir 第一部分 NameNode元数据第二部分 DataNode数据You need to look in your hdfs-default.xml configuration file for the dfs.data.dir setting. The default setting is: ${hadoop.tmp.dir}/dfs/data and note that the ${hadoop.tmp.dir} is actually in core-default.xml described here. 参考文献及资料[1] HDFS NameNode内存全景，链接：https://tech.meituan.com/2016/08/26/namenode.html]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pyspark实现原理和源码分析]]></title>
    <url>%2F2021%2F05%2F06%2F2020-10-06-Pyspark%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Pyspark%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景https://mikolaje.github.io/2019/pyspark_slower.html https://blog.csdn.net/oTengYue/article/details/105379628 https://www.readfog.com/a/1631040025628086272 spark为了保证核心架构的统一性，在核心架构外围封装了一层python，spark的核心架构功能包括计算资源的申请，task的管理和分配， driver与executor之间的通信，executor之间的通信，rdd的载体等都是在基于JVM的 spark的这种设计可以说是非常方便的去进行多种开发语言的扩展。但是也可以明显看出与在jvm内部运行的udf相比，在python worker中执行udf时，额外增加了数据在executor jvm和pythin worker之间序列化、反序列化、及通信IO等损耗，并且在程序运行上python相比java的具有一定的性能劣势。在计算逻辑比重比较大的spark任务中，使用自定义udf的pyspark程序会明显有更多的性能损耗。当然在spark sql 中使用内置udf会降低或除去上述描述中产生的性能差异。 程序模型提交命令： 1234567[root@quickstart pysparkExample]# cat run.sh /usr/lib/spark/bin/spark-submit \--master yarn \--deploy-mode cluster \--archives hdfs:///user/admin/python/python3.5.2.zip \--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./python3.5.2.zip/conda/bin/python \test.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@quickstart pysparkExample]# ./run.sh21/05/16 00:02:32 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/172.17.0.2:803221/05/16 00:02:32 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers21/05/16 00:02:32 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (2816 MB per container)21/05/16 00:02:32 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead21/05/16 00:02:32 INFO yarn.Client: Setting up container launch context for our AM21/05/16 00:02:32 INFO yarn.Client: Setting up the launch environment for our AM container21/05/16 00:02:32 INFO yarn.Client: Preparing resources for our AM container21/05/16 00:02:33 INFO yarn.YarnSparkHadoopUtil: getting token for namenode: hdfs://quickstart.cloudera:8020/user/admin/.sparkStaging/application_1621088965108_000221/05/16 00:02:33 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 9 for admin on 172.17.0.2:802021/05/16 00:02:34 INFO hive.metastore: Trying to connect to metastore with URI thrift://quickstart.cloudera:908321/05/16 00:02:34 INFO hive.metastore: Opened a connection to metastore, current connections: 121/05/16 00:02:34 INFO hive.metastore: Connected to metastore.21/05/16 00:02:34 INFO hive.metastore: Closed a connection to metastore, current connections: 021/05/16 00:02:34 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/admin/python/python3.5.2.zip21/05/16 00:02:34 INFO yarn.Client: Uploading resource file:/home/pyspark/pysparkExample/test.py -&gt; hdfs://quickstart.cloudera:8020/user/admin/.sparkStaging/application_1621088965108_0002/test.py21/05/16 00:02:34 INFO yarn.Client: Uploading resource file:/tmp/spark-f65e84d5-0438-473c-9dff-03aeb95d4f18/__spark_conf__7787627711692930444.zip -&gt; hdfs://quickstart.cloudera:8020/user/admin/.sparkStaging/application_1621088965108_0002/__spark_conf__7787627711692930444.zip21/05/16 00:02:35 INFO spark.SecurityManager: Changing view acls to: root,admin21/05/16 00:02:35 INFO spark.SecurityManager: Changing modify acls to: root,admin21/05/16 00:02:35 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, admin); users with modify permissions: Set(root, admin)21/05/16 00:02:35 INFO yarn.Client: Submitting application 2 to ResourceManager21/05/16 00:02:35 INFO impl.YarnClientImpl: Submitted application application_1621088965108_000221/05/16 00:02:36 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)21/05/16 00:02:36 INFO yarn.Client: client token: Token &#123; kind: YARN_CLIENT_TOKEN, service: &#125; diagnostics: N/A ApplicationMaster host: N/A ApplicationMaster RPC port: -1 queue: root.admin start time: 1621094555087 final status: UNDEFINED tracking URL: http://quickstart.cloudera:8088/proxy/application_1621088965108_0002/ user: admin21/05/16 00:02:37 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)21/05/16 00:02:38 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)21/05/16 00:02:39 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)21/05/16 00:02:40 INFO yarn.Client: Application report for application_1621088965108_0002 (state: FINISHED)21/05/16 00:02:40 INFO yarn.Client: client token: Token &#123; kind: YARN_CLIENT_TOKEN, service: &#125; diagnostics: N/A ApplicationMaster host: 172.17.0.2 ApplicationMaster RPC port: 0 queue: root.admin start time: 1621094555087 final status: SUCCEEDED tracking URL: http://quickstart.cloudera:8088/proxy/application_1621088965108_0002/history/application_1621088965108_0002/1 user: admin21/05/16 00:02:40 INFO yarn.Client: Deleting staging directory .sparkStaging/application_1621088965108_000221/05/16 00:02:40 INFO util.ShutdownHookManager: Shutdown hook called21/05/16 00:02:40 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f65e84d5-0438-473c-9dff-03aeb95d4f18 yarn日志： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606121/05/15 16:19:29 INFO yarn.ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]21/05/15 16:19:30 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1621095402395_0001_00000121/05/15 16:19:30 INFO spark.SecurityManager: Changing view acls to: admin21/05/15 16:19:30 INFO spark.SecurityManager: Changing modify acls to: admin21/05/15 16:19:30 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)21/05/15 16:19:30 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread21/05/15 16:19:30 INFO yarn.ApplicationMaster: Waiting for spark context initialization21/05/15 16:19:30 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... 21/05/15 16:19:31 INFO spark.SparkContext: Running Spark version 1.6.021/05/15 16:19:31 INFO spark.SecurityManager: Changing view acls to: admin21/05/15 16:19:31 INFO spark.SecurityManager: Changing modify acls to: admin21/05/15 16:19:31 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)21/05/15 16:19:31 INFO util.Utils: Successfully started service 'sparkDriver' on port 46545.21/05/15 16:19:31 INFO slf4j.Slf4jLogger: Slf4jLogger started21/05/15 16:19:31 INFO Remoting: Starting remoting21/05/15 16:19:31 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.17.0.2:35921]21/05/15 16:19:31 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@172.17.0.2:35921]21/05/15 16:19:31 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 35921.21/05/15 16:19:31 INFO spark.SparkEnv: Registering MapOutputTracker21/05/15 16:19:31 INFO spark.SparkEnv: Registering BlockManagerMaster21/05/15 16:19:31 INFO storage.DiskBlockManager: Created local directory at /yarn/nm/usercache/admin/appcache/application_1621095402395_0001/blockmgr-edbfee4f-522d-4c06-81a0-5b83b750e88a21/05/15 16:19:31 INFO storage.MemoryStore: MemoryStore started with capacity 491.7 MB21/05/15 16:19:31 INFO spark.SparkEnv: Registering OutputCommitCoordinator21/05/15 16:19:31 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter21/05/15 16:19:31 INFO util.Utils: Successfully started service 'SparkUI' on port 43419.21/05/15 16:19:31 INFO ui.SparkUI: Started SparkUI at http://172.17.0.2:4341921/05/15 16:19:31 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler21/05/15 16:19:31 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40989.21/05/15 16:19:31 INFO netty.NettyBlockTransferService: Server created on 4098921/05/15 16:19:31 INFO storage.BlockManager: external shuffle service port = 733721/05/15 16:19:31 INFO storage.BlockManagerMaster: Trying to register BlockManager21/05/15 16:19:31 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.2:40989 with 491.7 MB RAM, BlockManagerId(driver, 172.17.0.2, 40989)21/05/15 16:19:31 INFO storage.BlockManagerMaster: Registered BlockManager21/05/15 16:19:32 INFO scheduler.EventLoggingListener: Logging events to hdfs://quickstart.cloudera:8020/user/spark/applicationHistory/application_1621095402395_0001_121/05/15 16:19:32 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.821/05/15 16:19:32 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done21/05/15 16:19:32 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@172.17.0.2:46545)21/05/15 16:19:32 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/172.17.0.2:803021/05/15 16:19:32 INFO yarn.YarnRMClient: Registering the ApplicationMaster21/05/15 16:19:32 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals21/05/15 16:19:32 INFO yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 021/05/15 16:19:32 INFO spark.SparkContext: Invoking stop() from shutdown hook21/05/15 16:19:32 INFO ui.SparkUI: Stopped Spark web UI at http://172.17.0.2:4341921/05/15 16:19:32 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors21/05/15 16:19:32 INFO cluster.YarnClusterSchedulerBackend: Asking each executor to shut down21/05/15 16:19:32 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!21/05/15 16:19:32 INFO storage.MemoryStore: MemoryStore cleared21/05/15 16:19:32 INFO storage.BlockManager: BlockManager stopped21/05/15 16:19:32 INFO storage.BlockManagerMaster: BlockManagerMaster stopped21/05/15 16:19:32 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!21/05/15 16:19:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.21/05/15 16:19:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.21/05/15 16:19:32 INFO spark.SparkContext: Successfully stopped SparkContext21/05/15 16:19:32 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED21/05/15 16:19:32 INFO Remoting: Remoting shut down21/05/15 16:19:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.21/05/15 16:19:32 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.21/05/15 16:19:32 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1621095402395_000121/05/15 16:19:32 INFO util.ShutdownHookManager: Shutdown hook called21/05/15 16:19:32 INFO util.ShutdownHookManager: Deleting directory /yarn/nm/usercache/admin/appcache/application_1621095402395_0001/spark-76c43a00-f562-4e4e-be1a-be3a2fcefc21/pyspark-b56e6390-71a2-4475-8ffe-4164798ab6c421/05/15 16:19:32 INFO util.ShutdownHookManager: Deleting directory /yarn/nm/usercache/admin/appcache/application_1621095402395_0001/spark-76c43a00-f562-4e4e-be1a-be3a2fcefc21 http://sharkdtu.com/posts/pyspark-internal.html https://cloud.tencent.com/developer/article/1589011 https://cloud.tencent.com/developer/article/1558621 https://www.nativex.com/cn/blog/2019-12-27-2/ 但是在大数据场景下，JVM和Python进程间频繁的数据通信导致其性能损耗较多，恶劣时还可能会直接卡死，所以建议对于大规模机器学习或者Streaming应用场景还是慎用PySpark，尽量使用原生的Scala/Java编写应用程序，对于中小规模数据量下的简单离线任务，可以使用PySpark快速部署提交。 pyspark与py4j线程模型简析https://www.jianshu.com/p/013fe44422c9 pyspark日志输出：12345678910from pyspark import SparkContextsc = SparkContext()log4jLogger = sc._jvm.org.apache.log4jLOGGER = log4jLogger.LogManager.getLogger('MYLOGGER')# same call as you'd make in java, just using the py4j methods to do soLOGGER.setLevel(log4jLogger.Level.WARN)# will no longer printLOGGER.info("pyspark script logger initialized") 对于Spark1.5.1版本（）， 12345678910from pyspark import SparkContextsc = SparkContext()log4jLogger = sc._jvm.org.apache.log4j.LoggerLOGGER = log4jLogger.getLogger('MYLOGGER')# same call as you'd make in java, just using the py4j methods to do soLOGGER.setLevel(log4jLogger.Level.WARN)# will no longer printLOGGER.info("pyspark script logger initialized") 参考文献及资料1、Job Scheduling，链接：https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark中的动态伸缩和反压机制]]></title>
    <url>%2F2021%2F05%2F02%2F2021-05-02-Spark%E4%B8%AD%E7%9A%84%E5%8A%A8%E6%80%81%E4%BC%B8%E7%BC%A9%E5%92%8C%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景https://fares.codes/posts/dynamic-scaling-and-backpressure/ 第一部分 建议采用以下做法以实现更好的自动缩放比例： 最好从相当大的集群和数量的执行程序开始，并在必要时进行缩减。（执行程序映射到YARN容器。） 执行者的数量应至少等于接收者的数量。 设置每个执行器的核心数，以使执行器具有一些多余的容量，这些容量超出了运行接收器所需的容量。 内核总数必须大于接收器数量；否则，应用程序将无法处理收到的数据。 设置spark.streaming.backpressure.enabled为true，则Spark Streaming可以控制接收速率（基于当前的批处理调度延迟和处理时间），以便系统仅以其可以处理的速度接收数据。 为了获得最佳性能，请考虑使用Kryo序列化程序在Spark数据的序列化表示和反序列化表示之间进行转换。这不是Spark的默认设置，但是您可以显式更改它：将spark.serializer属性设置 为org.apache.spark.serializer.KryoSerializer。 开发人员可以通过在不再需要DStream时取消缓存它们来减少内存消耗。 参考文献及资料1、Job Scheduling，链接：https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark on Yarn任务动态伸缩机制介绍]]></title>
    <url>%2F2021%2F05%2F02%2F2021-05-02-Spark%20on%20Yarn%E4%BB%BB%E5%8A%A1%E5%8A%A8%E6%80%81%E4%BC%B8%E7%BC%A9%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 配置实现 第二部分 动态配置原理和源码分析 第三部分 总结 参考文献及资料 背景Spark默认使用的是资源预分配的模式。即在任务运行之前，需要提前指定任务运行需要的资源量。但是在实际线上生产环境使用过程就存在资源浪费和不足的问题，特别是Spark Streaming类型的任务。例如很多日志数据在一天中量并不是均匀分布的，而是一个“双驼峰”。对于预分配模式，就存在日志峰值期间，运算资源不足导致数据处理的延迟，而在日志低峰时期存在资源闲置却无法释放（特别是资源管理器粗粒度模式）。使得生产线上环境资源未能高效使用。 Spark在Spark 1.2版本后，对于Spark On Yarn模式，开始支持动态资源分配（Dynamic Resource Allocation，后文我们也简称DRA）。该机制下Spark Core和Spark Streaming任务就可以根据Application的负载情况，动态的增加和减少Executors。 第一部分 配置实现对于Spark on Yarn模式需要提前配置Yarn服务，主要是配置External shuffle service（Spark 1.2开始引入）。Spark计算需要shuffle时候，每个Executor 需要把上一个 stage 的 mapper 输出写入磁盘，然后作为 server 等待下一个stage 的reducer 来获取 map 的输出。因此如果 Executor 在 map 阶段完成后被回收，reducer 将无法找到 block的位置。所以开启 Dynamic Resource Allocation 时，必须开启 External shuffle service。这样，mapper 的输出位置（元数据信息）将会由 External shuffle service（长期运行的守护进程） 来登记保存，Executor 不需要再保留状态信息，可以安全回收。 1.1 Yarn服务配置首先需要对Yarn的NodeManager服务进行配置，使其支持Spark的Shuffle Service。 修改每台NodeManager上的配置文件yarn-site.xml： 12345678910111213&lt;!--修改和增加--&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle,spark_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;spark.shuffle.service.port&lt;/name&gt; &lt;value&gt;7337&lt;/value&gt;&lt;/property&gt; 配置服务依赖包。将$SPARK_HOME/lib/spark-1.6.0-yarn-shuffle.jar（注意实际版本号）复制到每台NodeManager的${HADOOP_HOME}/share/hadoop/yarn/lib/下。 重启所有NodeManager生效配置调整。 1.2 Spark core 任务配置1.2.1 配置方法通常配置Saprk应用任务的参数有三种方式： 修改配置文件spark-defaults.conf，全局生效； 配置文件位置：$SPARK_HOME/conf/spark-defaults.conf，具体参数如下： 123456789101112//启用External shuffle Service服务spark.shuffle.service.enabled true//Shuffle Service服务端口，必须和yarn-site中的一致spark.shuffle.service.port 7337//开启动态资源分配spark.dynamicAllocation.enabled true//每个Application最小分配的executor数spark.dynamicAllocation.minExecutors 1//每个Application最大并发分配的executor数spark.dynamicAllocation.maxExecutors 30spark.dynamicAllocation.schedulerBacklogTimeout 1sspark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s spark-submit 命令配置，个性化生效； 参考下面的案例： 12345678910111213spark-submit --master yarn-cluster \ --driver-cores 2 \ --driver-memory 2G \ --num-executors 10 \ --executor-cores 5 \ --executor-memory 2G \ --conf spark.dynamicAllocation.enabled=true \ --conf spark.shuffle.service.enabled=true \ --conf spark.dynamicAllocation.minExecutors=5 \ --conf spark.dynamicAllocation.maxExecutors=30 \ --conf spark.dynamicAllocation.initialExecutors=10 --class com.spark.sql.jdbc.SparkDFtoOracle2 \ Spark-hive-sql-Dataframe-0.0.1-SNAPSHOT-jar-with-dependencies.jar 代码中配置，个性化生效； 参考下面的scala代码案例： 123456val conf: SparkConf = new SparkConf()conf.set("spark.dynamicAllocation.enabled", true);conf.set("spark.shuffle.service.enabled", true);conf.set("spark.dynamicAllocation.minExecutors", "5");conf.set("spark.dynamicAllocation.maxExecutors", "30");conf.set("spark.dynamicAllocation.initialExecutors", "10"); 接下来我们介绍详细的参数含义。 1.2.2 配置说明 Property Name Default Meaning Since Version spark.dynamicAllocation.enabled false Whether to use dynamic resource allocation, which scales the number of executors registered with this application up and down based on the workload. For more detail, see the description here. This requires spark.shuffle.service.enabled or spark.dynamicAllocation.shuffleTracking.enabled to be set. The following configurations are also relevant: spark.dynamicAllocation.minExecutors, spark.dynamicAllocation.maxExecutors, and spark.dynamicAllocation.initialExecutors spark.dynamicAllocation.executorAllocationRatio 1.2.0 spark.dynamicAllocation.executorIdleTimeout 60s If dynamic allocation is enabled and an executor has been idle for more than this duration, the executor will be removed. For more detail, see this description. 1.2.0 spark.dynamicAllocation.cachedExecutorIdleTimeout infinity If dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration, the executor will be removed. For more details, see this description. 1.4.0 spark.dynamicAllocation.initialExecutors spark.dynamicAllocation.minExecutors Initial number of executors to run if dynamic allocation is enabled. If --num-executors (or spark.executor.instances) is set and larger than this value, it will be used as the initial number of executors. 1.3.0 spark.dynamicAllocation.maxExecutors infinity Upper bound for the number of executors if dynamic allocation is enabled. 1.2.0 spark.dynamicAllocation.minExecutors 0 Lower bound for the number of executors if dynamic allocation is enabled. 1.2.0 spark.dynamicAllocation.executorAllocationRatio 1 By default, the dynamic allocation will request enough executors to maximize the parallelism according to the number of tasks to process. While this minimizes the latency of the job, with small tasks this setting can waste a lot of resources due to executor allocation overhead, as some executor might not even do any work. This setting allows to set a ratio that will be used to reduce the number of executors w.r.t. full parallelism. Defaults to 1.0 to give maximum parallelism. 0.5 will divide the target number of executors by 2 The target number of executors computed by the dynamicAllocation can still be overridden by the spark.dynamicAllocation.minExecutors and spark.dynamicAllocation.maxExecutors settings 2.4.0 spark.dynamicAllocation.schedulerBacklogTimeout 1s If dynamic allocation is enabled and there have been pending tasks backlogged for more than this duration, new executors will be requested. For more detail, see this description. 1.2.0 spark.dynamicAllocation.sustainedSchedulerBacklogTimeout schedulerBacklogTimeout Same as spark.dynamicAllocation.schedulerBacklogTimeout, but used only for subsequent executor requests. For more detail, see this description. 1.2.0 spark.dynamicAllocation.shuffleTracking.enabled false Experimental. Enables shuffle file tracking for executors, which allows dynamic allocation without the need for an external shuffle service. This option will try to keep alive executors that are storing shuffle data for active jobs. 3.0.0 spark.dynamicAllocation.shuffleTracking.timeout infinity When shuffle tracking is enabled, controls the timeout for executors that are holding shuffle data. The default value means that Spark will rely on the shuffles being garbage collected to be able to release executors. If for some reason garbage collection is not cleaning up shuffles quickly enough, this option can be used to control when to time out executors even when they are storing shuffle data. 3.0.0 1.3 Spark Streaming 任务配置对于Spark Streaming 流处理任务，Spark官方并未在文档中给出介绍。Dynamic Resource Allocation配置指引如下： 必要配置（Spark 3.0.0） 123456# 开启Spark Streaming流处理动态资源分配参数开关（默认关闭）spark.streaming.dynamicAllocation.enabled=true# 设置最大和最小的Executor数量spark.streaming.dynamicAllocation.minExecutors=1（必须正整数）spark.streaming.dynamicAllocation.maxExecutors=50（必须正整数，默认Int.MaxValue，即无限大） 可选配置（Spark 3.0.0） 这些参数可以不用配置，都已经提供了一个较为合理的默认值。 123spark.streaming.dynamicAllocation.scalingUpRatio（必须正数，默认0.9）spark.streaming.dynamicAllocation.scalingInterval（单位秒，默认60）spark.streaming.dynamicAllocation.scalingDownRatio（必须正数，默认0.3） 第二部分 动态配置原理和源码分析介绍完使用配置后，接下来将详细介绍实现原理。以便理解各参数的含义和参数调优。 2.1 Spark Core任务为了动态伸缩Spark任务的计算资源（Executor为基本分配单位），首先需要确定的度量是任务的繁忙程度。DRA机制将Spark任务是否有挂起任务(pending task)作为判断标准，一旦有挂起任务表示当前的Executor数量不够支撑所有的task并行运行，所以会申请增加资源。 2.1.1 资源请求（Request）策略当Spark任务开启DRA机制，SparkContext会启动后台ExecutorAllocationManager，用来管理集群的Executors。 1234567891011121314151617//package org.apache.spark SparkContext.scalaval dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf) _executorAllocationManager = if (dynamicAllocationEnabled) &#123; schedulerBackend match &#123; case b: ExecutorAllocationClient =&gt; Some(new ExecutorAllocationManager( schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf, cleaner = cleaner, resourceProfileManager = resourceProfileManager)) case _ =&gt; None &#125; &#125; else &#123; None &#125; _executorAllocationManager.foreach(_.start()) Start()方法将ExecutorAllocationListener加入到listenerBus中，ExecutorAllocationListener通过监听listenerBus里的事件，动态添加，删除Executor。并且通过Thread不断添加Executor，遍历Executor，将超时的Executor杀掉并移除。 Spark会周期性（intervalMillis=100毫秒）计算实际需要的Executor的最大数量maxNeeded。公式如下。 12val maxNeeded = math.ceil(numRunningOrPendingTasks * executorAllocationRatio / tasksPerExecutor).toInt 逻辑代码： 12345678910111213141516private def updateAndSyncNumExecutorsTarget(now: Long): Int = synchronized &#123; if (initializing) &#123; 0 &#125; else &#123; val updatesNeeded = new mutable.HashMap[Int, ExecutorAllocationManager.TargetNumUpdates] numExecutorsTargetPerResourceProfileId.foreach &#123; case (rpId, targetExecs) =&gt; val maxNeeded = maxNumExecutorsNeededPerResourceProfile(rpId) if (maxNeeded &lt; targetExecs) &#123; decrementExecutorsFromTarget(maxNeeded, rpId, updatesNeeded) &#125; else if (addTime != NOT_SET &amp;&amp; now &gt;= addTime) &#123; addExecutorsToTarget(maxNeeded, rpId, updatesNeeded) &#125; &#125; doUpdateRequest(updatesNeeded.toMap, now) &#125;&#125; 当集群中有Executor出现pending task，计算判断条件maxNeeded &gt; targetExecs，并且等待时间超过schedulerBacklogTimeout(默认1s)，则会触发方法addExecutorsToTarget(maxNeeded, rpId, updatesNeeded)。对于首次增加Executor。 1spark.dynamicAllocation.schedulerBacklogTimeout = 1s（秒） 后续按照周期性时间sustainedSchedulerBacklogTimeout来检测pending task，一旦出现pending task，即触发增加Executor。 1spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = 1s(秒) 每次（轮）触发增加Executor资源请求，增加的数量翻倍，即是一个指数数列（2的n次方），例如：1、2、4、8。 2.1.2 资源释放（Remove）策略对于移除策略如下： 如果Executor闲置（maxNeeded &lt; targetExecs）时间超过以下参数，并且executor中没有cache（数据缓存在内存），则spark应用将会释放该Executor。 1spark.dynamicAllocation.executorIdleTimeout（单位为秒） 默认60s 如果空闲Executor中有cache，那么这个超时参数为： 1spark.dynamicAllocation.cachedExecutorIdleTimeout 默认值：Integer.MAX_VALUE（即永不超时） 对于Executor的退出，设计上需要考虑状态的问题，主要： 需要移除的Executor存在cache。 如果需要移除的Executor含有RDD cache。这时候超时时间为整型最大值（相当于无限）。 123456private[spark] val DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT = ConfigBuilder("spark.dynamicAllocation.cachedExecutorIdleTimeout") .version("1.4.0") .timeConf(TimeUnit.SECONDS) .checkValue(_ &gt;= 0L, "Timeout must be &gt;= 0.") .createWithDefault(Integer.MAX_VALUE) Shuffle状态的保存问题。如果需要移除的Executor包含了Shuffle状态数据（在shuffle期间，Spark executor先要将map的输出写入到磁盘，然后该executor充当一个文件服务器，将这些文件共享给其他的executor访问）。需要提前启动External shuffle service，由专门外置服务提供存储，Executor中不再负责保存，架构上功能解耦。 另外添加和移除Executor之后，需要告知DAGSchedule进行相关信息更新。 2.1.3 配置建议Spark的动态伸缩机制的几点建议： 给Executor数量设置一个合理的伸缩区间，即[minExecutors-maxExecutors]区间值。 配置资源粒度较小的Executor，例如CPU数量为3-4个。动态伸缩的最小伸缩单位是单个Executor，如果出现资源伸缩，特别是Executor数目下降后业务量突增，新申请资源未就绪，已有的Executor就可能由于任务过载而导致集群崩溃。 如果程序中有shuffle,例如(reduce,groupBy),建议设置一个合理的并行数，避免杀掉过多的Executors。 对于每个Stage持续时间很短的应用，不适合动态伸缩机制。这样会频繁增加和移除Executors，造成系统颠簸。特别是在 Spark on Yarn模式下资源的申请处理速度并不快。 2.2 Spark Streaming 任务Spark Streaming任务可以看成连续运行的微（micro-batch）批任务，如果直接套用Spark Core的动态伸缩机制就水土不服了。一般一个微批任务较短（默认60秒），实际线上任务可能更小，动态伸缩的反应时间较长（特别是on Yarn模式），一个微批任务结束，动态伸缩策略还没生效。所以针对Spark Streaming任务，项目组设计新的动态机制（Spark 2.0.0 版本引入）。 提案：https://issues.apache.org/jira/browse/SPARK-12133 2.2.1 源码分析Spark Streaming任务会统计微批任务运行时间的延迟时间，最朴素的想法就是按照这个度量指标来作为动态伸缩的触发指标。这部分源码在org.apache.spark.streaming.scheduler中： 周期性计算微批运行完成的平均时间，然后和batch interval进行比较； 这里的周期大小由参数spark.streaming.dynamicAllocation.scalingInterval决定，大小为scalingIntervalSecs * 1000。例如默认值为：60*1000毫秒，即60秒。 通过streamingListener计算微批平均处理时间（averageBatchProcTime），然后计算微批处理率（ratio，微批平均处理时间/微批处理周期）。 然后和参数值上限（scalingUpRatio）和下限（scalingDownRatio）进行比较。详细控制函数如下： 123456789101112131415161718private def manageAllocation(): Unit = synchronized &#123; logInfo(s"Managing executor allocation with ratios = [$scalingUpRatio, $scalingDownRatio]") if (batchProcTimeCount &gt; 0) &#123; val averageBatchProcTime = batchProcTimeSum / batchProcTimeCount val ratio = averageBatchProcTime.toDouble / batchDurationMs logInfo(s"Average: $averageBatchProcTime, ratio = $ratio" ) if (ratio &gt;= scalingUpRatio) &#123; logDebug("Requesting executors") val numNewExecutors = math.max(math.round(ratio).toInt, 1) requestExecutors(numNewExecutors) &#125; else if (ratio &lt;= scalingDownRatio) &#123; logDebug("Killing executors") killExecutor() &#125; &#125; batchProcTimeSum = 0 batchProcTimeCount = 0 &#125; 增加Executor数量；如果ratio &gt;= scalingUpRatio，然后按照下面的公司增加数量： 1val numNewExecutors = math.max(math.round(ratio).toInt, 1) 例如ratio=1.6&gt;0.9(scalingUpRatio)，这时候说明有大量微批任务出现了延迟，按照公式计算numNewExecutors=2。接下来会调用requestExecutors(numNewExecutors)方法去申请2个Executor。 减少Executor数量；如果ratio &lt;= scalingDownRatio，这直接调用killExecutor()方法（方法中判断没有receiver运行的Executor）去kill Executor。 2.2.2 配置建议Spark Streaming动态资源分配起作用前，需要至少完成一个Batch处理(batchProcTimeCount &gt; 0)。 Spark Core和Spark Streaming的动态配置开关配置是分别设置的。 如果两个配置开关同时配置为true，会抛出错误。建议如下配置： 12spark.dynamicAllocation.enabled=false （默认是false，可以不配置）spark.streaming.dynamicAllocation.enabled=true 第三部分 总结3.1 对比Spark Core中动态伸缩机制是基于空闲时间来控制回收Executor。而在Spark Streaming中，一个Executor每隔很短的时间都会有一批作业被调度，所以在streaming里面是基于平均每批作业处理的时间。 3.2 Structed Streaming任务动态伸缩在spark Streaming中，最小的可能延迟受限于每批的调度间隔以及任务启动时间。所以这不能满足更低延迟的需求。如果能够连续的处理，尤其是简单的处理而没有任何的阻塞操作。这种连续处理的架构可以使得端到端延迟最低降低到1ms级别，而不是目前的10-100ms级别，这就是Spark 2.2.0版本引入新的Spark流处理框架：Structed Streaming。 https://issues.apache.org/jira/browse/SPARK-20928 当然项目组自然也会考虑该框架的资源伸缩机制（未完成） https://issues.apache.org/jira/browse/SPARK-24815 后续趋势上看，Spark项目会将更多精力放在Structed Streaming。 3.3 Spark Streaming 背压机制为了应对Spark Streaming处理数据波动，除了资源动态伸缩机制，在Spark 1.5版本项目在Spark Streaming 中引入了的背压（Backpressure）机制。 Spark Streaming任务中，当batch的处理时间大于batch interval时，意味着数据处理速度跟不上数据接收速度。这时候在数据接收端(Receiver)Executor就会开始积压数据。如果数据存储采用MEMORY_ONLY模式（内存）就会导致OOM，采用MEMORY_AND_DISK多余的数据保存到磁盘上，增加数据IO时间。 背压（Backpressure）机制，通过动态控制数据接收速率来适配集群数据处理能力。这是被动防守型的应对，将数据缓存在Kafka消息层。如果数据持续保持高量级，就需要主动启停任务来增加计算资源。 参考文献及资料1、Job Scheduling，链接：https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup 2、About Spark Streaming，链接：https://www.turbofei.wang/spark/2019/05/26/about-spark-streaming]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Nginx常见使用场景总结]]></title>
    <url>%2F2021%2F04%2F15%2F2021-05-30-Nginx%E5%B8%B8%E8%A7%81%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景 HTTP服务器（含动静分离） 负载均衡 反向代理 正向代理 跨域请求 第一部分 HTTP服务器（含动静分离）Nginx本身是一个静态资源的服务器，当只有静态资源的时候，就可以使用Nginx来做服务器，如下，我们使用Nginx来部署一个打包好的vue项目 12345678#vue项目server&#123; listen 8081; #监听端口 server_name 209.250.235.145; root /app/vue/dist/; # 我们的资源在服务器中的路径 index index.html; #指定资源的入口文件&#125;复制代码 完成后我们nginx -s reload一下，然后访问209.250.235.145:8081，只要路径没错静态资源就访问的到了 第二部分 正向代理第三部分 反向代理反向代理应该是Nginx做的最多的一件事了，什么是反向代理呢，以下是百度百科的说法：反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。简单来说就是真实的服务器不能直接被外部网络访问，所以需要一台代理服务器，而代理服务器能被外部网络访问的同时又跟真实服务器在同一个网络环境，当然也可能是同一台服务器，端口不同而已。 下面贴上一段简单的实现反向代理的代码 server{ listen80; server_namelocalhost; client_max_body_size1024M; location/{ proxy_passhttp://localhost:8080; proxy_set_headerHost$host:$server_port; } } 保存配置文件后启动Nginx，这样当我们访问localhost的时候，就相当于访问localhost:8080了 第四部分 负载均衡在线上生产环境，为了承载较大流量，通常需要以集群方式并发处理，这就需要有代理服务对流量进行智能负载。通过算法将流量合理的分配给集群中各个处理节点。实现方式有硬件和软件两种，硬件常见的是F5专用设备，成本较高。如果流量不大可以由软件来实现。 而Nginx就是常见的软负载组件。利用upstream定义集群服务器。负载均衡配置一般都需要同时配置反向代理，通过反向代理跳转到负载均衡。 Nginx目前支持自带3种负载均衡策略，还有2种常用的第三方策略。 4.1 配置首先需要在http节点中，配置upstream，例如： 1234upstream upstreamTest &#123; server 192.168.88.1:9200; server 192.168.88.2:9200; &#125; 将server节点下的location节点中的proxy_pass配置为：http:// + upstream名称，即 12345location / &#123; root html; index index.html index.htm; proxy_pass http://upstreamTest; &#125; 4.2 负载模式4.2.1 轮询 （round-robin）（默认方式）轮询为负载均衡中最为朴素的算法，不需要配置额外参数。假设共有N台服务器，算法将遍历服务器节点列表，并按节点次序每轮选择一台服务器处理请求。当所有节点均被调用过一次后，算法将从第一个节点开始重新一轮遍历。如果列表中服务有下宕的，算法能主动将服务器从轮询列表中去除。 这个算法前提需要服务器列表中每台服务器的处理能力是均衡的，否则会有分配不均的问题。 配置案例： 1234upstream upstreamTest &#123; server 192.168.88.1:9200; server 192.168.88.2:9200; &#125; 4.2.2 加权轮询但后端负载集群性能不均的时候，可以通过加权方式分配流量，这就是加权轮询。例如： 1234upstream upstreamTest &#123; server 192.168.88.1:9200 weight=5; server 192.168.88.2:9200 weight=10; &#125; 上面的配置给每一台服务指定了weight值，weight 的值越大意味着该服务器的性能越好，可以承载更多的请求。也可以从概率角度去理解，192.168.88.2的流量分配概率比192.168.88.1大一倍。 4.2.3 IP 哈希（IP hash）轮询和加权轮询，每次访问后端是随机不同的机器，对于一些场景就不太适应。当程序有状态的时候，例如采用了session保存数据，把登录信息保存到了session中，那么跳转到另外一台服务器的时候就需要重新登录。所以这时候需要原IP客户端固定访问同一台服务器。 ip hash函数将每个请求按访问ip的hash结果分配，同一个IP客户端访问的负载后端服务不变。配置如下： 12345upstream upstreamTest &#123; ip_hash; server 192.168.88.1:9200; server 192.168.88.2:9200; &#125; 4.2.4 fair（第三方）对于上面的负载算法没有动态的考虑服务器的性能变化。fair算法根据负载后端服务器的响应时间来动态分配请求，响应时间短优先分配流量。配置参考： 12345upstream upstreamTest &#123; fair; server 192.168.88.1:9200; server 192.168.88.2:9200; &#125; 4.2.5 url_hash（第三方）按訪问url的hash结果来分配请求，使每一个url定向到同一个后端服务器。后端服务器为缓存时有效。静态资源缓存,节约存储，加快速度。配置参考： 123456upstream upstreamTest &#123; server 192.168.88.1:9200; server 192.168.88.2:9200; hash $request_uri; hash_method crc32;&#125; 其中hash_method crc32配置为指定hash算法为crc32。 4.3 补充upstream还能够为每一个设备设置状态值，这些状态值的含义分别例如以下： down 后端节点不参与负载； max_fails和fail_timeout Nginx基于连接探测，如果发现后端异常，在单位周期为fail_timeout设置的时间，中达到max_fails次数，这个周期次数内，如果后端同一个节点不可用，那么接将把节点标记为不可用，并等待下一个周期（同样时常为fail_timeout）再一次去请求，判断是否连接是否成功。如果成功，将恢复之前的轮询方式，如果不可用将在下一个周期(fail_timeout)再试一次。 backup backup 不能和ip_hash一起使用，backup 参数是指当所有非备机都宕机或者不可用的情况下，就只能使用带backup标准的备机。 max_conns 允许最大连接数。 slow_start 当节点恢复，不立即加入 例如下面的案例： 123456upstream upstreamTest &#123; server 192.168.88.1:9200 down; server 192.168.88.2:9200 backup; server 192.168.88.3:9200 max_fails=2 fail_timeout=60s;&#125; 如上配置表明如果后端节点60秒内出现2次不可用情况，判定节点不可用。判定不可用后10秒内请求不会转发到此节点，直到60秒后重新检测节点健康情况。 第五部分 跨域请求前后端分离的项目中由于前后端项目分别部署到不同的服务器上，我们首先遇到的问题就是跨域，在这个场景我们下nginx可以帮助我们很好地解决这个问题 12345678910111213#跨域请求serverserver&#123; listen 9000; server_name 209.250.235.145; root /app/crossDomain/; index index.html; location /douban/ &#123; #添加访问目录为/apis的代理配置 rewrite ^/douban/(.*)$ /$1 break; proxy_pass https://m.douban.com; &#125;&#125;复制代码 在我的服务器下我写了一个 index.html请求豆瓣接口，模拟跨域 123456789101112function nginxClick()&#123; $.ajax(&#123; url: '/douban/rexxar/api/v2/muzzy/columns/10018/items?start=0&amp;count=3', dataType: 'json', type: 'get', data: "", success:(res)=&gt;&#123; console.log(res) &#125; &#125;)&#125;复制代码 当我们访问点击请求时，匹配到location下的/douban/ 1rewrite ^/douban/(.*)$ /$1 break;复制代码 这段配置将请求路径重写为/rexxar/api/v2/muzzy/columns/10018/items?start=0&amp;count=3，其中$1代表正则模糊匹配到的第一个参数， 1proxy_pass https://m.douban.com;复制代码 这段配置是将请求域名代理到豆瓣的域名下面，所以从本地服务器发出去的请求将被重新重写为： https://m.douban.com/rexxar/api/v2/muzzy/columns/10018/items?start=0&amp;count=3，我们就能拿到豆瓣api提供的数据。详情可以看看这篇[文章](https://www.jianshu.com/p/10ecc107b5ee) 演示地址：http://209.250.235.145:9000/ 参考文献及资料[1] Orange官网，链接：http://orange.sumory.com/ [2] Orange网关官网docker，链接：https://hub.docker.com/r/syhily/orange]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparkSql使用总结]]></title>
    <url>%2F2021%2F04%2F15%2F2021-06-06-sparkSql%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景https://blog.csdn.net/weixin_40035337/article/details/108018058 Spark SQL和Hive的继承select 语法 1234567SELECT [ALL | DISTINCT] attr_expr_list FROM table_reference[WHERE where_condition][GROUP BY col_name_list][ORDER BY col_name_list][ASC | DESC][CLUSTER BY col_name_list | DISTRIBUTE BY col_name_list][SORT BY col_name_list]][LIMIT number]; 内置函数数学函数日期函数字符串函数字符串连接： 函数 用途 返回值 描述 concat(string A,string B…) 字符串连接 string 连接多个字符串，合并为一个字符串，可以接受任意数量的输入字符串。 concat_ws(string SEP, string A,string B…) 字符串连接 string 连接多个字符串，字符串之间以指定的分隔符分隔。 ltrim(string A)、rtrim(string A)、trim(string A) 字符串处理 string 删除字符串空格，其他的空格保留。 regexp_extract(string subject, string pattern, int index) 正则提取 string 通过下标返回正则表达式指定的部分。regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 2) 返回：’bar.’ regexp_replace(string A, string B,string C) 正则替换 字符串A中的B字符被C字符替代 substring(string A,int start)、substring(string A,int start, int len) 字符串截取 从文本字符串A中截取指定的起始位置后的字符。 参考文献及资料[1] Orange官网，链接：http://orange.sumory.com/ [2] Orange网关官网docker，链接：https://hub.docker.com/r/syhily/orange]]></content>
      <categories>
        <category>orange</category>
      </categories>
      <tags>
        <tag>orange</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[suse操作系统rpm命令]]></title>
    <url>%2F2021%2F04%2F15%2F2021-05-30-suse%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9Frpm%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[背景RPM 安装操作 命令： rpm -i 需要安装的包文件名 举例如下： rpm -i example.rpm 安装 example.rpm 包； rpm -iv example.rpm 安装 example.rpm 包并在安装过程中显示正在安装的文件信息； rpm -ivh example.rpm 安装 example.rpm 包并在安装过程中显示正在安装的文件信息及安装进度； RPM 查询操作 命令： rpm -q … 附加查询命令： a 查询所有已经安装的包以下两个附加命令用于查询安装包的信息； i 显示安装包的信息； l 显示安装包中的所有文件被安装到哪些目录下； s 显示安装版中的所有文件状态及被安装到哪些目录下；以下两个附加命令用于指定需要查询的是安装包还是已安装后的文件； p 查询的是安装包的信息； f 查询的是已安装的某文件信息； 举例如下： rpm -qa | grep tomcat4 查看 tomcat4 是否被安装； rpm -qip example.rpm 查看 example.rpm 安装包的信息； rpm -qif /bin/df 查看/bin/df 文件所在安装包的信息； rpm -qlf /bin/df 查看/bin/df 文件所在安装包中的各个文件分别被安装到哪个目录下； RPM 卸载操作 命令： rpm -e 需要卸载的安装包 在卸载之前，通常需要使用rpm -q …命令查出需要卸载的安装包名称。 举例如下： rpm -e tomcat4 卸载 tomcat4 软件包 RPM 升级操作 命令： rpm -U 需要升级的包 举例如下： rpm -Uvh example.rpm 升级 example.rpm 软件包 RPM 验证操作 命令： rpm -V 需要验证的包 举例如下： rpm -Vf /etc/tomcat4/tomcat4.conf 输出信息类似如下： S.5….T c /etc/tomcat4/tomcat4.conf 其中，S 表示文件大小修改过，T 表示文件日期修改过。限于篇幅，更多的验证信息请您参考rpm 帮助文件：man rpm RPM 的其他附加命令 –force 强制操作 如强制安装删除等； –requires 显示该包的依赖关系； –nodeps 忽略依赖关系并继续操作； 参考文献及资料[1] Orange官网，链接：http://orange.sumory.com/ [2] Orange网关官网docker，链接：https://hub.docker.com/r/syhily/orange]]></content>
      <categories>
        <category>orange</category>
      </categories>
      <tags>
        <tag>orange</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka中序列化和反序列化总结]]></title>
    <url>%2F2021%2F04%2F15%2F2021-04-15-Kafka%E4%B8%AD%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景https://www.vijaykonnackal.com/protobuf-kafka-message/ https://blog.csdn.net/weixin_26717681/article/details/108499713#t6 https://codingharbour.com/apache-kafka/how-to-use-protobuf-with-apache-kafka-and-schema-registry/ https://codingharbour.com/apache-kafka/how-to-use-protobuf-with-apache-kafka-and-schema-registry/ https://data-flair.training/blogs/kafka-serialization-and-deserialization/ https://blog.csdn.net/weixin_40929150/article/details/88775559 参考文献及资料https://blog.csdn.net/shirukai/article/details/82152172 https://shirukai.github.io/blog/kafka-custom-message-serialization-and-deserialization-mode.html]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[orange网关原理和源码分析]]></title>
    <url>%2F2021%2F04%2F15%2F2021-05-29-orange%E7%BD%91%E5%85%B3%E5%8E%9F%E7%90%86%E7%9A%84%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[背景Orange 属于流量网关（Api Geteway），项目托管在Github（https://github.com/orlabs/orange）上，目前活跃度较弱（2年未更新）。通常将`orange`和另一个流行的网关项目`Kong`进行比较，其实`orange`大部分组件都是参考`Kong`实现的，但是活跃度远不及`Kong`。 orange和Kong本质上都是OpenResty 应用，Lua语言编写，Lua在葡萄牙语里代表美丽的月亮。Openresty和Lua语言的关系类似JVM虚拟机和Java语言的关系。OpenResty可以理解为一个集成了很多模块的定制版nginx。 本文以orange 0.7.0版本介绍项目的整体架构和原理，最后通过案例介绍如何研发实现一个orange插件。 第一部分 知识准备1.1 Openresty介绍Nginx组件已经被广泛应用在web应用架构中，例如：负载均衡、反向代理、代理缓存以及流量限流等场景。Nginx开发则需要C/C++语言，还需要了解底层接口，学习门槛较高。2009年，淘宝的章亦春和王晓哲一起设计了第二代的 OpenResty。在王晓哲的提议下，选择了小而美的脚本语言 lua 进行开发。这就是ngx_lua 模块，并且将Nginx核心、LuaJIT、ngx_lua模块、许多有用的Lua库和常用的第三方Nginx模块组合在一起成为OpenResty。 Lua脚本语言，不需要编译就可以执行，更加灵活。而且 OpenResty 还把 Lua 自身的协程与 Nginx 的事件机制完美结合在一起，优雅地实现了许多其他语言所没有的同步非阻塞编程范式，能够轻松开发出高性能的 Web 应用。 我们知道Nginx的配置文件是文件存储，修改后需要reload。而Lua 有代码热加载特性，不需要重启进程，就能够从磁盘、Redis 或者任何其他地方加载数据，随时替换内存里的代码片段。这就带来了“动态配置”，让 OpenResty 能够永不停机，在微秒、毫秒级别实现配置和业务逻辑的实时更新，比起 Nginx 秒级的重启是一个极大的进步。 OpenResty 还选用了LuaJIT作为 Lua 语言的运行时（Runtime），提升效率。LuaJIT 是一个高效的 Lua 虚拟机，支持 JIT（Just In Time）技术，可以把 Lua 代码即时编译成“本地机器码”，这样就消除了脚本语言解释运行的劣势，让Lua 脚本跑得和原生 C 代码一样快。 1.2 Openresty阶段式处理Nginx 处理请求的过程一共划分为 11 个阶段，按照执行顺序依次是 post-read、server-rewrite、find-config、rewrite、post-rewrite、preaccess、access、post-access、try-files、content 以及 log。 基于Nginx底层的阶段处理，OpenResty 也使用“流水线”来处理 HTTP 请求。Nginx 的处理流水线是由一个个 C 模块组成的，只能在静态文件里配置，开发困难，配置麻烦（相对而言）。而 OpenResty 的处理流水线则是由一个个的 Lua 脚本组成的，不仅可以从磁盘上加载，也可以从Redis、MySQL 里加载，而且编写、调试的过程非常方便快捷。 Nginx 把一个请求分成了很多阶段，这样第三方模块就可以根据自己行为，挂载到不同阶段进行处理达到目的。OpenResty 也应用了同样的特性。所不同的是，OpenResty 挂载的是我们编写的 Lua 代码。OpenResty 的阶段，比起 Nginx，OpenResty 的阶段更注重对 HTTP 请求响应报文的加工和处理。 ​ OpenResty 里有几个阶段与 Nginx 是相同的，比如 rewrite、access、content、filter，这些都是标准的 HTTP 处理。 ​ 在这几个阶段里可以用“xxx_by_lua”指令嵌入 Lua 代码，执行重定向跳转、访问控制、产生响应、负载均衡、过滤报文等功能。因为 Lua 的脚本语言特性，不用考虑内存分配、资源回收释放等底层的细节问题，可以专注于编写非常复杂的业务逻辑，比 C 模块的开发效率高很多，即易于扩展又易于维护。 第二部分版本是一个重构版本， 着重为了解决之前版本在有大量规则配置时性能损耗的问题。 基本的设计思路是将原来的规则细分成两层， 第一层叫做selector, 用于将流量进行第一步划分， 在进入某个selector后才按照之前的设计进行规则匹配， 匹配到后进行相关处理。 https://lengrongfu.github.io/2019/05/21/orange-%E5%8E%9F%E7%90%86/ https://book.aikaiyuan.com/openresty/understanding-orange.html https://zhuanlan.zhihu.com/p/67481992 网关优化项目https://github.com/starjiang/xorange orange运行后根据nginx.conf文件中location配置来进行顺序匹配流量。 123456789101112131415161718192021222324252627282930313233343536373839404142location / &#123; set $upstream_host $host; set $upstream_request_uri ''; set $upstream_url ''; set $upstream_scheme ''; set $target ''; rewrite_by_lua_block &#123; local orange = context.orange orange.redirect() orange.rewrite() &#125; access_by_lua_block &#123; local orange = context.orange orange.access() &#125; # proxy proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Scheme $scheme; proxy_pass $upstream_scheme$upstream_url$upstream_request_uri; header_filter_by_lua_block &#123; local orange = context.orange orange.header_filter() &#125; body_filter_by_lua_block &#123; local orange = context.orange orange.body_filter() &#125; log_by_lua_block &#123; local orange = context.orange orange.log() &#125;&#125; 第一部分 orange 中设计概念Nginx 的请求处理阶段共有 11 个之多，我们先介绍其中 3 个比较常见的。按照它们执行时的先后顺序，依次是 rewrite 阶段、access 阶段以及 content 阶段（后面我们还有机会见到其他更多的处理阶段）。 orange 缓存在nginx.conf文件中： 1234567891011lua_code_cache on;lua_shared_dict orange_data 20m; # should not removed. used for orange data, e.g. plugins configurations..lua_shared_dict status 1m; # used for global statistic, see plugin: statlua_shared_dict waf_status 1m; # used for waf statistic, see plugin: waflua_shared_dict monitor 10m; # used for url monitor statistic, see plugin: monitorlua_shared_dict rate_limit 10m; # used for rate limiting count, see plugin: rate_limitinglua_shared_dict property_rate_limiting 10m; # used for rate limiting count, see plugin: rate_limitinglua_shared_dict consul_upstream 5m; # used for consul_upstream, see plugin consul_balancerlua_shared_dict consul_upstream_watch 5m; # used for consul_upstream_watch, consul_balancer 这些配置是插件缓存数据。 orange中数据持久化orange中插件基本构成选择器（selector）规则（rule）第二部分 orange的启动过程2.11234567891011121314151617init_by_lua_block &#123; local orange = require("orange.orange") local env_orange_conf = os.getenv("ORANGE_CONF") print(string.char(27) .. "[34m" .. "[INFO]" .. string.char(27).. "[0m", [[the env[ORANGE_CONF] is ]], env_orange_conf) local config_file = env_orange_conf or ngx.config.prefix().. "/conf/orange.conf" local config, store = orange.init(&#123; config = config_file &#125;) -- the orange context context = &#123; orange = orange, store = store, config = config &#125;&#125; 第三部分 orange中的插件第四部分 如何开发一个orange插件通常一个插件的代码分为两个部分：后端和前端。 需求分析业务逻辑介绍插件对过滤后的目标流量访问的接口进行验签认证（SignatureAuth）。 插件支持多用户，在实际线上环境，对于一个借口，通常是多用户需要访问该接口。就需要我们针对不同用户分配密钥（accessKey）； 插件支持用户将认证字段放在header还是query中； 插件支持用户自由添加字段； 客户端包装认证参数 传入参数：accessKey,accessSecret 生成参数 paramStr：method=akauth&amp;client=$UUID&amp;rand=Math.rand() timeStr：String.valueOf(System.currentTimeMillis()/1000) sginStr：accessKey+timeStr+paramStr 签名计算：signature=base64(HmacSHA1.init(accessSecret).doFinal(signStr)) 传递参数：accessKey,timeStr,paramStr,signature 参数放在header中。新增三个字段，分别是：timestamp、sign、appName 其中timestamp 作为判断密钥的有效期。 服务端转发认证参数 接收参数：accessKey,timeStr,paramStr,signature 请求 认证服务接口 后端后端代码位于：orange/pligins中，在源码中创建新增插件名称命名的子目录，例如：signature_auth_header。 每个插件的子目录中约定必须有两个文件（api.lua、handler.lua）： 1234signature_auth_header/api.luahandler.luaREADME.md 其中 下面的函数可以从流量请求中提取header表。 12local headers = ngx.req.get_headers()real = headers[condition.name] 前端插件前后端使用经典MVC模式。前端代码文件在dashboad中，目录结构如下： 123routes # 路由static # js及静态资源路径views # 前端展示 在views中主页中增加插件： 1dashboard\views\common\left_nav.html 代码： 123456 &lt;li id="nav-signature-auth-header"&gt; &lt;a href="/signature_auth_header"&gt; &lt;i class="fa fa-minus-circle"&gt;&lt;/i&gt; &lt;span class="nav-label"&gt;Signature Auth In Header&lt;/span&gt; &lt;/a&gt;&lt;/li&gt; 最后提交git的清单如下： 12 参考文献及资料[1] Orange官网，链接：http://orange.sumory.com/ [2] Orange网关官网docker，链接：https://hub.docker.com/r/syhily/orange [3] agentzh 的 Nginx 教程（版本 2020.03.19）,链接：http://openresty.org/download/agentzh-nginx-tutorials-zhcn.html [4] OpenResty最佳实践，链接：https://moonbingbing.gitbooks.io/openresty-best-practices/content/]]></content>
      <categories>
        <category>orange</category>
      </categories>
      <tags>
        <tag>orange</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用spark streaming将Kafka汇入Mysql实践]]></title>
    <url>%2F2021%2F04%2F15%2F2021-04-20-%E4%BD%BF%E7%94%A8spark%20streaming%E5%B0%86Kafka%E6%B1%87%E5%85%A5Mysql%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[背景http://www.biancheng666.com/article_147327.html 参考文献及资料]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[orange网关使用手册]]></title>
    <url>%2F2021%2F04%2F15%2F2021-04-19-orange%E7%BD%91%E5%85%B3%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[背景Orange是一个基于OpenResty的API Gateway，提供API及自定义规则的监控和管理，如访问统计、流量切分、API重定向、API鉴权、WEB防火墙等功能。Orange可用来替代前置机中广泛使用的Nginx/OpenResty， 在应用服务上无痛前置一个功能丰富的网关系统。 流量网关 业务网关； API内置插件以HTTP Restful形式开放全部API，详细可查看API文档 注意 现实中由于用户的业务系统多种多样，对于复杂应用，Orange并不是一个开箱即用的组件，需要调整一些配置才能集成到现有系统中。 Orange提供的的配置文件和示例都是最简配置，用户使用时请根据具体项目或业务需要自行调整，这些调整可能包括但不限于: 使用的各个shared dict的大小， 如ngx.shared.status nginx.conf配置文件中各个server、location的配置及其权限控制，比如orange dashboard/API的server应该只对内部有权限的机器开放访问 根据不同业务而设置的不同nginx配置，如timeout、keepalive、gzip、log、connections等等 LicenceOrange采用MIT协议开源 其它 Orange的插件模式参考了Kong，Kong是一个功能比较全面的API Gateway实现，推荐关注。 Orange与Kong的不同(刨除基础设计，如存储、API)主要体现在针对”插件”和”API”的组织方式， Orange在流量筛选和变量提取方面相对来说更灵活一些。 第一部分 Nginx知识准备1.1 Nginx配置文件nginx 的配置文件结构中 HTTP 配置主要包括三个区块，结构如下： 1234567Global: nginx 运行相关Events: 与用户的网络连接相关http http Global: 代理，缓存，日志，以及第三方模块的配置 server server Global: 虚拟主机相关 location: 地址定向，数据缓存，应答控制，以及第三方模块的配置 从上面展示的 nginx 结构中可以看出 location 属于请求级别配置，这也是我们最常用的配置。 1.2 location介绍1.2.1 location 语法Location 块通过指定模式来与客户端请求的URI相匹配。Location基本语法： 匹配 URI 类型，有四种参数可选，当然也可以不带参数。 命名location，用@来标识，类似于定义goto语句块。 12location [ = | ~ | ~* | ^~ | !~ | !~* ] /uri/&#123; … &#125;location @/name/ &#123; … &#125; 各类参数含义： =表示请求字符串与其精准匹配，成功则立即处理，nginx停止搜索其他匹配； ~ 表示区分大小写正则匹配； ~* 表示不区分大小写正则匹配； ^~ 表示URI以某个常规字符串开头，并要求一旦匹配到就会立即处理，不再去匹配其他的正则 URI，一般用来匹配目录； !~ 表示区分大小写正则不匹配； !~* 表示不区分大小写正则不匹配； / 通用匹配，任何请求都会匹配到； @ 定义一个命名的 location，@ 定义的locaiton名字一般用在内部定向，例如error_page, try_files命令中。它的功能类似于编程中的goto。 1.2.2 location匹配顺序nginx有两层指令来匹配请求 URI 。第一个层次是 server 指令，它通过域名、ip 和端口来做第一层级匹配，当找到匹配的 server 后就进入此 server 的 location 匹配。 location 的匹配并不完全按照其在配置文件中出现的顺序来匹配，请求URI 会按如下规则进行匹配： 先精准匹配 = ，精准匹配成功则会立即停止其他类型匹配； 没有精准匹配成功时，进行前缀匹配。先查找带有 ^~ 的前缀匹配，带有 ^~ 的前缀匹配成功则立即停止其他类型匹配，普通前缀匹配（不带参数 ^~ ）成功则会暂存，继续查找正则匹配； = 和 ^~ 均未匹配成功前提下，查找正则匹配 ~ 和 ~\* 。当同时有多个正则匹配时，按其在配置文件中出现的先后顺序优先匹配，命中则立即停止其他类型匹配； 所有正则匹配均未成功时，返回步骤 2 中暂存的普通前缀匹配（不带参数 ^~ ）结果 以上规则简单总结就是优先级从高到低依次为（序号越小优先级越高）： 1234561. location = # 精准匹配2. location ^~ # 带参前缀匹配3. location ~ # 正则匹配（区分大小写）4. location ~* # 正则匹配（不区分大小写）5. location /a # 普通前缀匹配，优先级低于带参数前缀匹配。6. location / # 任何没有匹配成功的，都会匹配这里处理 1.2 网关中流量筛选1.2.1 orange中流量选择器orange本质是使用web的方式动态配置nginx，就需要能过滤流量。实现方式是：流量选择器，如下图： 名称，定义流量选择器名称； 类型，可选参数有：全流量、自定义流量； 全流量匹配就是对原始流量不过滤。 自定义流量，需要设置匹配方式与条件，符合条件的请求才会被进行流量管理。 规则，自定义流量开启参数。可选参数有：单一条件匹配、and匹配、or匹配、复杂匹配。 单一条件匹配，单个条件，只能配置一个条件； and匹配，多个条件以且的逻辑过滤； or匹配，多个条件以或的逻辑过滤； 复杂匹配，即自定义条件之间逻辑关系； 按照表达式对所有条件求值，表达式不能为空。表达式中每个值的格式为v[index], 比如v[1]对应的就是第一个条件的值。 例如我们编写了3个条件，表达式为：(v[1] or v[2]) and v[3]。即前两个条件至少一个为真并且第三个条件为真时，规则为真。3个条件按照顺序分别对应：v[1] 、v[2]、v[3]。 条件编写，一条完整的调优有三个要素：条件类型、匹配类型、正则表达式。 条件类型有： Random， URI 根据你请求路径中的 uri 来进行匹配，在接入网关的时候，前端几乎不用做任何更改。 在选择器中，推荐使用 uri 中的前缀来进行匹配，而在规则中，则使用具体路径来进行匹配。 Header，K/V类型 根据 http 请求头中的字段值来匹配。这个类型的name非空。 Query，K/V类型 根据 uri 中的查询参数来进行匹配，比如 /test?a=1&amp;b=2 ，那么可以选择该匹配方式。 Cookie Cookie是用于维持服务端会话状态的，通常由服务端写入，在后续请求中，供服务端读取。 Postparams，K/V类型 IP 根据 http 调用方的 ip 来进行匹配。尤其是在 waf 插件里面，如果发现一个 ip 地址有攻击，可以新增一条匹配条件，填上该 ip ，拒绝该 ip 的访问。 UserAgent User-Agent会告诉网站服务器，访问者是通过什么工具来请求的。包含了一个特征字符串，用来让网络协议的对端来识别发起请求的用户代理软件的应用类型、操作系统、软件开发商以及版本号。例如火狐浏览器发起的请求，User-Agent字段为： 1Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0 Host 根据 http 调用方的 host 来进行匹配。尤其是在 waf 插件里面，如果发现一个 host 地址有攻击，可以新增一条匹配条件，填上该 host ，拒绝该 host 的访问。 Referer HTTP 协议在请求（request）的头信息里面，设计了一个Referer字段，给出”引荐网页”的 URL。这个字段是可选的。客户端发送请求的时候，自主决定是否加上该字段。 HttpMethod HTTP 请求可以使用多种请求方法。HTTP1.0 定义了三种请求方法： GET, POST 和 HEAD方法。HTTP1.1 新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT 方法。一共有8种请求方法。 匹配的类型有： Match，正则匹配 Not Match，正则不匹配 =，精确相等 !=，精确不相等 &gt; &gt;= &lt; &lt;= % 表达式，这里填写正则表达式或者匹配值。 处理，两个参数：继续后续选择器、略过后续选择器； 继续后续选择器，流量继续被其他选择器过滤。 略过后续选择器，流量终止后面的选择器过滤。 是否开启，是否生效选择器。 时候记录日志，过滤结果是否记录日志。 在多个插件里（比如URL重定向插件、WAF插件、自定义监控插件）都使用了选择器来将流量进行第一步划分， 在流量被一个选择器命中后才会进入它内部的规则过滤。 使用选择器的目的是减少不必要的规则判断，进一步提高性能。 1.2.2 orange中的流量规则流量规则必须归属一个流量选择器，用来进一步过滤被流量选择器过滤后的流量。 一个流量选择器，可以有多个所属规则器。 下图是一个规则器，不同的插件的规则器不相同，我们后续在插件中分别介绍。 1.3 变量提取变量提取模块是很多orange插件都使用到的一个概念， 它主要用来从请求中提取出各种信息， 比如query string, form表单里的字段， 某个header头等等。 它支持两种提取方式： 索引式提取 模板式提取 1.3.1 索引式提取顾名思义， 索引式提取的含义是将提取出的所有变量按照顺序保存在一个数组中， 后续以下标的方式来使用。 比如我们在”变量提取模块”提取了三个值： 那么之后就可以通过\${1}、​\${2}、​\${3}来使用， 其中 ${1}指的是header头里的app_version字段， 如果header没有此字段， 则赋一个默认值v0.1 ${2}指的是query中的uid字段 ${3}指的是query中age字段， 若无则给予默认值18 1.3.2 模板式提取模板时提取主要为了解决索引式提取必须要按序使用的问题， 并且当需要从uri中提取多个值时索引式提取方式并不友好。 如以下示例， 我们提取了四个值： 则之后我们可以通过以下方式来使用： 指的是从query中提取出的uid字段 指的是从query中提取出的age字段, 若无则给予默认值18 指的是从格式为1^/start/(.*)/(.*)/end 的URI中提取出的第1个分段值 比如， 如果URI为/start/abc/123/end, 则此时值为abc 如果URI为/start/momo/sharp/end, 则此时值为momo 指的是从格式为1^/start/(.*)/(.*)/end 的URI中提取出的第2个分段值 比如， 若URI为/start/abc/123/end, 则此时值为123 如果URI为/start/momo/sharp/end, 则此时值为sharp 注意， 若从URI中提取， 仍然要根据顺序来使用， 如、、. 设计原理:https://github.com/orlabs/orange/issues/15 第二部分 核心组件介绍最新稳定版本0.8.1，该版本对第三方组件进行去除。这里核心组件主要是0.6.4版本中组件。 2.1 全局统计可统计API访问情况、Nginx连接情况、流量统计、QPS、应用版本、服务器信息等。如下图： 2.2 自定义监控可根据配置的规则筛选出流量并监控，统计其各个指标。当前的监控指标有： 请求总次数： 分别统计200/300/400/500区间的请求数 QPS 请求总耗时/平均请求耗时 总流量/平均请求流量 案例： 例如筛选出指定流量，进行监控。下面是监控视图： 2.3 URL重定向（redirect）网关实现的重定向主要是：当客户端向网关请求URL资源的时候，网关通知客户端实际的资源地址，然后客户端向实际的URL请求资源。重定向是指当浏览器请求一个URL时，服务器返回一个重定向指令，告诉浏览器地址已经变了，麻烦使用新的URL再重新发送新请求。 网关实现了通过UI配置各种rewrite策略，省去手写nginx rewrite和重启。redirect是浏览器和服务器发生两次请求，也就是服务器命令客户端“去访问某个页面”。 2.3.1 案例我们使用重定向来代理网关的官网(http://orange.sumory.com/)。 首先，添加选择器： 然后在选择器中创建新的规则： 配置完成后，当我们访问192.168.52.137:8888/to_orange时候，会自动跳转为：http://orange.sumory.com/。 2.3.2 参数说明重定向有两种：一种是302响应，称为临时重定向，一种是301响应，称为永久重定向。两者的区别是，如果服务器发送301永久重定向响应，浏览器会缓存/hi到/hello这个重定向的关联，下次请求/hi的时候，浏览器就直接发送/hello请求了。 2.4 URI 重写(Rewrite)Url重写主要用于站内请求的重写。rewrite则是服务器内部的一个接管，在服务器内部告诉“某个页面请帮我处理这个用户的请求”，浏览器和服务器只发生一次交互，浏览器不知道是该页面做的响应，浏览器只是向服务器发出一个请求。 URL重写用于将页面映射到本站另一页面，若重写到另一网络主机（域名），则按重定向处理。 rewrite是把一个地址重写成另一个地址。地址栏不跳转。相当于给另一个地址加了一个别名一样。 2.4.1 案例我们使用重写（rewrite）来重写上面案例中地址。 首先添加选择器： 然后在选择器中创建新的规则： 配置完成后，当我们访问http://192.168.52.137:8888/to_orange_test时候，流量被映射到本站另一个地址http://192.168.52.137:8888/to_orange。而后面地址被重定向到http://orange.sumory.com/。 2.4.2 参数说明2.5 HTTP Basic Authorizationbasic auth是最简单权限认证方式，密钥被base64加密，但是网络传输是非加密的，一旦被截取，解密是容易的。所以通常用于安全的内部网络。 案例参数说明2.6 HTTP Key Auth案例参数说明2.7 Signature Auth案例参数说明https://www.cnblogs.com/Sinte-Beuve/p/12093307.html 2.8 Rate Limiting 访问限速案例参数说明2.9 Rate Limiting 防刷案例参数说明2.10 WAF 防火墙案例参数说明2.11 代理 &amp; 分流 &amp; ABTesting当前divide分流插件是静态的，需要提前在nginx.conf里配置upstream，但是这样不利于灵活管理，能否实现动态配置upstream。 分流插件，可分为三个使用场景： 作为proxy，如代理后端的多个HTTP应用 用于AB测试 用于动态分流，API版本控制等 https://book.aikaiyuan.com/openresty/orange-divide.html#%E8%AF%95%E9%AA%8C%E7%8E%AF%E5%A2%83 http://bbs.orchina.org/topic/160/view 案例我们使用该插件代理elasticsearch集群节点。 我们对es进行负载配置 http://www.ttlsa.com/nginx/nginx-elasticsearch/ 123upstream es_upstream &#123; server 192.168.31.3:9200; &#125; https://github.com/orlabs/orange/issues/136 2.12 KV Store第三部分 第三方插件https://zhjwpku.com/2017/11/14/orange-balancer-plugin-tutorial.html The balancer plugin migrated tov0.9.0-dev due to conflicts with existing features. The dynamic_upstream plugin migrated tov0.9.0-dev due to conflicts with existing features. The consul_balancer plugin migrated tov0.9.0-dev due to conflict with existing functions. The persist plugin migrated tov0.9.0-dev due to conflicts with existing features. 3.1 node插件该插件主要用户网关集群管理。 node plugin（容器集群节点管理插件） 新增集群节点注册命令 orange register 通过 dashboard 面板同步节点配置信息 配合 persist 插件，可以查看历史统计信息 12345678influxdb:/usr/local/orange/conf # opm install ledgetech/lua-resty-http* Fetching ledgetech/lua-resty-http Downloading https://opm.openresty.org/api/pkg/tarball/ledgetech/lua-resty-http-0.14.opm.tar.gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 20862 100 20862 0 0 7545 0 0:00:02 0:00:02 --:--:-- 7545 Package ledgetech/lua-resty-http 0.14 installed successfully under /usr/local/openresty/site/ . https://github.com/orlabs/orange/issues/353 3.2 headers 插件用于修改请求头。 3.3 balancer插件用户负载多个upstream https://zhjwpku.com/2017/11/14/orange-balancer-plugin-tutorial.html 3.4 Consul Upstream3.4 Dynamic Upstream案例参数说明3.6 HTTP Jwt Auth3.7 HTTP Hmac Auth3.8 持久日志第四部分 插件的优先级orange中所有插件都是继承基本组件的，文件plugins\base_handler.lua中定义如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344local Object = require("orange.lib.classic")local BasePlugin = Object:extend()function BasePlugin:new(name) self._name = nameendfunction BasePlugin:get_name() return self._nameendfunction BasePlugin:init_worker() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": init_worker")endfunction BasePlugin:redirect() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": redirect")endfunction BasePlugin:rewrite() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": rewrite")endfunction BasePlugin:access() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": access")endfunction BasePlugin:balancer() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": balancer")endfunction BasePlugin:header_filter() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": header_filter")endfunction BasePlugin:body_filter() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": body_filter")endfunction BasePlugin:log() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": log")endreturn BasePlugin 第五部分 Dashboard 用户管理从v0.1.1版本开始，Orange Dashboard支持授权验证（默认未开启）。当配置开启后，只有通过成功登录的账户才能登陆展现Dashboard。 配置这部分功能在配置文件conf/orange.conf中： 12345678"dashboard": &#123; "auth": false,//是否开启用户鉴权，默认为false "session_secret": "y0ji4pdj61aaf3f11c2e65cd2263d3e7e5",//用于加密cookie的盐 "whitelist": [ "^/auth/login$", "^/error/$" ]&#125; Dashboard 用户的用户信息存储在Mysql的dashboard_user表中。 默认系统管理员用户名和密钥如下，首次登陆后可以修改和添加其他用户： 12用户名：admin密码：orange_admin 参考文献及资料[1] Orange官网，链接：http://orange.sumory.com/ [2] Orange网关官网docker，链接：https://hub.docker.com/r/syhily/orange]]></content>
      <categories>
        <category>orange</category>
      </categories>
      <tags>
        <tag>orange</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[orange网关原理的源码分析]]></title>
    <url>%2F2021%2F04%2F15%2F2021-05-30-orange%E7%BD%91%E5%85%B3%E7%94%9F%E4%BA%A7%E7%BB%B4%E6%8A%A4%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[背景Orange v0.6.0版本是一个重构版本， 着重为了解决之前版本在有大量规则配置时性能损耗的问题。 基本的设计思路是将原来的规则细分成两层， 第一层叫做selector, 用于将流量进行第一步划分， 在进入某个selector后才按照之前的设计进行规则匹配， 匹配到后进行相关处理。 https://lengrongfu.github.io/2019/05/21/orange-%E5%8E%9F%E7%90%86/ https://book.aikaiyuan.com/openresty/understanding-orange.html https://zhuanlan.zhihu.com/p/67481992 生产单机部署生产集群部署配置更新日志切割Nginx日志对于统计、系统服务排错很有用。Nginx日志主要分为两种：access_log(访问日志)和error_log(错误日志)。通过访问日志我们可以得到用户的IP地址、浏览器的信息，请求的处理时间等信息。错误日志记录了访问出错的信息，可以帮助我们定位错误的原因。本文将详细描述一下如何配置Nginx日志。 访问日志主要记录客户端的请求。客户端向Nginx服务器发起的每一次请求都记录在这里。客户端IP，浏览器信息，referer，请求处理时间，请求URL等都可以在访问日志中得到。当然具体要记录哪些信息，你可以通过log_format指令定义。 错误日志在Nginx中是通过error_log指令实现的。该指令记录服务器和请求处理过程中的错误信息。 Nginx中通过access_log和error_log指令配置访问日志和错误日志，通过log_format我们可以自定义日志格式。如果日志文件路径中使用了变量，我们可以通过open_log_file_cache指令来设置缓存，提升性能。 access_log，level表示日志等级，日志等级分为[ debug | info | notice | warn | error | crit ]，从左至右，日志详细程度逐级递减，即debug最详细，crit最少 error_log level可以是debug, info, notice, warn, error, crit, alert,emerg中的任意值。可以看到其取值范围是按紧急程度从低到高排列的。只有日志的错误级别等于或高于level指定的值才会写入错误日志中。默认值是error。 orange中日志和默认配置orange项目logs/目录下面有日志文件： 123456789101112# 主日志access.logerror.log# orange管理界面日志dashboard_access.logdashboard_error.log# 管理api接口日志api_access.logapi_error.log# default_upstream_access.logdefault_upstream_error.log 在配置文件conf/nginx.conf中这样定义： 12345678910111213141516171819202122http &#123;# ...access_log ./logs/access.log main;error_log ./logs/error.log info;# ...server &#123; listen 8001; access_log ./logs/default_upstream_access.log main; error_log ./logs/default_upstream_error.log; &#125;server &#123; listen 9999; access_log ./logs/dashboard_access.log main; error_log ./logs/dashboard_error.log info; &#125;server &#123; listen 7777; access_log ./logs/api_access.log main; error_log ./logs/api_error.log info; &#125;&#125; 日志切割日常运维中orange的日志文件随着时间逐渐增多，日志单文件较大不方便日常文件生命周期清理。所以需要对原日志进行切割，每日一个日志文件。 常见方法有： 使用shell脚本重命名日志文件，使用crontab定时调用该脚本。 使用logrotate工具。 从nginx 0.7.6版本开始access_log的路径配置可以包含变量。 建议的配置： 12345678map $time_iso8601 $logdate &#123; '~^(?&lt;ymd&gt;\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;)' $ymd; default 'date-not-found';&#125;access_log logs/access-$logdate.log main;error_log ./logs/error-$logdate.log info;open_log_file_cache max=10; https://jingsam.github.io/2019/01/15/nginx-access-log.html 参考文献及资料[1] Orange官网，链接：http://orange.sumory.com/ [2] Orange网关官网docker，链接：https://hub.docker.com/r/syhily/orange]]></content>
      <categories>
        <category>orange</category>
      </categories>
      <tags>
        <tag>orange</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch中Mapping总结]]></title>
    <url>%2F2021%2F04%2F12%2F2021-04-12-Elasticsearch%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Elasticsearch%E4%B8%ADMapping%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 版本升级指引 第二部分 升级方法和具体步骤 总结 参考文献及资料 背景第一部分 MappingMapping在Elasticsearch中作用类似于传统数据库中的表结构定义（schema），即Index的元数据信息。主要作用有： 定义索引（Index）中字段名称； 定义字段的数据类型； 定义倒排索引的配置； 在Elasticsearch 7.x版本前，单个index中支持多个type。每个index都会对应有自己的mapping。这时候index可以类比关系型数据库中库，而不同type类比库中表。 在索引中定义太多字段可能会导致索引膨胀，出现内存不足和难以恢复的情况，下面有几个设置： index.mapping.total_fields.limit：一个索引中能定义的字段的最大数量，默认是 1000 index.mapping.depth.limit：字段的最大深度，以内部对象的数量来计算，默认是20 index.mapping.nested_fields.limit：索引中嵌套字段的最大数量，默认是50 1.1 Elasticsearch中数据类型1.1.1 基本类型 text类型 该类型的字段将经过分词器分词，用于全文检索； keyword类型 不经过分词器，用于精确检索（只能检索该字段的完整值），用于过滤（filtering） 数值类型 和编程语言相同，主要有： long：有符号64-bit integer：-2^63 ~ 2^63 - 1 integer：有符号32-bit integer，-2^31 ~ 2^31 - 1 short：有符号16-bit integer，-32768 ~ 32767 byte： 有符号8-bit integer，-128 ~ 127 double：64-bit IEEE 754 浮点数 float：32-bit IEEE 754 浮点数 half_float：16-bit IEEE 754 浮点数 scaled_float 布尔类型 逻辑型，即true/false 日期类型 由于Json没有date类型，所以es通过识别字符串是否符合format定义的格式来判断是否为date类型。ormat默认为：strict_date_optional_time||epoch_millis https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html 二进制类型 该类型的字段把值当做经过 base64 编码的字符串，默认不存储，且不可搜索 范围类型 范围类型表示值是一个范围，而不是一个具体的值 譬如 age 的类型是 integer_range，那么值可以是 {“gte” : 10, “lte” : 20}；搜索 “term” : {“age”: 15} 可以搜索该值；搜索 “range”: {“age”: {“gte”:11, “lte”: 15}} 也可以搜索到 range参数 relation 设置匹配模式 INTERSECTS ：默认的匹配模式，只要搜索值与字段值有交集即可匹配到 WITHIN：字段值需要完全包含在搜索值之内，也就是字段值是搜索值的子集才能匹配 CONTAINS：与WITHIN相反，只搜索字段值包含搜索值的文档 integer_range float_range long_range double_range date_range：64-bit 无符号整数，时间戳（单位：毫秒） ip_range：IPV4 或 IPV6 格式的字符串 1.1.2 复杂类型 数组类型 字符串数组 [ “one”, “two” ] 整数数组 [ 1, 2 ] 数组的数组 [ 1, [ 2, 3 ]]，相当于 [ 1, 2, 3 ] Object对象数组 [ { “name”: “Mary”, “age”: 12 }, { “name”: “John”, “age”: 10 }] 同一个数组只能存同类型的数据，不能混存，譬如 [ 10, “some string” ] 是错误的 数组中的 null 值将被 null_value 属性设置的值代替或者被忽略 空数组 [] 被当做 missing field 处理 对象类型 对象类型可能有内部对象 被索引的形式为：manager.name.first 嵌套类型 地理位置数据类型 专用数据类型 记录IP地址 ip 实现自动补全 completion 记录分词数 token_count 记录字符串hash值 murmur3 Percolator 1.2 多字段特性（multi-fields） 允许对同一个字段采用不同的配置，比如分词，常见例子如对人名实现拼音搜索，只需要在人名中新增一个子字段为 pinyin 即可 通过参数 fields 设置 1.3 不可修改性Mapping中的字段类型一旦设定后，禁止直接修改，原因是Lucence实现的倒排索引生成后不允许修改，除非重新建立新的索引，然后做reindex操作。但是允许新增字段。通过dynamic参数来控制字段的新增： true(默认）允许自动新增字段 false不允许自动新增字段，但是文档可以正常写入，但无法对字段进行查询等操作 strict 文档不能写入，报错 第二部分 设置Mapping在创建一个索引的时候，可以对 dynamic 进行设置，可以设成 false、true 或者 strict。 第三部分 Dynamic MappingDynamic Mapping 机制使我们不需要手动定义 Mapping，ES 会自动根据文档信息来判断字段合适的类型，但是有时候也会推算的不对，比如地理位置信息有可能会判断为 Text，当类型如果设置不对时，会导致一些功能无法正常工作，比如 Range 查询。 3.1 类型自动推断3.2 更新mapping如果是新增加的字段，根据 Dynamic 的设置分为以下三种状况： 当 Dynamic 设置为 true 时，一旦有新增字段的文档写入，Mapping 也同时被更新。 当 Dynamic 设置为 false 时，索引的 Mapping 是不会被更新的，新增字段的数据无法被索引，也就是无法被搜索，但是信息会出现在 _source 中。 当 Dynamic 设置为 strict 时，文档写入会失败。 另外一种是字段已经存在，这种情况下，ES 是不允许修改字段的类型的，因为 ES 是根据 Lucene 实现的倒排索引，一旦生成后就不允许修改，如果希望改变字段类型，必须使用 Reindex API 重建索引。 不能修改的原因是如果修改了字段的数据类型，会导致已被索引的无法被搜索，但是如果是增加新的字段，就不会有这样的影响。 第四部分 Index Template### 参考材料1、Elasticsearch官网 链接：https://www.elastic.co/cn/ https://www.cnblogs.com/wupeixuan/p/12514843.html]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Clickhouse学习笔记]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-27-Clickhouse%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料1、项目地址，链接：https://github.com/ClickHouse/ClickHouse]]></content>
      <categories>
        <category>Clickhouse</category>
      </categories>
      <tags>
        <tag>Clickhouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Presto使用介绍]]></title>
    <url>%2F2021%2F03%2F21%2F2021-04-01-%E9%93%B6%E8%A1%8C%E7%A7%91%E6%8A%80%E7%9A%84%E8%A1%B0%E8%90%BD%2F</url>
    <content type="text"><![CDATA[背景这次响应乌克兰副总理号召最先决定退出俄罗斯的甲骨文，让很多人想起一个今天已经有点陌生的名词：“去IOE”。 IOE中，IOE分别指IBM的小型机、Oracle的数据库和EMC的存储。它们代表的是一套由西方科技公司主导的IT底层架构。20年前的全球IT市场，各个公司的大批预算投向了IOE架构里。 2009年，阿里巴巴内部开始提出“去IOE”的技术路线：“低成本、线性可控、去中心化（分布式）：去IBM，PCSever替代小型机；去Oracle，用MySQL替代；去EMC，用中低端存储”。第二年，进一步提出用MySQL+自研数据库替代Oracle，并不再使用高端存储。也因此，阿里云在2009年由此诞生，同一时期，阿里也开始自研数据库。 在这之前，中国也有自己官方资金扶持下的数据库，但思路都没有离开甲骨文三十多年领先奠定的技术规则。哪怕在局部的性能上有突破，也只能像俄罗斯的一些国产竞品那样，在全局依然比不过一个已经成熟三十年的庞大生态的产品。 但中国与俄罗斯的不同在于，中国有庞大而复杂的市场。随着消费互联网在中国快速的铺开，中国产生了完全不同于甲骨文诞生时美国以军方应用以及企业级应用主导的新IT环境。去IOE 过程中出现的新路线满足了高并发的场景和各类产业数字化过程中对安全和性价比的需要。国产云厂商和云计算市场也因此繁荣起来，而这一次不再像数据库等技术最初发展起来时中国的滞后，云技术成了一个中外几乎同时开启的竞争。除了阿里云之外，腾讯、华为、金山和百度等也纷纷加入云计算和云技术的市场竞争。 2012年到2013年，阿里巴巴陆续告别了“IOE”体系。在这个过程中，阿里巴巴有了云数据库，和自研数据库。并借此开始向云操作系统、芯片等技术领域反向定义。 阿里云今天拥有了国内最丰富的数据库产品，覆盖关系型数据库、分析型数据库、NoSQL数据库及相关的服务与工具。PolarDB-X和云原生数据仓库AnalyticDB，应对双11期间超过上亿的订单业务峰值，10万人可同时查询包裹实时状态，并在今年疫情期间防疫物资的运送方面发挥了举足轻重的作用；在去IOE过程中自研的数据库OceanBase，也在性能上赶超老牌产品，根据“数据库领域世界杯”的数据库基准测试TPC-C，OceanBase在2019首次超过甲骨文，之后又连续打破纪录，性能分数已突破亿级达到7.07亿tpmC。 今天来看，正是因为中国自己的更大更复杂的技术应用场景带来的对技术迭代的需求，导致在由美国利益主导的稳固的IOE牢笼里出现了突破口，而许多中国科技公司勇敢的抓住了这个机会，在新的场景里从底层重新设计了自己的路线。 这个故事过去十年在诸多有技术追求的中国公司身上上演。 从底层的IT基础设施，到新的智能交互需求下的软件开发环境，再到人工智能时代的底层开发组件，和越来越多垂直行业的新技术方案，一整套“自根自生”的IT路线慢慢出现。 这些自主进程都有一个共同的特点，在它们发生之初，与“自主可控”的目的——或是统一部署的计划式推进相比，它们更多都是一场场由全新的需求带来的技术创新迭代。它们不是为了替代而替代，但都因为选择了从底层重构来解决新问题，而在今天拥有了更重要的一层意义：它们的实践让我们在国家基础科技设施的自主可控和安全进程里，走出了一条新路。用更直观的比喻来形容，就是俄罗斯有去I、去O和去E，但没有去IOE。后者本质是一场系统化的信息化底层设施改造，需要的是重新定义问题的机会、能力和勇气——俄罗斯一直在解美国公司自己提出、并且做了几十年的题，而这些中国公司的尝试让我们自己定义了新的问题，进而产生了新的解题思路，从而真正有可能吸引更多国产技术公司加入这个生态的搭建。 这条路一直是最艰难的，并且只会变得更难。除了庞大体量的基础架构在巨人转身中带来的巨大风险，在科技竞争冲击美国利益之下，美国也开始越发频繁的将本国科技公司作为“刀子”来维护“美国第一”的战略，这都使得建立科技基础设施安全的成本无限增加。 这种困难还来自不少“从业者”。最关乎信息安全的产业，也往往是那些依然自我麻痹在美国公司”科技无国界”的叙事所制造的幻想中的人们所习惯于妄自菲薄的领域。 但看着今天美国科技公司对俄罗斯的制裁中，它们不仅不回避，且迫切希望将自己政治化和把影响扩大化的表现，这些人也应该明白，基础设施性质的科技巨头在制裁中指向的一定是普通公众，当这些全球垄断平台被美国用来制造新的“国际贱民”，每个人都没法独善其身。这种无差别的“攻击”应该足以让所有人丢掉幻想。 今天看着俄罗斯，回顾我们自己在走的路，所有人应该都已清楚一件事：永远不要让自己国家最聪明的头脑只能沿着美国公司制定的规则去完善技术，哪怕会遭遇冷嘲热讽，哪怕这很难，但努力变成新问题的定义者，这事我们没得选。 参考文献及资料]]></content>
      <categories>
        <category>Presto</category>
      </categories>
      <tags>
        <tag>Presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机中存储概念介绍]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-27-%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E5%AD%98%E5%82%A8%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[背景扇区（sector）硬盘的读写以扇区为基本单位。硬盘中每个磁道被等分为若干弧段，这些弧段称为扇区。硬盘的读写以扇区为基本单位。 通常每个扇区的大小是512字节。 123456789101112131415root@deeplearning:~# fdisk -lDisk /dev/loop1: 55.5 MiB, 58159104 bytes, 113592 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/loop2: 55.4 MiB, 58073088 bytes, 113424 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes#（省略）Device Start End Sectors Size Type/dev/sda1 2048 1050623 1048576 512M EFI System/dev/sda2 1050624 909946879 908896256 433.4G Linux filesystem/dev/sda3 909946880 976771071 66824192 31.9G Linux swap 其中显示Sector size就是扇区的大小，案例中为512 bytes（0.5K）。 这是1956年由industry trade organization, International Disk Drive Equipment和Materials Association三家机构确定的行业标准。有时代和技术的限制，因为磁盘技术发展初期，存储容量非常小。512字节的扇区也够用，但是随着时代的发展，512字节大小的扇区（Sector）明显太小了，由于每个扇区（Sector）还要存放很多其他信息，因此增大sector size可以降低扇区（Sector）的数量，从而提高实际存储量，同时降低了差错校验等很多CPU计算量。但遗憾的是由于这个标准太根深蒂固，要想改变一些底层代码或架构势必非常困难，所以现在4KB扇区硬盘暂时还没有全部普及。 关于物理扇区（physical setctor）与逻辑扇区，这个还得扯上扇区大小，由于近年来，随着对硬盘容量的要求不断增加，为了提高数据记录密度，硬盘厂商往往采用增大扇区大小的方法，于是出现了扇区大小为4096字节的硬盘。我们将这样的扇区称之为“物理扇区”。但是这样的大扇区会有兼容性问题，有的系统或软件无法适应。为了解决这个问题，硬盘内部将物理扇区在逻辑上划分为多个扇区片段并将其作为普通的扇区（一般为512字节大小）报告给操作系统及应用软件。这样的扇区片段我们称之为“逻辑扇区”。实际读写时由硬盘内的程序（固件）负责在逻辑扇区与物理扇区之间进行转换，上层程序“感觉”不到物理扇区的存在。 逻辑扇区是硬盘可以接受读写指令的最小操作单元，是操作系统及应用程序可以访问的扇区，多数情况下其大小为512字节。我们通常所说的扇区一般就是指的逻辑扇区。物理扇区是硬盘底层硬件意义上的扇区，是实际执行读写操作的最小单元。是只能由硬盘直接访问的扇区，操作系统及应用程序一般无法直接访问物理扇区。一个物理扇区可以包含一个或多个逻辑扇区（比如多数硬盘的物理扇区包含了8个逻辑扇区）。当要读写某个逻辑扇区时，硬盘底层在实际操作时都会读写逻辑扇区所在的整个物理扇区。 注意，扇区是磁盘物理层面的概念，操作系统是不直接与扇区交互的，而是与多个连续扇区组成的磁盘块交互。由于扇区是物理层面的概念，所以无法在系统中进行大小的更改。 磁盘块，IO Block文件系统读写数据的最小单位，也称磁盘簇。扇区是磁盘最小的物理存储单元，操作系统将相邻的扇区组合在一起，形成一个快，对块进行管理。每个磁盘块可以包括2、4、8、16、64个扇区。磁盘块是操作系统所使用的逻辑概念，而非磁盘的物理概念。磁盘块的大小可以通过下面的命令查看： 123456789root@deeplearning:~# stat /boot File: '/boot' Size: 12288 Blocks: 24 IO Block: 4096 directoryDevice: 802h/2050d Inode: 15859713 Links: 4Access: (0755/drwxr-xr-x) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2021-03-27 12:54:48.241977827 +0800Modify: 2021-01-30 12:38:28.524642113 +0800Change: 2021-01-30 12:38:28.524642113 +0800 Birth: - 其中IO Block: 4096就是磁盘块的大小。案例中是4096 Bytes。也就是4K，连续8个扇区组成。 为了更好地管理磁盘空间和更高效地从硬盘读取数据，操作系统规定一个磁盘块中只能放置一个文件，因此文件所占用的空间，只能是磁盘块的整数倍，那就意味着会出现文件的实际大小，会小于其所占用的磁盘空间的情况。 页（page）页是内存最小存储单位。页的大小通常为磁盘块大小的2^n倍。可以通过命令查看大小。 12root@deeplearning:~# getconf PAGE_SIZE4096 案例中页大小为4096 Bytes,与磁盘块大小一致。 总结 扇区大小，fdisk -l 磁盘块大小，stat /boot 内存页大小，getconf PAGE_SIZE 参考文献及资料1、项目地址，链接：https://github.com/ClickHouse/ClickHouse]]></content>
      <categories>
        <category>Clickhouse</category>
      </categories>
      <tags>
        <tag>Clickhouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库的行式存储和列式存储]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-27-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E8%A1%8C%E5%BC%8F%E5%AD%98%E5%82%A8%E5%92%8C%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[背景数据库技术发展迅速，由原来的关系型数据库到越来越丰富的非关系型数据库。如果按照存储形式分类，主要有：行式存储（Row-Based）、列式存储（Column-Based）、键值（key-value）存储、文档（doc）存储、图形（graph）存储、时序数据库等。我们常用的传统关系型数据库（MySQL、Oracle、PostgreSQL 、DB2和 SQL Server）都是采用行式存储，而最新兴起的分布式数据库很多采用列式存储，例如：Druid、Kudu、Clickhouse等。 本文将详细介绍行式存储、列式存储以及业务场景的选型和对比。 第一部分 存储基础数据库按照存储介质，分为磁盘数据库和内存数据库。对于磁盘数据库，需要依赖于内置硬盘或外置集中式存储设备。首先我们介绍磁盘的存储原理。 1.1 磁盘存储首先我们先介绍几个概念： 磁盘扇区（sector） 磁盘中每个磁道等分为若干弧段，这些弧段即称为扇区。磁盘的读写以扇区为基本单位。可以使用fdisk -l命令来查看服务器中磁盘扇区的大小，通常是512 bytes=0.5K。为什么是这么大，这是一个行业标准（近期也有4K的扇区磁盘）。扇区是磁盘的最小存储单元。 扇区是一个物理层面的概念，操作系统是不直接和扇区交互的，而是和磁盘块交互。 磁盘块（IO Block） 操作系统将相邻的扇区组合在一起，形成一个块，对块进行管理，通常称为磁盘块（或磁盘簇）。可以使用stat /boot参看，一般一个磁盘块由2、4、8、16、64等个扇区组成，这是操作系统中的逻辑概念，所以可以调节。通常一个磁盘块为4096 Bytes=4K，即8个连续扇区。 inode 操作系统中文件数据存储在磁盘块中，那么还需要有地方存储文件的元数据信息（文件的创建用户、时间信息、权限等），最重要的是文件数据所在的磁盘块地址信息。这就是inode数据。 文件存储在磁盘中，操作系统有一定规范： 每个磁盘块中只能存储一个文件 例如一个文件大小为5K，操作系统中每个磁盘块大小为4K，那么这个文件实际使用2个磁盘块进行落盘存储。 磁盘块是操作系统最小存储单位 例如例子中的5K文件，存储在两个磁盘块，那么读取这个文件需要进行两次I/O操作。 文件的元数据信息 每个文件的数据存储在磁盘块中，inode进行登记。如果一个磁盘块不够会申请新的磁盘块，均在inode中登记。文件读取时候，顺序读取inode中磁盘块的数据，加载至内存中。 1.2 内存存储同样对于内存，操作系统同样定义了逻辑读取的基本单位为：页（page）。页的大小为磁盘的$2^n$倍数，可以使用命令getconf PAGE_SIZE参看。通常和磁盘块大小一致为4K。 1.3 数据库中的页类似磁盘和内存，数据库中同样页（page）的概念，显然这是一个逻辑的概念。数据库中的数据在磁盘上以文件的形式存储，数据库的存储引擎会以固定大小的page为单位组织文件，读写磁盘也以page为单位。不同数据库page大小有差异，比如：SQLite 1KB, Oracle/DB2 4KB, SQL Server 8KB, MySQL 16KB。 第二部分 行列存储原理磁盘数据库的内部组件组成是复杂的，我们只关注数据文件的存储方式（不同数据库系统会有复杂的处理机制）。 2.1 行式存储数据按照行数据为基础单元进行组织存储。例如下面的二维表数据： 表中每一行数据用|分割，整体存储在一个磁盘块中（图中每条行记录挤满一个磁盘块）； 表中其他记录连续写入文件分配的磁盘块中； 2.1.1 优势 具有随机写入优势，特别是频繁写和更新操作。对于写入的每条记录，只需要内存拼接好整行记录，一次性写入到磁盘块中，单条记录数量小于单个磁盘块就只需要1次IO操作。 2.1.2 短板 读取冗余。在查询操作中计算机是按照磁盘块为基本单位进行读取，同一个磁盘块中其他记录也需要读取。然后加载至内存，在内存中进行过滤处理。即使查询只涉及少数字段，也需要读取完整的行记录。 行存储数据会引入索引和分库分表技术来避免全量读取。 数据压缩。每行数据有不同类型数据，数据压缩率较低。 2.2 列式存储数据按照列为单元进行集中存储。例如下面的二维表数据： 数据列SSN的数据顺序存储在同一个磁盘块中； 其他列同样集中顺序存储。顺序写在前列后面或单独一个文件。 2.2.1 优势 大量查询操作优势。对于目标查询，列存只需要返回目标列的值。而且列值在磁盘中集中存储，会减少读取磁盘块的频次，较少IO的数量。最后在内存中高效组装各列的值，最终形成查询结果。另外列式记录每一列数据类型同质，容易解析。每列存储是独立的可以并发处理，进一步提升读取效率。 数据压缩优势。因为各列独立存储，且数据类型已知，可以针对该列的数据类型、数据量大小等因素动态选择压缩算法，以提高物理存储利用率。如果某一行的某一列没有数据，那在列存储时，就可以不存储该列的值，这将比行式存储更节省空间。整体上减轻IO的频次。 映射下推(Project PushDown)、谓词下推(Predicate PushDown)。可以减少不必要的数据扫描，尤其是表结构比较庞大的时候更加明显，由此也能够带来更好的查询性能 2.2.2 短板 写入更新短板。列式存储写入前需要将一条记录拆分成单列，分别追加写入到列所在的磁盘块中，如果列比较多，一条记录就会产生多次磁盘块的IO操作。对于实时的逐条写入性能会比行式弱。 但是对于大量的批写入，列可以在内存中拆分好各分列，然后集中写入，性能和行式数据比不一定差。 第三部分 业务场景适应性分析对于列存和行存，我们在技术选型的时候应该挑选哪个？技术没有包打一切银弹，只有适合对应场景的技术。通常将数据处理分为三大类：联机事务处理OLTP（on-line transaction processing）、联机分析处理OLAP（On-Line Analytical Processing）还有混合事务/分析处理（Hybrid transaction/analytical processing）。 3.1 联机事务处理（OLTP）联机事务处理类型通常表示事务性非常高的系统。一般都是联机在线系统，通常以小的事务和小的查询为主。评估系统性能主要看每秒执行的Transaction以及Execute SQL的数量。单个数据库每秒处理的Transaction往往超过几百个，或者是几千个，查询语句的执行量每秒几千甚至几万个。对于互联网秒杀场景要求会更高。典型的OLTP系统有银行、证券、电子商务系统等互联网在线业务系统等。 业务特点从主要有： 数据库视角： 每个事务的读、写、更改涉及的数据量非常小。 数据库的数据必须是最新的，所以对数据库的可用性要求很高。 系统使用用户较大，有海量访问。 要求业务处理快速响应，通常一个事务需要在秒级内完成。 存储视角： 每个I/O非常小，通常为2KB～8KB。 访问硬盘数据的位置非常随机，至少30％的数据是随机写操作。 REDO日志（重做日志文件）写入非常频繁。 3.2 联机分析处理（OLAP）联机分析处理 (OLAP) 的概念最早是由关系数据库之父E.F.Codd于1993年提出的。也称为决策支持系统，即数据仓库。该场景下语句的执行量不再是考核标准，通常一条语句的执行延迟非常长，读取的数据也是海量的。性能评估的指标变为查询的吞吐量，如能达到多少MB/s的流量。 业务特点主要有： 数据库视角： 数据更新操作少（以大批量写入为主）或没有数据更新和修改。 数据查询过程复杂。 系统用户较少，数据的使用频率逐渐减小，允许延迟。 查询结果以统计、聚合计算为主。 存储视角： 单个I/O数据量大（均为读），通常为MB和GB级别，甚至TB级别。 读取操作通常顺序读取。 当进行读取操作进行时，写操作的数据存放在临时表空间内。 对在线日志写入少。只有在批量加载数据时，写入操作增多。 3.3 混合事务/分析处理(HTAP)HTAP 就是 OLAP 和OLTP 两种场景的结合。在对新旧数据进行 OLAP 分析的情况下增加事务的处理来对数据进行更新。实际业务场景中，往往是 OLAP、OLTP是同时存在的（见下图）。对于HTAP系统，一种解决方案分别针对新旧数据构建两套引擎，一套负责 OLTP（热数据），一套负责 OLAP（冷数据），一个查询到达后，需要分别解析成两套查询，在两个查询引擎都得到结果后进行合并，还可能用到两阶段提交等分布式事务。 例如TiDB数据库就是一个典型的HTAP数据库。目前有两种存储节点，分别是 TiKV和 TiFlash。TiKV 采用了行式存储，更适合 TP（OLTP） 类型的业务；而TiFlash 采用列式存储，擅长 AP(OLAP) 类型的业务。TiFlash 通过 raft 协议从 TiKV 节点实时同步数据，拥有毫秒级别的延迟，以及非常优秀的数据分析性能。它支持实时同步 TiKV 的数据更新，以及支持在线 DDL。把 TiFlash 作为 Raft Learner 融合进 TiDB 的 raft 体系，将两种节点整合在一个数据库集群中，上层统一通过 TiDB 节点查询，使得 TiDB 成为一款真正的 HTAP 数据库。 3.4 选型通过上文的介绍， 联机事物处理（OLTP）、联机分析处理（OLAP）分别适合于行式存储、列式存储数据库。实际选型时，在时间允许的前提下，需要深入了解业务场景的特点，进行场景模拟测试后，根据评测结果最终选择合适的数据库。例如在联机分析处理场景中，如果大量的查询是读取的记录中的大多数或所有字段，主要由单条记录查询和范围扫描组成，则面向行的存储布局的数据库会更适合。 第四部分 总结在过去几年中，由于对不断增长的数据集和运行复杂分析查询的需求不断增长，产生许多新的面向列的文件格式，如Apache Parquet、Apache ORC、RCFile，以及面向列的存储，如Apache Kudu、ClickHouse，以及许多其他列式数据存储组件。 对于列式存储和行式存储存在的短板，在实际数据系统会有很多巧妙设计去弥补。例如行式存储数据库的索引、分库分表、读写分离等功能加入，大大提升了读写性能。而列式存储在面对数据更新短板，数据库设置缓存池（WAL机制解析），积累一定量更新操作后批量提交执行；对于删除操作，采用先标记后删除方式等设计来提升性能。 随着列式数据库的发展，很多传统的行式数据库也加入了列式存储的支持，形成具有两种存储方式的数据库系统。例如，Oracle在12c版本中推出了in memory组件，使得Oracle数据库具有了双模式数据存放方式。还有新兴数据库TiDB也同时支持行列两种存储模式，从而能够实现对混合类型应用的支持。 总之，技术没有包打一切的银弹，只有适合相应场景的技术，要学会tradeoff。 参考文献及资料1、TiDB项目地址，链接：https://github.com/pingcap/tidb 2、《数据库系统实现》，[美] 加西亚·莫利纳 等 著，杨冬青 等 译 3、《深入理解计算机系统》，作者: Randal E.Bryant / David O’Hallaron]]></content>
      <categories>
        <category>Row-Based，Column-Based</category>
      </categories>
      <tags>
        <tag>Row-Based、Column-Based</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark研发各类报错汇总]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-29-Spark%E7%A0%94%E5%8F%91%E5%90%84%E7%B1%BB%E6%8A%A5%E9%94%99%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[背景主要记录日常的爬坑记录，以Spark相关为主。 第一部分 报错关键字：System memory must be at least 1.1 报错背景本地ideal研发环境（windows）运行spark程序调试，报错如下： 1221/03/29 12:28:36 ERROR SparkContext: Error initializing SparkContext. java.lang.IllegalArgumentException: System memory 259522560 must be at least 471859200. Please increase heap size using the --driver-memory option or spark.driver.memory in Spark configuration. 1.2 原因分析从报错内容上看SparkContext没有初始化成功，错误示内存资源不够，需要至少471859200。 1.2.1 spark本地运行机制Spark在本地运行（local）原理是使用线程模拟进程，所以整个集群启动在一个进程中。 1.2.2 源码分析上源码(Spark 3.1.0，org.apache.spark.memory.UnifiedMemoryManager)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445object UnifiedMemoryManager &#123; // Set aside a fixed amount of memory for non-storage, non-execution purposes. // This serves a function similar to `spark.memory.fraction`, but guarantees that we reserve // sufficient memory for the system even for small heaps. E.g. if we have a 1GB JVM, then // the memory used for execution and storage will be (1024 - 300) * 0.6 = 434MB by default. private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024 def apply(conf: SparkConf, numCores: Int): UnifiedMemoryManager = &#123; val maxMemory = getMaxMemory(conf) new UnifiedMemoryManager( conf, maxHeapMemory = maxMemory, onHeapStorageRegionSize = (maxMemory * conf.get(config.MEMORY_STORAGE_FRACTION)).toLong, numCores = numCores) &#125; /** * Return the total amount of memory shared between execution and storage, in bytes. */ private def getMaxMemory(conf: SparkConf): Long = &#123; val systemMemory = conf.get(TEST_MEMORY) val reservedMemory = conf.getLong(TEST_RESERVED_MEMORY.key, if (conf.contains(IS_TESTING)) 0 else RESERVED_SYSTEM_MEMORY_BYTES) val minSystemMemory = (reservedMemory * 1.5).ceil.toLong if (systemMemory &lt; minSystemMemory) &#123; throw new IllegalArgumentException(s"System memory $systemMemory must " + s"be at least $minSystemMemory. Please increase heap size using the --driver-memory " + s"option or $&#123;config.DRIVER_MEMORY.key&#125; in Spark configuration.") &#125; // SPARK-12759 Check executor memory to fail fast if memory is insufficient if (conf.contains(config.EXECUTOR_MEMORY)) &#123; val executorMemory = conf.getSizeAsBytes(config.EXECUTOR_MEMORY.key) if (executorMemory &lt; minSystemMemory) &#123; throw new IllegalArgumentException(s"Executor memory $executorMemory must be at least " + s"$minSystemMemory. Please increase executor memory using the " + s"--executor-memory option or $&#123;config.EXECUTOR_MEMORY.key&#125; in Spark configuration.") &#125; &#125; val usableMemory = systemMemory - reservedMemory val memoryFraction = conf.get(config.MEMORY_FRACTION) (usableMemory * memoryFraction).toLong &#125;&#125; 主要功能点： systemMemory变量定义了系统内存资源。 123456package org.apache.spark.internal.configval TEST_MEMORY = ConfigBuilder("spark.testing.memory") .version("1.6.0") .longConf .createWithDefault(Runtime.getRuntime.maxMemory) 其中默认值为Runtime.getRuntime.maxMemory，这个值为java虚拟机（JVM）能够从操作系统获取的最大内存资源，如果启动虚拟机时候没有配置-Xmx参数，那么就是256M=256*1024*1024 beytes。 reservedMemory变量为系统保留内存资源。优先使用TEST_RESERVED_MEMORY的值，默认值是个表达式，如果IS_TESTING=True（测试模式）则值为0，否则为：RESERVED_SYSTEM_MEMORY_BYTES=300M。 1234567891011 val TEST_RESERVED_MEMORY = ConfigBuilder("spark.testing.reservedMemory") .version("1.6.0") .longConf .createOptional val IS_TESTING = ConfigBuilder("spark.testing") .version("1.0.1") .booleanConf .createOptional//定义private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024 minSystemMemory变量为系统最小内存资源。定义为reservedMemory的1.5倍。 从报错信息看，明显是触发下面的代码逻辑： 12345if (systemMemory &lt; minSystemMemory) &#123; throw new IllegalArgumentException(s"System memory $systemMemory must " + s"be at least $minSystemMemory. Please increase heap size using the --driver-memory " + s"option or $&#123;config.DRIVER_MEMORY.key&#125; in Spark configuration.") &#125; spark应用中未进行相关参数配置，reservedMemory值为300M，那么minSystemMemory值为450M，而应用程序为设置JVM参数，systemMemory默认是256M。显然触发systemMemory &lt; minSystemMemory条件。 可以使用下面的命令查看java环境的默认Xmx的默认值。 12345#In Windows:java -XX:+PrintFlagsFinal -version | findstr /i "HeapSize PermSize ThreadStackSize"#In Linux:java -XX:+PrintFlagsFinal -version | grep -iE 'HeapSize|PermSize|ThreadStackSize' 1.3 修复方法根据上面的源码分析，我们有下面的修复方法。 1.3.1 生产环境生产代码按照源码分析要求jvm虚拟机至少是450M以上的内存。 配置-Xmx参数，使得其远大于450M。例如： 1-Xmx1024m 1.3.2 测试调试 配置spark.testing.memory参数 这时候systemMemory=spark.testing.memory参数的值，例如： 123val sparkConf = new SparkConf() .set("spark.testing.memory","2147480000")//2147480000=2G 开启测试模式（IS_TESTING=True） 12val sparkConf = new SparkConf() .set("spark.testing","true") 这时候minSystemMemory即为0。这时候异常条件不会触发。 指定spark.testing.reservedMemory参数的值（尽可能的小） 12val sparkConf = new SparkConf() .set("spark.testing.reservedMemory","0") 上面的配置下，minSystemMemory的值也为0。 1.4 总结Spark运行对内存资源进行了门槛限制，如果降低这个限制必须要特意显示配置测试相关的指标配置。 第二部分 报错关键字：The maximum recommended task size is 100 KB1.1 报错背景1Stage 0 contains a task of very large size (183239 KB). The maximum recommended task size is 100 KB. 1.2 原因分析此错误消息意味着将一些较大的对象从driver端发送到executors。 spark rpc传输序列化数据是有大小的限制，默认大小是128M（即131072K, 134217728 字节）。所以需要修改spark.rpc.message.maxSize配置的值。 1.3 修复方法在Dirver程序中，修改spark.rpc.message.maxSize 值，例如，增大到1024M： 1--conf spark.rpc.message.maxSize=1024 1.4 总结暂无 第三部分 报错关键字：Spark java.lang.UnsupportedClassVersionError: xxxxxx: Unsupported major.minor version 52.01.1 报错背景123456789101112131415[root@quickstart bin]# ./run-example org.apache.spark.examples.SparkPiException in thread "main" java.lang.UnsupportedClassVersionError: org/apache/spark/launcher/Main : Unsupported major.minor version 52.0 at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:800) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:449) at java.net.URLClassLoader.access$100(URLClassLoader.java:71) at java.net.URLClassLoader$1.run(URLClassLoader.java:361) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:482) 1.2 原因分析Unsupported major.minor version 52.0错误信息时，就可以确定是由于JDK版本低于1.8导致的。即编译时使用了JDK1.8，但是运行环境中的JDK版本低于1.8导致的。 检查一下本地机器的JAVA_HOME环境： 12[root@quickstart home]# env|grep JAVA_HOMEJAVA_HOME=/usr/java/jdk1.7.0_67-cloudera 而实际需要运行的Spark版本为：Spark 2.3.0版本。 1.3 修复方法调整JAVA_HOME的版本为1.8版本： 1234[root@quickstart bin]# java -versionopenjdk version "1.8.0_41"OpenJDK Runtime Environment (build 1.8.0_41-b04)OpenJDK 64-Bit Server VM (build 25.40-b25, mixed mode) 1.4 总结参考文献及资料https://blog.csdn.net/wangshuminjava/article/details/79792961]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络的表达能力]]></title>
    <url>%2F2021%2F03%2F21%2F2021-04-03-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B%2F</url>
    <content type="text"><![CDATA[背景人工神经网络（Artificial Neural Network）首次提出要追溯到上个世纪40年代，由于当时缺少有效的算法支撑，学界研究较为低迷。直到1984年，Hinton等人提出了反向传播算法（Backpropagation），才重新点燃了学界的研究热潮，开始训练多层（更深）神经网络。这时候神经网络就改头换面，有了一个更高大上的名字：深度学习（Deep learning）。深度学习的工程应用在工业界异常火热，但是理论基础在学界的研究却是滞后的，所以人送外号：“炼金术”。 神经网络有很多基本问题需要理论解答： 为什么深度学习优于其他模型？泛化性能优于其他模型？ 为什么深度模型优于浅层模型？ 深度学习是否存在学习边界，即表达能力的极限在哪？ 为什么是深度学习，而不是宽度学习？ 数据分布在流形上，为什么深度学习直接使用欧式距离计算通常是有效的？ 上面的开放问题有些已经有接近的答案。 第一部分 Universal Approximation Theorem在人工神经网络领域的数学观点中，「通用近似定理 (Universal approximation theorem，一译万能逼近定理)」指的是：如果一个前馈神经网络具有线性输出层和至少一层隐藏层，只要给予网络足够数量的神经元，便可以实现以足够高精度来逼近任意一个在 \mathbb{R}^nRn 的紧子集 (Compact subset) 上的连续函数。 这一定理表明，只要给予了适当的参数，我们便可以通过简单的神经网络架构去拟合一些现实中非常有趣、复杂的函数。这一拟合能力也是神经网络架构能够完成现实世界中复杂任务的原因。尽管如此，此定理并没有涉及到这些参数的算法可学性 (Algorithmic learnablity)。 通用近似定理用数学语言描述如下： 令 $\varphi$ 为一单调递增、有界的非常数连续函数。记 $m$ 维单元超立方体 (Unit hypercube) $[0,1]^{m}$ 为 $I_{m},$ 并记在 $I_{m}$ 上 的连续函数的值域为 $C\left(I_{m}\right)$ 。则对任意实数 $\epsilon&gt;0$ 与函数 $f \in C\left(I_{m}\right),$ 存在整数 $N_{\text {、常数 }} v_{i}, b_{i} \in \mathbb{R}$ 与向量 $w_{i} \in \mathbb{R}^{m}(i=1, \ldots, n),$ 使得我们可以定义 :$$F(x)=\sum_{i=1}^{N} v_{i} \varphi\left(w_{i}^{T} x+b_{i}\right)$$为 $f$ 的目标拟合实现。在这里， $f$ 与 $\varphi$ 无关，亦即对任意 $x \in I_{m},$ 有：$$|F(x)-f(x)|&lt;\epsilon$$因此, 形为 $F(x)$ 这样的函数在 $C\left(I_{m}\right)$ 里是稠密的。替换上述 $I_{m}$ 为 $\mathbb{R}^{m}$ 的任意紧子集，结论依然成立。 在 1989 年，George Cybenko 最早提出并证明了这一定理在激活函数为 Sigmoid 函数时的特殊情况。那时，这一定理被看作是 Sigmoid 函数的特殊性质。但两年之后，Kurt Hornik 研究发现，造就「通用拟合」这一特性的根源并非 Sigmoid 函数，而是多层前馈神经网络这一架构本身。当然，所用的激活函数仍然必须满足一定的弱条件假设，常数函数便是显然无法实现的。 https://www.jiqizhixin.com/articles/2019-02-21-12 https://zhuanlan.zhihu.com/p/64802339 https://iphysresearch.github.io/blog/post/ml_notes/liweiwang_dl_from_theory_to_algorithm/ https://blog.csdn.net/KeEN_Xwh/article/details/113478160 http://neuralnetworksanddeeplearning.com/chap4.html 第四部分 总结1、数据流形中数据采样充分的时候，模型的训练效果较好 2、深度学习有效的重要假设前提是数据流形的训练数据是充分采样的。 参考文献及资料]]></content>
      <categories>
        <category>Presto</category>
      </categories>
      <tags>
        <tag>Presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一性原理]]></title>
    <url>%2F2021%2F03%2F21%2F2021-04-03-%E7%AC%AC%E4%B8%80%E6%80%A7%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料]]></content>
      <categories>
        <category>Presto</category>
      </categories>
      <tags>
        <tag>Presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用DeOldify项目修复老照片]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-21-%E4%BD%BF%E7%94%A8DeOldify%E9%A1%B9%E7%9B%AE%E4%BF%AE%E5%A4%8D%E8%80%81%E7%85%A7%E7%89%87%2F</url>
    <content type="text"><![CDATA[背景DeOldify是由Jason Antic创建的一个开源深度学习模型，用于为灰度图像添加高质量的色彩效果，效果令人赞叹。简而言之，这种深度学习模型的目标是对旧图像和胶片进行着色，还原并赋予新的生命。 第一部分 部署项目从github上拉取项目（考虑到墙的因素可能需要下载zip包方式）： 1git clone https://github.com/jantic/DeOldify.git 安装依赖包（过程较为缓慢）： 1root@deeplearning:/data/DeOldify# pip install -r requirements.txt 第二部分 准备预训练模型下载预训练模型： 1root@deeplearning:/data/DeOldify# mkdir models 照片类： 1wget https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth 视频类： 1wget https://data.deepai.org/deoldify/ColorizeVideo_gen.pth 注意：以上都是缓慢的过程。 第三部分 实验结果启动jupyter。 1root@deeplearning:/data/DeOldify# jupyter notebook --no-browser --port 8888 --ip=192.168.1.3 --allow-root &amp; 打开ImageColorizer.ipynb文件。运行前需要进行提前配置： 图片附件参数配置 123456source_url= None# 需要修复的老照片地址，如果来自网络使用url地址，如果是本地值为Nonesource_path = 'test_images/lihuanyin.jpg'# 需要修复的老照片路径result_path = 'result_images'# 输出结果路径 预训练模型文件配置 12root@deeplearning:/data/DeOldify/models# lsColorizeArtistic_gen.pth 最后我们运行模型，主要测试结果对比如下： 第一张：李焕英。 第二张:1860年上海豫园。 第四部分 总结效果上看图片还是模糊的，只是涂色。需要对图片进行增强。 参考文献及资料1、DeOldify项目地址，链接：https://github.com/jantic/DeOldify]]></content>
      <categories>
        <category>DeOldify</category>
      </categories>
      <tags>
        <tag>DeOldify</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用深度学习对图片进行增强]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-21-%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AF%B9%E5%9B%BE%E7%89%87%E8%BF%9B%E8%A1%8C%E5%A2%9E%E5%BC%BA%2F</url>
    <content type="text"><![CDATA[背景第一部分 环境部署https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life https://github.com/aiff22/DPED https://www.zhihu.com/question/319291048 https://github.com/CrazyVertigo/awesome-data-augmentation 参考文献及资料1、DPED项目地址，链接：http://people.ee.ethz.ch/~ihnatova/#dataset]]></content>
      <categories>
        <category>DeOldify</category>
      </categories>
      <tags>
        <tag>DeOldify</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡的总结]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-30-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%9A%84%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景常见的负载均衡算法包含：1、轮询法（Round Robin）2、加权轮询法（Weight Round Robin）3、随机法（Random）4、加权随机法（Weight Random）5、平滑加权轮询法（Smooth Weight Round Robin）6、源地址哈希法（Hash）7、最小连接数法（Least Connections） 轮询调度算法 (Round-Robin)轮询调度算法的原理是每一次把来自用户的请求轮流分配给内部中的服务器，从1开始，直到N(内部服务器个数)，然后重新开始循环。算法的优点是其简洁性，它无需记录当前所有连接的状态，所以它是一种无状态调度。 参考文献及资料]]></content>
      <categories>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用深度学习对图片进行动作驱动]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-21-%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AF%B9%E5%9B%BE%E7%89%87%E8%BF%9B%E8%A1%8C%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[背景https://github.com/AliaksandrSiarohin/first-order-model 参考文献及资料1、DeOldify项目地址，链接：https://github.com/jantic/DeOldify]]></content>
      <categories>
        <category>DeOldify</category>
      </categories>
      <tags>
        <tag>DeOldify</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Presto使用介绍]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-31-Presto%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[背景Presto是一个分布式SQL查询引擎， 它被设计为用来专门进行高速、实时的数据分析。它支持标准的ANSI SQL，包括复杂查询、聚合（aggregation）、连接（join）和窗口函数（window functions)。Presto的运行模型和Hive或MapReduce有着本质的区别。Hive将查询翻译成多阶段的MapReduce任务， 一个接着一个地运行。 每一个任务从磁盘上读取输入数据并且将中间结果输出到磁盘上。 然而Presto引擎没有使用MapReduce。它使用了一个定制的查询和执行引擎和响应的操作符来支持SQL的语法。除了改进的调度算法之外， 所有的数据处理都是在内存中进行的。 不同的处理端通过网络组成处理的流水线。 这样会避免不必要的磁盘读写和额外的延迟。 这种流水线式的执行模型会在同一时间运行多个数据处理段， 一旦数据可用的时候就会将数据从一个处理段传入到下一个处理段。 这样的方式会大大的减少各种查询的端到端响应时间。 参考文献及资料]]></content>
      <categories>
        <category>Presto</category>
      </categories>
      <tags>
        <tag>Presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink各类资源管理器部署总结]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-21-Flink%E5%90%84%E7%B1%BB%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%99%A8%E9%83%A8%E7%BD%B2%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景第一部分 Flink集群基础架构Flink集群通常下图所示的组件组成。Flink客户端将Flink应用转换成JobGraph，然后提交给JobManager。JobManager将任务分发给TaskManager运行，当然集群通常还有一些支持组件组成。 1.1 Flink 客户端（Flink Client）本地执行Flink任务的main()方法，解析生成JobGraph对象，最后将JobGraph提交至JobManager，同时监控job认为运行状态。 1.2 JobManager JobManager为Flink集群中管理服务，管理整个集群的计算资源、job任务管理和调度。 1.3 TaskManager集群中具体执行计算任务的服务节点。 1.4 High Availability Service Provider1.5 File Storage and Persistency1.6 Resource Provider1.7 Metrics Storage1.8 Application-level data sources and sinks第二部分 Flink部署模式Flink应用有3种运行模式： Session Mode Per-Job Mode Application Mode 3种模式区别主要是： 第三部分 Flink资源管理器第四部分 Flink Stabdalone模式第五部分 Flink on Yarn模式 第六部分 Flink on K8s模式第七部分 高可用模式原理参考文献及资料1、DeOldify项目地址，链接：https://github.com/jantic/DeOldify]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中国经济疫情期间分析]]></title>
    <url>%2F2021%2F03%2F21%2F2022-03-12-%E4%B8%AD%E5%9B%BD%E7%BB%8F%E6%B5%8E%E7%96%AB%E6%83%85%E6%9C%9F%E9%97%B4%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[背景记录日本。日本股市表现最优秀的行业是廉价零售业、精密制造业、互联网行业、医药制造业。银行、航空、电力表现落后于日经指数。 \1. Fast retailing 优衣库母公司 涨幅约60倍 \2. CyberAgent 互联网广告 涨约74倍 \3. Z Holding Corp 运行Yahoo! Japan 约涨223倍 \4. Keyence Corp 精密仪器制造 约涨17倍 \5. Pan Pacific International Holdings Corp 唐吉坷德连锁折扣店 约涨19倍 \6. Soft Bank 软银集团 投资阿里巴巴闻名 约涨23倍 \7. Cosmos Pharmaceutical Corp 廉价药品及日用品零售 约涨18倍 \8. MonotaRo 桃太郎网上商城 主营各种工具 约涨12倍 每一个经济危机一般都有一个繁荣时代做前奏。经济危机是指资本主义再生产过程中周期性爆发的生产过剩的危机，而生产力过剩都是一个演变过程，即从供不应求到充足到有剩余到过剩的过程。一般如果在有剩余时不及时控制，就会出现生产过剩的情况。生产过剩一般会导致物价的非正常波动，有些企业为缩小差距就会刻意压低或抬升物价，进而导致市场紊乱，这就是经济危机。 参考文献及资料]]></content>
      <categories>
        <category>Presto</category>
      </categories>
      <tags>
        <tag>Presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot系列文章（使用SpringBoot部署Hive Restful接口项目）]]></title>
    <url>%2F2021%2F03%2F20%2F2021-03-20-SpringBoot%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88%E4%BD%BF%E7%94%A8SpringBoot%E9%83%A8%E7%BD%B2Hive%20Restful%E6%8E%A5%E5%8F%A3%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景https://shalishvj.wordpress.com/2018/06/02/hive-jdbc-spring-boot-restful-webservice-in-pivotal-cloud-foundry/ 参考文献及资料1、Spring官网，链接： http://blog.hming.org/2018/11/19/spring-boot-shi-yong-jdbc-lian-jie-hive/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Vue开发、测试、打包以及生产环境部署总结]]></title>
    <url>%2F2021%2F03%2F20%2F2021-03-20-Vue%E5%BC%80%E5%8F%91%E3%80%81%E6%B5%8B%E8%AF%95%E3%80%81%E6%89%93%E5%8C%85%E4%BB%A5%E5%8F%8A%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发、测试环境项目部署 第二部分 项目打包 第三部分 生产环境部署 第四部分 总结 参考文献及资料 背景本文简略介绍了使用vue-cli工具构建研发和测试环境，最后介绍了项目的生产部署。 第一部分 研发环境准备vue-cli 是一个官方发布 vue.js 项目脚手架，使用 vue-cli 可以快速创建 vue 项目。 直接使用webpack创建项目： 123456789# 安装vue-cli脚手架$ npm install -g vue-cli# 初始化项目$ vue init webpack my-project# 交互式回显：项目名称、项目描述、作者、打包方式、是否使用ESLint规范代码等等，根据需要自行考虑yes还是no$ cd my-project# 安装项目依赖$ npm install 项目文件树结构： 1234567891011121314151617181920212223242526272829303132333435vue-test├── build/ # 脚本目录：webpack 编译任务配置文件:(开发环境与生产环境)│ └── ...├── config/ # 配置目录│ ├── index.js # 项目核心配置│ └── ...├ ── node_module/ # 依赖的node工具包目录：项目中安装的依赖模块├── src/ # 源码文件目录：自己的项目文件都需要放到 src 文件夹下│ ├── main.js # 程序入口JS文件│ ├── App.vue # 程序入口vue组件│ ├── components/ # 组件目录│ │ └── ...│ └── assets/ # 资源文件目录：一般放一些静态资源文件│ └── ...├── static/ # 静态资源文件目录 (直接拷贝到dist/static/里面)├── test/ # 测试文件目录│ └── unit/ # 单元测试│ │ ├── specs/ # 测试规范│ │ ├── index.js # 测试入口文件│ │ └── karma.conf.js # 测试运行配置文件│ └── e2e/ # 端到端测试│ │ ├── specs/ # 测试规范│ │ ├── custom-assertions/ # 端到端测试自定义断言│ │ ├── runner.js # 运行测试的脚本│ │ └── nightwatch.conf.js # 运行测试的配置文件├── .babelrc # babel 配置文件├── .editorconfig # 编辑配置文件├── .eslintignore # ├── .eslintrc.js # ES语法检查配置├── .gitignore # 用来过滤一些版本控制的文件，比如node_modules文件夹 ├── .postcssrc.js # ├── index.html # 入口页面├── package.json # 项目描述文件：记载着一些命令和依赖还有简要的项目描述信息 ├── package-lock.json #└── README.md #介绍自己这个项目的，可参照 github上star多的项目。 最后启动开发环境： 12# 启动开发环境$ npm run dev 默认在本地监听8080端口，打开地址：http://localhost:8080/。如果本地端口资源冲突，可以在config/index.js文件中修改port的值进行端口替换。 第二部分 生产环境打包项目开发完成之后，需要将项目部署在生产环境，通常使用下面的命令打包： 1# npm run build 打包命令执行后，将整个项目编译成静态文件，会在项目目录中生成dist文件夹（index.html文件和static文件夹）。只需要将 dist文件夹放到web服务器上即可。 这里的web服务器可以是Tomcat、Nginx、Apache等web服务器。 第三部分 生产环境部署3.1 Tomcat部署首先当然是部署好tomcat基础环境，然后将项目打包文件部署在tomcat/apache-tomcat-9.0.41/webapps，各自的tomcat版本差异不同哈。 主要的实施步骤为： config/index.js (修改assetsPublicPath, 修改目的是为了解决js找不到的问题) 1assetsPublicPath: '/VueTest/' 这里VueTest是项目的名称，用户自行定义。 当项目中使用vue-router的时候，调整src/router/index.js： 1234567891011export default new Router(&#123; routes: [ &#123; path: '/', name: 'HelloWorld', component: HelloWorld &#125; ], mode: 'history', base: '/VueTest/'&#125;) 这里主要增加了两个参数：mode和base。 执行npm run build,进行打包生成dist文件 最后上传打包文件： 123# tomcat/apache-tomcat-9.0.41/webapps/VueTest# -rw------- 1 root root 545 7月 24 05:45 index.htmldrwx------ 4 root root 4096 7月 24 13:38 static/ 启动tomcat，服务访问url为：http://ip:8080/VueTest/，其中ip为tomcat的部署服务器,8080为tomcat的默认服务端口。 3.2 Nginx部署首先完成nginx部署，文件目录如下： 12root@deeplearning:/data/nginx# lsconf html logs sbin 按照tomcat的方式配置和打包后，放在html目录下面，即： 123# nginx/html/VueTest# llindex.htmlstatic/ 接下来需要配置nginx配置文件： 1234567891011121314151617181920212223242526272829303132333435363738http &#123; server &#123; # 监听的端口号 listen 8081; # 服务名称 生产环境要修改成 公网ip 如 47.105.134.120 server_name localhost; # 配置根目录的地址是以 nginx 下的 html 文件夹为根目录来查找的 root html; # 配置默认的主页显示 比如 47.105.134.120:8080 显示的 index 页面 location / &#123; try_files $uri $uri/ /index.html; &#125; # 配置我们的 admin 的前台服务 比如 47.105.134.120:8080/admin/index.html location ^~ /VueTest &#123; # 处理 Vue 单页面应用 路由模式为 history 模式刷新页面 404 的问题 try_files $uri $uri/ /VueTest/index.html; &#125; # 如果你想配置多个项目的话，可以复制上面的，修改一下就可以同时部署多个前端项目了 # 比如 # location ^~ /blog &#123; # 处理 Vue 单页面应用 路由模式为 history 模式刷新页面 404 的问题 # try_files $uri $uri/ /blog/index.html; # &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; include servers/*;&#125; 最后启动Nginx： 1root@deeplearning:/data/nginx/sbin# ./nginx &amp; 打开url地址：http://ip:8081/VueTest/ 最后补充一个坑： 打开网页报403权限问题，通常是html中的项目文件权限问题。 12345# 更改文件夹权限find . -type d -exec chmod 755 &#123;&#125; \;# 更改普通文件权限find . -type f -exec chmod 644 &#123;&#125; \;重启nginx即可 参考文献及资料1、vue项目文档，链接：https://vuejs.org/]]></content>
      <categories>
        <category>Vue</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[若依spring-cloud项目前端模块（Vue）介绍]]></title>
    <url>%2F2021%2F03%2F20%2F2021-03-20-%E8%8B%A5%E4%BE%9Dspring-cloud%E9%A1%B9%E7%9B%AE%E5%89%8D%E7%AB%AF%E6%A8%A1%E5%9D%97%EF%BC%88Vue%EF%BC%89%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 基础环境准备 第二部分 开发环境项目部署 第三部分 生产部署 第四部分 总结 参考文献及资料 背景http://doc.ruoyi.vip/ruoyi-cloud/document/qdsc.html 第一部分 项目目录结构简介1234567891011121314151617181920212223242526272829303132ruoyi-ui├── babel.config.js├── bin│ ├── build.bat│ ├── package.bat│ └── run-web.bat├── build│ └── index.js├── node_modules├── package-lock.json├── package.json├── public│ ├── favicon.ico│ ├── index.html│ └── robots.txt├── README.md├── src│ ├── api│ ├── App.vue│ ├── assets│ ├── components│ ├── directive│ ├── layout│ ├── main.js│ ├── permission.js│ ├── router│ ├── settings.js│ ├── store│ ├── utils│ └── views├── tree.md└── vue.config.js 参考文献及资料1、RuoYi-Cloud项目文档，链接：http://doc.ruoyi.vip/ruoyi-cloud/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[若依Spring-cloud项目部署介绍]]></title>
    <url>%2F2021%2F03%2F20%2F2021-03-20-%E8%8B%A5%E4%BE%9Dspring-cloud%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 基础环境准备 第二部分 开发环境项目部署 第三部分 生产部署 第四部分 总结 参考文献及资料 背景第一部分 基础环境准备1.1 MySql环境本地或远程部署mysql环境，验证服务正常。 1.2 Nacos环境部署Nacos支持单机、集群和多集群部署，对于测试环境我们部署为单机模式。另外单机模式我们使用对接mysql数据库而不是内置的嵌入式数据库存储。主要步骤有： 部署项目文件系统 从github（https://github.com/alibaba/nacos）上下载稳定编译版本包（`nacos-server-$version.zip`），在目标路径解压即可： 1unzip nacos-server-$version.zip 确认mysql数据准备就绪； 创建系统库和表（conf/nacos-mysql.sql 提供初始化语句）； 修改配置文件（conf/application.properties）中mysql链接参数； 1234567891011#*************** Config Module Related Configurations ***************#### If use MySQL as datasource:spring.datasource.platform=mysql### Count of DB:db.num=1### Connect URL of DB:db.url.0=jdbc:mysql://localhost:3306/ry-config?characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useUnicode=true&amp;useSSL=false&amp;serverTimezone=UTCdb.user=rootdb.password=root 1.3 启动Nacos本地研发环境是windows，使用下面的命令进行启动： 1D:\RuoYi-Cloud\nacos\bin&gt;startup.cmd -m standalone 服务的监听端口为：8848，页面控制台地址为：http://localhost:8848/nacos/#/login，默认用户名和密码为：`nacos/nacos`。 第二部分 开发环境项目部署前往Gitee下载页面(https://gitee.com/y_project/RuoYi-Cloud (opens new window))下载解压到目标目录。使用ideal导入项目。 若依项目模块较多我们启动基础模块： RuoYiGatewayApplication （网关模块 必须） RuoYiAuthApplication （认证模块 必须） RuoYiSystemApplication （系统模块 必须） RuoYiMonitorApplication （监控中心 可选） RuoYiGenApplication （代码生成 可选） RuoYiJobApplication （定时任务 可选） RuoYFileApplication （文件服务 可选） 2.1 数据库初始化项目中有个sql子目录，里面有项目初始化的数据文件： 123quartz.sql # 定时任务表ry_20210108.sql #项目主库ry_config_20201222.sql # nacos服务初始化表 使用sql文件初始化表和库。 2.2 前端启动前端使用Vue编写，项目目录为ruoyi-ui，启动步骤如下： 安装依赖包 12cd ruoyi-uinpm install 启动项目 1npm run dev 回显： 123456App running at:- Local: http://localhost:80/- Network: http://192.168.1.1:80/Note that the development build is not optimized.To create a production build, run npm run build. 前端服务监听端口为：80，页面地址为：http://localhost:80 ，默认账户/密码 admin/admin123，界面如下： 2.3 启动后台服务研发环境分别在项目中启动： RuoYiGatewayApplication （网关模块 必须） RuoYiAuthApplication （认证模块 必须） RuoYiSystemApplication （系统模块 必须） 这是就可以通过前台进行登录： 第三部分 生产部署在ruoyi项目的bin目录下执行package.bat打包Web工程，生成war/jar包文件。然后会在项目下生成target文件夹包含war或jar 第四部分 源码阅读4.1 整体项目4.2 服务网关4.2.1 依赖Spring Cloud Gateway12345&lt;!-- spring cloud gateway 依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt; 参考文献及资料1、RuoYi-Cloud项目文档，链接：http://doc.ruoyi.vip/ruoyi-cloud/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java面向对象基础总结]]></title>
    <url>%2F2021%2F03%2F20%2F2021-07-24-Java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发、测试环境项目部署 第二部分 项目打包 第三部分 生产环境部署 第四部分 总结 参考文献及资料 背景第一部分 方法1.1 类变量的私有性1234567891011public class Person &#123; private String name; private int age;&#125;public class PersonMain &#123; public static void main(String[] args) &#123; Person xiaoming = new Person(); xiaoming.name = "xiao ming"; xiaoming.age = 21; &#125; 运行报错，Person中变量的私有，外部无法直接修改。 123456789101112131415161718192021222324252627282930313233343536373839public class Person &#123; private String name; private int age; public String getName() &#123; return name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; if (age &lt; 0 || age &gt; 100) &#123; throw new IllegalArgumentException("invalid age value"); &#125;else &#123; this.age = age; &#125; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return "Person&#123;" + "name='" + name + '\'' + ", age=" + age + '&#125;'; &#125;&#125;public class PersonMain &#123; public static void main(String[] args) &#123; Person xiaoming = new Person(); xiaoming.setName("xiao ming"); xiaoming.setAge(21); System.out.println(xiaoming.getAge()); &#125;&#125; 1.2 this变量在方法内部，可以使用一个隐含的变量this，它始终指向当前实例。因此，通过this.field就可以访问当前实例的字段。例如下面的案例： 12345678public void setAge(int age) &#123; if (age &lt; 0 || age &gt; 100) &#123; throw new IllegalArgumentException("invalid age value"); &#125;else &#123; this.age = age; //前面的this不可少，少了就变成局部变量name了 &#125;&#125; 1.3 方法参数和可变参数1234567class Group &#123; private String[] names; public void setNames(String[] names) &#123; this.names = names; &#125;&#125; 1.4 参数绑定调用方把参数传递给实例方法时，调用时传递的值会按参数位置一一绑定。 基本类型参数的传递，是调用方值的复制。双方各自的后续修改，互不影响。 第二部分 构造方法2.1 自建构造方法能否在创建对象实例时就把内部字段全部初始化为合适的值？这就是构造方法： 1234public Person(String name, int age) &#123; this.name = name; this.age = age;&#125; 2.2 默认构造方法如果一个类没有定义构造方法，编译器会自动为我们生成一个默认构造方法，它没有参数，也没有执行语句。 没有在构造方法中初始化字段时，引用类型的字段默认是null，数值类型的字段用默认值，int类型默认值是0，布尔类型默认值是false。 1234class Person &#123; public Person() &#123; &#125;&#125; 如果我们自定义了一个构造方法，那么，编译器就不再自动创建默认构造方法。 第三部分 方法重载在一个类中，我们可以定义多个方法。如果有一系列方法，它们的功能都是类似的，只有参数有所不同，那么，可以把这一组方法名做成同名方法。 1234567891011121314151617class Hello &#123; public void hello() &#123; System.out.println("Hello, world!"); &#125; public void hello(String name) &#123; System.out.println("Hello, " + name + "!"); &#125; public void hello(String name, int age) &#123; if (age &lt; 18) &#123; System.out.println("Hi, " + name + "!"); &#125; else &#123; System.out.println("Hello, " + name + "!"); &#125; &#125;&#125; 这种方法名相同，但各自的参数不同，称为方法重载（Overload）。 第四部分 继承继承是面向对象编程中非常强大的一种机制，它首先可以复用代码。当我们让Student从Person继承时，Student就获得了Person的所有功能，我们只需要为Student编写新增的功能。 12345678class Student extends Person &#123; // 不要重复name和age字段/方法, // 只需要定义新增score字段/方法: private int score; public int getScore() &#123; … &#125; public void setScore(int score) &#123; … &#125;&#125; 另外由于子类不能继承父类的构造方法，因此，如果要调用父类的构造方法，可以使用 super 关键字。super 可以用来访问父类的构造方法、普通方法和属性。 super 关键字的功能： 在子类的构造方法中显式的调用父类构造方法 访问父类的成员方法和变量。 12345678public class Student extends Person &#123; public Student(String name, int age, String birth) &#123; super(name, age); // 调用父类中含有2个参数的构造方法 &#125; public Student(String name, int age, String sex, String birth) &#123; super(name, age, sex); // 调用父类中含有3个参数的构造方法 &#125;&#125; 在OOP的术语中，我们把Person称为超类（super class），父类（parent class），基类（base class），把Student称为子类（subclass），扩展类（extended class）。 Java只允许一个class继承自一个类，因此，一个类有且仅有一个父类。只有Object特殊，它没有父类。 当用final修饰一个类时，表明这个类不能被继承。也就是说，如果一个类你永远不会让他被继承，就可以用final进行修饰。final类中的成员变量可以根据需要设为final，但是要注意final类中的所有成员方法都会被隐式地指定为final方法。 第五部分 多态在继承关系中，子类如果定义了一个与父类方法签名完全相同的方法，被称为覆写（Override）。 例如，在Person类中，我们定义了run()方法： 12345class Person &#123; public void run() &#123; System.out.println("Person.run"); &#125;&#125; 在子类Student中，覆写这个run()方法： 123456class Student extends Person &#123; @Override public void run() &#123; System.out.println("Student.run"); &#125;&#125; Override和Overload不同的是，如果方法签名不同，就是Overload，Overload方法是一个新方法；如果方法签名相同，并且返回值也相同，就是Override。 第六部分 抽象类如果一个class定义了方法，但没有具体执行代码，这个方法就是抽象方法，抽象方法用abstract修饰。 因为无法执行抽象方法，因此这个类也必须申明为抽象类（abstract class）。 1234567891011121314151617public class Main &#123; public static void main(String[] args) &#123; Person p = new Student(); p.run(); &#125;&#125;abstract class Person &#123; public abstract void run();&#125;class Student extends Person &#123; @Override public void run() &#123; System.out.println("Student.run"); &#125;&#125; 第七部分 接口12345678910111213141516171819202122232425public class Main &#123; public static void main(String[] args) &#123; Person p = new Student("Xiao Ming"); p.run(); &#125;&#125;interface Person &#123; String getName(); default void run() &#123; System.out.println(getName() + " run"); &#125;&#125;class Student implements Person &#123; private String name; public Student(String name) &#123; this.name = name; &#125; public String getName() &#123; return this.name; &#125;&#125; 第八部分 静态字段和静态方法第九部分 包第十部分 作用域第十一部分 classpath和jar第十二部分 模块参考文献及资料1、vue项目文档，链接：https://vuejs.org/]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mysql中读写事务和只读事务总结]]></title>
    <url>%2F2021%2F03%2F20%2F2021-07-27-Mysql%E4%B8%AD%E8%AF%BB%E5%86%99%E4%BA%8B%E5%8A%A1%E5%92%8C%E5%8F%AA%E8%AF%BB%E4%BA%8B%E5%8A%A1%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发、测试环境项目部署 第二部分 项目打包 第三部分 生产环境部署 第四部分 总结 参考文献及资料 背景念：从这一点设置的时间点开始（时间点a）到这个事务结束的过程中，其他事务所提交的数据，该事务将看不见！（查询中不会出现别人在时间点a之后提交的数据） 应用场合： 如果你一次执行单条查询语句，则没有必要启用事务支持，数据库默认支持SQL执行期间的读一致性；如果你一次执行多条查询语句，例如统计查询，报表查询，在这种场景下，多条查询SQL必须保证整体的读一致性，否则，在前条SQL查询之后，后条SQL查询之前，数据被其他用户改变，则该次整体的统计查询将会出现读数据不一致的状态，此时，应该启用事务支持。【注意是一次执行多次查询来统计某些信息，这时为了保证数据整体的一致性，要用只读事务】 怎样设置： 对于只读查询，可以指定事务类型为readonly，即只读事务。由于只读事务不存在数据的修改，因此数据库将会为只读事务提供一些优化手段，例如Oracle对于只读事务，不启动回滚段，不记录回滚log。 （1）在JDBC中，指定只读事务的办法为： connection.setReadOnly(true); （2）在Hibernate中，指定只读事务的办法为： session.setFlushMode(FlushMode.NEVER);此时，Hibernate也会为只读事务提供Session方面的一些优化手段 （3）在Spring的Hibernate封装中，指定只读事务的办法为： bean配置文件中，prop属性增加“readOnly”或者用注解方式@Transactional(readOnly=true)【 if the transaction is marked as read-only, Spring will set the Hibernate Session’s flush mode to FLUSH_NEVER,and will set the JDBC transaction to read-only】也就是说在Spring中设置只读事务是利用上面两种方式 在将事务设置成只读后，相当于将数据库设置成只读数据库，此时若要进行写的操作，会出现错误 参考文献及资料1、vue项目文档，链接：https://vuejs.org/]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink学习笔记]]></title>
    <url>%2F2021%2F03%2F18%2F2021-03-18-Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[背景第一部分 Flink入门第二部分 Flink应用第三部分 核心抽象第四部分 时间和窗口第五部分 类型和序列化第六部分 内存管理第七部分 状态原理第八部分 作业提交第九部分 资源管理第十部分 作业调度第十一部分 作业执行第十二部分 数据交换第十三部分 应用容错第十四部分 Flink SQL第十五部分 运维监控第十六部分 RPC框架参考文献及资料]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中日志体系整理]]></title>
    <url>%2F2021%2F03%2F13%2F2021-03-13-Java%E4%B8%AD%E6%97%A5%E5%BF%97%E4%BD%93%E7%B3%BB%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 日志发展历程 第二部分 日志分类 第三部分 最佳实践 参考文献及资料 背景对于一个应用程序来说日志记录是必不可少的一部分。线上问题追踪，基于日志的业务逻辑统计分析等都离不日志。 第一部分 日志发展历程1.1 第一阶段2001年前，Java还没有日志库，通过System.out和System.err打印和记录日志。这个方式通常作为开发调试时候使用，线上生产环境使用主要缺点有：打印过程有大量IO、输出只能在控制台打印无法持久化保存在文件。 该阶段还未有成熟的日志框架（Logging Framework ）。 1.2 第二阶段2001 年，瑞士工程师ceki Gulcü的研发了一个日志框架 log4j（后来将log4j 项目捐献给Apache，成为 Apache 项目，Ceki 加入 Apache 组织）。 log4j 的前生是SEMPER（欧洲安全电子市场项目组）日志工具： Tracing API 。后通过迭代优化后，在IBM 公共许可证下开源。参考Ceki Gülcü博客文章：Log4j delivers control over logging。 ceki Gulcü是Java 日志发展中关键人物，整个Java的日志体系几乎都有他参与或者受到了ceki Gulcü的影响。 再后来Log4j捐献给Apache基金会,ceki Gulcü也一同”嫁入”Apache。Log4j逐渐成为Java社区的日志标准。还有一种说法，Apache基金会曾经建议Sun引入Log4j到Java的标准库中，但是被拒绝了。原因是Sun是有’’私心”的，因为他想搞自己的日志标准库。 已经出现第一个日志框架（Logging Framework ）：Log4j。 1.3 第三阶段2002年2月JDK 1.4正式发布，Sun终于拿出了自己的日志标准库JUL(Java Util Logging)。其实是”借鉴” Log4j 实现的，而且直到 JDK1.5 后，性能和可用性才有了提升。JUL 出来晚，还不如Log4j好用，程序员们的眼光是雪亮的，所以Log4j仍然比JUL使用广泛。 虽然Log4j比JUL好用，但是JUL是原生在JDK中的，根正苗红，仍然有一部分用户。作为用户研发，可以任选一个日志架构。但是如果在一个项目中应用程序和第三方库使用不同日志框架呢？标准不统一影响整个软件业的生产力。 出现第二个日志框架（Logging Framework ）：JUL(Java Util Logging)。日志框架出现竞争。 1.4 第四阶段紧接着，同年（2002年）8月Apache又推出JCL(Jakarta Commons Logging)。JCL其实只是定义了一套日志接口(其内部也提供一个Simple Log的简单实现)，支持运行时动态加载日志组件的实现。也就是说，在你应用代码里，只需调用JCL的接口，底层实现可以是Log4j，也可以是Sun的标准库JUL 。 Apache的心思就是想通过增加一层Interface，然后统一两种日志框架。 注意这各阶段出现了第一个日志门面（Logging Interface）：JCL(Jakarta Commons Logging) 。 1.5 第五阶段2006 年 ceki Gulcü 离开 Apache 基金会（原因参考他自己的博客：The forces and vulnerabilities of the Apache model），自立门户，然后就搞了一套新的日志标准接口规范 Slf4j (Simple Logging Facade for Java)。Slf4j对标JCL，比JCL优秀是自然的（Slf4j知道JCL的痛点和不足）。另外为了吸引其他日志框架的用户，还搞了一系列桥接包，帮助 Slf4j 与其他日志框架、日志门面的对接，称为桥接设计模式(Adapter)。 JCL （日志门面）与 Slf4j （日志门面）之间的桥接包是 jcl-over-slf4j.jar; Log4j（日志框架）与 Slf4j （日志门面）之间的桥接包是 slf4j-log4j12.jar; 详细如下图： 这样项目代码使用 Slf4j 接口，可以实现日志的统一标准化。以后如果想改变日志实现，只需引入对应的 Slf4j 桥接包和具体的日志标准库就可以了。 但是还有这样的场景：当前项目日志框架和日志门面为：Log4j+Slf4j，但是后续项目计划使用日志门面JCL，这时候难道要拔掉Slf4j？没事，ceki Gulcü又搞出了新的桥接：slf4j-jcl.jar，如下图。 桥接（ Bridging legacy）劫持所以第三方日志输出并重定向至 SLF4j，最终实现统一日志上层 API（编码） 与下层实现（输出日志位置、格式统一） 上面的图中还有多个连接（含箭头方向）没有补完整。后续如图又补充下面的桥接。 JUL （日志框架）与 Slf4j （日志门面）之间的桥接包是 jul-to-slf4j.jar、slf4j-jdk14.jar; Log4j（日志框架）与 Slf4j （日志门面）之间的桥接包是 log4j-over-slf4j.jar; 最终Slf4j一统江湖： 1.6 第六阶段发展到这个阶段（如上图），日志生态已经很复杂了。但是ceki Gulcü 认为最初他实现是日志框架Log4j存在不足想进一步优化，但是现状是Log4j在Apache手上。所以只能另起炉灶创建新的日志框架： Logback，并且作为 Slf4j 日志门面的默认实现。Logback在功能完整性和性能上超越了所有现有的日志框架。 Logback作为日志门面Sil4j的默认日志框架，无需额外的日志桥接。如下图： Logback和Sil4j结合使用，避免复杂的桥接包和桥接关系。新项目考虑直接以这种方式实现，减少日志的维护成本，并且具有性能优势。事情在向着好的方向发展。 1.7 第七阶段但是Apache并不会放弃竞争。2012年，Apache也推出新的日志项目Log4j2（不兼容Log4j，相当于重写），Log4j2借鉴 Slf4j+Logback。Log4j2分为两个部分： log4j-core.jar，属于日志框架（Logging Framework）； log4j-api，属于日志门面（Logging Interface）； Log4j2性能有提高，支持异步日志打印等。但是由于和原来Log4j日志框架（Log4j 1.x）不兼容，仍然需要通过大量的日志桥接来互相兼容。 1.8 小结整体上看，整个Java日志生态的发展都是 ceki Gulcü和Apache基金会的竞争。目前Java日志生态，对于新手这是复杂的。在日常使用过程中，如果没有了解背后的体系，使用门框较高，还容易出现各种问题。但目前这个状态已经是既定现实。 本节我们按照时间线梳理了日志生态的形成和发展过程。接下来我们会按照功能，介绍目前日志生态的细节。 第一部分 日志依赖分类2.1 日志框架（Logging Framework）从上面章节的介绍，日志框架是日志生态中底层日志实现类。用来控制日志信息输送的目的地是控制台、文件、GUI组件，甚至是套接口服务器、NT的事件记录器、UNIX Syslog守护进程等。也可以控制每一条日志的输出格式。通过定义每一条日志信息的级别（七种日志级别：OFF、FATAL、ERROR、WARN、INFO、DEBUG和TRACE），更加细致地控制日志的生成过程。 按照时间顺序，常见日志框架有4种： Log4j log4j是Java日志框架的元老。1999年发布首个版本，2012年发布最后一个版本，2015年正式宣布终止。至今还有还有很多项目在使用log4j。 JUL(jdk-logging) Sun公司出品，JDK自带原生日志框架。 Logback 作为日志门面Sil4j的默认日志框架，性能优异。 Log4j2 Apache基金会出品，Log4j的继任者。 2.2 日志门面（Logging Interface）日志门面的出现是为了解决J.U.L(jdk-logging)和Log4j两个日志框架统一的问题。日志门面作为应用程序和日志关键之间的中间件层（日志框架和应用程序的桥梁作用）。解耦之后，应用程序可以在不修改源代码，只需要调整日志框架依赖包和配置，就可以更换日志框架。这就需要日志门面有强大的兼容多日志框架的兼容能力。 计算机学科一句哲理：“Any problem in computer science can be solved by another layer of indirection.” “计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决”. 按照时间顺序，常见的日志门面有： JCL(Apache Commons Logging) Apache基金会出品，最早的日志门面。原名：Jakarta Commons Logging（所以简称还是JCL）。 Slf4j Ceki Gülcü出品。 Log4j2（log4j-api） Apache基金会出品。 2.3 桥接类多种日志实现框架混用情况下，需要借助桥接类进行日志的转换，最后统一成一种进行输出。 slf4j-jdk14 slf4j-log4j12 log4j-slf4j-impl logback-classic slf4j-jcl jul-to-slf4j log4j-over-slf4j icl-over-slf4j log4j-to-slf4j 第三部分 日志配置文件 日志框架 配置文件名 Log4j log4j.properties JUL(jdk-logging) logging.properties Logback logback.xml Log4j2 log4j2.xml 注意:log4j2则已经弃用了.properties方式，采用的是.xml https://www.cnblogs.com/chanshuyi/p/something_about_java_log_framework.html 3.1 日志配置常见日志配置文件常用配置项目说明如下： Loggers：被称为记录器，应用程序通过获取Logger对象，调用其API来来发布日志信息。Logger通常时应用程序访问日志系统的入口程序。 Appenders：也被称为Handlers，每个Logger都会关联一组Handlers，Logger会将日志交给关联Handlers处理，由Handlers负责将日志做记录。Handlers在此是一个抽象，其具体的实现决定了日志记录的位置可以是控制台、文件、网络上的其他日志服务或操作系统日志等。 Layouts：也被称为Formatters，它负责对日志事件中的数据进行转换和格式化。Layouts决定了数据在一条日志记录中的最终形式。 Level：每条日志消息都有一个关联的日志级别。该级别粗略指导了日志消息的重要性和紧迫，我可以将Level和Loggers，Appenders做关联以便于我们过滤消息。 Filters：过滤器，根据需要定制哪些信息会被记录，哪些信息会被放过。 3.2 日志级别各级别按降序排列如下： 日志级别 说明 SEVERE（最高值） SEVERE 是指示严重失败的消息级别 WARNING WARNING 是指示潜在问题的消息级别 INFO INFO 是报告消息的消息级别 CONFIG CONFIG 是用于静态配置消息的消息级别 FINE FINE 是提供跟踪信息的消息级别 FINER FINEST 指示一条最详细的跟踪消息 FINEST（最低值） FINEST 指示一条最详细的跟踪消息 特殊级别 OFF：可用来关闭日志记录。ALL：启用所有消息的日志记录。 ### 第四部分 最佳实践3.1 规范面多日志生态中，这么丰富的选择（4个日志框架、3个日志门面），新的研发项目在选择日志方案的时候，如何抉择呢？当然，作为企业研发规范肯定是需要内部统一。 例如阿里Java开发手册中日志规约中有下面的约定： 【强制】应用中不可直接使用日志系统 (Log4j、Logback) 中的 API, 而应依赖使用日志框架 SLF4] 中的 API， 使用门面模式的日志框架, 有利于维护和各个类的日志处理方式统一。 12345&gt; import org.slf4j.Logger; &gt; import org.slf4j.LoggerFactory; &gt; &gt; private static final Logger logger = LoggerFactory.getLogger(Abc.class); &gt; 规范中，要求研发中我们应该使用日志门面，而不是直接使用日志框架。即依赖日志的抽象，而不是日志的实现。总结如下： 使用日志门面； 使用一个日志框架； 依赖不传递； 了解了日志的发展历史，那现在我们再回过头来看看如果，你的系统在选择日志方案的时候，如何抉择呢？毕竟我们3个日志接口，以及4个日志产品 显然第一点是使用日志接口的API而不是直接使用日志产品的API这一条也是必须的，也是符合依赖倒置原则的，我们应该依赖日志的抽象，而不是日志的实现 日志产品的依赖只添加一个当然也这个也是必须的，依赖多个日志产品，只会让自己的应用处理日志显得更复杂，不可统一控制 把日志产品的依赖设置为Optional和runtime scope其中Optional是为了依赖不会被传递，比如别的人引用你这个jar，就会被迫使用不想用的日志依赖 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 而scope设置为runtime，是可以保证日志的产品的依赖只有在运行时需要，编译时不需要，这样，开发人员就不会在编写代码的过程中使用到日志产品的API了 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 3.2 SLF4J+Logback3.2.1 配置新建项目中pom文件引入下面的依赖包： 12345&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.11&lt;/version&gt;&lt;/dependency&gt; 项目会自动导入下面的依赖： 123ch.qos.logback:logback-classic:1.2.11ch.qos.logback:logback-core:1.2.11org.slf4j:slf4j-api:1.7.32 然后在项目resources目录创建相关日志配置文件（logback.xml）。配置文件详细介绍参考其他文章。 12345678910111213141516&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder&gt; &lt;Pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/Pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;logger name="com.test" level="TRACE"/&gt; &lt;root level="debug"&gt; &lt;appender-ref ref="STDOUT"/&gt; &lt;/root&gt;&lt;/configuration&gt; 程序文件中如下使用： 12345678910111213import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class TestClass &#123; static final Logger logger = LoggerFactory.getLogger(TestClass.class); public static void main(String[] args) &#123; logger.debug("debug"); logger.info("info"); &#125;&#125;//输出//19:25:30.653 [main] DEBUG TestClass - debug//19:25:30.657 [main] INFO TestClass - info 3.2.2 配置文件加载顺序 logback允许多配置文件，其加载时读取配置文件的顺序如下： 在classpath查找logback-test.xml（一般classpath为src/test/resources） 如果该文件不存在，logback尝试寻找logback.groovy 如果该文件不存在，logback尝试寻找logback.xml 如果该文件不存在，logback会在META-INF下查找[com.qos.logback.classic.spi.Configurator](http://logback.qos.ch/xref/ch/qos/logback/classic/spi/Configurator.html)接口的实现 如果依然找不到，则会使用默认的BasicConfigurator，导致日志直接打印到控制台，日志等级为DEBUG，日志的格式为%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n 3.3 桥接包使用log4j -&gt; log4j2那么就得去掉 log4j 1.x jar，添加log4j-1.2-api.jar，配合 log4j-api-2.x.x.jar 和 log4j-core-2.x.x.jar 即可，依赖如下 log4j-1.2-api.jar log4j-api-2.x.x.jar log4j-core-2.x.x.jar123log4j2 -&gt; log4j这里我们只是演示如何做桥接，实际上当然不建议这么做的。理清了上面的jar包作用，就会发现，可以通过 log4j2 -&gt; slf4j -&gt; log4j 的方式来实现。 需要的jar包，根据依赖关系分别是： log4j-api-2.x.x.jar log4j-to-slf4j.jar slf4j-api-x.x.x.jar slf4j-log4j12-x.x.x.jar log4j-1.2.17.jar12345log4j -&gt; slf4j将代码中的log4j日志桥接到 slf4j，需要如下jar包 log4j-over-slf4j-x.x.x.jar slf4j-api-x.x.x.jar12log4j2 -&gt; slf4j将代码中的log4j2日志桥接到 slf4j，需要如下jar包 log4j-api-2.x.x.jar log4j-to-slf4j-2.x.x.jar slf4j-api-x.x.x.jar123slf4j -&gt; log4j2 将slf4j日志，采用log4j2实现进行输出，需要如下jar包 slf4j-api-x.x.x.jar log4j-slf4j-impl.jar log4j-api.jar log4j-core.jar 3.4 Spring-Boot 实践第五部分 大数据研发日志实践5.1 MapReduceMapReduce默认采用log4j作为日志框架 share/hadoop/mapreduce/lib 1log4j-1.2.17.jar 5.2 SparkSpark默认采用log4j作为日志框架（为了和log4j2区别，也有称为log4j1），并且采用${SPARK_HOME}/conf/log4j.properties，通常是通过其中的log4j.properties.template 模板文件创建。 官网说明：https://spark.apache.org/docs/latest/configuration.html#configuring-logging 依赖包在jars目录中，例如Saprk-2.3.2目录中： 12345678slf4j-api-1.7.16.jarslf4j-log4j12-1.7.16.jarapache-log4j-extras-1.2.17.jarlog4j-1.2.17.jarcommons-logging-1.1.3.jar# 日志桥接包jcl-over-slf4j-1.7.16.jarjul-to-slf4j-1.7.16.jar 如果项目中使用Spark默认的log4j1，那么pom文件中无需额外引入日志依赖包。下面的spark-core包中已经有相关依赖了。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt;&lt;/dependency&gt; 已有相关依赖： 12345org.slf4j:slf4j-api:1.7.10org.slf4j:jul-to-slf4j:1.7.10org.slf4j:jcl-over-slf4j:1.7.10log4j:log4j:1.2.17org.slf4j:slf4j-log4j12:1.7.10 代码中直接使用SLF4J日志门面即可。底层默认使用 将spark默认日志log4j替换为logback 1.将jars文件夹下apache-log4j-extras-1.2.17.jar，commons-logging-1.1.3.jar, log4j-1.2.17.jar, slf4j-log4j12-1.7.16.jar 替换成log4j-over-slf4j-1.7.23.jar,logback-access-1.2.1.jar, logback-classic-1.2.1.jar, logback-core-1.2.1.jar。 2.将conf文件夹下的log4j.properties.template通过 log4j.properties Translator 转换成logback.xml即可 运行试例： 5.3 FlinkFlink 中使用 SLF4J日志门面实现，但是底层日志实现却默认使用Log4j2作为日志框架。链接：https://nightlies.apache.org/flink/flink-docs-master/zh/docs/deployment/advanced/logging/ 依赖包位置在lib/ 12345# flink-1.13.2 案例log4j-1.2-api-2.12.1.jarlog4j-api-2.12.1.jarlog4j-core-2.12.1.jarlog4j-slf4j-impl-2.12.1.jar 研发项目的日志依赖配置如下，注意scope 参数为provided： 123456789101112131415161718&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 默认日志配置文件位置：conf/，配置文件有： log4j-cli.properties：Flink 命令行使用（例如 flink run）； log4j-session.properties：Flink 命令行在启动基于 Kubernetes/Yarn 的 Session 集群时使用（例如 kubernetes-session.sh/yarn-session.sh）； log4j-console.properties：Job-/TaskManagers 在前台模式运行时使用（例如 Kubernetes）； log4j.properties： Job-/TaskManagers 默认使用的日志配置。 Log4j 会定期扫描这些文件的变更，并在必要时调整日志记录行为。默认情况下30秒检查一次，监测间隔可以通过 Log4j 配置文件的 monitorInterval 配置项进行设置。 官方说使用 -Dlog4j.configurationFile= 参数可以传递日志文件 第六部分 拾遗6.1 log4j和log4j2前文我们知道log4j2是Apache对log4j的重新和升级的日志框架，为了区别通常称为log4j2，以示和旧的区别。注意在Maven引用的时候区别（groupId不同）： 12345678910111213141516171819&lt;!-- log4j --&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt;&lt;!-- log4j2 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.14.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- log4j-core包中已经引入了log4j-api --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.14.1&lt;/version&gt;&lt;/dependency&gt; 代码中引入区别： 123456789//log4j：import org.apache.log4j.Logger;private final Logger logger = Logger.getLogger(Test.class.getName());//log4j2：import org.apache.logging.log4j.Level;import org.apache.logging.log4j.LogManager;import org.apache.logging.log4j.Logger;private static Logger logger = LogManager.getLogger(Test.class.getName()); ### 参考文献及资料1.slf4j官网，链接：https://www.slf4j.org/manual.html 2.《阿里Java开发手册》，链接：https://developer.aliyun.com/special/tech-java 3、Flink日志使用手册，链接：https://nightlies.apache.org/flink/flink-docs-master/zh/docs/deployment/advanced/logging/]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[比特币的数学原理]]></title>
    <url>%2F2021%2F02%2F28%2F2021-02-17-%E7%BB%8F%E6%B5%8E%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%AF%94%E7%89%B9%E5%B8%81%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料1、]]></content>
      <categories>
        <category>bitcoin</category>
      </categories>
      <tags>
        <tag>bitcoin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-数据科学中的几何]]></title>
    <url>%2F2021%2F02%2F15%2F2021-03-10-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%87%A0%E4%BD%95%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[背景 数据科学赋予了几何才有了灵魂。 在正文前，我们先介绍一个机器学习案例。 案例提供二维数据集，数据有：(116.331398,39.897445)、(121.545451,31.178669）、(120.184051,30.320602)、(117.038113,30.496019)等，然后要求对数据进行聚类。 对于这个案例通常做法就是计算数据之间的距离（欧式距离），使用各类聚类算法寻找质心。这样就默认给数据集赋予了二维欧式距离，即这些点是取自二维欧式空间的。这很有可能已经破坏了数据集中所蕴含的信息。事实上，这些数据是地球上经纬度坐标数据，它有天然的几何结构：球面几何。在几何学中，它属于非欧几何范畴。案例数据属于球面空间的子空间，自然应该使用球面距离(Haversine公式)。 我们在处理数据集的时候不能先入为主的认为是欧式空间的子集。如果直接使用欧式度量，这是一个很强的假设前提（先验）。 很多数据处理算法均是默认欧式距离。例如神经网络模型中，假定数据为欧式空间，学习的函数空间就是欧式空间之间的非线性函数。数据特征工程中将各种无关属性的数据拼接成高维特征均是欧式空间。在数学角度看都是很强的假设前提，甚至已经破坏了数据的内在关系，从信息论角度已经丢失信息或者引进的噪声信息。 但是面对高维数据，低维生命无法肉眼看清复杂的结构关系（或目前还没有有效的刻画技术）。但是可以尝试减少先验，给数据集合赋予一个比欧式空间较弱的几何结构：拓扑流形。特别是对于非线性数据，赋予流形结构提供的视角和高度是欧式距离无法给予的。 第一部分 数据微分流形对于给定的多维数据集，数学上首先它是一个集合$S$，这时候数据和数据是没有关系的（结构）。接下来我们给数据集定义几何结构： 赋予拓扑结构（数据集是一个拓扑空间）：数据集合$S$ 的子集构成的新的集合 $\tau$, 称为数据集$S$上的拓扑 (topology)，需要满足以下条件： (1) $\emptyset \in \tau$,$S \in \tau$; (2) $\tau$ 中有限个集合的交集和任意（有限、可数、不可数）个集合的并集还属于$\tau$。简单来说就是$\tau$ 中的集合对于有限交和任意并运算封闭。 赋予流形结构：数据集$S$上可以赋予很多拓扑结构，并不是唯一的。我们进一步增加约束条件：令$S$是一个Hausdorff空间。若对任一点 $p\in S $ , 都存在 $x$ 在$S$中的一个邻域 $U\subset S$ ,使得 $U$ 同胚于$n$ 维欧氏空间$R^n$ , 则$S$就称为是一个$n$维流形。 赋予微分流形结构：为了能在数据流形上有微分运算（类似欧式空间），我们需要数据流形是可微的：一个$n$维微分流形$S$就是一个赋予了微分结构的$n$维流形，即存在一组坐标卡 $\mathcal{A}=\left{\left(U_{i}, \varphi_{i}\right)\right}$（即流形定义中的局部同胚映射），使得：（i）$\mathcal{A}$ 为$S$ 的一个开覆盖；（ii）任意两个不相交的坐标卡；（iii）$(U, \varphi)$ 和 $(V, \psi)$ 满足$ \psi \circ \varphi^{-1}$$与$ $\varphi \circ \psi^{-1}$皆为 $ C^{\infty}$函数为$\mathcal{A}$ 极大的（该条件可由前两条唯一生成）。 最后数据集集合具有微分流形几何结构，而经典数据科学中常用的欧式空间是微分流形的特例，我们弱化了假设前提。但是微分流形仍具有较好的性质： 微分流形局部同胚与欧式空间（每个点都存在一个邻域同胚于欧氏空间中的开集）； 微分流形可以嵌入至欧式空间； 其中第二个性质是微分拓扑中重要定理：Whitney嵌入定理。 Whitney嵌入定理：设$M$是$m$维微分（光滑）流形，存在$M$到欧氏空间$R^n$的光滑嵌入映射$f$，其中$n&gt;=2m+1$。并且$f$的像是$R^n$中的闭集。 Whitney嵌入定理是微分拓扑中重要的定理，也许可以认为正是Whitney发现了这个定理，开创了微分拓扑。在这个定理之前，人们对于流形还是把握不定的。但是在这个定理后，由于流形可以嵌入到维数较大的欧氏空间中，所以有了一系列的关于流形的重要结果，形成了微分拓扑这个分支。 第二部分 流行学习有了Whitney嵌入定理的理论保证，高维数据集可以假设是嵌入在高维欧式空间中低维流形。基于这个假设就有了一个研究分支：流形学习（manifold learning）。 对于地球位置的数据集，在3维欧式空间可以用3维数据坐标$(x,y,z)$ 来表示，实际上又可以用2维经纬度坐标（内蕴坐标）表示，地球位置数据实际是一个二维球面子流形嵌入在3维欧式空间中。当数据集是3维欧式空间表示时，我们可以降维到二维流形。 上面的分析前提是我们开启了上帝视角。实际应用中，对于任意给定的数据集，很难判断或假设合理的内蕴流形结构。 2.1 数据降维数据科学中对于线性空间中数据，有较多成熟的降维技术：例如主成分分析（PCA）、独立分量分析（ICA）、Fish判别法（FDA）、多尺度分析（MDS）等。那么在降维的时候，寻找恰当的降维映射是算法的目的。降维映射空间同样是庞大的，需要定义相关的性能度量来度量映射的优劣。 然而对于非线性数据流形，空间上是没有距离（欧氏距离）概念的。这时候我们需要利用微分流形另一个重要特性：微分流形局部同胚与欧式空间。 宇宙是一个微分流形，我们人类所处的局部是一个三维欧式空间。 2.1.1 保持局部线性关系映射需要保持流形上每个很小的局部欧式线性。代表算法有：局部改线嵌入（local Linear Embedding，LLE）。算法大体分成三个部分：寻找近邻、线性重构、低维嵌入。 寻找近邻 对于每个数据点，需要定义邻域。邻域有两种方式：(1)最近的K个点;(2)邻域距离阀值限定局部大小。LLE算法使用前者，其中K是一个预先给定值（超参数），使用欧式距离度量。 线性重构 每个数据点由K个近邻线性表达； 低维嵌入 每个点在低维空间中的降维映射值在项空间同样有近似的线性表达。 具体算法过程，参考论文：《Nonlinear Dimensionality Reduction by Locally Linear Embedding》。LEE算法对于闭合流形、稀疏数据集效果有限，另外结果对于K值选取较为敏感。 2.1.2 保持点之间测地线距离微分流形中整体是没有距离概念的，但是有推广的测地线距离（geodesic）。代表算法有Isomap(Isometric Feature Mapping)算法。算法步骤大体分成三个部分：构造近邻图、计算最短路径、低维嵌入。 构造近邻图 和LLE算法相同，基于欧式距离找出邻近点，建立整体集合的近邻连接图（图论理论中图概念）。 计算最短路径 近邻连接图中近邻点之间存在连接关系，而非近邻点之间不存在连接关系。这样计算两点之间测地线距离的问题就转变为计算近邻连接图上两点之间的最短路径问题。最短路径上逐点距离和近似代替几何意义上的测地线。最短路径计算，采用图论中Dikstra算法或Floyd算法。 低维嵌入 在得到任意两点的距离之后，结果就可以直接应用 MDS(Multidimensional Scaling) 算法了。 上图瑞士卷数据集是常用的验证数据集。A图中蓝色线表示流形上两点之间的实际测地线距离，B图中红色是Dijkstra算法在近邻图上找到的最短距离，C图显示了降维后三维数据集在二维平面中的嵌入效果，结果较为近似。但是当流形的曲率较大、数据稀疏时或者流形上有孔洞的话，算法效果较差。而对于空间中的数据点稠密时，近似效果较好。 2.1.3 保持图的局部邻接关系Isomap 算法使用点与点之间欧式距离构建了整体数据集合的近邻图。在这个图中，点与点之间关系为欧式距离。而这个关系可以使用高斯核(Gaussian Kernel)来定义。 对于连通的两个点 $i,i′$,令图关系值为:$w_{ii′}=k(i,i’)=exp(−∥i−i′∥2/σ2)$ 。而其它点关系值为: 0。这样所有点之间的关系值可以得到图的邻接矩阵W，第i行的第i&#39;个值对应权重：$w_{ii’}$。 如果记在降维映射后的值为$j,j’$ ,那么就是在假设空间中寻找函数使得下面的性能度量最小：$$\sum_{jj’}(j-j’)^2 w_{jj’}=\sum_{jj’}\left(j^{2}+j’^{2}-2j j’\right) W_{j j’}=2 \mathbf{y}^{T} L \mathbf{y}$$其中$$D_{jj’}=\sum_{j’} W_{j j’}, L=D-W$$ ，L为Laplacian矩阵。 这个目标函数形式是不是和PCA十分的相似。目标函数的求解是一个广义特征值分解问题：$$L \mathbf{y}=\lambda D \mathbf{y}$$计算L的特征值，将特征值从小到大排序，取前k个特征值，并计算前k个特征值$\lambda_{0}, \lambda_{2}, \ldots, \lambda_{k-1}$。这样就得到了键数据映射到特征空间中。 这个算法过程和谱聚类(Spectral Clustering)算法技术是相同的。 这就是Laplacian Eigenmaps算法，具体算法过程，参考论文：《Laplacian Eigenmaps for Dimensionality Reduction and Data Representation》。LEE算法对于闭合流形、稀疏数据集效果有限，另外结果对于K值选取较为敏感。 LE算法是非线性的，另外还有推广的线性版本：保持保持投影LPP（Locality Preserving Projections）算法，参考论文：《Locality Preserving Projections》。 2.1.4 保持概率分布对于数据流形中，Isomap 算法使用近似测地线来度量近邻点之间的距离（相似性）。2002年Hinton提出一个奇妙的思路，使用概率密度来度量这个相似性。这就是SNE（Stochastic Neighbor Embedding）算法。 对于数据流形$X$中两个数据点$x_i$和$x_j$，考虑$x_i$为中心的高斯分布（$\sigma_i$为分布的方差），定义一个条件概率$p_{ij}$（数据点$x_i$选择$x_j$为近邻点的概率）：$$p_{i j}=\frac{\exp \left(-d_{i j}^{2}\right)}{\sum_{k \neq i} \exp \left(-d_{i k}^{2}\right)},其中d_{i j}^{2}=\frac{\left|\mathbf{x}{i}-\mathbf{x}{j}\right|^{2}}{2 \sigma_{i}^{2}},(局部欧式使用欧式度量)$$当映射将流形数据映射到低维空间$Y$后，需要保持数据点的相似性。记在低维空间中数据点$x_i$和$x_j$的值分别为$y_i$和$y_j$，同样赋予条件概率度量，特别的将高斯分布的方差固定为$1/\sqrt{2}$。$$q_{i j}=\frac{\exp \left(-\left|\mathbf{y}{i}-\mathbf{y}{j}\right|^{2}\right)}{\sum_{k \neq i} \exp \left(-\left|\mathbf{y}{i}-\mathbf{y}{k}\right|^{2}\right)}$$我们定义了数据流形中点$x_i$和值空间中点$y_i$条件概率。如果遍历所有点，就定义了一个关于$x_i$和$y_i$的概率分布：$$P_i={p_{ij}(x_j)}_{j\in X},Q_i={q_{ij}(y_j)}{j\in Y}$$映射保持概率分布，这时候概率分布的度量KL（Kullback-Leibler Divergence）散度就用上了,最后得到假设空间的性能度量指标：$$C=\sum{i} K L\left(P_{i} | Q_{i}\right)=\sum_{i} \sum_{i} p_{j \mid i} \log \frac{p_{j \mid i}}{q_{j \mid i}}$$最后算法根据这个目标函数寻找最优解。 我们知道KL散度是非对称的，SNE算法更关注于局部，容易忽视全局结构。后续Hinton将低维空间的分布由高斯分布换成t分布（利用t分布的长尾性特征），就是降维可视化中最常用的算法t-SNE算法。 下图为mnist在t-SNE算法下降维至2维空间的可视化效果。 .png) 对于t-SNE算法，实际工程实现时需要计算大量的概率值，计算量较大。实际上，对于相距较远的点成为邻域点的概率会很小，所以只需要考虑一定范围邻域的点集即可，提前构建近邻图。 2.2 可视化数据降维的一个衍生功能就是可视化化。将数据映射到二维或者三维欧式空间中（映射保持指定的信息结构），便于三维生物体更容易感知到数据之间的结构关系。 但是原始数据蕴含的流形并不一定是恰好的三维或二维，强行降维会破坏数据的真实结构信息。 2.3 半监督学习在实际数据处理中，标签数据是珍贵而稀缺的。更多的应用场景是：有少量已经标记的标签数据和大量的无标签数据。在流形学习假设前提下，相似数据应该在流形的同一个局部邻域中（或者说局部领域中数据标签相似）。基于这个假设，大量的非标签数据就可以通过少量标签获取到标签。 2.4 工程实现对于工程实现，Python中scikit-learn包提供相关流形学习的算法实现包。主要有： 封装类名 对应算法 manifold.Isomap() Isomap Embedding manifold.LocallyLinearEmbedding() Locally Linear Embedding manifold.SpectralEmbedding() Spectral embedding for non-linear dimensionality reduction. manifold.TSNE) t-distributed Stochastic Neighbor Embedding. 第三部分 总结3.1 谱图理论Isomap算法和LE算法在构建数据近邻图的过程使用的思想其实是谱图理论(spectral graph theory)。其本质是利用数据相似度的关系矩阵，然后计算矩阵的特征值和特征向量，最后选择合适的特征向量投影得到的数据的低维嵌入。图谱理论的技术要求数据流形的数据采样是稠密的、流形曲率不能太大，否则对噪声是敏感的。 图谱理论的技术使得数据流形学习由理论落地为实践。近几年在图神经网络方向仍然是重要技术。 3.2 流形学习中问题3.2.1 基本问题1对于给定的多维数据集，研究和刻画数据微分流形的几何和拓扑性质。寻找一些基本判别定理确定数据内蕴微分流形结构。 3.2.2 基本问题2在对数据流形处理过程中，如何刻画数据微分流形中信息量，如何建立数据中信息量和几何结构的关系。数据微分流形在哪一类函数变换下能保持信息量的不变。 3.3 解释性对于一些常用的数据集，很多研究结果表明具有流形结构，例如： 1、MNIST数据是一个6维流形； 2、从不同角度拍摄得到的一系列图片数据集实际上是分布在一个低维流形中； 通常机器学习中，在数据上构建模型的时候，人们更多的是关注于模型的泛化性能，强调模型是端到端的，就是我们经常在英文文献中看到的概念：end-to-end learning 。将大量时间花费在缺乏理论指导的实验性调参上（自嘲为”炼丹师”）。例如深度学习应用中，更多是优化网络层结构和参数，忽略背后数据特征的变化原理。当然也引起了人们的怀疑态度，即模型的可解释性危机。 数据科学中很多问题本质是几何问题，解释性危机极大可能由几何理论来最终解答。例如国内顾险峰团队使用最优传输理论（Optimal Mass Transportation ），凸几何（Convex Geometry），蒙日-安培方程（Monge-Ampere Equation）的交汇给出了生成模型GAN（Generative Adversarial Networks）的几何观点。 3.4 交叉学科除了流形学习，近些年数据科学和几何学出现很多交叉方向，例如信息几何学、计算共性几何。 信息几何学 概率是信息学和数据科学中重要概念。信息几何学将概率密度函数集合看成微分流形，把Fisher信息作为黎曼度量，使用测度距离作为概率密度函数之间的距离，获得丰富结果。 计算共形几何 计算共形几何将现代几何拓扑理论与计算机科学相融合，将经典微分几何、Riemann面理论、代数拓扑、几何偏微分方程的概念、定理和方法推广至离散情形，转换成计算机算法，广泛应用于计算机图形学、计算机视觉、计算机辅助几何设计、数字几何处理、计算机网络、计算力学、机械设计以及医学影像等领域中。 由于笔者水平有限，不能保证全部内容均正确无误，若读者有不同意见，非常欢迎留言指教一起探讨。 参考文献1、《微分拓扑新讲》，张筑生，北京大学出版社； 2、《黎曼几何引论》，陈维恒 李兴校，北京大学出版社； 3、Nonlinear Dimensionality Reduction by Locally Linear Embedding，Sam T. Roweis和Lawrence K. Saul，《Science》2000 4、Laplacian Eigenmaps for Dimensionality Reduction and Data Representation，M.Belkin，NIPS2002论文 5、Locality Preserving Projections，何晓飞，NIPS (2003) 6、深度学习的几何学解释，链接：http://www.engineering.org.cn/ch/10.1016/j.eng.2019.09.010]]></content>
      <categories>
        <category>Distance Measures</category>
      </categories>
      <tags>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-特征工程综述]]></title>
    <url>%2F2021%2F02%2F15%2F2022-02-21-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[背景 特征工程是什么？ 特征工程解决了什么问题？ 特征工程的原理？ 如何实现特征工程的最佳实现？ 在讲解特征工程（Feature Engineering）之前，我们需要明白数据科学中，什么是特征。维基百科中，对于“特征”定义如下： In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. 翻译：在机器学习和模式识别中，特征指的是独立可测量属性或现象的特性。 维基百科中的定义： Feature engineering (or feature extraction) is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data. The motivation is to use these extra features to improve the quality of results from a machine learning process, compared with supplying only the raw data to the machine learning process. 特征工程( 或特征萃取)是指：使用领域知识从原始数据中萃取特征的过程。这些萃取得到的特征将会提升机器学习的性能。 事物运行的结果，会受到很多因素的影响，由于技术限制我们通常只能量化提取有限个影响因素。而在这些影响因素（含复合因素）集合中，通常影响权重是不同的。为了减少模型复杂度和计算量等因素，我们会选取权重最重要的影响因素。这个过程其实就是特征工程背后的哲学思想。 过拟合，其实本质是模型过多的关注部分数据，导致性能函数挑选函数会追究极致。针对这部分数据模型性能达到极致。这样模型针对全局数据时候，性能就很难有良好的泛化性能。所以本质还是训练数据的采样未能很好的覆盖所有数据集。 如果数据很好的采样，是否存在模型训练次数的增加，导致参数过拟合呢？在工程实践中，经常有这样的现象。这是否是模型的性能函数选取不当呢？导致性能函数本身挑选函时候有“偏见”。 模型的专注力 参考文献及资料1、深度了解特征工程，链接：https://zhuanlan.zhihu.com/p/111296130]]></content>
      <categories>
        <category>Distance Measures</category>
      </categories>
      <tags>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-数据科学中度量距离总结]]></title>
    <url>%2F2021%2F02%2F15%2F2021-02-17-%E7%BB%8F%E6%B5%8E%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-2021%E5%B9%B4%E7%BB%8F%E6%B5%8E%E8%B6%8B%E5%8A%BF%E4%B8%AA%E4%BA%BA%E8%A7%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料1、]]></content>
      <categories>
        <category>Distance Measures</category>
      </categories>
      <tags>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-独热编码]]></title>
    <url>%2F2021%2F02%2F15%2F2022-02-21-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料1、深度了解特征工程，链接：https://zhuanlan.zhihu.com/p/111296130]]></content>
      <categories>
        <category>Distance Measures</category>
      </categories>
      <tags>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-数据科学中度量距离总结]]></title>
    <url>%2F2021%2F02%2F15%2F2021-02-15-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%AD%E5%BA%A6%E9%87%8F%E8%B7%9D%E7%A6%BB%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景在数据科学中，需要对数据空间中点度量差异（相似度），就需要对数据集合赋予恰当合适的测度（度量），用来刻画点与点之间的差异（距离）。通常这种度量种类很多，选取时需要针对集合特点进行选取。 https://www.cnblogs.com/ranjiewen/p/5958803.html https://www.cnblogs.com/wt869054461/p/7156397.html 第一部分 前言机器学习中有监督和无监督算法都会用到距离度量。例如常用算法k-NN，UMAP，HDBSCAN中经常使用的度量：欧氏度量和余弦相似性度量等。其中欧式距离是我们使用最多的距离，但是它是否适应所有的数据集合呢？答案是否定的，欧式距离不是银弹。 特别的当我们数据集合中数据本身具有遗传度量，例如地球上物体经纬度坐标，这个本身就有自己的球面距离，如果使用欧式距离就破坏了数据天然的度量。 第一部分 欧氏距离我们从最常见的距离度量开始，即欧几里得距离。最好将距离量度解释为连接两个点的线段的长度。该公式非常简单，因为使用勾股定理从这些点的笛卡尔坐标计算距离。 1.1 缺点尽管这是一种常用的距离度量，但欧几里得距离并不是比例不变的，这意味着所计算的距离可能会根据要素的单位而发生偏斜。通常，在使用此距离度量之前，需要对数据进行标准化。 此外，随着数据维数的增加，欧氏距离的用处也就越小。这与维数的诅咒有关，维数的诅咒与高维空间不能像直觉上期望的二维或3维空间那样起作用。有关摘要，请参阅这篇文章。 1.2 用例当您拥有低维数据并且向量的大小非常重要时，欧几里得距离的效果非常好。如果在低维数据上使用欧几里得距离，则kNN和HDBSCAN之类的方法将显示出出色的结果。 尽管已开发出许多其他措施来解决欧几里得距离的缺点，但出于充分的原因，它仍然是最常用的距离措施之一。它使用起来非常直观，易于实现，并且在许多用例中都显示出了极好的效果。 第二部分 余弦相似度余弦相似度经常被用作解决高维数欧几里德距离问题的方法。余弦相似度只是两个向量之间角度的余弦。如果将向量归一化为长度均为1，则向量的内积也相同。 两个方向完全相同的向量的余弦相似度为1，而两个彼此相对的向量的相似度为-1。注意，它们的大小并不重要，因为这是方向的度量。 余弦相似度 缺点余弦相似度的主要缺点之一是不考虑向量的大小，仅考虑其方向。实际上，这意味着没有充分考虑值的差异。例如，如果采用推荐系统，则余弦相似度不会考虑不同用户之间的评分等级差异。 用例当我们拥有高维数据并且向量的大小不重要时，通常会使用余弦相似度。对于文本分析，当数据由字数表示时，此度量非常常用。例如，当一个单词在一个文档中比另一个单词更频繁出现时，这并不一定意味着一个文档与该单词更相关。可能是文件长度不均匀，计数的重要性不太重要。然后，我们最好使用忽略幅度的余弦相似度。 3.海明距离海明距离。图片由作者提供。 汉明距离是两个向量之间不同的值的数量。它通常用于比较两个等长的二进制字符串。它也可用于字符串，以通过计算彼此不同的字符数来比较它们彼此之间的相似程度。 缺点如您所料，当两个向量的长度不相等时，很难使用汉明距离。您可能希望将相同长度的向量相互比较，以了解哪些位置不匹配。 此外，只要它们不同或相等，就不会考虑实际值。因此，当幅度是重要指标时，建议不要使用此距离指标。 用例典型的用例包括通过计算机网络传输数据时的纠错/检测。它可以用来确定二进制字中失真比特的数量，作为估计错误的一种方法。 此外，您还可以使用汉明距离来测量分类变量之间的距离。 4.曼哈顿距离 曼哈顿距离。图片由作者提供。 曼哈顿距离（通常称为出租车距离或城市街区距离）可计算实值向量之间的距离。想象一下在统一的网格上描述对象的矢量，例如棋盘。然后，曼哈顿距离是指两个向量只能以直角移动时的距离。计算距离时不涉及对角线运动。 曼哈顿距离 缺点尽管曼哈顿距离对于高维数据似乎还可以，但它比欧几里德距离直观性差一些，特别是在使用高维数据时。 此外，由于它不是最短的路径，因此比欧几里得距离更有可能提供更高的距离值。这不一定会带来问题，但是您应该考虑这一点。 用例当您的数据集具有离散和/或二进制属性时，Manhattan似乎工作得很好，因为它考虑了可以在这些属性的值内实际采用的路径。以欧几里得距离为例，实际上可能不可能在两个向量之间创建一条直线。 5.切比雪夫距离 切比雪夫距离。图片由作者提供。 切比雪夫距离定义为沿着任何坐标维度的两个向量之间的最大差。换句话说，它只是一个轴上的最大距离。由于其性质，通常将其称为棋盘距离，因为国王从一个正方形到另一个正方形所需的最小移动次数等于切比雪夫距离。 切比雪夫距离 缺点切比雪夫（Chebyshev）通常用在非常特定的用例中，这使得很难将其用作通用距离度量标准，例如欧几里得距离或余弦相似度。因此，建议仅在绝对确定它适合您的用例时才使用它。 用例如前所述，切比雪夫距离可用于提取从一个正方形移动到另一个正方形所需的最小移动次数。此外，在允许无限制八向移动的游戏中，这可能是有用的措施。 实际上，切比雪夫距离通常用于仓库物流中，因为它与高架起重机移动物体所需的时间非常相似。 6.闵可夫斯基 Minkowski距离。图片由作者提供。 Minkowski距离比大多数距离更复杂。它是在范数向量空间（n维实空间）中使用的度量，这意味着它可以在距离可以表示为具有长度的向量的空间中使用。 该措施具有三个要求： 零向量—零向量的长度为零，而每个其他向量的长度为正。例如，如果我们从一个地方到另一个地方旅行，那么该距离始终为正。但是，如果我们从一个地方到自己的地方旅行，则该距离为零。 标量因数—当向量与正数相乘时，其长度会更改，同时保持其方向。例如，如果我们在一个方向上走了一定距离并添加了相同的距离，则方向不会改变。 三角形不等式—两点之间的最短距离是一条直线。 Minkowski距离的公式如下所示： 明可夫斯基距离 关于此距离度量最有趣的是使用parameter **p**。我们可以使用此参数来操纵距离度量以使其与其他度量非常相似。 的常见值p是： p = 1 —曼哈顿距离 p = 2 —欧几里德距离 p = ∞-切比雪夫距离 缺点Minkowski与它们所代表的距离度量具有相同的缺点，因此对曼哈顿，欧几里得和契比雪夫距离等度量的良好理解非常重要。 而且，p根据您的用例，使用合适的参数可能会很麻烦，因为找到正确的值可能在计算上效率低下。 用例好处p是可以对其进行迭代并找到最适合您的用例的距离度量。它为您的距离度量提供了极大的灵活性，如果您熟悉p并且有很多距离度量，那么这将是一个巨大的好处。 7.贾卡德指数 Jaccard索引。图片由作者提供。 Jaccard索引（或“联合上的交集”）是用于计算样本集的相似性和多样性的度量。它是交集的大小除以样本集并集的大小。 实际上，它是集合之间相似实体的总数除以实体的总数。例如，如果两个集合共有1个实体，并且共有5个不同的实体，则Jaccard索引将为1/5 = 0.2。 要计算Jaccard距离，我们只需从1中减去Jaccard索引： 提卡距离 缺点Jaccard索引的主要缺点是它受到数据大小的很大影响。大型数据集可能会对索引产生很大影响，因为它可以显着增加联合并同时保持相交相似。 用例Jaccard索引通常用于使用二进制或二进制数据的应用程序中。当您拥有一个预测图像片段（例如汽车）的深度学习模型时，可以使用Jaccard索引来计算给定真实标签的预测片段的准确性。 同样，它可用于文本相似性分析中，以衡量文档之间单词选择重叠的程度。因此，它可以用来比较模式集。 8.哈弗碱 Haversine距离。图片由作者提供。 Haversine距离是指球面上两个点之间的经度和纬度。它与欧几里得距离非常相似，因为它可以计算两点之间的最短线。主要区别在于不可能有直线，因为这里的假设是两个点都在一个球面上。 两点之间的正弦距离 缺点这种距离测量的一个缺点是假定这些点位于一个球体上。实际上，这种情况很少出现，例如，地球不是完美的圆形，在某些情况下可能会使计算变得困难。取而代之的是，将目光转向假定椭圆形的Vincenty距离。 用例如您所料，Haversine距离通常用于导航。例如，您可以使用它来计算两个国家之间的飞行距离。请注意，如果距离本身不那么大，则不太适合。曲率不会产生太大的影响。 9.索伦森-骰子指数 Sørensen-骰子系数。图片由作者提供。 Sørensen-Dice索引与Jaccard索引非常相似，因为它可以测量样本集的相似性和多样性。尽管它们的计算方式相似，但索伦森-迪斯指数却更直观一些，因为可以将其视为两组之间重叠的百分比，该值介于0和1之间： 索伦森–骰子系数 缺点像Jaccard索引一样，它们都夸大了几乎没有或没有地面真理肯定集的集合的重要性。结果，它可以控制多组平均得分。它会按相关集合的大小成反比地加权每个项目，而不是平等对待它们。 用例用例与Jaccard索引相似（如果不同）。您会发现它通常用于图像分割任务或文本相似性分析中。 注意：比这里提到的9种距离测量更多。如果您正在寻找更有趣的指标，我建议您研究以下内容之一：马哈拉诺比斯（Mahalanobis），堪培拉（Canberra），Braycurtis和KL分歧。 切比雪夫距离闵可夫斯基距离(Minkowski Distance)标准化欧氏距离 (Standardized Euclidean distance )马氏距离(Mahalanobis Distance)巴氏距离（Bhattacharyya Distance）皮尔逊系数(Pearson Correlation Coefficient)参考文献及资料1、9 Distance Measures in Data Science，链接：https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa]]></content>
      <categories>
        <category>Distance Measures</category>
      </categories>
      <tags>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache DolphinScheduler调度平台调研]]></title>
    <url>%2F2021%2F01%2F02%2F2021-08-14-Apache%20DolphinScheduler%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 单机部署： https://dolphinscheduler.apache.org/zh-cn/docs/1.3.4/user_doc/standalone-deployment.html 最新架构图： https://www.analysys.cn/developer/apache-dolphinscheduler/ 源码分析: https://www.cnblogs.com/gabry/p/12217966.html 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kong API 网关使用]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-31-Kong%20API%20%E7%BD%91%E5%85%B3%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 https://zhuanlan.zhihu.com/p/242260216 # 本次分享我们从百亿流量交易系统API网关（API Gateway）的现状和面临问题出发，阐述微服务架构与 API 网关的关系，理顺流量网关与业务网关的脉络，带来最全面的 API 网关知识与经验。内容涉及： 第一部分：API网关概述 分布式服务架构、微服务架构与 API 网关 API 网关的定义与职能、关注点 API 网关的分类与技术分析 第二部分：开源网关的分析与调研 常见的开源网关介绍 四大开源网关的对比分析（OpenResty/Kong/Zuul2/SpringCloudGateway 等） 开源网关的技术总结 第三部分：百亿流量交易系统 API 网关设计 百亿流量 API 网关的现状和面临问题 业务网关的设计与最佳实践 对API网关的发展展望 第一部分：API网关概述 计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决。 – David Wheeler 分布式服务架构、微服务架构与 API 网关什么是API网关（API Gateway）其实网关跟面向服务架构（Service Oriented Architecture，SOA）和微服务架构（MicroServices Architecture，MSA）有很深的渊源。 十多年以前，银行等金融机构完成全国业务系统大集中以后，分散的系统都变得集中，同时也带来各种问题：业务发展过快如何应对，对接系统过多如何集成和管理。为了解决这些问题，业界实现了作用于渠道与业务系统之间的中间层网关，即综合前置系统，由其适配各类渠道和业务，处理各种协议接入、路由与报文转换、同步异步调用等。 图1 人们基于SOA的理念，在综合前置的基础上，进一步增加了服务的元数据管理、注册、中介、编排、治理等功能，逐渐形成了企业服务总线（ESB，Enterprise Service Bus）。 图2（作者参与设计开发的Primeton ESB系统） 面向服务架构（SOA）是一种建设企业IT生态系统的架构指导思想。SOA的关注点是服务，服务最基本的业务功能单元，由平台中立性的接口契约来定义。通过将业务系统服务化，可以将不同模块解耦，各种异构系统间可以轻松实现服务调用、消息交换和资源共享。 不同于以往的孤立业务系统，SOA强调整个企业IT生态环境是一个大的整体。整个IT生态中的所有业务服务构成了企业的核心IT资源。各系统的业务拆解为不同粒度和层次的模块和服务，服务可以组装到更大的粒度，不同来源的服务可以编排到同一个处理流程，实现非常复杂的集成场景和更加丰富的业务功能。 SOA从更高的层次对整个企业IT生态进行统一的设计与管理，应用软件被划分为具有不同功能的服务单元，并通过标准的软件接口把这些服务联系起来，以SOA架构实现的企业应用可以更灵活快速地响应企业业务变化，实现新旧软件资产的整合和复用，降低软件整体拥有成本。 当然基于ESB这种集中式管理的SOA方案也存在着种种问题，特别是面向互联网技术领域的爆发式发展的情况下。 分布式服务架构、微服务架构与API网关近年来，随着互联网技术的飞速发展，为了解决以ESB为代表的集中式管理的SOA方案的种种问题，以Apache Dubbo（2011年开源后）与Spring Cloud为代表的分布式服务化技术的出现，给了SOA实现的另外一个选择：去中心化的分布式服务架构（DSA）。分布式服务架构技术不再依赖于具体的服务中心容器技术（比如ESB），而是将服务寻址和调用完全分开，这样就不需要通过容器作为服务代理。 之后又在此基础上随着REST、Docker容器化、领域建模、自动化测试运维等领域的发展，逐渐形成了微服务架构（MSA）。在微服务架构里，服务的粒度被进一步细分，各个业务服务可以被独立地设计、开发、测试、部署和管理。这时，各个独立部署单元可以选择不同的开发测试团队维护，可以使用不同的编程语言和技术平台进行设计，但是要求必须使用一种语言和平台无关的服务协议作为各个单元之间的通信方式，如图7-3所示。 图3 在微服务架构中，由于系统和服务的细分，导致系统结构变得非常复杂，RESTAPI由于其简单、高效、跨平台、易开发、易测试、易集成，成为不二选择。此时一个类似综合前置的系统就产生了，这就是API网关（API Gateway）。API网关作为分散在各个业务系统微服务的API聚合点和统一接入点，外部请求通过访问这个接入点，即可访问内部所有的REST API服务。 跟SOA/ESB类似，企业内部向外暴露的所有业务服务能力，都可以通过API网关上管理的API服务得以体现，所以API网关上也就聚合了企业所有直接对外提供的IT业务能力。 API网关的技术趋势我们从百度指数趋势看到，SpringCloud和SOA非常火，MSA、gRPC、Gateway也都有着非常高的关注度，而且这些技术的搜索趋势都正相关。 另一方面，我们可以通过Github的搜索来看，Gateway类型的项目也非常多。 图4 github.com/search?o=de… 可以看到，前10页的100个项目，使用Go语言实现的Gateway差不多占一半，语言分类上来看： Go&gt;NodeJS/JavaScript&gt;Java&gt;Lua&gt;C/C++&gt;PHP&gt;Python/Ruby/Perl API 网关的定义、职能与关注点API网关的定义 网关的角色是作为一个API架构，用来保护、增强和控制对于API服务的访问。（The role of a Gateway in an API architecture is to protect, enrich and control access to API services.）– github.com/strongloop/… API网关是一个处于应用程序或服务（提供REST API接口服务）之前的系统，用来管理授权、访问控制和流量限制等，这样REST API接口服务就被API网关保护起来，对所有的调用者透明。因此，隐藏在API网关后面的业务系统就可以专注于创建和管理服务，而不用去处理这些策略性的基础设施。 这样，网关系统就可以代理业务系统的业务服务API。此时网关接受外部其他系统的服务调用请求，也需要访问后端的实际业务服务。在接受请求的同时，可以实现安全相关的系统保护措施。在访问后端业务服务的时候，可以根据相关的请求信息做出判断，路由到特定的业务服务上，或者调用多个服务后聚合成新的数据返回给调用方。网关系统也可以把请求的数据做一些过程和预处理，同理也可以把返回给调用者的数据做一些过滤和预处理，即根据需要对请求头/响应头、请求报文/响应报文做一些修改处理。如果不做这些额外的处理，最简单直接的代理服务API功能，我们一般叫做透传。 同时，由于REST API的语言无关性，我们可以看出基于API网关，我们的后端服务可以是任何异构系统，不论是Java、Dotnet、Python，还是PHP、ROR、NodeJS等，只要是支持REST API，就可以被API网关管理起来。 API网关的职能 图5 一般来说，API网关有四大职能： 请求接入：作为所有API接口服务请求的接入点，管理所有的接入请求； 业务聚合：作为所有后端业务服务的聚合点，所有的业务服务都可以在这里被调用； 中介策略：实现安全、验证、路由、过滤、流控，缓存等策略，进行一些必要的中介处理； 统一管理：提供配置管理工具，对所有API服务的调用生命周期和相应的中介策略进行统一管理。 API网关的关注点通过以上的分析可以看出，API网关不是一个典型的业务系统， 而是一个为了让业务系统更专注与业务服务本身，给API服务提供更多附加能力的一个中间层。 这样在设计和实现API网关时，两个目标需要考虑： 开发维护简单，节约人力成本和维护成本。这要求我们使用非常成熟的简单可维护的技术体系。 高性能，节约设备成本，提高系统吞吐能力。这要求我们需要针对API网关的特点，进行一些特定的设计和权衡。 当并发量小的时候，这些都不是问题。然后一旦系统的API访问量非常大的时候，这些都会成为关键的问题。 海量并发的Gateway最重要的三个关注点： 保持大规模的inbound请求接入能力（长短连接），比如基于netty实现。 最大程度的复用outbound的HTTP连接能力，比如基于HttpClient4的asynchronizedHttpclient实现。 方便灵活地实现安全、验证、过滤、聚合、限流、监控等各种策略。 API网关的分类与技术分析API网关的分类如果我们对于上述的目标和关注点进行更深入的思考，就会发现一个很重要的问题：所有需要考虑的问题和功能可以分为两类。 一类是全局性的，跟具体的后端业务系统和服务完全无关的部分，比如安全策略、全局性流控策略、流量分发策略等。 一类是针对具体的后端业务系统，或者是服务和业务有一定关联性的部分，并且一般被直接部署在业务服务的前面。 图6 这样，随着互联网的复杂业务系统的发展，这两类功能集合逐渐形成了现在常见的两种网关系统：流量网关和业务网关。 流量网关与WAF我们定义全局性的、跟具体的后端业务系统和服务完全无关的策略网关，即为流量网关。这样流量网关关注于全局流量的稳定与安全，具体比如防止各类SQL注入，黑白名单控制，接入请求到业务系统的Loadbalance等，通常有如下的一些通用性功能： 全局性流控 日志统计 防止SQL注入 防止Web攻击 屏蔽工具扫描 黑白名单控制 等等。 通过这个功能清单，我们可以发现，流量网关的功能跟Web应用防火墙（WAF）非常类似。WAF一般是基于Nginx/OpenResty的ngx_lua模块开发的Web应用防火墙。 WAF一般代码很简单，关注于使用简单，高性能和轻量级。简单的说就是在Nginx本身的代理能力以外，添加了安全相关功能。一句话来描述其原理，就是解析HTTP请求（协议解析模块），规则检测（规则模块），做不同的防御动作（动作模块），并将防御过程（日志模块）记录下来。 一般的WAF具有如下功能： 防止SQL注入，本地包含，部分溢出，fuzzing测试，XSS/SSRF等Web攻击 防止Apache Bench之类压力测试工具的攻击 屏蔽常见的扫描黑客工具，扫描器 屏蔽图片附件类目录执行权限、防止webshell上传 支持IP白名单和黑名单功能，直接将黑名单的IP访问拒绝 支持URL白名单，将不需要过滤的URL进行定义 支持User-Agent的过滤、支持CC攻击防护、限制单个URL指定时间的访问次数 支持支持Cookie过滤，URL与URL参数过滤 支持日志记录，将所有拒绝的操作，记录到日志中去 几个WAF开源实现以上WAF的内容主要参考如下两个项目： github.com/unixhot/waf github.com/loveshell/n… 流量网关的开源实例，还可以参考著名的开源项目Kong（基于OpenResty）。 业务网关我们定义针对具体的后端业务系统，或者是服务和业务有一定关联性的策略网关，即为业务网关。比如针对某个系统、某个服务或者某个用户分类的流控策略，针对某一类服务的缓存策略，针对某个具体系统的权限验证方式，针对某些用户条件判断的请求过滤，针对具体几个相关API的数据聚合封装等等。 业务网关一般部署在流量网关之后，业务系统之前，比流量网关更靠近系统。我们大部分情况下说的API网关，狭义上指的是业务网关。并且如果系统的规模不大，我们也会将两者合二为一，使用一个网关来处理所有的工作。具体的业务网关设计实现，将在下面的篇章详细介绍。 第二部分：开源网关的分析与调研常见的开源网关介绍 图7（开源网关技术图谱） 目前常见的开源网关大致上按照语言分类有如下几类： Nginx+lua：Open Resty、Kong、Orange、Abtesting gateway等 Java：Zuul/Zuul2、Spring Cloud Gateway、Kaazing KWG、gravitee、Dromara soul等 Go：Janus、fagongzi、Grpc-gateway Dotnet：Ocelot NodeJS：Express Gateway、Micro Gateway 按照使用数量、成熟度等来划分，主流的有4个： OpenResty Kong Zuul/Zuul2 Spring Cloud Gateway Nginx+LuaOpenResty*项目地址：openresty.org/ OpenResty® 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。 OpenResty® 通过汇聚各种设计精良的 Nginx 模块（主要由 OpenResty 团队自主开发），从而将 Nginx 有效地变成一个强大的通用 Web 应用平台。这样，Web 开发人员和系统工程师可以使用 Lua 脚本语言调动 Nginx 支持的各种 C 以及 Lua 模块，快速构造出足以胜任 10K 乃至 1000K 以上单机并发连接的高性能 Web 应用系统。 OpenResty® 的目标是让你的Web服务直接跑在 Nginx 服务内部，充分利用 Nginx 的非阻塞 I/O 模型，不仅仅对 HTTP 客户端请求,甚至于对远程后端诸如 MySQL、PostgreSQL、Memcached 以及 Redis 等都进行一致的高性能响应。 以上介绍来自于OpenResty网站中文版：openresty.org/cn 简单的说，OpenResty基于Nginx，集成了Lua语言和Lua的各种工具库，可用的第三方模块，这样我们就在Nginx既有的高效HTTP处理的基础上，同时获得了Lua提供的动态扩展能力。因此，我们可以做出各种符合我们需要的网关策略的Lua脚本，以其为基础实现我们的网关系统。 Kong*项目地址：konghq.com/ 与 github.com/kong/kong Kong基于OpenResty，是一个云原生、快速、可扩展、分布式的微服务抽象层（Microservice Abstraction Layer），也叫API网关（API Gateway），在Service Mesh里也叫API中间件（API Middleware）。 Kong开源于2015年，核心价值在于高性能和扩展性。从全球5000强的组织统计数据来看，Kong是现在依然在维护的，在生产环境使用最广泛的API网关。 Kong宣称自己是世界上最流行的开源微服务API网关（The World’s Most Popular Open Source Microservice API Gateway）。 核心优势： 可扩展：可以方便的通过添加节点水平扩展，这意味着可以在很低的延迟下支持很大的系统负载。 模块化：可以通过添加新的插件来扩展Kong的能力，这些插件可以通过RESTful Admin API来安装和配置。 在任何基础架构上运行：Kong可以在任何地方都能运行，比如在云或混合环境中部署Kong，单个或全球的数据中心。 图8 ABTestingGateway项目地址：github.com/CNSRE/ABTes… ABTestingGateway是一个可以动态设置分流策略的网关，关注与灰度发布相关领域，基于nginx和ngx-lua开发，使用 redis 作为分流策略数据库，可以实现动态调度功能。 ABTestingGateway 是新浪微博内部的动态路由系统 dygateway 的一部分，目前已经开源。在以往的基于 nginx 实现的灰度系统中，分流逻辑往往通过 rewrite 阶段的 if 和 rewrite 指令等实现，优点是性能较高，缺点是功能受限、容易出错，以及转发规则固定，只能静态分流。ABTestingGateway则采用 ngx-lua，通过启用lua-shared-dict和lua-resty-lock作为系统缓存和缓存锁，系统获得了较为接近原生nginx转发的性能。 功能特性： 支持多种分流方式，目前包括iprange、uidrange、uid尾数和指定uid分流 支持多级分流，动态设置分流策略，即时生效，无需重启 可扩展性，提供了开发框架，开发者可以灵活添加新的分流方式，实现二次开发 高性能，压测数据接近原生nginx转发 灰度系统配置写在nginx配置文件中，方便管理员配置 适用于多种场景：灰度发布、AB测试和负载均衡等 据了解，美团内部的Oceanus也是基于Nginx和ngx_lua扩展实现，主要提供服务注册与发现、动态负载均衡、可视化管理、定制化路由、安全反扒、session ID复用、熔断降级、一键截流和性能统计等功能。 JAVAZuul/Zuul2*项目地址：github.com/Netflix/zuu… Zuul是Netflix开源的API网关系统，它的主要设计目标是动态路由、监控、弹性和安全。 Zuul的内部原理可以简单看做是很多不同功能filter的集合（PS：作为对比，ESB也可以简单被看做是管道（channel）和过滤器（filter）的集合。），这些filter可以使用groovy或其他基于JVM的脚本编写（当然Java也可以编写），放置在指定的位置，然后可以被Zuul Server轮询发现变动后动态加载并实时生效。 Zuul目前有两个大的版本，1.x和2.x，这两个版本差别很大。 Zuul1.x基于同步IO，也是Spring Cloud全家桶的一部分，可以方便的配合Spring Boot/Spring Cloud配置和使用。 在Zuul1.x里，filter的种类和处理流程可以参见下图，最主要的就是pre、routing、post这三种过滤器，分别作用于调用业务服务API之前的请求处理、直接响应、调用业务服务API之后的响应处理。 图9（Zuul 1.x示意图） Zuul2.x最大的改进就是基于Netty Server实现了异步IO来接入请求，同时基于Netty Client实现了到后端业务服务API的请求。这样就可以实现更高的性能、更低的延迟。此外也调整了filter类型，将原来的三个核心filter显式命名为：Inbound Filter、Endpoint Filter和Outbound Filter。 图10（Zuul 2.x） Zuul2.x核心功能： Service Discovery Load Balancing Connection Pooling Status Categories Retries Request Passport Request Attempts Origin Concurrency Protection HTTP/2 Mutual TLS Proxy Protocol GZip WebSockets Spring Cloud Gateway*项目地址：github.com/spring-clou… Spring Cloud Gateway基于Java8、Spring 5.0、Spring Boot 2.0、Project Reactor，发展的比Zuul2要早，目前也是Spring Cloud全家桶的一部分。 Spring Cloud Gateway可以看做是一个Zuul1.x的升级版和代替品，比Zuul2更早的使用Netty实现异步IO，从而实现了一个简单、比Zuul1.x更高效的、与Spring Cloud紧密配合的API网关。 Spring Cloud Gateway里明确的区分了Router和Filter，并且一个很大的特点是内置了非常多的开箱即用功能，并且都可以通过SpringBoot配置或者手工编码链式调用来使用。 比如内置了10种Router，使得我们可以直接配置一下就可以随心所欲的根据Header，或者Path，或者Host，或者Query来做路由。 比如区分了一般的Filter和全局Filter，内置了20种Filter和9种全局Filter，也都可以直接用。当然自定义Filter也非常方便。 核心特性： Able to match routes on any request attribute. Predicates and filters are specific to routes. Hystrix Circuit Breaker integration. Spring Cloud DiscoveryClient integration Easy to write Predicates and Filters Request Rate Limiting Path Rewriting gravitee gateway项目地址：gravitee.io/ 与 github.com/gravitee-io… Kaazing WebSocket Gateway项目地址：github.com/kaazing/gat… 与 kaazing.com/products/we… Kaazing WebSocket Gateway是一个专门针对和处理Websocket的网关，其宣称提供世界一流的企业级WebSocket服务能力。 具体如下特性： 标准WebSocket支持，支持全双工的双向数据投递 线性扩展，无状态架构意味着可以部署更多机器来扩展服务能力 验证，鉴权，单点登录支持，跨域访问控制 SSL/TLS加密支持 Websocket keepalive和TCP半开半关探测 通过负载均衡和集群实现高可用 Docker支持 JMS/AMQP等支持 IP白名单 自动重连和消息可靠接受保证 Fanout处理策略 实时缓存等 Dromara soul项目地址： github.com/Dromara/sou… Soul是一个异步的、高性能的、跨语言的、响应式的API网关，提供了统一的HTTP访问。 支持各种语言，无缝集成Dubbo和SpringCloud； 丰富的插件支持鉴权、限流、熔断、防火墙等； 网关多种规则动态配置，支持各种策略配置； 插件热插拔，易扩展； 支持集群部署，支持A/B Test。 图11 Gofagongzi项目地址：github.com/fagongzi/ga… fagongzi gateway是一个Go实现的功能全面的API Gateway，自带了一个rails实现的web UI管理界面。 功能特性： 流量控制 熔断 负载均衡 服务发现 插件机制 路由(分流，复制流量) API 聚合 API 参数校验 API 访问控制（黑白名单） API 默认返回值 API 定制返回值 API 结果Cache JWT Authorization API Metric导入Prometheus API 失败重试 后端server的健康检查 开放管理API(GRPC、Restful) 支持Websocket协议 Janus项目地址：github.com/hellofresh/… Janus是一个轻量级的API Gateway和管理平台，它能帮你实现控制谁，什么时候，如何访问这些REST API，同时它也记录了所有的访问交互细节和错误。使用Go实现API网关的一个好处在于，一般只需要一个单独的二进制文件即可运行，没有复杂的依赖关系（No dependency hell）。 功能特性： 热加载配置，不需要重启网关进程 HTTP连接的优雅关闭 支持OpenTracing，从而可以进行分布式跟踪 支持HTTP/2 可以针对每一个API实现断路器 重试机制 流控，可以针对每一个用户或者key CORS过滤，可以针对具体的API 多种开箱即用的验证协议支持，比如JWT、OAuth2.0和Basic Auth docker image支持 DotnetOcelot项目地址：github.com/ThreeMammal… 路由 请求聚合 服务发现（基于Consul或Eureka） 服务Fabric WebSockets 验证与鉴权 流控 缓存 重试策略与QoS 负载均衡 日志与跟踪 请求头、Query字符串转换 自定义的中间处理 配置和管理REST API NodeJSExpress Gateway项目地址：github.com/ExpressGate… 与 www.express-gateway.io/ Express Gateway是一个基于NodeJS开发，Express和Express中间件实现的REST API网关。 功能特性： 动态中心化配置 API消费者和凭证管理 插件机制 分布式数据存储 命令行工具CLI microgateway项目地址：github.com/strongloop/… 与 developer.ibm.com/apiconnect StrongLoop是IBM的一个子公司，Microgateway网关基于Node.js/Express和Nginx构建，作为IBM API Connect，同时也是IBM云生态的一部分。 Microgateway是一个聚焦于开发者，可扩展的网关框架，它可以增强我们对微服务和API的访问能力。 核心特性： 安全和控制，基于Swagger(OpenAPI)规范 内置了多种网关策略，API Key验证，流控，OAuth2.0，JavaScript脚本支持 使用Swagger扩展（API Assembly）实现网关策略（安全、路由、集成等） 方便地自定义网关策略 此外，Microgateway还有几个特性： 通过集成Swagger，实现基于Swagger API定义的验证能力， 使用datastore来保持需要处理的API数据模型， 使用一个流式引擎来处理多种策略，使得API设计者可以更好的控制API的生命周期 核心架构如下图所示： 图12 四大开源网关的对比分析（OpenResty/Kong/Zuul2/SpringCloudGateway等）OpenResty/Kong/Zuul2/SpringCloudGateway重要特性对比 网关 限流 鉴权 监控 易用性 可维护性 成熟度 Spring Cloud Gateway 可以通过IP，用户，集群限流，提供了相应的接口进行扩展 普通鉴权、auth2.0 Gateway Metrics Filter 简单易用 spring系列可扩展强，易配置 可维护性好 spring社区成熟，但gateway资源较少 Zuul2 可以通过配置文件配置集群限流和单服务器限流亦可通过filter实现限流扩展 filter中实现 filter中实现 参考资料较少 可维护性较差 开源不久，资料少 OpenResty 需要lua开发 需要lua开发 需要开发 简单易用，但是需要进行的lua开发很多 可维护性较差，将来需要维护大量lua脚本 很成熟资料很多 Kong 根据秒，分，时，天，月，年，根据用户进行限流。可在原码的基础上进行开发 普通鉴权，Key Auth鉴权，HMAC，auth2.0 可上报datadog，记录请求数量，请求数据量，应答数据量，接收于发送的时间间隔，状态码数量，kong内运行时间 简单易用，api转发通过管理员接口配置，开发需要lua脚本 可维护性较差，将来需要维护大量lua库 相对成熟，用户问题汇总，社区，插件开源 以限流功能为例： Spring Cloud Gateway目前提供了基于Redis的Ratelimiter实现，使用的算法是令牌桶算法，通过yml文件进行配置； Zuul2可以通过配置文件配置集群限流和单服务器限流亦可通过filter实现限流扩展； OpenResty可以使用resty.limit.count、resty.limit.conn、resty.limit.req来实现限流功能可实现漏桶或令牌通算法； Kong拥有基础限流组件，可在基础组件源代码基础上进行lua开发。 对Zuul/Zuul2/Spring Cloud Gateway的一些功能点分析可以参考Spring Cloud Gateway作者Spencer Gibb的文章： spencergibb.netlify.com/preso/detro… OpenResty/Kong/Zuul2/SpringCloudGateway性能测试对比分别使用3台4Core16G内存的机器，作为API服务提供者、Gateway、压力机，使用wrk作为性能测试工具，对OpenResty/Kong/Zuul2/SpringCloudGateway进行简单小报文的情况进行性能测试。 图13（Spring Cloud Gateway、Zuul2、OpenResty、Kong的性能对比） 上图中y轴坐标是QPS，x轴是一个gateway的数据，每根线是一个场景下的不同网关数据，测试结论如下： 实测情况是性能 SCG~Zuul2 &lt;&lt; OpenResty ~&lt; Kong &lt;&lt; Direct（直连）； Spring Cloud Gateway、Zuul2的性能差不多，大概是直连的40%； OpenResty、Kong差不多，大概是直连的60-70%； 大并发下，例如模拟200并发用户、1000并发用户时，Zuul2会有很大概率返回出错。 开源网关的技术总结开源网关的测试分析脱离场景谈性能，都是耍流氓。性能就像温度，不同的场合下标准是不一样的。同样是18摄氏度，老人觉得冷，小孩觉得很合适，企鹅觉得热，冰箱里的蔬菜可能要坏了。 同样基准条件下，不同的参数和软件，相对而言的横向比较，才有价值。比如同样的机器（比如16G内存/4Core），同样的server（用spring boot，配置路径api/hello返回一个helloworld），同样的压测方式和工具（比如用wrk，10线程，20并发连接），我们测试直接访问server得到的极限QPS（QPS-Direct，29K），和配置了一个spring cloud gateway做网关访问的极限QPS（QPS-SCG，11K）、同样方式配置一个Zuul2做网关压测得到的极限QPS（QPS-Zuul2，13K），Kong得到的极限QPS（QPS-Kong，21K），OpenResty得到的极限QPS（QPS-OR，19K），这个对比就有意义了。 Kong的性能非常不错，非常适合做流量网关，并且对于 service，route，upstream，consumer，plugins的抽象，也是自研网关值得借鉴的。 对于复杂系统，不建议业务网关用Kong，或者更明确的说是不建议在Java技术栈的系统深度定制Kong或OpenResty，主要是工程性方面的考虑。举个例子：假如我们有很多个不同业务线，鉴权方式五花八门，都是与业务多少有点相关的。这时如果把鉴权在网关实现，就需要维护大量的Lua脚本，引入一个新的复杂技术栈是一个成本不低的事情。 Spring Cloud Gateway/Zuul2对于Java技术栈来说比较方便，可以依赖业务系统的一些common jar。Lua不方便，不光是语言的问题，更是复用基础设施的问题。另外，对于网关系统来说，性能不是差一个数量级，问题不大，多加2台机器就可以搞定。 目前测试的总结来看，如果服务都是2ms级别，直连的性能假如是100，Kong可以到60，OpenResty是50，Zuul2和Spring Cloud Gateway是35，如果服务本身的latency大一点，这些个差距会逐步缩小。 目前来看Zuul2的坑还是比较多的： 不成熟，没文档，刚出不久，还没有太多的实际应用案例 高并发时出错率较高，1000并发时我们的测试场景近50%的出错 所以简单使用或者轻度定制业务网关系统，目前比较建议使用Spring Cloud Gateway作为基础骨架。 各类网关的demo与测试以上测试用到的模拟服务和网关demo代码，大部分可以在这里找到：github.com/kimmking/sp… 也简单模拟了一个NodeJS做的Gateway，加了keep-alive和pool，demo的性能测试结果大概是直连的1/9，也就是Spring Cloud Gateway或Zuul2的1/4左右。 第三部分：百亿流量交易系统 API 网关设计百亿流量交易系统 API网关的现状和面临问题百亿流量系统面对的业务现状 图14 我们目前面临的现状是日常十几万的并发在线长连接数（不算短连接），每天长连接总数3000万+，每天API的调用次数超过100亿，每天交易订单数1.5亿。 在这个情况下，API网关设计的一个重要目标就是：如何借助API网关为各类客户提供精准、专业、个性化的服务，保障客户实时的获得业务系统的数据和业务能力。 网关系统与其他系统的关系我们的业务里，API网关系统与其他系统的关系大致如下图所示： 图15 网关系统典型的应用场景我们的API网关系统为Web端、移动APP端客户提供服务，同时也为大量API客户提供API调用服务，同时支持REST API和WebSocket协议。 作为实时交易系统的前置系统，必须精准及时为客户提供最新的行情和交易信息。一旦出现数据的延迟或者错误，都会给客户造成无法挽回的损失。 另外针对不同的客户和渠道，网关系统需要提供不同的安全、验证、流控、缓存策略，同时可以随时聚合不同视角的数据进行预处理，保障系统的稳定可靠和数据的实时精确。 图16、图17 交易系统API的特点作为一个全球性的交易系统，API的特点总结如下： 访问非常集中：最核心的一组API，占据了访问量的一半以上 访问非常频繁：QPS非常高，日均访问量非常大 数据格式固定：交易系统处理的数据格式非常固定 报文数据量小：每次请求传输的数据一般不超过10K 用户全世界分布：客户分布在全世界的各个国家 分内部调用和外部调用：除了API客户直接调用的API，其他的API都是由内部其他系统调用的 7x24小时不间断服务：系统需要提供高可用、不间断的服务能力，以满足不同时区客户的交易和自动化策略交易 外部用户有一定技术能力：外部API客户，一般是自己集成我们的API，实现自己的交易系统 交易系统API网关面临的问题问题1：流量的不断增加 如何合理控制流量，如何应对突发流量，怎么样最大程度的保障系统稳定，都是重要的问题。特别网关作为一个直接面对客户的系统，任何问题都会放大百倍。很多千奇百怪的重来没人遇到的问题都随时可能出现。 问题2：网关系统越来越复杂 现有的业务网关经过多年发展，里面有大量的业务嵌入，并且存在很多个不同的业务网关，相互之间没有任何关系，也没有沉淀出基础设施。 同时技术债务太多，系统里硬编码实现了全局性网关策略以及很多业务规则，导致维护成本较大。 问题3：API网关管理比较困难 海量并发下API的监控指标设计和数据的收集也是一个不小的问题。7x24小时运行的技术支持也导致维护成本较高。 问题4：推送还是拉取的选择 使用短连接还是长连接，REST API还是WebSocket？ 业务渠道较多（多个不同产品线的Web、App、API等形成十几个不同的渠道），导致用户的使用行为难以控制。 业务网关的设计与最佳实践API网关1.0我们的API网关1.0版本是多年前开发的，是直接使用OpenResty定制的，全局的安全测试、流量的路由转发策略、针对不同级别的限流等都是直接用Lua脚本实现。 这样就导致在经历了业务飞速发展以后，系统里存在了非常多的相同功能或不同功能的Lua脚本，每次上线或维护都需要找到影响的其中几个或几十个Lua脚本，进行策略调整，非常不方便，策略控制的粒度也不够细。 API网关2.0在区分了流量网关和业务网关以后，2017年开始实现了流量网关和业务网关的分离，流量网关继续使用OpenResty定制，只保留少量全局性，不经常改动的配置功能和对应的Lua脚本。 业务网关使用Vert.x实现的Java系统，部署在流量网关和后端业务服务系统之间，利用Vert.x的反应式编程能力和异步非阻塞IO能力、分布式部署的扩展能力，这样就初步解决了问题1和问题2。 图18 Vert.x是一个基于事件驱动和异步非阻塞IO、运行于JVM上的框架，如下图所示。在Vert.x里，Verticle是最基础的开发和部署单元，不同的Vert.x可以通过Event Bus传递数据，进而方便的实现高并发性能的网络程序。关于Vert.x原理的分析可以参考阿里宿何的blog：www.sczyh30.com/tags/Vert-x… 图19 Vert.x同时也很好的支持Websocket协议，所以可以方便的实现支持REST API和Websocket、完全异步的网关系统。 图20 一个高性能的API网关系统，缓存是必不可少的部分。无论是分发冷热数据，降低对业务系统的压力，还是作为中间数据源，为服务聚合提供高效可复用的业务数据，都发挥了巨大作用。 而一个优秀、高效的缓存系统，也必须是需要针对所承载的业务数据特点，进行特定设计和实现的。 图21 API网关的日常监控我们使用多种工具对API进行监控和管理，全链路访问跟踪、连接数统计分析、全世界重要国家和城市的波测访问统计。网关技术团队每时每刻都关注着数据的变化趋势。各个业务系统研发团队，每天安排专人关注自己系统的API性能，推进性能问题解决和持续优化。这就初步解决了问题3。 图22、23、24、25 推荐外部客户使用Websocket由于外部客户需要自己通过API网关调用API服务来集成业务服务能力到自己的系统。各个客户的技术能力和系统处理能力有较大差异，使用行为也各有不同。对于不断发展变动的交易业务数据，客户调用API频率太低则会影响数据实时性，调用频率太高则可能会浪费双方的系统资源。同时利用Websocket的消息推送特点，我们可以在网关系统控制客户接受消息的频率、单个用户的连接数量等，随时根据业务系统的情况动态进行策略调整。 综合考虑，Websocket是一个比REST API更加实时可靠，更加易于管理的方式。通过逐步协助和鼓励客户使用Websocket协议上，基本解决了问题4。 API网关的性能优化API网关系统作为API的统一接入点，为了给用户提供最优质的用户体验，必须长期做性能优化工作。 不仅API网关自己做优化，同时可以根据监控情况，时刻发现各业务系统的API服务能力，以此为出发点，推动各个业务系统不断优化API性能。 在此举一个具体的例子，某个网关系统发现连接经常剧烈抖动（如下图所示），严重影响系统的稳定性、浪费系统资源，经过排除发现： 有爬虫IP不断爬取我们的交易数据，且这些IP所在网段都没有在平台产生任何实际交易，最高单爬虫IP的每日新建连接近100万次，平均每秒10几次； 有部分API客户的程序存在bug，且处理速度有限，不断的断开并重新连接，尝试重新对API数据进行处理，严重影响了客户的用户体验。 针对如上分析，我们采取了几个处理方式： 对于每天认定的爬虫IP，加入黑名单，直接在流量网关限制其访问我们的API网关； 对于存在bug的API客户，协助对方进行问题定位和bug修复，增强客户使用信心； 对于处理速度和技术能力有限的客户，基于定制的Websocket服务，使用滑动时间窗口算法，在业务数据变化非常大时，对分发的消息进行批量优化； 对于未登录和识别身份的API调用，流量网关实现全局的流控策略，增加缓存时间和限制调用次数，保障系统稳定； 业务网关则根据API服务的重要等级和客户的分类，进一步细化和实时控制网关策略，最大程度保障核心业务和客户的使用。 优化前： 图26 优化后： 图27 对API网关的发展展望 图28 现有的API Gateway是以Vert.x为基础、结合业务自研的网关系统Gateway 2.0。 目前计划年底前基于Spring Cloud和Spring Cloud Gateway实现新一代微服务架构的网关系统Gateway 3.0。 计划整合了流量网关和业务网关、并增加了很多开箱即用功能组件的微服务架构网关，作为Apollo Gateway 1.0开源。 期待大家的共同参与。 参考文献及资料1、 案例，链接]]></content>
      <categories>
        <category>Api网关</category>
      </categories>
      <tags>
        <tag>Api网关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Airflow数据调度平台调研]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-03-Airflow%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 https://www.jianshu.com/p/e878bbc9ead2 https://airflow.apache.org/community/ 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Orange API 网关部署安装总结]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-18-Orange%20API%20%E7%BD%91%E5%85%B3%E9%83%A8%E7%BD%B2%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 编译安装 第二部分 高版本安装 第三部分 docker安装 第四部分 总结 参考文献及资料 背景orange是基于Nginx+lua研发的API网关项目。该项目为国人自研项目，大部分组件参考Kong网关项目。但是项目活跃性不大，文档不够丰富详细（至今官网文档还是0.6.4版本，大量issue状态open），插件缺少详细的说明。另外项目部署自动化较低，文档不够详细，部署有一定困难。 本文在suse系统上部署orange高低版本（0.6.1和0.7.1两个版本）。详细说明的编译部署的过程和注意事项，算是为项目文档做点贡献，用爱发电。 文中讲解了对接Tidb数据的配置注意事项。 第一部分 编译安装编译安装以0.6.4版本为例。 1.1 编译依赖环境安装为了部署安装OpenResty，需要提前准备编译环境，依赖有： GCC 编译器 nginx是C语言编写的，编译安装需要安装 gcc和gcc-c++。不同操作系统安装有差异，在线情况较为简单，例如ubuntu使用下面的命令即可： 1# apt-get install gcc g++ libreadline-dev libncurses5-dev libpcre3-dev libssl-dev perl make build-essential 如果是suse或者centos操作系统： 12# yum install -y gcc# yum install -y gcc-c++ 注：离线安装较为繁琐，例如suse需要离线下载rpm包安装。 1.2 OpenResty安装openResty是一个基于nginx+lua的WEB服务器,所以安装此软件的过程中也会将nginx一并安装好，配置要把http_stub_status_module模块加上。具体安装如下： 1.2.1 安装介质准备将安装介质上传至/approot1/orangesoft(路径自定义),介质清单： 123456lor-0.3.4.tar.gzopenresty-1.19.3.1.tar.gzorange-0.6.4.tar.gzpcre-8.39.tar.gzzlib-1.2.11.tar.gzopenssl-1.0.2p.tar.gz 使用tar命令将包依次解压。 1.2.2 安装安装命令： 1234567root@deeplearning:/approot1/orangesoft # tar -zxvf pcre-8.39.tar.gzroot@deeplearning:/approot1/orangesoft # tar -zxvf zlib-1.2.11.tar.gzroot@deeplearning:/approot1/orangesoft # tar -zxvf openssl-1.0.2p.tar.gz# root@deeplearning:/approot1/orangesoft # tar -zxvf openresty-1.19.3.1.tar.gzroot@deeplearning:/approot1/orangesoft # cd openresty-1.19.3.1root@deeplearning:/approot1/orangesoft/openresty-1.19.3.1 # ./configure --with-luajit --with-http_stub_status_module --with-openssl=/approot1/orangesoft/openssl-1.0.2p --with-zlib=/approot1/orangesoft/zlib-1.2.11 --with-pcre=/approot1/orangesoft/pcre-8.39 注意： 参数：--with-http_stub_status_module，必须依赖，orange启动依赖组件 参数：--prefix=/approot1/openresty，参数可以指定安装目录。这里没有配置，会使用默认的路径：/usr/local/openresty。 配置完成后，开始编译和安装： 12root@deeplearning:/approot1/orangesoft/openresty-1.19.3.1 # makeroot@deeplearning:/approot1/orangesoft/openresty-1.19.3.1 # make install 1.2.3 配置环境变量配置环境变量，在/etc/profile文件中最佳下面的配置： 12export PATH=$PATH:/usr/local/openresty/nginx/sbin export PATH=$PATH:/usr/local/openresty/bin 最后当前shell中生效环境变量（或者重新开一个shell）： 1# source /etc/profile 1.2.4 验证验证nginx： 12# nginx -vnginx version: openresty/1.19.3.1 验证openresty： 1234567# resty -vresty 0.27nginx version: openresty/1.19.3.1built by gcc 4.8.5 (SUSE Linux) built with OpenSSL 1.0.2p 14 Aug 2018TLS SNI support enabledconfigure arguments: --prefix=/usr/local/openresty/nginx --with-cc-opt=-O2 --add-module=../ngx_devel_kit-0.3.1 --add-module=../echo-nginx-module-0.62 --add-module=../xss-nginx-module-0.06 --add-module=../ngx_coolkit-0.2 --add-module=../set-misc-nginx-module-0.32 --add-module=../form-input-nginx-module-0.12 --add-module=../encrypted-session-nginx-module-0.08 --add-module=../srcache-nginx-module-0.32 --add-module=../ngx_lua-0.10.19 --add-module=../ngx_lua_upstream-0.07 --add-module=../headers-more-nginx-module-0.33 --add-module=../array-var-nginx-module-0.05 --add-module=../memc-nginx-module-0.19 --add-module=../redis2-nginx-module-0.15 --add-module=../redis-nginx-module-0.3.7 --add-module=../rds-json-nginx-module-0.15 --add-module=../rds-csv-nginx-module-0.09 --add-module=../ngx_stream_lua-0.0.9 --with-ld-opt=-Wl,-rpath,/usr/local/openresty/luajit/lib --with-http_stub_status_module --with-openssl=/approot1/orange/soft/openssl-1.0.2p --with-zlib=/approot1/orange/soft/zlib-1.2.11 --with-pcre=/approot1/orange/soft/pcre-8.39 --with-openssl-opt=-g --with-pcre-opt=-g --with-zlib-opt=-g --with-stream --with-stream_ssl_module --with-stream_ssl_preread_module --with-http_ssl_module openresty 组件安装成功。 1.3 安装lor1.3.1 介质准备加压安装包： 12root@deeplearning:/approot1/orangesoft # tar -zxvf lor-0.3.4.tar.gzroot@deeplearning:/approot1/orangesoft # cd lor-0.3.4 执行编译安装： 123456root@deeplearning:/approot1/orangesoft/lor-0.3.4 # make install install lor runtime files to /usr/local/lorlor runtime files installed.install lord cli to /usr/local/bin/lord cli installed.lor framework installed successfully. 1.4 安装orange4.1 介质准备12root@deeplearning:/approot1/orangesoft # tar -zxvf orange-0.6.4.tar.gzroot@deeplearning:/approot1/orangesoft # cd orange-0.6.4 4.2 编译安装1root@deeplearning:/approot1/orangesoft/orange-0.6.4 # make install 默认安装目录为/usr/local/orange： 12345678910111213141516root@deeplearning:/usr/local # cd orange/root@deeplearning:/usr/local/orange # lltotal 52drwxrwxr-x 2 sysop root 4096 May 16 2017 apidrwxrwxr-x 5 sysop root 4096 May 16 2017 bindrwx------ 2 nobody root 4096 Jun 4 01:25 client_body_tempdrwxrwxr-x 2 sysop root 4096 Jun 4 01:25 confdrwxrwxr-x 7 sysop root 4096 May 16 2017 dashboarddrwx------ 2 nobody root 4096 Jun 4 01:25 fastcgi_tempdrwxrwxr-x 2 sysop root 4096 May 16 2017 installdrwxr-xr-x 2 sysop root 4096 Jun 4 01:25 logsdrwxrwxr-x 6 sysop root 4096 May 16 2017 orangedrwx------ 2 nobody root 4096 Jun 4 01:25 proxy_tempdrwx------ 2 nobody root 4096 Jun 4 01:25 scgi_tempdrwxr-xr-x 2 sysop root 4096 Jun 4 01:25 tmpdrwx------ 2 nobody root 4096 Jun 4 01:25 uwsgi_temp 4.3 配置连接mysql或Tidb4.3.1 更新mysql依赖包主要使用最新版本的包替换旧版本的包： 1# /usr/local/openresty/lualib/resty/mysql.lua 注意文件的大小检查： 1-rw-r--r-- 1 root bin 34779 Mar 3 07:47 mysql.lua 4.3.2 配置连接mysql或Tidb的连接信息更新conf/orange.conf配置文件中mysql的相关配置： 1root@deeplearning:/usr/local/orange/conf # vi orange.conf 主要有配置： 123456789101112131415161718"store": "mysql","store_mysql": &#123; "timeout": 5000, "connect_config": &#123; "host": "192.169.8.8", "port": 4001, "database": "orange", "user": "orange", "password": "orange", "max_packet_size": 1048576, "charset": "utf8mb4" &#125;, "pool_config": &#123; "max_idle_timeout": 10000, "pool_size": 3 &#125;, "desc": "mysql configuration"&#125;, 特别注意的参数是： 字符集，&quot;charset&quot;: &quot;utf8mb4&quot;为新增字符集参数，适配tidb使用； 4.3.3 数据库初始化数据库的初始化sql文件如下，提前通过mysql客户端多数据库进行初始化： 12root@deeplearning:/approot1/orangesoft/orange-0.6.4/install # ll orange-v0.6.4.sql-rw-rw-r-- 1 root root 10406 May 16 2017 orange-v0.6.4.sql 数据库初始化： 123456# CREATE DATABASE orange;# CREATE USER 'orange'@'%' IDENTIFIED BY 'orange';# GRANT ALL PRIVILEGES ON orange.* TO 'orange'@'%';# FLUSH PRIVILEGES; 然后初始化表：# mysql -uorange -porange orange &lt; orange-v0.6.4.sql 4.3.4 配置用户认证12345678"dashboard": &#123; "auth": true, "session_secret": "y0ji4pdj61aaf3f11c2e65cd2263d3e7e5", "whitelist": [ "^/auth/login$", "^/error/$" ] &#125;, 其中参数&quot;auth&quot;: true为开启权限认证。默认用户名密钥如下，初次登陆可以修改，并新增用户。 12用户名： admin密钥： orange_admin 4.4 启动orange安装完成后，orange执行文件在/usr/local/bin/orange，所用使用下面的命令： 1234567891011121314root@deeplearning:/usr/local/orange/conf # orange helpOrange v0.6.4, OpenResty/Nginx API Gateway.Usage: orange COMMAND [OPTIONS]The commands are: stop Stop current Orangehelp Show help tipsrestart Restart Orangeversion Show the version of Orangereload Reload the config of Orangestart Start the Orange Gatewaystore Init/Update/Backup Orange store 使用下面命令启动： 1root@deeplearning:/usr/local/orange/conf # orange start 4.5 验证启动后orange的前端监听端口在9999，浏览器使用下面的地址验证界面： 1http://192.168.31.3:9999 第二部分 高版本安装前面的编译安装主要针对低版本的0.6.4，对于高版本的0.7.1版本，由于引入了较多的第三方插件，所以在安装orange前需要补充安装一些依赖包。 openresty自带包管理工具opm，新版本建议使用新的包管理工具luarocks。 2.1 安装luarocks首先配置，安装在/usr/local/luarocks。 1234567891011121314151617181920root@deeplearning:/approot1/orange/soft/luarocks-3.0.3 # ./configure --with-lua=/usr/local/openresty/luajit --prefix=/usr/local/luarocksConfiguring LuaRocks version 3.0.3...Lua version detected: 5.1Lua interpreter found: /usr/local/openresty/luajit/bin/luajitlua.h found: /usr/local/openresty/luajit/include/luajit-2.1/lua.hunzip found in PATH: /usr/binDone configuring.LuaRocks will be installed at......: /usr/local/luarocksLuaRocks will install rocks at.....: /usr/local/luarocksLuaRocks configuration directory...: /usr/local/luarocks/etc/luarocksUsing Lua from.....................: /usr/local/openresty/luajit* Type make and make install: to install to /usr/local/luarocks as usual.* Type make bootstrap: to install LuaRocks into /usr/local/luarocks as a rock. 然后安装： 1root@deeplearning:/approot1/orange/soft/luarocks-3.0.3 # make &amp;&amp; make install 2.2 依赖包安装orange高版本引入多个第三方包，这些包依赖如下，需要在线环境进行安装。 1234567891011121314151617181920212223root@deeplearning:/usr/local/orange/logs # luarocks install luafilesystem# ...root@deeplearning:/usr/local/orange/logs # luarocks listRocks installed for Lua 5.1---------------------------binaryheap 0.4-1 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1lrandom 20180729-1 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1lua-resty-dns-client 1.0.0-1 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1lua-resty-http 0.13-0 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1lua-resty-jwt 0.2.0-0 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1lua-resty-timer 1.1.0-1 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1luafilesystem 1.8.0-1 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1luasocket 3.0rc1-2 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1penlight 1.5.4-1 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1 2.3 依赖配置依赖包安装的路径需要添加到rerquire搜索路径中。更新conf/nginx.conf配置文件中下面两个参数： 123#----------------------------Orange configuration----------------------------lua_package_cpath '/usr/local/luarocks/lib/lua/5.1/?.so;/usr/local/luarocks/lib/lua/5.1/socket/?.so;/usr/local/luarocks/lib/lua/5.1/mime/?.so;;';lua_package_path './lualib/?.lua;./lualib/resty/?.lua;/usr/local/orange/?.lua;/usr/local/lor/?.lua;/usr/local/luarocks/share/lua/5.1/?.lua;/usr/local/luarocks/share/lua/5.1/resty/?.lua;;'; 完成以上依赖配置后，参考低版本部署orange即可。 2.4 注意事项2.4.1 高版本sql文件缺失在部署高版本0.7.1时候，数据库表初始化sql文件orange-v0.7.0.sql中缺失组件headers表的初始化语句，可以在版本0.8.1中的初始化文件orange-v0.8.1.sql中获取。 说明项目的版本管理较为混乱。 2.4.2 lua-resty-mysql版本问题orange项目和mysql或者tidb交互使用的是依赖包lua-resty-mysql。版本差异有： 0.6.4版本使用0.19版本 0.7.1版本使用0.23版本 经过测试目前0.23版本兼容tidb，而低版本0.19版本不兼容。所以在部署低版本orange的时候注意替换和安装新版lua-resty-mysql包。 依赖包的位置，可以直接替换文件生效。 1/usr/local/openresty/lualib/resty/mysql.lua 第三部分 docker安装另外orange项目组也制作了docker镜像，项目地址为：syhily/orange。需要注意的是该镜像中orange版本为0.6.4版本，详细可以参考项目的Dockerfile文件。 2.1 部署mysql容器1# docker run --name orange-database -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=orange -p 3306:3306 mysql:5.7 2.2 部署orange集群启动主节点： 123456789101112# docker run -d --name orange \ --link orange-database:orange-database \ -p 7777:7777 \ -p 8888:8888 \ -p 9999:9999 \ --security-opt seccomp:unconfined \ -e ORANGE_DATABASE=orange \ -e ORANGE_HOST=orange-database \ -e ORANGE_PORT=3306 \ -e ORANGE_USER=root \ -e ORANGE_PWD=root \ syhily/orange 启动从节点： 12345678910# docker run -d --name orange \ -p 7777:7777 \ -p 8888:8888 \ -p 9999:9999 \ --security-opt seccomp:unconfined \ -e ORANGE_DATABASE=orange \ -e ORANGE_HOST=192.168.31.3 \ -e ORANGE_PORT=4000 \ -e ORANGE_USER=root \ syhily/orange 注意：官网的镜像配置中映射的api端口为8888，但是启动配置是80，这是错误配置。本文在这里进行了纠正。 第四部分 总结5.1 非编译安装前面的过程都是编译安装的，特别是组件openresty编译过程比较漫长。如果测试环境已经编译完成，只需要将部署目录整体拷贝至生产环境后即可（注意环境变量），不再赘述。 5.2 集群部署orange网关启动后将所有配置信息加载在内存中，mysql或者Tidb数据库只是作为配置的持久化保存。所有可以启动多个orange网关共用一套数据库，架构上组成集群模式。维护中需要注意的是： 同步配置 在集群部署模式下，一台网关应用服务器调整配置，需要及时手动同步配置到数据库持久化，另外其他网关节点需要手动同步将数据库中最新配置加载至网关内存。 研发配置自动同步组件 集群中每个节点登记注册加入，其中一台节点更新了配置，自动将登记的节点集合中所有配置自动刷新。 数据库故障 网关集群中，数据库只是配置的持久化。若短时间数据库故障不可访问，并不影响网关的对外服务。 5.3 动态调整低版本orange项目还不支持动态upstream配置功能。需要通过在nginx配置文件中新增。新增完成后需要使用下面的命令动态加载： 1# orange reload 高版本中有Dynamic Upstream组件实现动态upstream配置功能。 5.4 域名问题在实际生产线上环境，orange背后的持久化数据库对外地址通常是高可用的域名模式，另外网关代理的API地址通常也是内部域名。操作系统配置的DNS并不会被nginx读取，本地hosts是有效的。 这时候就需要将内部DNS地址配置在conf/nginx.conf文件中，例如： 1resolver 192.168.12.1 192.168.12.2 ipv6=off; 上面两个DNS地址以轮询方式请求，解析间隔默认是3600秒。 写在最后期望orange项目组未来能投入更多的精力继续演进项目。尽快发布新版本，丰富文档，答复issue。否则项目也荒凉的风险呀。 参考文献及资料[1] OpenResty官网，链接：http://openresty.org/cn/download.html [2] orange 使用文档，链接：http://orange.sumory.com/docs/guides/usages/ [3] orange官网，链接：http://orange.sumory.com/]]></content>
      <categories>
        <category>Api网关</category>
      </categories>
      <tags>
        <tag>Api网关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[streamsets数据采集平台介绍]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-03-streamsets%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%B9%B3%E5%8F%B0%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 https://blog.csdn.net/ffjl1985/article/details/81391333 https://blog.csdn.net/xfg0218/article/details/80731557?utm_source=blogxgwz0&amp;utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-6&amp;spm=1001.2101.3001.4242 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache NiFi数据调度平台调研]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-03-Apache%20NiFi%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 https://cloud.tencent.com/developer/news/89963 https://blogs.apache.org/nifi/ http://nifi.apache.org/quickstart.html https://github.com/apache/nifi 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache DolphinScheduler调度平台调研]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-03-Apache%20DolphinScheduler%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 单机部署： https://dolphinscheduler.apache.org/zh-cn/docs/1.3.4/user_doc/standalone-deployment.html 最新架构图： https://www.analysys.cn/developer/apache-dolphinscheduler/ 源码分析: https://www.cnblogs.com/gabry/p/12217966.html 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat介绍]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-02-Tomcat%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 # 第一部分 Tomcat的基础知识1.1 Web 概念1.1.1 软件架构Web应用有两种架构： C/S架构；QQ软件 B/S架构；各类网站 1.1.2 资源分类 静态资源； 所有用户访问后，返回的结果相同，称为静态资源。静态资源可以直接被浏览器解析。例如：html、css、js、图片文件等。 动态资源； 用户访问资源后，返回的结果不同，称为动态资源。动态资源被访问后，需要先转换成静态资源，然后再返回给浏览器，通过浏览器进行最后解析。例如：servlet/jsp、php、asp等 1.1.3 网络通讯 ip地址；ip地址作为用户在互联网上唯一标识。 端口；端口是计算机中应用进程的唯一标识。 通信协议；定义了数据传输的规则，例如：tcp/udp、http。 1.2 常见的Web服务器1.2.1 概念 服务器；安装服务器软件的计算机节点。 服务器软件；接受用户的请求，处理请求和应答请求。在Web服务软件中，可以部署Web项目，用户可以通过浏览器进行访问项目。 1.2.2 常见的Web服务器软件 BEA WebLogic 是用于开发、集成、部署和管理大型分布式Web应用、网络应用和数据库应用的Java应用服务器。将Java的动态功能和Java Enterprise标准的安全性引入大型网络应用的开发、集成、部署和管理之中。 BEA WebLogic Server拥有处理关键Web应用系统问题所需的性能、可扩展性和高可用性 Apache Apache 是世界使用排名第一的Web服务器软件。它可以运行在几乎所有广泛使用的计算机平台上。Apache源于NCSAhttpd服务器，经过多次修改，成为世界上最流行的Web服务器软件之一。Apache取自”a patchy server”的读音，意思是充满补丁的服务器，因为它是自由软件，所以不断有人来为它开发新的功能、新的特性、修改原来的缺陷。Apache的特点是简单、速度快、性能稳定，并可做代理服务器来使用。 IIS 是英文Internet Information Server的缩写，译成中文就是”Internet信息服务”的意思。它是微软公司主推的服务器，最新的版本是Windows2008里面包含的IIS 7，IIS与Window Server完全集成在一起，因而用户能够利用Windows Server和NTFS（NT File System，NT的文件系统）内置的安全特性，建立强大，灵活而安全的Internet和Intranet站点。 WebSphere软件平台能够帮助客户在Web上创建自己的业务或将自己的业务扩展到Web上，为客户提供了一个可靠、可扩展、跨平台的解决方案。作为IBM电子商务应用框架的一个关键组成部分，WebSphere软件平台为客户提供了一个使其能够充分利用Internet的集成解决方案。 Tomcat是一个开放源代码、运行servlet和JSP Web应用软件的基于Java的Web应用软件容器。Tomcat Server是根据servlet和JSP规范进行执行的，因此我们就可以说Tomcat Server也实行了Apache-Jakarta规范且比绝大多数商业应用软件服务器要好。 其中Apache和Tomcat是开源免费的。 1.3 Tomcat的历史Tomcat是Apache 软件基金会(Apache Software Foundation)的Jakarta 项目中的一个核心项目，由Apache、Sun 和其他一些公司及个人共同开发而成。由于有了Sun 的参与和支持，最新的Servlet 和JSP 规范总是能在Tomcat 中得到体现，Tomcat 5支持最新的Servlet 2.4 和JSP 2.0 规范。因为Tomcat 技术先进、性能稳定，而且免费，因而深受Java 爱好者的喜爱并得到了部分软件开发商的认可，成为目前比较流行的Web 应用服务器。 Tomcat 服务器是一个免费的开放源代码的Web 应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试JSP 程序的首选。对于一个初学者来说，可以这样认为，当在一台机器上配置好Apache 服务器，可利用它响应HTML(标准通用标记语言下的一个应用)页面的访问请求。实际上Tomcat是Apache 服务器的扩展，但运行时它是独立运行的，所以当你运行tomcat 时，它实际上作为一个与Apache 独立的进程单独运行的。 诀窍是，当配置正确时，Apache 为HTML页面服务，而Tomcat 实际上运行JSP 页面和Servlet。另外，Tomcat和IIS等Web服务器一样，具有处理HTML页面的功能，另外它还是一个Servlet和JSP容器，独立的Servlet容器是Tomcat的默认模式。不过，Tomcat处理静态HTML的能力不如Apache服务器。目前Tomcat最新版本为9.0.37。 1.4 Tomcat安装1.4.1 下载可以在Tomcat官网下载安。官网地址为：http://tomcat.apache.org/ 1.4.2 安装ubuntu系统中下载安装介质： 1root@deeplearning:/data/tomcat# wget https://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-9/v9.0.41/bin/apache-tomcat-9.0.41.tar.gz 解压： 1root@deeplearning:/data/tomcat# tar -zxvf apache-tomcat-9.0.41.tar.gz 1.5 Tomcat 目录结构目录结构如下： 目录及文件 说明 bin 用于存放 Tomcat的启动、停止等批处理脚本和Shell脚本 bin/startup. bat 用于在 Windows下启动 Tomcat bin/startup.sh 用于在 Linux下启动 Tomcat bin/shutdown. bat 用于在 Windows下停止 Tomcat bin/shutdown.sh 用于在 Linux下停止 Tomcat conf 用于存放 Tomcat的相关配置文件 conf/Catalina 用于存储针对每个虚拟机的 Context 配置 conf/context.xml 用于定义所有Web应用均需要加载的 Context 配置，如果Web应用指定了自己的context.xml，那么该文件的配置将被覆盖 conf/catalina.properties Tomcat环境变量配置 conf/catalina.policy 当 Tomcat在安全模式下运行时，此文件为默认的安全策略配置 conf/logging.properties Tomcat日志配置文件，可通过该文件修改 Tomcat日志级别以及日志路径等 conf/server.xml Tomcat服务器核心配置文件，用于配置 Tomcat的链接器、监听端口、处理请求的虚拟主机等。可以说，Tomcat主要根据该文件的配置信息创建服务器实例 conf/tomcat-users.xml 用于定义 Tomcat默认用户及角色映射信息，Tomcat的 Manager模块即用该文件中定义的用户进行安全认证 conf/web.xml Tomcat中所有应用默认的部署描述文件，主要定义了基础 Servlet和MIME映射。如果应用中不包含 web. xml，那么 Tomcat将使用此文件初始化部署描述，反之，Tomcat会在启动时将默认部署描述与自定义配置进行合并 lib Tomcat服务器依赖库目录，包含 Tomcat服务器运行环境依赖jar包 logs Tomcat默认的日志存放路径 webapps Tomcat默认的Web应用部署目录 work 存放Web应用JSP代码生成和编译后产生的class文件目录 temp 存放tomcat在运行过程中产生的临时文件 1.6 Tomcat启停 启动 使用startup.sh脚本启动如下： 12345678root@deeplearning:/data/tomcat/apache-tomcat-9.0.41/bin# ./startup.shUsing CATALINA_BASE: /data/tomcat/apache-tomcat-9.0.41Using CATALINA_HOME: /data/tomcat/apache-tomcat-9.0.41Using CATALINA_TMPDIR: /data/tomcat/apache-tomcat-9.0.41/tempUsing JRE_HOME: /usr/lib/jvm/java-8-openjdk-amd64Using CLASSPATH: /data/tomcat/apache-tomcat-9.0.41/bin/bootstrap.jar:/data/tomcat/apache-tomcat-9.0.41/bin/tomcat-juli.jarUsing CATALINA_OPTS: Tomcat started. tomcat的默认监听端口为8080，这时候浏览器打开网址：http://ip:8080 ,显示出tomcat的网址。 停止 使用shutdown.sh脚本停止如下： 1234567root@deeplearning:/data/tomcat/apache-tomcat-9.0.41/bin# ./shutdown.shUsing CATALINA_BASE: /data/tomcat/apache-tomcat-9.0.41Using CATALINA_HOME: /data/tomcat/apache-tomcat-9.0.41Using CATALINA_TMPDIR: /data/tomcat/apache-tomcat-9.0.41/tempUsing JRE_HOME: /usr/lib/jvm/java-8-openjdk-amd64Using CLASSPATH: /data/tomcat/apache-tomcat-9.0.41/bin/bootstrap.jar:/data/tomcat/apache-tomcat-9.0.41/bin/tomcat-juli.jarUsing CATALINA_OPTS: 1.7 Tomcat 源码在Tomcat官网可以下载源码压缩包，然后进行研读。 第二部分 Tomcat的架构2.1 Http工作原理Http协议是浏览器和服务器之间的数据传送协议，属于应用层协议。Http协议是基于TCP/IP协议来传递数据（html文件、图片、查询结构等）。Http协议不涉及数据包的传输，主要是规定客户端和服务端直接的通讯格式。 参考这篇文章：https://www.cnblogs.com/an-wen/p/11180076.html 主要过程为： 用户通过浏览器进行了一个操作，比如输入网址并回车，或者点击了链接，接着浏览器获取了这个事件。 浏览器向服务端发出TCP请求。 服务端程序接受到浏览器的连接请求，并经过了三次握手建立连接。 浏览器开始数据通信，将请求数据打包成一个http协议格式的数据包。 浏览器将数据包推入网络，数据包经过网络传输，最终到达服务端。 服务端程序拿到数据包后，使用http协议格式进行解包，获取客户端的请求。 服务端程序解析获得请求意图，比如提供静态文件等。 服务端程序将响应结果按照http协议格式进行打包。 服务器将响应结果的包推入网络，数据包经过网络传输最终到达浏览器程序。 浏览器拿到数据包后，同样以http协议格式进行解包，然后解析。 浏览器将响应内容展示在页面上。 2.2 Tomcat的整体架构2.2.1 Http服务器请求处理浏览器发给服务端的是一个http格式的请求，http服务器收到这个请求后，需要调用服务端程序来处理，所谓的服务端程序就是Java类，不同的请求需要由不同的Java类来处理。 两种方案： 方案一、Http服务直接和业务类交互 结构上紧耦合。 方案二、Servlet容器中间层 Http服务器不直接调用业务类，所有请求先发给Servlet容器。容器通过Servlet接口调用业务类，因此servlet接口和servlet容器的出现，达到了http服务器和业务类解耦的目的。servlet容器和servlet接口这一套规范称为Servlet规范。Tomcat按照这个规范要求实现了servlet容器，同时也具有http服务器的功能。作为java程序员。如果我们要实现新的业务功能，只需要实现一个servlet，并把它注册到Tomcat中，剩下的事情交给Tomcat帮助我们处理。 2.2.2 Servlet容器的工作流程为了解耦，http服务器不直接调用servlet，而是把请求交给servlet容器处理。当客户端请求资源的时候，http服务器会用一个servletRequest对象把客户的请求封装起来，然后调用servlet容器的service方法。servlet容器拿到请求后，根据请求的URL和servlet映射关系，找到相应的servlet。在这个过程中如果servlet没有被加载，就用反射机制创建这个servlet，并调用servlet的init方法来完成初始化。接着调用servlet方法来处理请求，把servletResponse对象返回给http服务器，http服务器会把相应发送到客户端。 2.2.3 Tomcat整体架构Tomcat整体架构主要是实现两个核心功能。 1、处理socket链接，负责网络字节流与Request和Response对象的转化 2、加载和管理servlet，以及具体处理Request请求。 Tomcat设计了两个核心组件：连接器（Connector）和容器（Container）来分别负责这两个功能。连接器负责对外交流，容器负责内部的处理。 2.3 连接器（Coyote）2.3.1 架构介绍2.3.2 IO模型与协议2.3.3 连接器组件2.3.4 源码解析2.4 容器2.5 Tomcat的启动流程2.6 Tomcat请求处理流程第三部分 Jasper第四部分 Tomcat服务器配置第五部分 Web应用配置第六部分 JVM配置第七部分 Tomcat集群第八部分 Tomcat 安全第九部分 Tomcat性能调优第十部分 Tomcat 附加功能参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源大数据调度平台调研报告]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-03-elasticsearch%E5%A4%B8%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Superset项目部署]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-23-Apache%20Superset%20%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[背景Apache Superset由airbnb公司 (知名在线房屋短租公司)开源的数据可视化项目。项目于 2015 年 6 月开源，活跃度极高。2021年1 月 21 日，Apache Superset项目组宣布毕业并成为 Apache 软件基金会（ASF）的顶级项目（Top-Level Project）。superset 在国内外都有着广泛的应用。superset 的国内比较知名的互联网公司有：bilibili、Douban、Kuaishou、Qunar 等等。 技术栈上，Superset 的前端主要用到了 React 和 NVD3/D3，而后端则基于 Python 的 Flask 框架和 Pandas、SQLAlchemy 等依赖库，主要提供的功能有： 集成数据查询功能，支持多种数据库，包括 MySQL、PostgresSQL、Oracle、SQL Server、SQLite、SparkSQL 等，并深度支持 Druid。 通过 NVD3/D3 预定义了多种可视化图表，满足大部分的数据展示功能。如果还有其他需求，也可以自开发更多的图表类型，或者嵌入其他的 JavaScript 图表库（如 HighCharts、ECharts）。 提供细粒度安全模型，可以在功能层面和数据层面进行访问控制。支持多种鉴权方式（如数据库、OpenID、LDAP、OAuth、REMOTE_USER 等）。 目前中文介绍项目部署的材料较少，本文将详细介绍多种部署方式。需要注意的是：Superset项目更新较快，很多命令最新版本和老版本有较大差异。 目录 背景 第一部分 传统部署安装 第二部分 基于docker部署 第三部分 基于helm在k8s集群部署 第四部分 生产高可用部署 参考文献及资料 第一部分 传统部署安装单机安装环境操作系统为：ubuntu 16.04，Python环境版本为：Python 3.7.6。 需要注意的是项目不支持在windows上运行，部署可以基于虚拟机。 1.1 基础环境准备Superset 使用Python语言编写，运行需要提前部署Python环境（要求版本大于Python 3.6）。如果具备互联网环境下，直接使用pip安装（非生产）即可： 1root@deeplearning:/data/superset# pip install apache-superset 1.2 配置应用Superset默认后台数据库使用sqllite。在生产环境中建议使用Mysql数据库（并具备高可用架构）。使用pip安装后，superset项目的Home目录通常在Python的包目录（具体环境目录存在差异）： 1/usr/anaconda3/lib/python3.7/site-packages/superset 其中目录中文件config.py是superset的配置文件(superset项目使用Flask框架编写)。通常我们关注的配置有： 应用服务配置 12345SUPERSET_WEBSERVER_PROTOCOL = "http"# 服务监听地址SUPERSET_WEBSERVER_ADDRESS = "0.0.0.0"# 服务监听端口SUPERSET_WEBSERVER_PORT = 8088 注意：使用pip安装部署的测试环境的配置在运行的时候是无效的，需要时命令行中重新指定。 后台数据库配置 1234# The SQLAlchemy connection string.SQLALCHEMY_DATABASE_URI = "sqlite:///" + os.path.join(DATA_DIR, "superset.db")# SQLALCHEMY_DATABASE_URI = 'mysql://myapp@localhost/myapp'# SQLALCHEMY_DATABASE_URI = 'postgresql://root:password@localhost/myapp' 配置中默认使用的就是sqlite。生产环境中我们调整配置为mysql： 1SQLALCHEMY_DATABASE_URI = 'postgresql://root:root@localhost/superset' 另外只要sqlalchemy支持的数据源，superset都是支持的，列举如下： | database | pypi package | SQLAlchemy URI prefix || :——— | :————————————– | :———————————————————– || MySQL | pip install mysqlclient | mysql:// || Postgres | pip install psycopg2 | postgresql+psycopg2:// || Presto | pip install pyhive | presto:// || Hive | pip install pyhive | hive:// || Oracle | pip install cx_Oracle | oracle:// || sqlite | | sqlite:// || Snowflake | pip install snowflake-sqlalchemy | snowflake:// || Redshift | pip install sqlalchemy-redshift | redshift+psycopg2:// || MSSQL | pip install pymssql | mssql:// || Impala | pip install impyla | impala:// || SparkSQL | pip install pyhive | jdbc+hive:// || Greenplum | pip install psycopg2 | postgresql+psycopg2:// || Athena | pip install &quot;PyAthenaJDBC&gt;1.0.9&quot; | awsathena+jdbc:// || Athena | pip install &quot;PyAthena&gt;1.2.0&quot; | awsathena+rest:// || Vertica | pip install sqlalchemy-vertica-python | vertica+vertica_python:// || ClickHouse | pip install sqlalchemy-clickhouse | clickhouse:// || Kylin | pip install kylinpy | kylin:// || BigQuery | pip install pybigquery | bigquery:// || Teradata | pip install sqlalchemy-teradata | teradata:// || Pinot | pip install pinotdb | pinot+http://controller:5436/ query?server=http://controller:5983/ | 其中对大数据领域常见的 Druid、ClickHouse、Kylin、Presto等OLAP数据库的支持，是最具吸引力的。 汉化 修改Setup default language，BABEL_DEFAULT_LOCALE调整为zh。 1.3 初始化应用首先初始化管理员账号密码： 12345678910111213root@deeplearning:/usr/anaconda3/lib/python3.7/site-packages/superset# superset fab create-adminlogging was configured successfullyINFO:superset.utils.logging_configurator:logging was configured successfully/usr/anaconda3/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled. "Flask-Caching: CACHE_TYPE is set to null, "Username [admin]: adminUser first name [admin]: adminUser last name [user]: adminEmail [admin@fab.org]: admin@fab.orgPassword: Repeat for confirmation: Recognized Database Authentications.Admin User admin created. 查看mysql数据库新增下面的用户表： 1234567891011121314mysql&gt; show tables;+-------------------------+| Tables_in_superset |+-------------------------+| ab_permission || ab_permission_view || ab_permission_view_role || ab_register_user || ab_role || ab_user || ab_user_role || ab_view_menu |+-------------------------+8 rows in set (0.01 sec) 继续初始化应用的表： 1root@deeplearning:/usr/anaconda3/lib/python3.7/site-packages/superset# superset db upgrade 查看数据库会新增应用依赖表。最后创建默认角色和权限： 1root@deeplearning:/usr/anaconda3/lib/python3.7/site-packages/superset# superset init 1.4 启动服务启动服务前我们加载一些案例数据： 1root@deeplearning:/usr/anaconda3/lib/python3.7/site-packages/superset# superset load_examples 注意：加载案例数据需要互联网环境，从互联网加载数据至数据库。完成上面的配置后就可以启动服务。 12# 启动命令root@deeplearning:# superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger 其中端口可以重新制定（覆盖配置文件中定义）。打开浏览器:http://localhost:8088/login/，弹出登陆界面，输入账号密码登陆。 第二部分 基于docker部署首先具备docker、docker-compose（version 1.24.1）和git环境。 12345# 拉取（如果国内拉取慢，可以下载zip包）git clone https://github.com/apache/superset.gitcd superset# you can run this command everytime you need to start superset now:docker-compose up 宿主机会启动下面的容器（一共6个）： IMAGE NAMES apache/superset:latest-dev superset_worker node:12 superset_node apache/superset:latest-dev superset_app apache/superset:latest-dev superset_init redis:3.2 superset_cache postgres:10 superset_db 使用默认用户名：admin/admin登录控制台界面：http://地址:8088/ 第三部分 基于helm在k8s集群部署首先具备helm和k8s集群环境。首先拉取Chart 加入库中： 12# helm repo rm cloudposse-incubator 2&gt;/dev/null# helm repo add cloudposse-incubator https://charts.cloudposse.com/incubator/ 然后安装： 123456789101112131415161718192021222324252627282930root@deeplearning:/data/helm# helm install cloudposse-incubator/supersetNAME: killjoy-squidLAST DEPLOYED: Sun Jan 31 16:06:02 2021NAMESPACE: defaultSTATUS: DEPLOYEDRESOURCES:==&gt; v1/DeploymentNAME READY UP-TO-DATE AVAILABLE AGEkilljoy-squid-superset 0/1 1 0 0s==&gt; v1/Pod(related)NAME READY STATUS RESTARTS AGEkilljoy-squid-superset-6587d9797f-wv5t4 0/1 ContainerCreating 0 0s==&gt; v1/SecretNAME TYPE DATA AGEkilljoy-squid-superset Opaque 2 0s==&gt; v1/ServiceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkilljoy-squid-superset ClusterIP 10.98.190.160 &lt;none&gt; 9000/TCP 0sNOTES:Superset can be accessed via port 9000 on the following DNS name from within your cluster:killjoy-squid-superset.default.svc.cluster.localInitially you can login with username/password: admin/admin.WARNING: Persistence is DISABLED ! 使用宿主机上地址，打开：http://宿主机IP:9000/login/。查看应用： 123root@deeplearning:/data/superset/superset-master/helm/superset# helm list --allNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACEkilljoy-squid 1 Sun Jan 31 16:06:02 2021 DEPLOYED superset-1.2.0 0.35.2 default 如果要删除应用： 1# helm delete killjoy-squid 第四部分 生产高可用配置以上部署均是测试部署，对于生产环境部署建议如下。 4.1 使用web服务部署目前支持web服务器有：Gunicorn，Nginx，Apache进行部署。 4.2 使用生产数据库默认后台数据库使用sqlite，生产建议使用MySQL，Postgres，MariaDB等数据库引擎，消息和缓存层使用redis。 4.3 开启负载均衡使用F5或者软件负载（Nginx）进行负载，组成web集群。 4.4 开源https和权限认证目前支持与OAuth2服务对接，并使用https协议。 第五部分 业务框架介绍5.1 整体框架superset整体使用业务框架并不复杂。主要业务概念有： 数据源。理论上sqlalchemy包支持的数据源，superset都是支持的。数据源配置中【配置读写权限。 图表（Chart）。superset 在这里定义了字段和指标（Metric）的概念。指标是对字段的某种统计结果，比如字段上值的求和、平均值、最大值、最小值等。 大屏板（Dashboard）。通过定义好的图表，组成一个大屏板。 5.2 权限介绍Superset的权限体系是通过Flask AppBuilder (FAB)完成，Flask-AppBuilder是基于Flask实现的一个用于快速构建Web后台管理系统的简单的框架。 Superset的默认角色有：Admin、Alpha、Gamma、sql_lab、Public Admin 管理员有所有的权利，其中包括授予或撤销其他用户和改变其他人的切片和仪表板的权利。 Alpha alpha可以访问所有数据源，但不能授予或撤消其他用户的访问权限，并且他们也只能修改自己的数据。alpha用户可以添加和修改数据源。 Gamma Gamma访问有限。他们只能使用他们通过另一个补充角色访问的数据源中的数据。他们只能访问查看从他们有权访问的数据源制作的切片和仪表板。目前，Gamma用户无法更改或添加数据源。我们假设他们大多是内容消费者，虽然他们可以创建切片和仪表板。 还要注意，当Gamma用户查看仪表板和切片列表视图时，他们只会看到他们有权访问的对象。 sql_lab sql_lab角色用于授予需要访问sql lab的用户，而管理员用户可以访问所有的数据库，默认情况下，Alpha和Gamma用户需要一个数据库的访问权限。 Public 允许登录用户访问一些Superset的一些功能。 另外Superset支持用户自定义创建角色，例如：您可以创建一个角色Financial Analyst，该角色将由一组数据源（表）和/或数据库组成。然后用户将被授予Gamma，Financial Analyst，或者sql_lab角色都可以。 第六部分 技术选型建议数据可视化开源项目较多，github上有个项目收集了大量项目清单，可以参考： https://github.com/thenaturalist/awesome-business-intelligence 6.1 优点 项目使用Python语言研发，用户具备技术栈后可以快速二次开发。社区较为活跃，项目演进较快。 可视化功能选项较为丰富。 6.2 缺点 权限管理。开源项目引入需要改造和内部权限打通。图表和Dashboard没有引入文件夹或者分组的理念，只有检索功能。权限系统特别复杂， 权限体系小规模使用还算方便，大规模使用需要很高的配置和运维成本。 使用Python语言，依赖环境较为复杂，传统环境（非云）部署较为复杂。建议基于云原生部署。 目前只支持每次可视化一张表，对于多表join的情况还无能为力。 依赖于数据库的快速响应，如果数据库本身太慢Superset也没什么办法 6.3 总结对于静态的日报、报表等业务需求，选择superset较好。整体定位上，Superset属于轻量级的BI项目，对于较为复杂的数据关联等逻辑应该在ETL过程中完成，Superset只是读取可视化结果表。 参考文献及资料1、Apache Superset项目代码托管地址，链接：https://github.com/apache/superset 2、pip源地址，链接：https://pypi.org/project/superset/ 3、在线文档，链接：https://apache-superset.readthedocs.io/en/0.36/installation.html 4、docker-superset，链接：https://abhioncbr.github.io/docker-superset/ 5、superset helm库，链接：https://artifacthub.io/packages/helm/cloudposse/superset]]></content>
      <categories>
        <category>Superset</category>
      </categories>
      <tags>
        <tag>Superset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源大数据调度平台调研报告]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-03-%E5%BC%80%E6%BA%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 https://mp.weixin.qq.com/s/2Pou8FoHVLuE4UR23wGM6w https://cloud.tencent.com/developer/article/1748204 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase使用总结]]></title>
    <url>%2F2020%2F12%2F30%2F2020-12-11-Hbase%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景HBase是一个高可靠性、高性能、可伸缩的分布式存储系统：• HBase是一个构建在HDFS上的分布式NoSQL数据库；• HBase是基于Google BigTable模型开发的，典型的key/value系统；• 具有松散的表结构；原生海量数据分布式存储；支持随机查询、范围查询• 高吞吐，低延迟；• 列存储，多版本，增量导入，多维删除 • HDFS Vs HBase：• Hadoop是一个高容错、高延时的分布式文件系统和高并发的批处理系统，不适用于提供实时计算；• HBase是可以提供实时计算的分布式数据库，数据被保存在HDFS分布式文件系统上，由HDFS保证其高容错性 HDFS Vs HBase：• HBase上的数据是以StoreFile(HFile)二进制流的形式存储在HDFS上block块儿中• HDFS并不知道的hbase存的是什么，它只把存储文件视为二进制文件，也就是说，hbase的存储数据对于HDFS文件系统是透明的 HBase特性：• 大：一个表可以有数十亿行，上百万列；• 无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列；• 面向列：面向列（族）的存储和权限控制，列（族）独立检索；• 稀疏、多维、排序的map：空（null）列并不占用存储空间，表可以设计的非常稀疏；每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；• 数据类型单一：Hbase中的数据都是字符串，没有类型。• 强一致性读写:HBase 不是“最终一致性”数据存储。这让它很适合高速计数聚合类任务；• 自动分片(Automatic sharding)： HBase 表通过 region 分布在集群中。数据增长时，region会自动分割并重新分布；• RegionServer 自动故障转移和负载均衡；• Hadoop/HDFS 集成：HBase 支持开箱即用地支持 HDFS 作为它的分布式文件系统；• MapReduce： HBase 通过 MapReduce 支持大并发处理；• 实时、随机地大数据访问；HBase内部使用LSM-tree(log-structured merge-tree)作为数据存储架构，LSM-tree周期性地合并小文件到较大的文件，以减少硬盘寻址• Java 客户端 API：HBase 支持易于使用的 Java API 进行编程访问；• Thrift/REST API：HBase 也支持 Thrift 和 REST 作为非 Java 前端的访问；• Block Cache 和 Bloom Filter：对于大容量查询优化， HBase 支持 Block Cache 和 Bloom Filter• 快照支持 面向列的数据存储 Vs 面向行的数据存储：HBase的逻辑视图面向行的数据存储面向列的数据存储对于记录的增加/修改效率较高对于读取数据效率较高读取包含整个行的页面只需要读取列最适合用于OLTP 对于OLTP还没有优化将一行中所有的值一起序列化，然后是下一行的值 ，等等将列中的值一起序列化，依次类推行数据存储在内存或磁盘中的连续页面中列以页面的列式存储在内存或磁盘中 参考文献及资料1、 Apache Spark support，链接]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark Mllib模块的学习总结]]></title>
    <url>%2F2020%2F12%2F12%2F2020-12-19-Spark%20Mllib%E6%A8%A1%E5%9D%97%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 alias配置和取消 第二部分 alias查看 参考文献及资料 背景MLlib 是 Spark 的机器学习库，旨在简化机器学习的工程实践工作，并方便扩展到更大规模。 MLlib 由一些通用的学习算法和工具组成，包括分类、回归、聚类、协同过滤、降维等，同时还包括底层的优化原语和高层的管道 API。 本节将对 Spark MLlib 进行简单介绍，在介绍数据挖掘算法时，将使用 Spark MLlib 提供的算法进行实例讲解。 Spark MLlib的构成Spark 是基于内存计算的，天然适应于数据挖掘的迭代式计算，但是对于普通开发者来说，实现分布式的数据挖掘算法仍然具有极大的挑战性。因此，Spark 提供了一个基于海量数据的机器学习库 MLlib，它提供了常用数据挖掘算法的分布式实现功能。 开发者只需要有 Spark 基础并且了解数据挖掘算法的原理，以及算法参数的含义，就可以通过调用相应的算法的 API 来实现基于海量数据的挖掘过程。 MLlib 由 4 部分组成：数据类型，数学统计计算库，算法评测和机器学习算法。 名称 说明 数据类型 向量、带类别的向量、矩阵等 数学统计计算库 基本统计量、相关分析、随机数产生器、假设检验等 算法评测 AUC、准确率、召回率、F-Measure 等 机器学习算法 分类算法、回归算法、聚类算法、协同过滤等 具体来讲，分类算法和回归算法包括逻辑回归、SVM、朴素贝叶斯、决策树和随机森林等算法。用于聚类算法包括 k-means 和 LDA 算法。协同过滤算法包括交替最小二乘法（ALS）算法。 Spark 机器学习库从 1.2 版本以后被分为两个包： spark.mllib包含基于RDD的原始算法API。Spark MLlib 历史比较长，在1.0 以前的版本即已经包含了，提供的算法实现都是基于原始的 RDD。 spark.ml 则提供了基于DataFrames 高层次的API，可以用来构建机器学习工作流（PipeLine）。ML Pipeline 弥补了原始 MLlib 库的不足，向用户提供了一个基于 DataFrame 的机器学习工作流式 API 套件。 使用 ML Pipeline API可以很方便的把数据处理，特征转换，正则化，以及多个机器学习算法联合起来，构建一个单一完整的机器学习流水线。这种方式给我们提供了更灵活的方法，更符合机器学习过程的特点，也更容易从其他语言迁移。Spark官方推荐使用spark.ml。如果新的算法能够适用于机器学习管道的概念，就应该将其放到spark.ml包中，如：特征提取器和转换器。开发者需要注意的是，从Spark2.0开始，基于RDD的API进入维护模式（即不增加任何新的特性），并预期于3.0版本的时候被移除出MLLib。 Spark在机器学习方面的发展非常快，目前已经支持了主流的统计和机器学习算法。纵观所有基于分布式架构的开源机器学习库，MLlib可以算是计算效率最高的。MLlib目前支持4种常见的机器学习问题: 分类、回归、聚类和协同过滤。下表列出了目前MLlib支持的主要的机器学习算法： Spark MLlib 的优势相比于基于 Hadoop MapReduce 实现的机器学习算法（如 Hadoop Manhout），Spark MLlib 在机器学习方面具有一些得天独厚的优势。 首先，机器学习算法一般都有由多个步骤组成迭代计算的过程，机器学习的计算需要在多次迭代后获得足够小的误差或者足够收敛时才会停止。如果迭代时使用 Hadoop MapReduce 计算框架，则每次计算都要读/写磁盘及完成任务的启动等工作，从而会导致非常大的 I/O 和 CPU 消耗。 而 Spark 基于内存的计算模型就是针对迭代计算而设计的，多个迭代直接在内存中完成，只有在必要时才会操作磁盘和网络，所以说，Spark MLlib 正是机器学习的理想的平台。其次，Spark 具有出色而高效的 Akka 和 Netty 通信系统，通信效率高于 Hadoop MapReduce 计算框架的通信机制。 在 Spark 官方首页中展示了 Logistic Regression 算法在 Spark 和 Hadoop 中运行的性能比较，可以看出 Spark 比 Hadoop 要快 100 倍以上。 MLlib(Machine Learnig lib) 是Spark对常用的机器学习算法的实现库，同时包括相关的测试和数据生成器。Spark的设计初衷就是为了支持一些迭代的Job, 这正好符合很多机器学习算法的特点。在Spark官方首页中展示了Logistic Regression算法在Spark和Hadoop中运行的性能比较，如图下图所示。 第四部分 参数调优参考文献及资料1、Reindex from a remote cluster，链接：https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[nohup命令的日志重定向总结]]></title>
    <url>%2F2020%2F12%2F12%2F2020-12-19-nohup%E5%91%BD%E4%BB%A4%E7%9A%84%E6%97%A5%E5%BF%97%E9%87%8D%E5%AE%9A%E5%90%91%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 alias配置和取消 第二部分 alias查看 参考文献及资料 背景第一部分 Linux shell的输入和输出一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件： 标准输入文件(stdin)：stdin的文件描述符为0，默认从stdin读取数据。硬件为键盘。 标准输出文件(stdout)：stdout 的文件描述符为1，默认向stdout输出数据。硬件为屏幕。 标准错误文件(stderr)：stderr的文件描述符为2，会向stderr流中写入错误信息。硬件为屏幕。 Linux 程序在执行任何形式的 I/O 操作时，都是在读取或者写入一个文件描述符。一个文件描述符只是一个和打开的文件相关联的整数，它的背后可能是一个硬盘上的普通文件、FIFO、管道、终端、键盘、显示器，甚至是一个网络连接。 第二部分 重定向重定向的使用有如下规律： 1）标准输入0、输出1、错误2需要分别重定向，一个重定向只能改变它们中的一个。 2）标准输入0和标准输出1可以省略。（当其出现重定向符号左侧时） 3）文件描述符在重定向符号左侧时直接写即可，在右侧时前面加&amp;。 4）文件描述符与重定向符号之间不能有空格！ 2.1 输入重定向 标准输入重定向至文件： 12command &lt; filenamecommand 0&lt; filename 2.2 输出重定向 标准输出重定向至文件(覆盖)： 12command &gt; filenamecommand 1&gt; filename 标准输出重定向至文件(文件尾追加)： 12command &gt;&gt; filenamecommand 1&gt;&gt; filename 错误输出重定向至文件 12command 2&gt; filenamecommand 2&gt;&gt; filename dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到”禁止输出”的效果。 参考文献及资料1、Reindex from a remote cluster，链接：https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统中命令别名设置]]></title>
    <url>%2F2020%2F12%2F12%2F2020-12-12-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%91%BD%E4%BB%A4%E5%88%AB%E5%90%8D%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 alias配置和取消 第二部分 alias查看 参考文献及资料 背景linux中提供用户自定义命名别名，即alias命令。 第一部分 alias配置和取消1.1 配置 临时设置 使用下面的命令格式： 1alias 新的命令='原命令 -选项/参数' 例如下面的例子中，我们将命令cd ..简化成..，方便使用。 1alias ..='cd ..' ​ 但是这种设置是临时在当前shell中生效了，重启开启新的shell就会失效。 永久生效 如果需要永久生效就需要将设置配置在环境变量中。需要注意的是环境变量有效范围（用户环境和系统环境变量）。例如系统环境变量中，在/etc/profile中追加： 12alias rm='rm –i'source /etc/profile 所有用户shell均具有该命令别名。 1.2 取消如果需要取消命令别名，可以使用下面的命令： 1unalias rm='rm –i' 对于配置在环境变量中的就需要手动注释，并source生效（当前shell）。 第二部分 alias查看如果需要参看当前shell环境已经配置的命令别名，可以直接使用命令： 12345678root@VM-0-5-ubuntu:~# aliasalias egrep='egrep --color=auto'alias fgrep='fgrep --color=auto'alias grep='grep --color=auto'alias l='ls -CF'alias la='ls -A'alias ll='ls -alF'alias ls='ls --color=auto' 上面是ubuntu系统自带的命令别名。另外比较常用的还有： 123alias ..='cd ..'alias ...='cd ../..'alias ....='cd ../../../' 日常操作中目录的进退是常用了，上面的命令别名大大提高了输入效率。 参考文献及资料1、Using alias Command in Linux to Improve Your Efficiency，链接：https://linuxhandbook.com/linux-alias-command/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Zeppelin安装部署介绍]]></title>
    <url>%2F2020%2F12%2F12%2F2020-12-19-Zeppelin%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 alias配置和取消 第二部分 alias查看 参考文献及资料 背景参考文献及资料1、Reindex from a remote cluster，链接：https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html]]></content>
      <categories>
        <category>Zeppelin</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Anaconda相关信息汇总]]></title>
    <url>%2F2020%2F12%2F12%2F2020-12-13-Anaconda%E7%9B%B8%E5%85%B3%E4%BF%A1%E6%81%AF%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[目录 背景 参考文献及资料 背景登记Anaconda相关的下载信息，备用查阅。 anaconda 镜像下载地址 https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=N&amp;O=D https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/?C=N&amp;O=D 官方首页 https://www.anaconda.com/distribution/ https://docs.conda.io/en/latest/miniconda.html 官方下载地址 https://repo.anaconda.com/archive/ https://repo.anaconda.com/miniconda/ 官方文档 https://docs.anaconda.com/anaconda/ old package lists https://docs.anaconda.com/anaconda/packages/oldpkglists/ release notes https://docs.anaconda.com/anaconda/reference/release-notes/ 使用tips1、消除环境的base在命令提示符的开头多了一个(base)，可以在终端中运行如下命令，消除这个(base)： 1conda config --set auto_activate_base false 参考文献及资料1、官网文档，链接：https://docs.anaconda.com/]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用Reindex实现Elasticsearch数据迁移]]></title>
    <url>%2F2020%2F12%2F12%2F2020-12-13-%E4%BD%BF%E7%94%A8Reindex%E5%AE%9E%E7%8E%B0Elasticsearch%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 alias配置和取消 第二部分 alias查看 参考文献及资料 背景在实际生产环境中，需要对线上Elasticsearch集群进行置换。业务上需要集群对外服务不中断的前提上，将原集群的数据迁移至新集群。目前迁移有多种方案，主要有：elasticsearch-dump、logstash、reindex、snapshot等方式。 elasticsearch-dump logstash snapshot reindex 基本原理 逻辑备份，类似mysqldump将数据一条一条导出后再执行导入 从一个 ES 集群中读取数据然后写入到另一个 ES 集群 从源 ES 集群通过备份api创建数据快照，然后在目标 ES 集群中进行恢复 reindex是Elasticsearch提供的一个api接口，可以把数据从一个集群迁移到另外一个集群 网络要求 网络需要互通 网络需要互通 无网络互通要求 网络需要互通 迁移速度 慢 一般 快 一般 运维配置复杂度 复杂，索引的分片数量和副本数量需要对每个索引单独进行迁移，或者直接在目标集群提前将索引创建完成，再迁移数据 复杂，需要提前在目标集群创建mapping和setting等，再迁移数据 简单 需要在目标ES集群中配置reindex.remote.whitelist参数，指明能够reindex的远程集群的白名单 适合场景 适用于数据量小的场景 适用于数据量一般，近实时数据传输 适用于数据量大，接受离线数据迁移的场景 本地索引更新Mapping实现索引层面迁移，或者跨集群的索引迁移 scroll query + bulk: 批量读取旧集群的数据然后再批量写入新集群，elasticsearch-dump、logstash、reindex都是采用这种方式 snapshot: 直接把旧集群的底层的文件进行备份，在新的集群中恢复出来，相比较scroll query + bulk的方式，snapshot的方式迁移速度最快。 https://cloud.tencent.com/developer/article/1611786 第一部分 Reindex接口介绍reindex 是 ES 提供的一个 api 接口，可以把数据从源 ES 集群导入到当前 ES 集群，实现集群内部或跨集群同步数据。 但仅限于腾讯云 ES 的实现方式（跨集群迁移需要elasticsearch.yml中加上ip白名单，并重启集群），所以腾讯云ES不支持 reindex 操作。具体见官方文档说明：https://www.elastic.co/guide/en/elasticsearch/reference/7.3/reindex-upgrade-remote.html 第三部分 Reindex迁移方案下面简单介绍 reindex 接口的使用方法： 1) 配置 elasticsearch.yml中的reindex.remote.whitelist 参数 需要在目标 ES 集群中配置该参数，指明能够 reindex 的远程集群的白名单。 2) 调用 reindex api 以下操作表示从源 ES 集群中查询名为 test1 的索引，查询条件为 title 字段为 elasticsearch，将结果写入当前集群的 test2 索引。 1234567891011121314151617POST _reindex &#123; "source": &#123; "remote": &#123; "host": "http://172.16.0.39:9200" &#125;, "index": "test1", "query": &#123; "match": &#123; "title": "elasticsearch" &#125; &#125; &#125;, "dest": &#123; "index": "test2" &#125; &#125; 从源索引中提取文档源，并将文档索引到目标索引中。可以将所有文档复制到目标索引，或为文档的子集重新索引。_reindex获取源索引的快照，但是其目标必须是其他索引，因此不会发生版本冲突。 第四部分 参数调优https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html https://www.cnblogs.com/Ace-suiyuan008/p/9985249.html https://elkguide.elasticsearch.cn/elasticsearch/api/reindex.html 参考文献及资料1、Reindex from a remote cluster，链接：https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何将文件数据导入hive表中]]></title>
    <url>%2F2020%2F11%2F23%2F2020-12-23-%E5%A6%82%E4%BD%95%E5%B0%86%E6%96%87%E4%BB%B6%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5hive%E8%A1%A8%E4%B8%AD%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景使用head命令看一下。 第一步 创建库和hive表使用下面的命令进入hive shell交互模式。 1root@hadoop01:/opt/hive/bin/#hive 创建库： 1CREATE database cda; 创建表： 12345678910111213CREATE TABLE IF NOT EXISTS cda.users (user_id string,item_id string,cat_id string,merchant_id string,brand_id string,month string,day string,action string,age_range string,gender string,province string )COMMENT 'user_log.csv Table' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' tblproperties("skip.header.line.count"="1"); 如果需要分区： 1234567891011CREATE TABLE IF NOT EXISTS cda.users (user_id string,item_id string,cat_id string,merchant_id string,brand_id string,action string,age_range string,gender string,province string )PARTITIONED BY(month string,day string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' tblproperties("skip.header.line.count"="1"); 第二部分 准备数据和导入将数据上传hdfs： 1root@hadoop03:/opt/data# hdfs dfs -put user_log.csv /data/user_log.csv 导入数据： 1LOAD DATA INPATH 'hdfs:///data/user_log.csv' INTO TABLE cda.users; 第三部分 查询1234567891011root@hadoop01:/opt/hive/bin# hiveSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/opt/hbase-1.4.13/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-2.3.7.jar!/hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.hive&gt; select * from cda.users limit 10; 回显数据，完成导入。 参考文献及资料1、 hive-load-csv-file-into-table，链接:https://sparkbyexamples.com/apache-hive/hive-load-csv-file-into-table/]]></content>
      <categories>
        <category>hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用Pyspark进行机器学习]]></title>
    <url>%2F2020%2F11%2F23%2F2020-12-24-Pyspark%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E4%BD%BF%E7%94%A8Pyspark%E8%BF%9B%E8%A1%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景第一部分 环境准备配置和启动jupyter notebook。 1.1 生成配置12root@hadoop01:/opt# jupyter notebook --generate-configWriting default config to: /root/.jupyter/jupyter_notebook_config.py 1.2 配置文件123456789101112131415161718192021root@hadoop01:/opt# vi /root/anaconda3/share/jupyter/kernels/python3/kernel.json&#123; "argv": [ "/root/anaconda3/bin/python", "-m", "ipykernel_launcher", "-f", "&#123;connection_file&#125;" ], "display_name": "Python 3", "language": "python","env": &#123;"SPARK_HOME": "/opt/spark-2.3.2/","PYSPARK_PYTHON": "/root/anaconda3/bin/python","PYSPARK_DRIVER_PYTHON": "ipython3", "PYTHONPATH": "/opt/spark-2.3.2/python/:/opt/spark-2.3.2/python/lib/py4j-0.10.7-src.zip","PYTHONSTARTUP": "/opt/spark-2.3.2/python/pyspark/shell.py","PYSPARK_SUBMIT_ARGS": "--name pyspark --master local pyspark-shell"&#125;&#125; 1.3 启动：1nohup jupyter notebook --ip=0.0.0.0 --no-browser --allow-root --notebook-dir=/opt/jupyter &amp; 1.4 配置密钥123# jupyter notebook passwordEnter password: Verify password: 1.5 验证123import pysparkfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate() 第一部分 数据的读取通常数据存储在csv文件中，需要使用pyspark进行读取。 1df = spark.read.option("header",True).csv("/data/test.csv") 如果是从hdfs文件系统读取： 1df = spark.read.options(header='True', delimiter=',').csv("hdfs://hadoop01:9000/data/test.csv") 可以使用show方法进行查看： 1df.show() 第二部分 数据清洗数据的清理，去除含有空字段的记录。 12345from pyspark.sql.functions import isnull, when, count, col# |user_id|age_range|gender|merchant_id|label|# 去除字段为空的异常数据df = df.filter(df.label.isNotNull()&amp;df.merchant_id.isNotNull()&amp;df.gender.isNotNull()&amp;df.age_range.isNotNull()&amp;df.user_id.isNotNull()) 另外还有些时候我们需要替换指定条件的。比如替换空值。 123# 使用数字0替换null字段df = df.na.fill(value=0).show()df = df.fillna(value=0,subset=["population"]).show() 对于一些不需要的列进行去除。 123# 去除列df = df.drop('user_id')df.show() 列值转换。 12345678910111213141516171819def encode_columns(df, col_list): indexers = [ StringIndexer(inputCol=c, outputCol=f'&#123;c&#125;_indexed').setHandleInvalid("keep") for c in col_list ] encoder = OneHotEncoderEstimator( inputCols = [indexer.getOutputCol()) for index in indexers]) #.setDropLast(False) newColumns = [] for f in col_list: colMap = df.select(f'&#123;f&#125;', f'&#123;f&#125;_indexed').distinct().rdd.collectAsMap() colTuple = sorted( (v, f'&#123;f&#125;_&#123;k&#125;') for k,v in colMap.items()) newColumns.append(v[1] for v in colTuple) pipeline = Pipeline(stages =indexers + [encoder]) piped_encoder = pipeline.fit(df) encoded_df = piped_encoder.transfrom(df) return piped_encoder, encoded_df, newColumnsdf = encode_columns(df, ['user_id']) 特征数据向量化。 12345678# Assemble all the features with VectorAssemblerfrom pyspark.ml.feature import VectorAssemblerrequired_features = ['age_range','gender','merchant_id']assembler = VectorAssembler(inputCols=required_features, outputCol='features')df = assembler.transform(df)df.show() 切分数据集： 1234567#Decide on the split between training and testing data from the dataframetrainingFraction = 0.7testingFraction = (1-trainingFraction)seed = 1234# Split the dataframe into test and training dataframestraining_data, test_data = df.randomSplit([trainingFraction, testingFraction], seed=seed) 第三部分 模型训练1234567from pyspark.ml.classification import RandomForestClassifierrf = RandomForestClassifier(labelCol='label', featuresCol='features', maxDepth=5)model = rf.fit(training_data)rf_predictions = model.transform(test_data) 第四部分 模型预测评分1234from pyspark.ml.evaluation import MulticlassClassificationEvaluatormulti_evaluator = MulticlassClassificationEvaluator(labelCol = 'label', metricName = 'accuracy')print('Random Forest classifier Accuracy:', multi_evaluator.evaluate(rf_predictions)) 第五部分 模型应用数据准备： 12df.write.option("header",True).csv("/data/output.csv")df.write.option("header",True).csv("hdfs://hadoop01:9000/data/output") 参考文献及资料1、 案例，链接：https://hackernoon.com/building-a-machine-learning-model-with-pyspark-a-step-by-step-guide-1z2d3ycd 2、案例，链接：https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-machine-learning-mllib-notebook 3、pyspark介绍，链接：https://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/]]></content>
      <categories>
        <category>pyspark</category>
      </categories>
      <tags>
        <tag>pyspark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yarn上长任务报Token失效问题总结(Invalid AMRMToken)]]></title>
    <url>%2F2020%2F11%2F12%2F2020-11-28-ubuntu%E6%90%AD%E5%BB%BAhadoop%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景https://cloud.tencent.com/developer/article/1350441 参考文献及资料1、 Apache Spark support，链接]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Yarn上长任务报Token失效问题总结(Invalid AMRMToken)]]></title>
    <url>%2F2020%2F11%2F12%2F2020-12-11-%E5%A6%82%E4%BD%95%E4%BB%8Ehive%E8%A1%A8%E4%B8%AD%E5%88%A0%E9%99%A4%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景\4. hive 从非分区表插入数据到分区表时出错： Cannot insert into target table because column number/types are different ‘’分区名’’: Table insclause-0 has 2 columns, but query has 3 columns. 首先解释一下这个错误：因为hive的分区列是作为元信息存放在mysql中的，他们并不在数据文件中，相反他们以子目录的名字被使用，因此你的分区表实际含有的数据列，注意是数据列是不包含分区列的，所以在你向分区表插入数据时，不能插入分区列； 举个简单的例子：体育课上站队时，老师经常让男生、女生各站一队，你觉得有必要再给他们每一个人加上一个性别的标签吗？ 下面是stackoverflow上大神关于这个问题的解释： *\*In Hive the partitioning “columns” are managed as** **metadata** **&gt;&gt; they are not included in the data files, instead they are used as sub-directory names. So your partitioned table has just 2 real columns, and you must feed just 2 columns with your SELECT.**** 比如你的非分区表non_part的内容如下： 123id name sex1 tom M2 mary F 假如你的分区表part是以sex来分区的，当你想把以上非分区表中的数据插入到分区表中： 你应该：insert into table part partition(sex=’M’) select id,name from non_part where sex=’M’;（对，你要插入的就是两列，分区列不作为数据保存在数据表中） \而不是\*：****insert into table part partition(sex=’M’) select \ from non_part where sex=’M’;**** 另外需要注意的地方： 1.从非分区表插入数据到分区表时hive会将HiveQL转换为MR来执行的,官方提示Hive-on-MR在将来可能不再被支持； 2.只要是向分区表内装数据，无论是load还是insert都要在表名后指明分区名；而且load时，会将你要load的文件内的所有内容放在指定的分区下； https://www.cnblogs.com/lemonu/p/11279979.html 参考文献及资料1、 Apache Spark support，链接]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS Federation架构介绍]]></title>
    <url>%2F2020%2F11%2F12%2F2020-11-28-HDFS%20Federation%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景http://dongxicheng.org/mapreduce/hdfs-federation-introduction/ 参考文献及资料1、 Apache Spark support，链接：http://dongxicheng.org/mapreduce/hdfs-federation-introduction/]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Unix和Windows中文本行末结束符]]></title>
    <url>%2F2020%2F11%2F12%2F2020-11-12-Unix%E5%92%8CWindows%E4%B8%AD%E6%96%87%E6%9C%AC%E8%A1%8C%E6%9C%AB%E7%BB%93%E6%9D%9F%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 回车和换行 第二部分 兼容性问题解决 参考文献及资料 背景或许你遇到过这样的坑。当你信心满满将自己编写的程序文件或配置文件上传到生产环境（linux），却发现无法运行或者生效。但是明明在本地（Windows）测试运行没有问题。那么很大几率遇到文本行末结束符的坑。 Unix和Windows中文本行末结束符是不同的。本文将详细讲解这个坑的背景、产生原因和解决办法。 第一部分 回车和换行1.1 历史背景关于回车（Carriage Return）和换行（Line Feed）的来历，需要从英文打字机讲起。在机械英文打字机中有个叫“字车”的部件，每打印一个英文字符，“字车”前进一位。但是纸张每行是有限制的，打满一行后，“字车”需要重置回到起始位置。从机械角度看，打印机有两个响应动作：（1）“字车”归位；（2）纸张的滚筒上卷一行，开始新的一行。这里“字车”归位就是“回车”，而滚筒上卷一行就是“换行”。 到了电传打印机时候，需要使用控制字符来通知打印机执行非打印操作的指令（回车（CR）和换行（NL））。而在ASCII码中分别使用\r（值13）和\n（值10）表示。 再后来计算机发明后，两个概念也就被搬到计算机中（计算机通常需要和打印机交互，保持兼容性）。考虑到当时存储资源的昂贵，就有人提出来文本中使用两个操作符表示行末结束较为浪费，于是分歧就产生了。 1.2 分歧目前主流操作系统（Unix、Windows、Mac）中分歧如下： 操作系统 系统行末结束符 备注 UNIX \n window \n\r MAC OS \n v9 之前 Mac OS 用 ‘\r’ 注：从2001年3月发布的Mac OS 10.0开始，系统行末结束符采用”\n”。 这种分歧就导致不同操作系统之间兼容问题。 第二部分 兼容性问题解决这种兼容问题通常发生在：不同操作系统之间传输纯文本文件。 Unix/Mac系统创建的文件在Windows里打开，文字会变成一行。因为没有\r。 Windows里的文件在Unix/Mac下打开的话，在每行会多出一个^M符号。多了\r。 2.1 Windows文件上传Linux问题我们经常遇到的问题是：Windows下编写的Shell脚本或者Python脚本，放到Linux下执行会出错。通常上传文件前，使用UE（Ultraedit）或者Nodepad++来转换。 UE中，执行“File-&gt;conversions-&gt;Dos to Unix”，将文件中\n\r转换成\n。 Nodepad++中，执行“编辑-&gt;档案格式转换-&gt;转换为UNIX格式”。 上传到Linux后还可以使用下面的命令查看是否准确： 1cat -v test.txt 如果每行换行处都有^M，这说明仍然是Windows下的文本文件。 注：cat -A命令：显示不可见字符。如换行符显示为“$”，TAB 显示为^I等。在这种模式下，回车（\r）字符将显示为^M 可以使用下面的命令进行统一替换： 1sed -i 's/^M$//g' test.txt 另外还有专用命令dos2unix(如果操作系统没有改命令需要安装一下)。 1dos2unix test.txt 最后还可以使用vi打开文件，输入:set fileformat=unix，回车。最后保存退出即可。 2.2 Linux文件上传Windows问题在Windows中使用Ultraedit或者Nodepad++文本编辑器查看Linux文件，而不是系统自带的记事本。 2.3 FTP中传输问题FTP软件在传输文件的时候，通常有两种模式：文本模式（ASCII模式）和二进制模式（BINARY模式）。两种模式的区别就是行末结束符的处理，BINARY模式不会对数据进行任何处理。而ASCII模式将行末结束符转换为本机操作系统的行末结束符。例如Windows系统将文本文件上传至Linux，就会将\r\n替换成\n。 在使用过程中需要注意两种模式的差别。特别的上实际生产环境上线投产过程中建议统一使用二进制模式，避免FTP对文件进行转换。 2.4 编程语言中 Python 使用 “Universal Newline“ 处理这个问题。文本使用 open() 方法打开时，会对行末结束符进行识别并一致处理成 ‘\n’，在文件写入的时候，使用 write(‘\n’) 即可，Python 会根据当前程序执行的操作系统自动处理。 Java中行末结束符使用下面的函数方法统一处理： 1System.getProperty("line.separator") 参考文献及资料1、Line_feed，链接：https://nl.wikipedia.org/wiki/Line_feed]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Yarn上长任务报Token失效问题总结(Invalid AMRMToken)]]></title>
    <url>%2F2020%2F11%2F12%2F2020-11-28-Yarn%E4%B8%8A%E9%95%BF%E4%BB%BB%E5%8A%A1%E6%8A%A5Token%E5%A4%B1%E6%95%88%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93(Invalid%20AMRMToken)%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景https://github.com/marsishandsome/marsishandsome.github.io/blob/master/slides/HadoopSecurity/Index.md https://www.shuzhiduo.com/A/xl5600X0zr/ http://flume.cn/2016/11/24/Spark%E8%B8%A9%E5%9D%91%E4%B9%8BStreaming%E5%9C%A8Kerberos%E7%9A%84hadoop%E4%B8%ADrenew%E5%A4%B1%E8%B4%A5/ http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/ https://www.shuzhiduo.com/A/xl5600X0zr/ 参考文献及资料1、 Apache Spark support，链接]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hive的分区和分桶总结]]></title>
    <url>%2F2020%2F11%2F05%2F2020-11-05-Hive%E7%9A%84%E5%88%86%E5%8C%BA%E5%92%8C%E5%88%86%E6%A1%B6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景https://data-flair.training/blogs/hive-partitioning-vs-bucketing/ https://blog.csdn.net/whdxjbw/article/details/82219022 分区和分桶参考文献及资料1、]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark和oracle交互总结]]></title>
    <url>%2F2020%2F11%2F05%2F2020-11-05-Spark%E5%92%8Coracle%E4%BA%A4%E4%BA%92%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景https://issues.apache.org/jira/browse/SPARK-10909 Spark SPARK-10909 Spark sql jdbc fails for Oracle NUMBER type columns https://blog.csdn.net/cuichunchi/article/details/107838633 https://stackoverflow.com/questions/34067124/the-java-lang-illegalargumentexception-requirement-failed-overflowed-precisio 参考文献及资料1、 Apache Spark support，链接：https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html 2、elasticsearch-hadoop项目，链接：https://github.com/elastic/elasticsearch-hadoop]]></content>
      <categories>
        <category>Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[云服务器部署WordPress介绍]]></title>
    <url>%2F2020%2F10%2F31%2F2020-10-31-%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2WordPress%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景第一部分 环境部署笔者购买了腾讯云的云主机，操作系统环境为：Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-88-generic x86_64)。主机具有互联网环境，所以依赖组件的安装将依赖在线安装。安装用户使用root用户。首先更新操作系统： 1apt-get update 1.1 部署Apache环境使用apt安装： 1apt install apache2 使用下面的命令检查进程状态： 123456789101112131415root@VM-0-5-ubuntu:/apphome# systemctl status apache2● apache2.service - The Apache HTTP Server Loaded: loaded (/lib/systemd/system/apache2.service; enabled; vendor preset: enabled) Drop-In: /lib/systemd/system/apache2.service.d └─apache2-systemd.conf Active: active (running) since Sun 2020-11-01 12:26:24 CST; 1min 9s ago Main PID: 704 (apache2) Tasks: 55 (limit: 2122) CGroup: /system.slice/apache2.service ├─704 /usr/sbin/apache2 -k start ├─706 /usr/sbin/apache2 -k start └─707 /usr/sbin/apache2 -k startNov 01 12:26:23 VM-0-5-ubuntu systemd[1]: Starting The Apache HTTP Server...Nov 01 12:26:24 VM-0-5-ubuntu systemd[1]: Started The Apache HTTP Server. 浏览器地址栏： 1http://主机公网IP/ 可以看到默认的Apache 欢迎页面。 1.2 部署Mysql环境1.3 部署PHP环境第二部分 部署和配置WordPresshttps://www.cnblogs.com/jiangfeilong/p/11142181.html https://cloud.tencent.com/developer/article/1627432#:~:text=%20%E5%A6%82%E4%BD%95%E5%9C%A8%20Ubuntu%2020.04%20%E4%B8%8A%E5%AE%89%E8%A3%85%20Apache%20%201,5%20%E4%BA%94%E3%80%81%E8%AE%BE%E7%BD%AE%E4%B8%80%206%20%E8%99%9A%E6%8B%9F%E4%B8%BB%E6%9C%BA%207%20%E5%85%AD%E3%80%81%E6%80%BB%E7%BB%93%20More%20 参考文献及资料1、 Apache Spark support，链接：https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html]]></content>
      <categories>
        <category>WordPress</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hive的Metastore介绍]]></title>
    <url>%2F2020%2F10%2F27%2F2020-10-27-Hive%E7%9A%84Metastore%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景Hive Metastore，也称为HCatalog，是一个关系数据库存储库，其中包含有关您在Hive中创建的对象的元数据。创建Hive表时，表定义（列名，数据类型，注释等）存储在Hive Metastore中。这是自动的，只是Hive架构的一部分。Hive Metastore之所以如此重要，是因为它充当中央架构存储库，可供其他访问工具（如Spark和Pig）使用。此外，通过Hiveserver2，您可以使用ODBC和JDBC连接访问Hive Metastore。这将为可视化工具（如PowerBi或Tableau）打开架构。 https://www.infoq.cn/article/uM7TSwszJlsvv7veixga https://bbs.huaweicloud.com/forum/viewthreaduni-66881-filter-reply-orderby-lastpost-page-7-1.html https://blog.csdn.net/lalaguozhe/article/details/9070203 https://www.infoq.cn/article/lXJisUVTgOjgHzRMSIBW https://www.codeobj.com/2019/01/hive-metastore%e5%b8%b8%e7%94%a8%e7%9a%84%e5%85%83%e6%95%b0%e6%8d%ae%e5%9c%a8mysql%e4%b8%ad%e5%af%b9%e5%ba%94%e7%9a%84%e8%a1%a8/ Hive Metastore Federation 在滴滴的实践 https://blog.didiyun.com/index.php/2019/03/25/hive-metastore-federation/ 网易杭研大数据实践：Apache Hive稳定性测试 https://dun.163.com/news/p/83abc4931c1349b086c73dfbad0fb57f 如何使用带有大量SPARK分区的HIVE表 https://andr83.io/en/1090/ 参考文献及资料1、 Apache Spark support，链接：https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html 2、elasticsearch-hadoop项目，链接：https://github.com/elastic/elasticsearch-hadoop]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark和Elasticsearch交互实践总结]]></title>
    <url>%2F2020%2F10%2F13%2F2020-10-13-Spark%E5%92%8CElasticsearch%E4%BA%A4%E4%BA%92%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景为了更好的支持Spark应用和Elasticsearch交互，Elasticsearch官方推出了elasticsearch-hadoop项目。本文将详细介绍Spark Java应用和Elasticsearch的交互细节。 第一部分 环境依赖1.1 配置Maven依赖12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-spark-13_2.11&lt;/artifactId&gt; &lt;version&gt;6.8.2&lt;/version&gt;&lt;/dependency&gt; 需要注意Spark版本和elasticsearch-hadoop版本的兼容性，参考版本对照表： Spark Version Scala Version ES-Hadoop Artifact ID 1.0 - 1.2 2.10 1.0 - 1.2 2.11 1.3 - 1.6 2.10 elasticsearch-spark-13_2.10 1.3 - 1.6 2.11 elasticsearch-spark-13_2.11 2.0+ 2.10 elasticsearch-spark-20_2.10 2.0+ 2.11 elasticsearch-spark-20_2.11 1.2 Spark配置关于elasticsearch集群的交互配置，定义在SparkConf中，例如下面的案例： 123456789import org.apache.spark.SparkConf;SparkConf sparkConf = new SparkConf().setAppName("JavaSpark").setMaster("local");//config elasticsearchsparkConf.set("es.nodes","192.168.31.3:9200");sparkConf.set("es.port","9200");sparkConf.set("es.index.auto.create","true");JavaSparkContext jsc = new JavaSparkContext(sparkConf); es.nodes，集群节点； es.port，服务端口； es.index.auto.create，参数指定index是否自动创建； 其他配置参考官方文档：https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html 第二部分 交互接口2.1 自定义id的写入在业务数据写入elasticsearch集群的时候，需要数据去重。这时候就需要自己指定元数据字段中的_id。elasticsearch在处理_id相同的数据时，会覆盖写入。例如下面的例子： 1234567891011121314151617181920212223242526272829303132import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaSparkContext;import org.elasticsearch.spark.rdd.Metadata;import org.elasticsearch.spark.rdd.api.java.JavaEsSpark;import scala.Tuple2;import java.util.ArrayList;import java.util.HashMap;import static org.elasticsearch.spark.rdd.Metadata.ID;try&#123; ArrayList&lt;Tuple2&lt;HashMap,HashMap&gt;&gt; metaList = new ArrayList&lt;&gt;(); for(int i=0;i&lt;100;i++) &#123; HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put("id", String.valueOf(i)); map.put("name", "one"); HashMap&lt;Metadata, String&gt; metamap = new HashMap&lt;Metadata, String&gt;(); metamap.put(ID, String.valueOf(i)); metaList.add(new Tuple2(metamap, map)); &#125; JavaPairRDD&lt;?, ?&gt; pairRdd = jsc.parallelizePairs(metaList); JavaEsSpark.saveToEsWithMeta(pairRdd,"spark/doc"); &#125;catch (Exception e)&#123; e.printStackTrace(); System.out.println("finish!"); jsc.stop(); &#125; 例子中我们使用ArrayList&lt;Tuple2&lt;HashMap,HashMap&gt;&gt;数据结构来存储待写入的数据，然后构造RDD，最后使用JavaEsSpark.saveToEsWithMeta方法写入。需要注意这里构造的两个HashMap: 数据HashMap，数据结构为：HashMap&lt;String, String&gt;，用于存储数据键值对。 元数据HashMap，数据结构为：HashMap&lt;Metadata, String&gt;，用于存储元数据键值对。例如ID即为_id。 其他类型读写可以参考官方网站：https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html 第三部分 任务提交最后编译运行。主要是setMaster()指定运行方式，分为如下几种。 运行模式 说明 local Run Spark locally with one worker thread (i.e. no parallelism at all). local[K] Run Spark locally with K worker threads (ideally, set this to the number of cores on your machine). local[*] Run Spark locally with as many worker threads as logical cores on your machine. spark://HOST:PORT Connect to the given Spark standalone cluster master. The port must be whichever one your master is configured to use, which is 7077 by default. mesos://HOST:PORT Connect to the given Mesos cluster. The port must be whichever one your is configured to use, which is 5050 by default. Or, for a Mesos cluster using ZooKeeper, use mesos://zk://.... To submit with --deploy-mode cluster, the HOST:PORT should be configured to connect to the MesosClusterDispatcher. yarn Connect to a YARN cluster in client or cluster mode depending on the value of --deploy-mode. The cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable. yarn-client Equivalent to yarn with --deploy-mode client, which is preferred to yarn-client yarn-cluster Equivalent to yarn with --deploy-mode cluster, which is preferred to yarn-cluster 除了在eclipse、Intellij中运行local模式的任务，也可以打成jar包，使用spark-submit来进行任务提交。 参考文献及资料1、 Apache Spark support，链接：https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html 2、elasticsearch-hadoop项目，链接：https://github.com/elastic/elasticsearch-hadoop]]></content>
      <categories>
        <category>Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python中局部函数]]></title>
    <url>%2F2020%2F10%2F10%2F2020-12-13-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Python%E4%B8%AD%E5%B1%80%E9%83%A8%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 参考文献及资料 背景通过前面的学习我们知道，Python 函数内部可以定义变量，这样就产生了局部变量，有读者可能会问，Python 函数内部能定义函数吗？答案是肯定的。Python 支持在函数内部定义函数，此类函数又称为局部函数。 那么，局部函数有哪些特征，在使用时需要注意什么呢？接下来就给读者详细介绍 Python 局部函数的用法。 首先，和局部变量一样，默认情况下局部函数只能在其所在函数的作用域内使用。举个例子： 1#全局函数def outdef (): #局部函数 def indef(): print("http://c.biancheng.net/python/") #调用局部函数 indef()#调用全局函数outdef() 程序执行结果为： http://c.biancheng.net/python/ 就如同全局函数返回其局部变量，就可以扩大该变量的作用域一样，通过将局部函数作为所在函数的返回值，也可以扩大局部函数的使用范围。例如，修改上面程序为： 1#全局函数def outdef (): #局部函数 def indef(): print("调用局部函数") #调用局部函数 return indef#调用全局函数new_indef = outdef()调用全局函数中的局部函数new_indef() 程序执行结果为： 调用局部函数 因此，对于局部函数的作用域，可以总结为：如果所在函数没有返回局部函数，则局部函数的可用范围仅限于所在函数内部；反之，如果所在函数将局部函数作为返回值，则局部函数的作用域就会扩大，既可以在所在函数内部使用，也可以在所在函数的作用域中使用。 以上面程序中的 outdef() 和 indef() 为例，如果 outdef() 不将 indef 作为返回值，则 indef() 只能在 outdef() 函数内部使用；反之，则 indef() 函数既可以在 outdef() 函数内部使用，也可以在 outdef() 函数的作用域，也就是全局范围内使用。 有关函数返回函数，更详细的讲解，可阅读《Python函数高级方法》一节。 另外值得一提的是，如果局部函数中定义有和所在函数中变量同名的变量，也会发生“遮蔽”的问题。例如： 1#全局函数def outdef (): name = "所在函数中定义的 name 变量" #局部函数 def indef(): print(name) name = "局部函数中定义的 name 变量" indef()#调用全局函数outdef() 执行此程序，Python 解释器会报如下错误： UnboundLocalError: local variable ‘name’ referenced before assignment 此错误直译过来的意思是“局部变量 name 还没定义就使用”。导致该错误的原因就在于，局部函数 indef() 中定义的 name 变量遮蔽了所在函数 outdef() 中定义的 name 变量。再加上，indef() 函数中 name 变量的定义位于 print() 输出语句之后，导致 print(name) 语句在执行时找不到定义的 name 变量，因此程序报错。 由于这里的 name 变量也是局部变量，因此前面章节讲解的 globals() 函数或者 globals 关键字，并不适用于解决此问题。这里可以使用 Python 提供的 nonlocal 关键字。 例如，修改上面程序为： 1#全局函数def outdef (): name = "所在函数中定义的 name 变量" #局部函数 def indef(): nonlocal name print(name) #修改name变量的值 name = "局部函数中定义的 name 变量" indef()#调用全局函数outdef() 程序执行结果为： 所在函数中定义的 name 变量 参考文献及资料1、YARN Application Security，链接：https://hadoop.apache.org/docs/r2.7.4/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python中的递归和限制]]></title>
    <url>%2F2020%2F10%2F10%2F2020-10-10-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Python%E4%B8%AD%E7%9A%84%E9%80%92%E5%BD%92%E5%92%8C%E9%99%90%E5%88%B6%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 参考文献及资料 背景参考文献及资料1、YARN Application Security，链接：https://hadoop.apache.org/docs/r2.7.4/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html]]></content>
      <categories>
        <category>Yarn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MapReduce on Yarn机制总结]]></title>
    <url>%2F2020%2F10%2F06%2F2020-10-06-MapReduce%20on%20Yarn%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景Yarn作为资源统一管理平台是从第一代MapReduce（MRv1）演进而来。在最初的MRv1架构中，主要构成有三部分： 编程模型，MapReduce API； 资源管理和作业控制模块，JobTracker（作业跟踪器）、TaskTracker（任务跟踪器）； 数据处理引擎，MapTask、ReduceTask； 其中JobTracker单一服务运行在主节点，负责调度集群所有的任务，并监控任务的状态。每个工作节点均运行一个TaskTracker负责执行JobTracker分发的任务。架构上不足有： JobTracker服务单点故障风险； JobTracker服务太重，承担了集群的资源管理和任务调度； JobTracker承担了集中式调度器角色，将集群中资源分配、任务分配和监控一个人扛，在高并发场景下容易出现负载过重，产生单点风险。 另外TaskTracker将资源强制划分为Map task slot和Reduce task slot，并且map task和reduce task只能使用对应的solts资源。当任务的map和reduce任务数量发生倾斜的时候，就会出现计算资源使用低效。 集群中solts资源由两个参数决定：（mapred.tasktracker.map.tasks.maximum、mapred.tasktracker.reduce.tasks.maximum）。 Yahoo和Hortonworks于2012年在Hadoop 2.0版中引入了YARN，架构上将JobTracker功能进行了拆分。将调度工作拆解为两层：中央调度器和计算框架调度器。中央调度器管理集群中所有资源的状态，它掌握整个集群的资源信息，按照一定的调度策略（如：FIFO、Fair、Capacity、Delay、Dominant Resource Fair等）将资源粗粒度地分配给计算框架调度器，各个框架收到资源后再根据作业特性细粒度地将资源分配给执行器执行具体的计算任务。双层调度架构大大减轻了中央调度器的负载，提升了集群并发性能和资源利用率。 新的架构就是Yahoo发布的Yarn架构（Yet Another Resource Negotiator）。原架构中的JobTarcker被拆解为： ResourceManger，集群管理器。负责整个集群的资源管理和调度，承担中央调度器角色。ResourceManager有两个组件构成： 应用程序管理器（Application Manager简称ASM）。主要负责：1.管理监控各个系统的应用，包括启动ApplicaitonMaster，监控ApplicaitonMaster运行状态; 2.跟踪分配给ApplicationMaster的进度和状态。 调度器(Scheduler)。调度器根据资源容量、队列等限制条件（如队列资源和队列任务数量限制），将集群资源分配给正在运行的应用程序。即分配资源容器（Resource Container）给ApplicaitonMaster，分配单位Resource Container将CPU和内存资源封装。调度器是一个可配置可插拔的组件，用户可以自己设计新的调度器，Yarn提供多个可选项（Fair Scheduler和Capacity Scheduler等）。 ApplicationMaster，负责应用任务内部的调度和监控，承担框架调度器角色。 用户提交的每个应用都会对应一个ApplicationMaster，主要负责监控应用，任务容错（重启失败的task）等。它同时会和ResourceManager和NodeManager有交互，向ResourceManager申请资源，请求NodeManager启动或停止task。 Yarn（MRv2）架构上已经不单单支持运行MapReduce任务，架构解耦后可以支持Tez、Spark、Storm、Flink等多种计算框架的运行。本文将详细阐述MapReduce任务在新架构Yarn上运行细节。 第一部分 角色说明MapReduce on Yarn运行模式下，主要涉及下面的角色： ResourceMange，中央调度器角色； NodeManagers，节点管理器，负责管理Container; MapReduce ApplicationManager，框架调度器角色。负责协调运行MapReduce任务。 整个调度过程如下： 第二部分 任务提交第三部分 任务初始化第四部分 任务分配第五部分 任务执行第六部分 任务监控第七部分 任务完成第八部分 总结参考文献及资料1、Hadoop 新 MapReduce 框架 Yarn 详解，链接：https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/ 2、Hadoop: Writing YARN Applications，链接：https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html 3、MapReduce Tutorial，链接：https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[监控Yarn资源调度平台资源状态]]></title>
    <url>%2F2020%2F10%2F06%2F2020-10-06-%E7%9B%91%E6%8E%A7Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E8%B5%84%E6%BA%90%E7%8A%B6%E6%80%81%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Yarn状态数据接口 第二部分 Java实现 第三部分 总结 参考文献及资料 背景目前国内大部分企业级的大数据平台资源调度系统都是基于Yarn集群。生产环境上，各种大数据计算框架运行在Yarn上，就需要对Yarn平台的资源情况进行实时监控。虽然Yarn本身提供一个Web管理界面展示平台资源使用情况，但是这些运行状态数据需要实时获取和监控。随着智能化运维推进，需要对监控数据能实时分析、异常检测、自动故障处理。这些场景都需要能实时获取到Yarn平台的状态监控数据。 本文将详细介绍各种监控实现的方法，并重点介绍Java实现。 第一部分 Yarn状态数据接口1.1 命令行方式yarn命令在{hadoop_home}/bin路径下，对于部署hadoop客户端的客户端需要加载命令环境变量。 参看任务信息 1234# 查看所有任务信息yarn application -list# 查看正在运行的任务信息（带过滤参数appStates）yarn application -list -appStates RUNNING 这里参数appStates的状态有：ALL,NEW,NEW_SAVING,SUBMITTED,ACCEPTED,RUNNING,FINISHED,FAILED,KILLED 另外还可以指定计算框架的类型，例如： 12# 参看所有MapReduce任务yarn application -list -appTypes MAPREDUCE 参看指定任务状态信息 1yarn application -status application_1575989345612_32134 1.2 Restful Api接口ResourceManager允许用户通过REST API获取有关群集的信息：群集上的状态、群集上的指标、调度程序信息，另外还有群集中节点的信息以及集群上应用程序的运行信息。 查询整个集群指标 1GET http://http address:port/ws/v1/cluster/metrics 查询集群调度器详情 1GET http://http address:port/ws/v1/cluster/scheduler 监控任务 12curl http://http address:port/ws/v1/cluster/apps/stateGET http://http address:port/ws/v1/cluster/apps/state 查看指定任务 1GET http://http address:port/ws/v1/cluster/apps/ 查看指定任务的详细信息 1curl http://http address:port/proxy/ws/v2/mapreduce/info 杀死任务 yarn application -kill application_id 12curl -v -X PUT -d '&#123;"state": "KILLED"&#125;' http://http address:port&gt;/ws/v1/cluster/apps/PUT http://http address:port/ws/v1/cluster/apps/state 1.2 JMX Metrics监控首先需要开启jmx，编辑{hadoop_home}/etc/hadoop/yarn-env.sh配置文件，最后天下下面三行配置： 123YARN_OPTS="$YARN_OPTS -Dcom.sun.management.jmxremote.authenticate=false"YARN_OPTS="$YARN_OPTS -Dcom.sun.management.jmxremote.port=8001"YARN_OPTS="$YARN_OPTS -Dcom.sun.management.jmxremote.ssl=false" 其中8001是服务监听端口。jmx提供了Cluster、Queue、Jvm、FSQueue等Metrics信息。 1234# 获取YARN相关的jmxhttp://http address:8088/jmx# 如果想获取NameNode的jmxhttp://http address:50070/jmx 上面的方式会获取服务所有的信息(json格式)。如果需要精准获得准确信息，org.apache.hadoop.jmx.JMXJsonServlet类支持三个参数：callback、qry、get。其中qry用于过滤，下面的url用于查询Yarn上spark用户在default队列上任务信息。 1http://192.168.1.2:8088/jmx?qry=Hadoop:service=ResourceManager,name=QueueMetrics,q0=root,q1=default,user=spark 更详细的信息参考官网：https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/jmx/JMXJsonServlet.html 1.3 Python Api接口对于Python有第三方包支持和yarn进行交互，github地址为:https://github.com/CODAIT/hadoop-yarn-api-python-client 案例代码： 1234567from yarn_api_client import ApplicationMaster, HistoryServer, NodeManager, ResourceManagerrm = ResourceManager(address='192.168.1.2', port=8088)# 获取到ResourceManager的所有apps的信息rm.cluster_applications().data# 获取到ResourceManager的具体任务的信息rm.cluster_application('application_1437445095118_265798').data 对于Hadoop安全集群，还需要部署认证包requests_kerberos。具体可以参考说明文档：https://python-client-for-hadoop-yarn-api.readthedocs.io/en/latest/index.html 第二部分 Java实现2.1 maven依赖根据Hadoop的版本添加下面的依赖包： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-api&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt;&lt;/dependency&gt; 2.2 接口实现我们将相关配置文件放在resources/conf路径下面，涉及的文件有： 1234567# 集群配置文件core-site.xmlhdfs-site.xmlyarn-site.xml# 安全认证文件user.keytabkrb5.conf 下面是案例代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122package com.main.yarnmonitor;import com.google.common.collect.Sets;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.security.UserGroupInformation;import org.apache.hadoop.yarn.api.records.ApplicationReport;import org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport;import org.apache.hadoop.yarn.api.records.ContainerReport;import org.apache.hadoop.yarn.api.records.YarnApplicationState;import org.apache.hadoop.yarn.client.api.YarnClient;import org.apache.hadoop.yarn.exceptions.YarnException;import java.io.File;import java.io.IOException;import java.util.EnumSet;import java.util.List;import java.util.Set;/** * @program: yarnmonitor * @description: * @author: rongxiang * @create: 2020-03-27 16:44 **/public class yarnmonitor &#123; //配置文件路径 private static String confPath = Thread.currentThread().getContextClassLoader().getResource("").getPath()+ File.separator + "conf"; public static void main(String[] args) &#123; //加载配置文件 Configuration configuration = initConfiguration(confPath); //初始化安全集群Kerberos配置 initKerberosENV(configuration); //初始化Yarn 客户端 YarnClient yarnClient = YarnClient.createYarnClient(); yarnClient.init(configuration); yarnClient.start(); try &#123; //获得运行的任务应用清单 List&lt;ApplicationReport&gt; applications = yarnClient.getApplications(EnumSet.of(YarnApplicationState.RUNNING)); //存储需要关注的任务信息 HashMap&lt;String, ArrayList&lt;String&gt;&gt; applicationInformation = new HashMap&lt;&gt;(); for (ApplicationReport application:applications) &#123; String applicationType = application.getApplicationType(); // 只关注CPU核数资源使用超过500的任务 if (getApplicationInfo(application).get(1)&gt;=500) &#123; applicationInformation.put(String.valueOf(application.getApplicationId()), new ArrayList&lt;String&gt;()&#123;&#123; add(String.valueOf(application.getName())); add(String.valueOf(application.getApplicationType())); add(String.valueOf(application.getQueue())); add(String.valueOf(getApplicationInfo(application).get(0))); add(String.valueOf(getApplicationInfo(application).get(1))); add(String.valueOf(getApplicationInfo(application).get(2))); &#125;&#125;); &#125; System.out.println(applicationInformation); &#125; //设置监控信息发送邮件 if(!applicationInformation.isEmpty())&#123; //发送邮件 sendMailYarn(); &#125; &#125; catch (YarnException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 从ApplicationReport获取信息 * @return applicationInfo */ public static ArrayList&lt;Integer&gt; getApplicationInfo(ApplicationReport Application) &#123; ArrayList&lt;Integer&gt; applicationInfo = new ArrayList&lt;&gt;(); ApplicationResourceUsageReport resourceReport = Application.getApplicationResourceUsageReport(); if (resourceReport != null) &#123; Resource usedResources = resourceReport.getUsedResources(); int allocatedMb = usedResources.getMemory(); int allocatedVcores = usedResources.getVirtualCores(); int runningContainers = resourceReport.getNumUsedContainers(); //赋值 applicationInfo.add(allocatedMb); applicationInfo.add(allocatedVcores); applicationInfo.add(runningContainers); &#125; return applicationInfo; &#125; /** * 初始化YARN Configuration * @return configuration */ public static Configuration initConfiguration(String confPath) &#123; Configuration configuration = new Configuration(); System.out.println(confPath + File.separator + "core-site.xml"); configuration.addResource(new Path(confPath + File.separator + "core-site.xml")); configuration.addResource(new Path(confPath + File.separator + "hdfs-site.xml")); configuration.addResource(new Path(confPath + File.separator + "yarn-site.xml")); return configuration; &#125; /** * 安全集群配置（如果非安全集群这无需该方法） */ public static void initKerberosENV(Configuration conf) &#123; System.setProperty("java.security.krb5.conf", confPath+File.separator+"krb5.conf"); System.setProperty("javax.security.auth.useSubjectCredsOnly", "false"); System.setProperty("sun.security.krb5.debug", "false"); try &#123; UserGroupInformation.setConfiguration(conf); UserGroupInformation.loginUserFromKeytab("user@HADOOP.COM", confPath+File.separator+"user.keytab"); System.out.println(UserGroupInformation.getCurrentUser()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; Yarn客户端YarnClient中定义了方法getApplications，获取到正在运行的任务清单，返回数据类型是：List&lt;ApplicationReport&gt;，如下： 1List&lt;ApplicationReport&gt; applications = yarnClient.getApplications(EnumSet.of(YarnApplicationState.RUNNING)); 对于数据类型ApplicationReport具有方法getApplicationResourceUsageReport()获得每个Yarn任务的ApplicationResourceUsageReport(任务资源报告)： 1ApplicationResourceUsageReport resourceReport = Application.getApplicationResourceUsageReport(); ApplicationResourceUsageReport提供了获取各类资源的方法： 1234567Resource usedResources = resourceReport.getUsedResources();//任务使用的内存资源int allocatedMb = usedResources.getMemory();//任务使用的CPU资源int allocatedVcores = usedResources.getVirtualCores();//任务使用的容器的数量int runningContainers = resourceReport.getNumUsedContainers(); 第三部分 总结Java的案例中我们使用了HashMap(applicationInformation)数据类型存储关注的任务信息，然后使用邮件接口发出。在实际使用中可以根据需要存储在elasticsearch集群。 另外对于其他方法，作者没有实际使用，可能存在部分信息未涵盖，可以参考官网文档使用。 参考文献及资料1、YARN Application Security，链接：https://hadoop.apache.org/docs/r2.7.4/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html 2、ApplicationResourceUsageReport接口，链接：http://hadoop.apache.org/docs/r2.7.0/api/org/apache/hadoop/yarn/api/records/ApplicationResourceUsageReport.html 3、ResourceManager REST API’s，链接：https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html 4、基于Yarn API的Spark程序监控，链接:https://yq.aliyun.com/articles/710902]]></content>
      <categories>
        <category>Yarn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[大数据资源调度平台粒度的说明]]></title>
    <url>%2F2020%2F10%2F06%2F2020-10-06-%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E7%B2%92%E5%BA%A6%E7%9A%84%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 各种资源调度器粒度 第二部分 动态分配（Dynamic Allocation） 参考文献及资料 背景我们在Yarn资源管理器上提交MapReduce任务的时候发现（Yarn Web控制台），任务使用的container数量是变化的。其实这是由于MapReduce任务在Yarn上资源分配的粒度是：细粒度的。接下来我们将介绍资源调度管理器的粒度。 第一部分 各种资源调度器粒度我们知道Spark最开始没有Yarn和Srandalone模式的时候，只有Mesos资源管理器。后来才有了Yarn，最后为了推广才产生了Standalone模式。 1.1 Mesos集群Spark on Mesos模式下，同时支持粗粒度（coarse-grained）和细粒度（fine-grained），等到高版本Spark 2.X版本后不再支持细粒度。低版本默认配置是细粒度。 Fine-grained mode is deprecated as of Spark 2.0.0。Consider using Dynamic Allocation for some of the benefits. For a full explanation see SPARK-11857 设置为粗粒度 Spark配置项spark.mesos.coarse设置为true（可以在spark-default.conf配置文件中或者代码配置中）。在粗粒度模式下，可以通过下面3个参数指定静态资源： 设置spark.cores.max，最大使用CPU资源； 设置spark.executor.memory，每个executor的内存资源； 设置spark.executor.cores，每个executor的CPU资源； 设置为细粒度 注释spark-default.conf文件中的配置参数：spark.mesos.coarse，或者将其设置为false。 在细粒度模式下，Spark执行器中的每个Spark任务都作为单独的Mesos任务运行。这允许Spark的多个实例（和其他框架）以非常精细的粒度共享内核，其中每个应用程序在逐步增加和减少时将分配或多或少的内核，但是在启动每个任务时会带来额外的开销。此模式不适用于低延迟要求。 在Spark 2.x版本后，取消了对Mesos细粒度的支持，而是引入： Dynamic Allocation 1.2 Yarn集群1.2.1 细粒度MapReduce任务在Yarn上是细粒度模式。资源按需动态分配，每一个task均可以去申请资源，使用完后立即释放回收。提高集群计算资源的高效利用。但是如果task任务轻而多，那么就会出现task实际使用计算资源时间短，但是申请数量多，这就导致大量运行时间其实是花费在资源申请分配和释放上。 注：在实际生产中，使用MapReduce定时任务拉取Kafka数据汇入hive，就会出现短时间申请大量资源，实际计算时间很短，使得集群资源出现脉冲现象，资源使用极不平稳，引起相关性能容量告警。 1.2.2 粗粒度Spark on Yarn是粗粒度模式。资源静态分配。Spark Application在运行前就已经分配好需要的计算资源，没有task申请资源的时间延迟。但是资源释放上需要等待所有的task均执行完毕，才会触发所有资源的释放回收。特别极端的例子就是，一个job有100个task，完成了99个，还有一个没完成，那么所有资源就会闲置在那里等待那个task完成后才释放 。 1.3 Standalone集群Spark自身的Standalone集群也是粗粒度，和Spark on Yarn的粗粒度类似。 第二部分 动态分配（Dynamic Allocation）Spark在2.x开始抛弃在Mesos集群上对细粒度模式的支持，而是转而在粗粒度模式下提供动态分配机制（Dynamic Allocation）。其实Spark在1.2版本后，在Spark on Yarn上就已经提供对动态分配机制（Dynamic Allocation）的支持。所以动态分配机制是未来架构的方向，可以补充粗粒度在资源有效利用方面的不足。我们将在其他文章中介绍这种资源调度模式。 参考文献及资料1、Running Spark on Mesos，链接：https://spark.apache.org/docs/latest/running-on-mesos.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch中的GC以及监控]]></title>
    <url>%2F2020%2F10%2F05%2F2020-10-05-Elasticsearch%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Elasticsearch%E4%B8%AD%E7%9A%84GC%E4%BB%A5%E5%8F%8A%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分实现原理 第二部分 实现源码 参考文献及资料 背景garbage collection 第一部分 实现原理第二部分 实现源码参考文献及资料1、Nodes stats API介绍，链接：https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-stats.html https://mincong.io/2020/08/30/gc-in-elasticsearch/ https://discuss.elastic.co/t/frequent-gc-in-elasticsearch/57681]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[监控Kafka的Topic数据]]></title>
    <url>%2F2020%2F10%2F05%2F2020-10-05-%E7%9B%91%E6%8E%A7Kafka%E7%9A%84Topic%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分实现原理 第二部分 实现源码 参考文献及资料 背景业务上需要实现对Kafka的Topic中数据进行监控。业务正常下，Kafka生产者是持续生产数据的。如果一段时间出现Kafka中指定Topic没有新的数据，那么说明业务生产者可能出现异常。 第一部分 实现原理1.1 Kafka生产者的写入原理Kafka每个Topic在创建时，划分为若干个Partition。消息在写入的时候，分布式写入指定的多个Partitions中。对于每个Partition，消息是顺序写入的。Topic在创建时，每个Partition的Offset是0，当消息顺序写入后逐步累加Offset值。 Kafka中Offset变量定义是一个长整型（Long），这个值最大为：9223372036854775807。那么逐步累加会不会用完呢？多虑了哈，这是一个天文级别的数值哈。 1.2 监控原理既然Offset记录了每个Topic的每个Partition的消息量，最朴素的方法就是监控这个值的变化来判断是否有新的数据写入。 编写语言我们选择Python，而且对于处理Offset这个超大数据，Python是天然支持。 那么可行性是没问题的。 第二部分 实现源码1.1 Python Api 接口与Kafka交互的Python包我们使用：Kafka-Python。对于消费者有下面的函数方法： end_offsets(partitions)[source] Get the last offset for the given partitions. The last offset of a partition is the offset of the upcoming message, i.e. the offset of the last available message + 1. This method does not change the current consumer position of the partitions. Note This method may block indefinitely if the partition does not exist. Parameters: partitions (list) – List of TopicPartition instances to fetch offsets for. Returns: int}: The end offsets for the given partitions. Return type: {TopicPartition Raises: UnsupportedVersionError – If the broker does not support looking up the offsets by timestamp.KafkaTimeoutError – If fetch failed in request_timeout_ms 我们写一个简单的测试： 1234567891011121314from kafka import KafkaConsumer, TopicPartition# Kafka配置BOOTSTRAP="192.168.1.1:9092"TOPIC="test"# 定义消费者consumer = KafkaConsumer(bootstrap_servers=[BOOTSTRAP])# 获取指定Topic的PartitionsPARTITIONS = []for partition in consumer.partitions_for_topic(TOPIC): PARTITIONS.append(TopicPartition(TOPIC, partition))# 获取Offset信息partitions = consumer.end_offsets(PARTITIONS)print(partitions)#&#123;TopicPartition(topic='test', partition=0): 4759, TopicPartition(topic='test', partition=1): 4823&#125; 接口比较简洁。案例中，我们创建了一个Topic，取名为test。一共2个分区，1个副本： 123Topic:test PartitionCount:2 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Topic: test Partition: 1 Leader: 0 Replicas: 0 Isr: 0 所以回显分别显示了目前Topic的两个Partition的Offset值。 另外我们还可以使用kafka-consumer-groups.sh脚本查看一下目前某个group的offset情况： 123456root@deeplearning:/data/kafka/kafka_2.12-2.1.0/bin# ./kafka-consumer-groups.sh --bootstrap-server 192.168.1.1:9092 --describe --group my-groupConsumer group 'my-group' has no active members.TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-IDtest 1 4827 4835 8 - - -test 0 4763 4772 9 - - - 其中回显： CURRENT-OFFSET，目前group-id名称为’my-group’的消费群组Offset； LOG-END-OFFSET，目前topic的的Offset，也就是我们的监控对象； LAG，这是LOG-END-OFFSET-CURRENT-OFFSET的差，即’my-group’群组还有多少消息未消费； 1.2 完整的代码实现对多个Topic的监控： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# -*- coding: utf-8 -*-"""Copyright 2020 RongXiang.Licensed under the terms of the Apache 2.0 license.Please see LICENSE file in the project root for terms."""from kafka import KafkaProducer, KafkaConsumer, TopicPartitionimport timeimport logging# Kafka 配置BOOTSTRAP = ["192.168.31.3:9092"]MONITORTOPIC = ["test", "testTopic"]def monitoroffset(bootstrap, topicList): try: consumer = KafkaConsumer(bootstrap_servers=bootstrap) topicoffset = &#123;&#125; for topic in topicList: PARTITIONS = [] for partition in consumer.partitions_for_topic(topic): PARTITIONS.append(TopicPartition(topic, partition)) partitions = consumer.end_offsets(PARTITIONS) print(partitions) topicoffset[topic] = sum([partitions[item] for item in partitions]) consumer.close() return topicoffset except Exception as e: print(e)def diffdict(dictFirst, dictEnd): try: diffdict = &#123;&#125; for item in dictFirst: diffdict[item] = dictEnd[item]-dictFirst[item] return diffdict except Exception as e: print(e)def sendMessage(): passif __name__ == "__main__": logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) firstOffset = monitoroffset(BOOTSTRAP, MONITORTOPIC) time.sleep(300) secondOffset = monitoroffset(BOOTSTRAP, MONITORTOPIC) print(diffdict(firstOffset, secondOffset)) 输出： 1&#123;'test': 4, 'testTopic': 0&#125; 参考文献及资料1、kafka-python API介绍，链接：https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用Dockerfile定制docker镜像总结]]></title>
    <url>%2F2020%2F10%2F03%2F2020-10-03-Docker%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Dockerfile%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Hive 性能瓶颈根源 Hive 配置优化 Hive 语句优化 总结 背景构建Docker镜像通常有两种方式： 基于容器制作； 通过Dockerfile； Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。 第一部分 Dockerfile文件规范Docker镜像的分层 Dockerfile中的每个指令都会创建一个新的镜像层 镜像层将被缓存和复用 当Dockerfile的指令被修改了，复制的文件变化了，或者构建镜像时指定的变量不同了，对应的镜像层缓存就会失效 某一层的镜像缓存失效后，其之后的镜像层缓存都会随之失效 镜像层是不可变的，如果在某一层中添加一个文件，然后在下一层中删除则镜像中依然会包含该文件 Dockerfile编写规则Dockerfile中是基于其指令进行编写的，其规则可以参考下面的表格，当然，在编写Dockerfile时，其格式是需要严格遵循的： 除注释外，第一行必须使用FROM指令所基于的镜像名称；之后使用MAINTAINER指明维护信息；然后就是一系列镜像操作指令，如RUN、 ADD等；最后便是CMD指令来指定启动容器时要运行的命令操作。其中RUN指令可以使用多条，CMD只有最后一条可以生效！ 第二部分 Dockerfile文件组成2.1 注释2.2 FROM命令第三部分 编译Dcokerfile文件]]></content>
      <categories>
        <category>docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用Dockerfile定制docker镜像总结]]></title>
    <url>%2F2020%2F10%2F03%2F2020-10-03-Docker%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Hive 性能瓶颈根源 Hive 配置优化 Hive 语句优化 总结 背景构建Docker镜像通常有两种方式： 基于容器制作； 通过Dockerfile； Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。 第一部分 Dockerfile文件规范Docker镜像的分层 Dockerfile中的每个指令都会创建一个新的镜像层 镜像层将被缓存和复用 当Dockerfile的指令被修改了，复制的文件变化了，或者构建镜像时指定的变量不同了，对应的镜像层缓存就会失效 某一层的镜像缓存失效后，其之后的镜像层缓存都会随之失效 镜像层是不可变的，如果在某一层中添加一个文件，然后在下一层中删除则镜像中依然会包含该文件 Dockerfile编写规则Dockerfile中是基于其指令进行编写的，其规则可以参考下面的表格，当然，在编写Dockerfile时，其格式是需要严格遵循的： 除注释外，第一行必须使用FROM指令所基于的镜像名称；之后使用MAINTAINER指明维护信息；然后就是一系列镜像操作指令，如RUN、 ADD等；最后便是CMD指令来指定启动容器时要运行的命令操作。其中RUN指令可以使用多条，CMD只有最后一条可以生效！ 第二部分 Dockerfile文件组成2.1 注释2.2 FROM命令第三部分 编译Dcokerfile文件]]></content>
      <categories>
        <category>docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch集群pending_task任务等待集群故障处理]]></title>
    <url>%2F2020%2F10%2F02%2F2021-04-29-Elasticsearch%E9%9B%86%E7%BE%A4pending_task%E4%BB%BB%E5%8A%A1%E7%AD%89%E5%BE%85%E9%9B%86%E7%BE%A4%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 静态分区 第二部分 动态分区 第三部分 两者的比较 第四部分 动态分区使用的问题 参考文献及资料 背景故障现象： 集群出现大量的put-mapping的任务堆积 （1200个左右），该任务属于HIGH级别，比创建索引等任务级别高。所以出现集群4月27号的索引未创建成功。 pending task 反应了master节点尚未执行的集群级别的更改任务（例如：创建索引，更新映射，分配分片）的列表。pending task的任务是分级别的（优先级排序：IMMEDIATE&gt;URGENT&gt;HIGH&gt;NORMAL&gt;LOW&gt;LANGUID）,只有当上一级别的任务执行完毕后才会执行下一级别的任务，这也说明了：当出现HIGH级别以上的pending task任务时，备份和创建索引等一些低级别任务虽然任务对资源占用不多，也将不会执行 put-mapping的原因分析： pendingtask只能由主节点来进行处理，这些任务包括创建索引并将shards分配给节点。任务分优先次序。如果任务的产生比处理速度更快，将会产生堆积。如果存在任务堆积，集群存在较大隐患，需要排查集群的任务，确认原因。 这一条优化建议在上面也提到了，因为创建索引及新加字段都是更新元数据操作，需要 master 节点将新版本的元数据同步到所有节点。 因此在集群规模比较大，写入qps较高的场景下，特别容易出现master更新元数据超时的问题，这可导致 master 节点中有大量的 pending_tasks 任务堆积，从而造成集群不可用，甚至出现集群无主的情况。 动态mapping非常容易引起性能问题，特别是集群比较大的情况下，容易因为大量的mapping更新任务会导致master过载。 并且动态mapping也容易因为脏数据的写入产生错误的字段类型。 我们的做法是完全禁用动态mapping，在索引的mapping设置中增加”dynamic”: “false” 这个选项即可。 集群的索引字段类型都需要预先设计好，数据必须严格按照设计的类型写入，否则会被reject。 优化建议： 1、静态mapping； 2、index创建提前，并且设定好mapping； 其中 priority 字段则表示该 task 的优先级，翻看 ES 的源码可以看到一共有六种优先级： 123456IMMEDIATE((byte) 0),URGENT((byte) 1),HIGH((byte) 2),NORMAL((byte) 3),LOW((byte) 4),LANGUID((byte) 5); 查看分片未分配的原因 其中 index和shard 列出了具体哪个索引的哪个分片未分配成功。reason 字段则列出了哪种原因导致的分片未分配。这里也将所有可能的原因列出来： 1234567891011121314151617181920212223INDEX_CREATED：由于创建索引的API导致未分配。CLUSTER_RECOVERED ：由于完全集群恢复导致未分配。INDEX_REOPENED ：由于打开open或关闭close一个索引导致未分配。DANGLING_INDEX_IMPORTED ：由于导入dangling索引的结果导致未分配。NEW_INDEX_RESTORED ：由于恢复到新索引导致未分配。EXISTING_INDEX_RESTORED ：由于恢复到已关闭的索引导致未分配。REPLICA_ADDED：由于显式添加副本分片导致未分配。ALLOCATION_FAILED ：由于分片分配失败导致未分配。NODE_LEFT ：由于承载该分片的节点离开集群导致未分配。REINITIALIZED ：由于当分片从开始移动到初始化时导致未分配（例如，使用影子shadow副本分片）。REROUTE_CANCELLED ：作为显式取消重新路由命令的结果取消分配。REALLOCATED_REPLICA ：确定更好的副本位置被标定使用，导致现有的副本分配被取消，出现未分配。 detail 字段则列出了更为详细的未分配的原因。下面我会总结下在日常运维工作中常见的几种原因。 如果未分配的分片比较多的话，我们也可以通过下面的API来列出所有未分配的索引和主分片： 新版本： https://github.com/elastic/elasticsearch/pull/48867 参考文献及资料1、记Elasticsearch 因不合理创建type导致集群故障 https://blog.csdn.net/sunjiangangok/article/details/80423852 https://support.cloudbees.com/hc/en-us/articles/115000089811-Elasticsearch-troubleshooting-guide?page=9]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hive中静态分区和动态分区总结]]></title>
    <url>%2F2020%2F10%2F02%2F2020-10-02-Hive%E4%B8%AD%E9%9D%99%E6%80%81%E5%88%86%E5%8C%BA%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 静态分区 第二部分 动态分区 第三部分 两者的比较 第四部分 动态分区使用的问题 参考文献及资料 背景在Hive中有两种类型的分区：静态分区(Static Partitioning)和动态分区(Dynamic Partitioning)。 静态分区。对于静态分区，从字面就可以理解：表的分区数量和分区值是固定的。 动态分区。会根据数据自动的创建新的分区。 本文会详细介绍两种分区方法、使用场景以及生产中常见问题和解决方法。 第一部分 静态分区静态分区的使用场景主要是分区的数量是确定的。例如人力资源信息表中使用“部门”作为分区字段，通常一段时间是静态不变的。例如： 1234567891011CREATE EXTERNAL TABLE employee_dept ( emp_id INT, emp_name STRING) PARTITIONED BY ( dept_name STRING )location '/user/employee_dept';LOAD DATA LOCAL INPATH 'hr.txt'INTO TABLE employee_deptPARTITION (dept_name='HR'); 上面的外部表以dept_name字段为分区字段，然后导入数据需要指定分区。 第二部分 动态分区通常在生产业务场景中，我们使用的都是灵活的动态分区。例如我们使用时间字段（天、小时）作为分区字段。新的数据写入会自动根据最新的时间创建分区并写入对应的分区。例如下面的例子： 123hive &gt; insert overwrite table order_partition partition (year,month) select order_id, order_date, order_status, substr(order_date,1,4) year, substr(order_date,5,2) month from orders;FAILED: SemanticException [Error 10096]: Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict 写入报错。这是因为Hive默认配置不启用动态分区，需要使用前开启配置。开启的方式有两种： 在Hive服务配置文件中全局配置； 每次交互时候进行配置（只影响本次交互）； 通常我们生产环境使用第二种。 12set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nonstrict; 其中参数hive.exec.dynamic.partition.mode表示动态分区的模式。默认是strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。 第三部分 两者的比较两种分区模式都有各自的使用场景，我们总结如下： 静态分区(Static Partitioning) 动态分区（Dynamic Partitioning） 分区创建 数据插入分区之前，需要手动创建每个分区 根据表的输入数据动态创建分区 适用场景 需要提前知道所有分区。适用于分区定义得早且数量少的用例 有很多分区，无法提前预估新分区，动态分区是合适的 另外动态分区的值是MapReduce任务在reduce运行阶段确定的，也就是所有的记录都会distribute by，相同字段(分区字段)的map输出会发到同一个reduce节点去处理，如果数据量大，这是一个很弱的运行性能。而静态分区在编译阶段就确定了，不需要reduce任务处理。所以如果实际业务场景静态分区能解决的，尽量使用静态分区即可。 第四部分 动态分区使用的问题Hive表中分区架构，使得数据按照分区分别存储在HDFS文件系统的各个目录中，查询只要针对指定的目录集合进行查询，而不需要全局查找，提高查询性能。 但是分区不是”银弹”，如果分区数据过多，就会在HDFS文件系统中创建大量的目录和文件，对于集群NameNode服务是有性能压力的，NameNode需要将大量元数据信息保留在内存中。另外大分区表在用户查询时候由于分析size太大，也容易造成Metastore服务出现OMM报错。 上面两个现象均在生产环境发生，分别造成NameNode和Metastore不可用。 事实上，Hive为了防止异常生产大量分区，甚至默认动态分区是关闭的。另外对于生成动态分区的数量也做了性能默认限制。 4.1 动态分区创建限制当我们在一个Mapreduce任务（hive写入会编译成mapreduce任务）中创建大量分区的时候，经常会遇到下面的报错信息： 122015-06-15 17:27:44,614 ERROR [LocalJobRunner Map Task Executor #0]: mr.ExecMapper (ExecMapper.java:map(171)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row ....Caused by: org.apache.hadoop.hive.ql.metadata.HiveFatalException: [Error 20004]: Fatal error occurred when node tried to create too many dynamic partitions. The maximum number of dynamic partitions is controlled by hive.exec.max.dynamic.partitions and hive.exec.max.dynamic.partitions.pernode. Maximum was set to: 256... 10 more 这个报错就是因为Hive对于动态分区创建的限制，涉及的参数有： 123hive.exec.max.dynamic.partitions = 1000;hive.exec.max.dynamic.partitions.pernode = 100;hive.exec.max.created.files = 10000 hive.exec.max.dynamic.partitions.pernode，参数限制MapReduce任务单个任务(mapper或reducer任务)创建的分区数量为100； hive.exec.max.dynamic.partitions，参数限制单次整体任务创建分区的数量上限为1000个； hive.exec.max.created.files，参数限制所有单次整体map和reduce任务创建的最大文件数量上限为10000个； 以上三个阀值超过就会触发错误，集群会杀死任务。为了解决报错，我们通常将两个参数调大。但是也需要用户对自己的Hive表的分区数量进行合理规划，避免过多的分区。 4.2 特殊分区如果动态分区列输入的值为NULL或空字符串，则Hive将该行将放入一个特殊分区，其名称由参数hive.exec.default.partition.name控制。默认值为__HIVE_DEFAULT_PARTITION__。 用户可以使用（查看表分区）命令进行查看： 1234show partitions 'table名称';# process_date=20160208#process_date=__HIVE_DEFAULT_PARTITION__ 有时候异常生产这些分区数据，需要进行清理。如果使用下面的语句： 1ALTER TABLE Table_Name DROP IF EXISTS PARTITION(process_date='__HIVE_DEFAULT_PARTITION__'); 这时候Hive会报错： 1Error: Error while compiling statement: FAILED: SemanticException Unexpected unknown partitions for (process_date = null) (state=42000,code=40000) 这是Hive一个已知bug（编号：HIVE-11208），在Hive 2.3.0版本修复。 但是有个有修复方法（不建议在生产环境中实施）： 123456-- update the column to be "string"ALTER TABLE test PARTITION COLUMN (p1 string);-- remove the default partitionALTER TABLE test DROP PARTITION (p1 = '__HIVE_DEFAULT_PARTITION__');-- then revert the column back to "int" typeALTER TABLE test PARTITION COLUMN (p1 int); 链接：https://cloudera.ericlin.me/2015/07/how-to-drop-hives-default-partition-__hive_default_partition__-with-int-partition-column/ 4.3 乱码分区字段有时候表分区字段由于处理不当，会出现乱码分区，例如： 1hp_stat_time=r_ready%3D91;r_load%3D351 原因是Hive会自动对一些UTF-8字符编码成Unicode（类似网址中中文字符和一些特殊字符的编码处理）。此处%3D解码后是’=’。可以使用在线转换进行解码：https://www.matools.com/code-convert-utf8。 最后使用解码后的字段即可（注意分号转义）： 1alter table dpdw_traffic_base drop partition(hp_stat_time='r_ready=91\;r_load=351'); 参考文献及资料1、动态分区，链接：https://cwiki.apache.org/confluence/display/Hive/DynamicPartitions 2、Hive Tutorial，链接：https://cwiki.apache.org/confluence/display/Hive/Tutorial 3、Apache Hive 中文手册，链接：https://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hive中外部表总结和实践]]></title>
    <url>%2F2020%2F10%2F02%2F2021-04-27-Hive%E4%B8%AD%E5%A4%96%E9%83%A8%E8%A1%A8%E6%80%BB%E7%BB%93%E5%92%8C%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 静态分区 第二部分 动态分区 第三部分 两者的比较 第四部分 动态分区使用的问题 参考文献及资料 背景参考文献及资料1、动态分区，链接：https://cwiki.apache.org/confluence/display/Hive/DynamicPartitions 2、Hive Tutorial，链接：https://cwiki.apache.org/confluence/display/Hive/Tutorial 3、Apache Hive 中文手册，链接：https://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mysql的两种连接方式总结]]></title>
    <url>%2F2020%2F10%2F02%2F2020-10-01-Mysql%E7%9A%84%E4%B8%A4%E7%A7%8D%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 TCP/IP Socket 第二部分 UNIX Domain Socket 第三部分 Mysql的两种连接方式 参考文献及资料 背景我们在使用Mysql客户端和Mysql交互的时候，如果客户端是远程（非本机）那么底层是通过TCP/IP的Socket方式进行交互。但是如果客户端和数据库在同一台服务器上时，Mysql支持通过UNIX Domain Socket方式交互。 Mysql客户端和Mysql数据库服务通信，不管是本机还是远程，其本质是两个计算机进程之间的通信。根据位置的不同分为：本机进程通信和网络间进程通信。 本机进程通信。Linux通常有：管道、信号量、消息队列、信号、共享内存、UNIX Domain Socket套接字。 网络间进程通信。 TCP/IP Socket 本文将介绍将介绍这两种交互方式。 第一部分 TCP/IP Socket提到通信，那么首先需要解决是身份识别问题。对于本机进程，不同的进程都有操作系统分配的进程号(process ID)作为唯一标识。但是网络间进程通信时候，PID就不能作为唯一标识了，另外操作系统支持的网络协议不同。所以网络间进程通信需要解决唯一身份标识和网络协议识别问题。 这时候出现的TCP/IP协议中，IP层的ip地址可以唯一标识网络计算机身份，传输层的“协议+端口”可以唯一标识进程。这样就有”三元坐标”:(IP地址，协议，端口)，这个坐标可以唯一标识网络中进程。 在网络编程中，TCP/IP和Socket两个概念没有必然的联系。Socket编程接口在设计的时候，也能支持其他的网络协议。所以，socket的出现只是可以更方便的使用TCP/IP协议栈而已，其对TCP/IP进行了抽象封装，形成了几个最基本的函数接口。例如create，listen，accept，connect，read和write等等。 第二部分 UNIX Domain Socket对于本机中进程通信，也是可以使用TCP/IP Socket方式，通过回环地址(loopback)地址 127.0.0.1,但是对于本机这是繁琐的，接着就发展出了Unix domain socket方式，又称IPC(inter-process communication进程间通信) socket。 UNIX domain socket 用于本机进程通信更有效率。不需要经过网络协议栈，不需要打包拆包、计算校验和、维护序号、路由和应答等，只是将应用层数据从一个进程拷贝到另一个进程。UNIX domain socket与 TCP/IP Socket相比较，在同一台主机的传输速度前者是后者的两倍。 UNIX domain socket 与TCP/IP Socket 最明显的不同在于地址格式不同，TCP/IP Socket 的 socket 地址是IP地址加端口号，而UNIX domain socket的地址是一个 socket 类型的文件在文件系统中的路径。 另外下面的命令可以查看当前操作系统中UNIX domain socket通信清单： 1netstat -a -p --unix 第三部分 Mysql的两种连接方式3.1 UNIX Domain Socket方式客户端部署在Mysql本机，配置好环境变量，就可以使用下面的命令登录Mysql。 1234567891011121314[root@mysql ~]# mysql -uroot -P*****Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 1Server version: 5.5.46-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.root@[(none)]&gt; 这时候就是通过UNIX Domain Socket方式和Mysql进行交互的。只是这时候我们没有指定参数–socket，这个参数指定就是UNIX Domain Socket依赖的socket 类型的文件。Mysql默认这个参数为：–socket=/tmp/mysql.sock。如果安装Mysql时候，配置文件my.cnf中下面配置错误或文件丢失，就会报错。 12[client] socket=/tmp/mysql.sock 报错找不到sock文件： 12[root@mysql ~]# mysql -uroot -P*****ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2) 遇到这种报错的处理方法： 使用find命令查找文件路径，调整配置，使其归位。如果文件不再配置文件指定位置，这时候需要在命令中指定具体的路径，命令如下： 1[root@mysql ~]# mysql -uroot -P***** -S /path/of/mysql.sock 重新创建。可以简单地通过重启Mysql服务重新创建得到它。因为服务在启动时重新创建它。 另外可以通过查看Mysql变量信息来查看这个文件路径配置路径： 12345678mysql&gt; SHOW VARIABLES LIKE 'socket';+---------------+-----------------+| Variable_name | Value |+---------------+-----------------+| socket | /tmp/mysql.sock |+---------------+-----------------+1 row in set (0.00 sec)mysql -uroot -S/tmp/mysql.sock 3.2 TCP/IP Socket方式在本机使用UNIX Domain Socket方式无法登陆时候，还可以使用TCP/IP Socket方式。命令需要指定IP信息，如： 1234567891011121314151617[root@mysql ~]# mysql -h192.1.1.20ERROR 2003 (HY000): Can't connect to MySQL server on '192.1.1.20' (111)[root@mysql ~]# mysql -h192.1.1.20 -P3307Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 3Server version: 5.5.46-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.root@[(none)]&gt; 那么如果本机中同时指定两个参数时候，Mysql会默认使用TCP/IP Socket的方式连接。 1234567891011121314[root@mysql ~]# mysql -h192.1.1.20 -P3306 -S /path/of/mysql.sockWelcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.5.46-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.root@[(none)]&gt; 对于和远程Mysql服务交互，需要指定远程Mysql服务监听IP和端口即可，不再赘述。 参考文献及资料1、Linux进程间通信方式有哪些？，链接：https://zhuanlan.zhihu.com/p/63916424 2、UNIX Domain Socket IPC，链接：http://docs.linuxtone.org/ebooks/C&amp;CPP/c/ch37s04.html]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark任务依赖jar包总结]]></title>
    <url>%2F2020%2F09%2F27%2F2020-09-27-Spark%E4%BB%BB%E5%8A%A1%E4%BE%9D%E8%B5%96jar%E5%8C%85%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景编写了Spark批任务（Java），使用maven打包成jar包，提交到Yarn集群，报jar包冲突的错误。查阅资料了解一下背后的原理。 https://stackoverflow.com/questions/16222748/building-a-fat-jar-using-maven 第一部分 Spark任务加载jar包原理Spark任务运行默认加载的配置从默认的配置文件中获取，如果配置参数用户重新定义，那么参数将会覆盖默认配置文件中加载的配置。 spark-defaults.conf https://exceptionshub.com/add-jars-to-a-spark-job-spark-submit.html 原因是本地的jar包被SPARK_HOME/lib中的jar覆盖。spark程序在提交到yarn时，除了上传用户程序的jar，还会上传SPARK_HOME的lib目录下的所有jar包（参考附录2 )。如果你程序用到的jar与SPARK_HOME/lib下的jar发生冲突，那么默认会优先加载SPARK_HOME/lib下的jar，而不是你程序的jar，所以会发生“ NoSuchMethodError”。 1mvn dependency:tree -Dverbose 参数spark.yarn.jars 官方文档的解释： 1List of libraries containing Spark code to distribute to YARN containers. By default, Spark on YARN will use Spark jars installed locally, but the Spark jars can also be in a world-readable location on HDFS. This allows YARN to cache it on nodes so that it doesn't need to be distributed each time an application runs. To point to jars on HDFS, for example, set this configuration to hdfs:///some/path. Globs are allowed. 参数（1.5.1）： spark.driver.extraClassPath spark.executor.extraClassPath 附加到driver的classpath的额外的classpath实体。 附加到executors的classpath的额外的classpath实体。这个设置存在的主要目的是Spark与旧版本的向后兼容问题。用户一般不用设置这个选项 额外的classpath条目需预先添加到驱动程序 classpath中。 注意 : 在客户端模式下，这一套配置不能通过 SparkConf 直接在应用在应用程序中，因为 JVM 驱动已经启用了。相反，请在配置文件中通过设置 –driver-class-path 选项或者选择默认属性。 目录使用hdfs文件 参数： spark.driver.extraLibraryPath spark.executor.extraLibraryPath 指定启动driver的JVM时用到的库路径 参数：（2.1.0版本后） spark.driver.userClassPathFirst spark.executor.userClassPathFirst (实验性)当在driver中加载类时，是否用户添加的jar比Spark自己的jar优先级高。这个属性可以降低Spark依赖和用户依赖的冲突。它现在还是一个实验性的特征。 （实验）在驱动程序加载类库时，用户添加的 Jar 包是否优先于 Spark 自身的 Jar 包。这个特性可以用来缓解冲突引发的依赖性和用户依赖。目前只是实验功能。这是仅用于集群模式。 参数： spark.yarn.dist.jars 第二部分 解决jar包冲突第四种方式操作：更改Spark的配置信息:SPARK_CLASSPATH, 将第三方的jar文件添加到SPARK_CLASSPATH环境变量中 注意事项：要求Spark应用运行的所有机器上必须存在被添加的第三方jar文件 ;) 1234567A.创建一个保存第三方jar文件的文件夹: 命令：$ mkdir external_jarsB.修改Spark配置信息 命令：$ vim conf/spark-env.sh修改内容：SPARK_CLASSPATH=$SPARK_CLASSPATH:/opt/cdh-5.3.6/spark/external_jars/*C.将依赖的jar文件copy到新建的文件夹中命令：$ cp /opt/cdh-5.3.6/hive/lib/mysql-connector-java-5.1.27-bin.jar ./external_jars/ 备注：（只针对spark on yarn(cluster)模式）spark on yarn(cluster)，如果应用依赖第三方jar文件最终解决方案：将第三方的jar文件copy到${HADOOP_HOME}/share/hadoop/common/lib文件夹中(Hadoop集群中所有机器均要求copy)https://blog.csdn.net/ifenggege/article/details/108327167 参考文献及资料1、Running Spark on YARN，链接：https://spark.apache.org/docs/1.5.1/running-on-yarn.html 2、解决jar包冲突新思路 - maven-shade-plugin，链接：https://zhuanlan.zhihu.com/p/62796806]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS小文件治理总结]]></title>
    <url>%2F2020%2F09%2F12%2F2020-09-13-HDFS%E5%B0%8F%E6%96%87%E4%BB%B6%E6%B2%BB%E7%90%86%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 回本溯源 第二部分 HDFS大量小文件的危害 第三部分 小文件治理方案总结 第四部分 总结 参考文献及资料 背景企业级Hadoop大数据平台在实际使用过程中，可能大部分会遭遇小文件问题，并体验它的破坏性。HDFS文件系统的 inode 信息和 block 信息以及 block 的位置信息，这些原数据信息均由 NameNode 的内存中维护，这使得 NameNode 对内存的要求非常高，特别是遭遇海量小文件。 例如：京东的 NameNode 内存是 512GB，甚至还有大厂的 NameNode 的机器是 1TB的内存。能力强的大厂，钱就不花在买机器上了，例如字节跳动使用C++重写 NameNode ，这样分配内存和释放内存都由程序控制。但是NameNode天生的架构缺陷，所以元数据的扩展性终是受限于单机物理内存大小。 本篇文章回本溯源，总结小文件的产生原理、对业务平台的危害，最后分析总结了治理方法。 第一部分 回本溯源HDFS基于Google的论文《分布式文件系统》思想实现的，设计目的是解决大文件的读写。 1.1 HDFS存储原理HDFS集群（hadoop 2.0 +）中，有两类服务角色：NameNode、DataNode。文件数据按照固定大小（block size，默认128M）切分后，分布式存储在DataNode节点上。而数据的元数据信息加载在NameNode服务内存中。为防止服务单机会持久化一份在文件中（即fsimage文件，最新的元数据存储在edits log日志中，一般为 64MB，当 edits log 文件大小达到 64MB时，就会将这些元数据追加到 fsimage 文件中）。 每个文件/目录和block元数据信息存储在内存中，内存中分别对应：INodeFile、INodeDirectory、BlockInfo，每个对象大约150-200 bytes。 1.2 检查文件系统1.2.1 命令 fsckHDFS提供了fsck命令用来检查HDFS文件系统的健康状态和Block信息。需要有HDFS的supergroup特权用户组的用户才有执行权限。参考下面的命令： 12345678910111213141516171819202122[root@quickstart cloudera]# hdfs fsck / -blocks -locationsConnecting to namenode via http://quickstart.cloudera:50070FSCK started by admin (auth:KERBEROS_SSL) from /172.17.0.2 for path / at Sun May 09 05:35:33 UTC 2021Status: HEALTHY Total size: 837797031 B Total dirs: 285 Total files: 921 Total symlinks: 0 Total blocks (validated): 916 (avg. block size 914625 B) Minimally replicated blocks: 916 (100.0 %) Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 0 (0.0 %) Mis-replicated blocks: 0 (0.0 %) Default replication factor: 1 Average block replication: 1.0 Corrupt blocks: 0 Missing replicas: 0 (0.0 %) Number of data-nodes: 1 Number of racks: 1FSCK ended at Sun May 09 05:35:34 UTC 2021 in 90 millisecondsThe filesystem under path '/' is HEALTHY 其中Total blocks (validated): 916 (avg. block size 914625 B)，表示集群一共有916个Block，平均一个Block存储大小为0.87M。 1.2.2 命令 countHDFS 提供查询目录中文件数量的命令，例如： 12[root@quickstart cloudera]# hdfs dfs -count / 285 923 837797031 / 回显说明，依次为：DIR_COUNT(文件目录数量), FILE_COUNT（文件数量）, CONTENT_SIZE（存储量） FILE_NAME（查询文件目录名） 1.2.3 NameNode WebUI界面NameNode WebUI提供展示界面，显示HDFS文件系统的关键指标信息。例如： 12345Security is on.Safemode is off.1,206 files and directories, 916 blocks = 2,122 total filesystem object(s).Heap Memory used 23.5 MB of 48.38 MB Heap Memory. Max Heap Memory is 48.38 MB.Non Heap Memory used 49.16 MB of 77.85 MB Commited Non Heap Memory. Max Non Heap Memory is 130 MB. 目前HDFS文件系统中有1206个文件和文件目录，916个block，一共2122个文件系统对象。 可以jmap命令参看jvm堆栈实际使用： 1234# jmap -histo:live 8204（namenode进程pid ）num #instances #bytes class name#（省略）Total 435970 61136008 实际使用61136008/1024/1024=58.3 M。 1.2.4 元数据内存资源估算NameNode的内存主要由NameSpace和BlocksMap占用，其中NameSpace存储的主要是INodeFile和INodeDirectory对象。BlocksMap存储的主要是BlockInfo对象，所以估算NameNode占用的内存大小也就是估算集群中INodeFile、INodeDirectory和BlockInfo这些对象占用的heap空间。下面是估算NameNode内存数据空间占用资源大小的预估公式。 1Total = 198 ∗ num(directories + Files) + 176 ∗ num(blocks) + 2% ∗ size(JVM Memory Size) 例如测试集群：（1206*198+176*916）/1024/1024+2%*1000=20.38M，和实际值23.5 M误差较小。 1.2.5 堆栈配置建议在NameNode WebUI界面的Summary可以看到文件系统对象（filesystem objects）的统计。下面是NameNode根据文件数量的堆栈大小配置建议。 文件数量(1文件对应1block) 文件系统对象数量（filesystem objects=files+blocks） 参考值（GC_OPTS） 5,000,000 10,000,000 -Xms6G -Xmx6G -XX:NewSize=512M -XX:MaxNewSize=512M 10,000,000 20,000,000 -Xms12G -Xmx12G -XX:NewSize=1G -XX:MaxNewSize=1G 25,000,000 50,000,000 -Xms32G -Xmx32G -XX:NewSize=3G -XX:MaxNewSize=3G 50,000,000 100,000,000 -Xms64G -Xmx64G -XX:NewSize=6G -XX:MaxNewSize=6G 100,000,000 200,000,000 -Xms96G -Xmx96G -XX:NewSize=9G -XX:MaxNewSize=9G 150,000,000 300,000,000 -Xms164G -Xmx164G -XX:NewSize=12G -XX:MaxNewSize=12G 1.3 产生的场景分析在实际生产环境中，很多场景会产生小文件。 1.3.1 MapReduce产生Mapreduce任务中reduce数量设置过多，reduce的个数和输出文件个数一致，从而导致输出大量小文件。 1.3.2 hive产生hive表设置过量分区，每次写入数据会分别落盘到各自分区中，每个分区的数据量越小，对应的分区表文件也就会越小。从而导致产生大量小文件。 1.3.3 实时流任务处理流任务处理数据通常要求短时间数据落盘，例如Spark Streaming 从外部数据源接收数据，每个微批（默认60s）需要落盘一次结果数据到HDFS，如果数据量小，会产生大量小文件落盘文件。 1.3.4 数据自身特性产生除了数据处理产生，还有是由于数据自身特性决定的。例如使用HDFS存储图片、短视频等数据。这些数据本身单体就不大，就会以小文件形式存储。 第二部分 HDFS大量小文件的危害2.1 MapReduce任务消耗大量计算资源MapReduce任务处理HDFS文件的时候回根据数据的Block数量启动对应数量Map task，如果是小文件系统，这会导致任务启动大量的Map task，一个task在Yarn上对应一个CPU，实际线上环境会短时间申请成千个CPU资源，造成集群运行颠簸。可以通过设置map端文件合并及reduce端文件合并来优化。 2.2 NameNode服务过载HDFS作为分布式文件系统的一个优点就是可以横向伸缩扩展，但是由于元数据存储在NameNode中，事实上，当数据量达到一定程度，NameNode服务单机内存资源（普通PC物理机内存通常是256G）反而成为横向扩展的瓶颈。特别是面对小文件系统，可能集群实际存储并不大，但是元数据信息已经使得NameNode服务过载，这时候横向扩容DataNode是无济于事的。 HDFS中元数据的操作均在NameNode服务完成，小文件系统造成服务过载后，元数据更新性能会下降。严重的时候，服务会经常Full GC，如果GC停顿过长甚至会导致服务故障。 如果导致NameNode出现故障，在没有HA保障时，服务启动是一个漫长的过程。服务需要重新将fsImage文件数据加载至服务内存，最新的日志数据editlogs需要回放，最后Checkpoint和DataNode的BlockReport。这个过程当元数据过大时候是个漫长过程。 例如美团的提供案例：当元数据规模达到5亿（Namespace中INode数超过2亿，Block数接近3亿），fsImage文件大小将接近到20GB，加载FsImage数据就需要约14min，Checkpoint需要约6min，再加上其它阶段整个重启过程将持续约50min，极端情况甚至超过60min。 笔者公司HA集群，曾经由于小文件系统导致NameNode服务故障，甚至出现主备元数据未同步，整个恢复过程需要完成先完成主备同步，整整需要8个小时，这个期间HDFS无法对外服务，影响较大。 第三部分 小文件治理方案总结面对HDFS的NameNode内存过载带来的线上问题，Hadoop社区给出治理方案和架构上优化。主要有： 横向扩展NameNode能力，分散单点负载；例如联邦（Federation）。 NameNode元数据调整为外置存储；例如LevelDB最为存储对象。 另外还有美团技术文章《HDFS NameNode内存全景》提到互联网大厂的最佳实践和尝试： 除社区外，业界也在尝试自己的解决方案。Baidu HDFS2[5]将元数据管理通过主从架构的集群形式提供服务，本质上是将原生NameNode管理的Namespace和BlockManagement进行物理拆分。其中Namespace负责管理整个文件系统的目录树及文件到BlockID集合的映射关系，BlockID到DataNode的映射关系是按照一定的规则分到多个服务节点分布式管理，这种方案与Lustre有相似之处（Hash-based Partition）。Taobao HDFS2[6]尝试过采用另外的思路，借助高速存储设备，将元数据通过外存设备进行持久化存储，保持NameNode完全无状态，实现NameNode无限扩展的可能。其它类似的诸多方案不一而足。 尽管社区和业界均对NameNode内存瓶颈有成熟的解决方案，但是不一定适用所有的场景，尤其是中小规模集群。 3.1 联邦HDFS在Hadoop 2.x发行版中引入了联邦（Federation）HDFS功能。联邦HDFS允许集群通过添加多个NameNode来实现扩展，每个NameNode管理一份元数据。架构上解决了横向扩展，但是这不是一个真正的分布式NameNode，仍然存在单点故障风险。具体可以参考美团技术的最佳实践[3]。 3.2 归档文件对于小文件问题，Hadoop自身提供了三种解决方案：Hadoop Archive、 Sequence File 和 CombineFileInputFormat。 3.2.1 Hadoop ArchiveHadoop Archives （HAR files）在 0.18.0版本中引入，目的是为了缓解大量小文件消耗 NameNode 内存的问题。HAR不会减少文件存储大小，而是减少NameNode 的内存资源。例如下图展示了将HDFS文件目录foo中大量小文件file-*归档为bar.har文件。 可以使用下面的命令对HDFS文件进行归档： 1# hadoop archive -archiveName name -p &lt;parent&gt; [-r &lt;replication factor&gt;] &lt;src&gt;* &lt;dest&gt; -archiveName指定创建归档文件的名称，如：foo.har，也就是说需要在归档名称后面添加一个*.har的扩展。 -p 指定需归档的文件的相对路径，如：-p /foo/bar a/b/c e/f/g，/foo/bar是根目录，a/b/c，e/f/g是相对根目录的相对路径。 -r 指定所需的复制因子，如果未被指定，默认为3。 例如下面的命令执行后，将提交一个Mapreduce任务。 1[root@quickstart /]# hadoop archive -archiveName test.har -p /user/test /tmp 使用下面的命令查看归档后的文件： 1234[root@quickstart /]# hadoop fs -lsr har:///tmp/test.harlsr: DEPRECATED: Please use 'ls -R' instead.-rw-r--r-- 3 admin supergroup 0 2021-05-10 23:02 har:///tmp/test.har/file-1(略) 需要注意的是： 归档文件一旦创建就不能改变，要增加或者删除文件，就需要重新创建。 归档不支持压缩数据，类似于Unix中的tar命令。 归档命令不会删除原HDFS文件，需要自行删除。 *.har在HDFS上是一个目录，不是一个文件。 Hive内置了将现有分区中的文件转换为Hadoop Archive (HAR)的支持，具体参数为： 123456# 启用归档hive&gt; set hive.archive.enabled=true;# 创建存档时是否可以设置父目录hive&gt; set hive.archive.har.parentdir.settable=true;# 控制组成存档的文件的大小hive&gt; set har.partfile.size=1099511627776; 实际线上环境，可以对长期保存的hive表以分区颗粒度进行归档，在需要查询的时候进行归档恢复。具体实践案例如下： 1234# hive分区归档ALTER TABLE tablename ARCHIVE PARTITION(ds='2008-04-08', hr='12')# hive归档分区恢复ALTER TABLE tablename UNARCHIVE PARTITION(ds='2008-04-08', hr='12') 数据归档为har后，数据仍然是可以被查询的（但是是不可写的），只是查询会比非归档慢，如果要提高效率需要归档恢复。 3.2.2 Sequence FileSequenceFile是Hadoop API提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中，使用Hadoop的标准Writable接口实现序列化和反序列化。Mapreduce计算的中间结果的落盘就是SequenceFile格式的文件。 线上生产环境中，将SequenceFile格式的文件作为HDFS小文件的容器。即读取小文件然后以Append追加的形式写入”文件容器”。 3.2.3 CombineFileInputFormatHadoop内置提供了一个 CombineFileInputFormat 类来专门处理合并小文件，其核心功能是将HDFS上多个小文件合并到一个 InputSplit中，然后会启用一个map来处理这里面的文件，以此减少Mapreduce整体作业的运行时间，同时也减少了map任务的数量。 这个接口天然具备处理小文件的能力，只要将合并后的小文件落盘即可。 另外对于Hive输入也是支持合并方式读取的，参数配置参考： 1234# 执行Map前进行小文件合并set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormatset mapreduce.input.fileinputformat.split.maxsize=1073741824set mapreduce.input.fileinputformat.split.minsize=1073741824 注意以上Mapreduce和Hive Sql使用CombineFileInputFormat方式并不会缓解NameNode内存管理问题，因为合并的文件并不会持久化保存到磁盘。只是提高Mapreduce和Hive作业的性能。 3.3 更换存储介质技术架构中没有最牛逼的技术，只有最合适的技术。HDFS并不适合小文件，那么可以改变存储介质。用HBase来存储小文件，也是比较常见的选型方案。HBase在设计上主要为了应对快速插入、存储海量数据、单个记录的快速查找以及流式数据处理。适用于存储海量小文件（小于10k）,并支持对文件的低延迟读写。 HBase同一类的文件存储在同一个列族下面，若文件数量太多，同样会导致regionserver内存占用过大，JVM内存过大时gc失败影响业务。根据实际测试，无法有效支持10亿以上的小文件存储。 3.4 客户端写入规范解决小文件问题的最简单方法就是在生成阶段就避免小文件的产生。 3.4.1 Hive 写入优化Hive SQL执行的背后实际是Mapreduce任务，所有优化涉及大量小文件读优化、总结过程小文件优化、输出小文件邮件。本次只介绍最后一种优化场景。主要优化目标就是减少Reduce数量和增加Reduce处理能力，涉及参数有： 123456# 设置reduce的数量set mapred.reduce.tasks = 1;# 每个reduce任务处理的数据量，默认为1Gset hive.exec.reducers.bytes.per.reducer = 1000000000# 每个任务最大的reduce数set hive.exec.reducers.max = 50 还可以对输出数据进行压缩，涉及参数有： 12345678# 开启hive最终输出数据压缩功能set hive.exec.compress.output=true;# 开启mapreduce最终输出数据压缩set mapreduce.output.fileoutputformat.compress=true;# 设置数据输出压缩方式set mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodec; # 设置mapreduce最终数据输出压缩为块压缩set mapreduce.output.fileoutputformat.compress.type=BLOCK; 另外就是输出结果进行归档，这在前文已介绍。 3.5 合并小文件系统3.5.1 离线合并HDFS提供命令将hdfs多个文件合并后下载到本地文件系统，然后可以将文件重新上传HDFS文件系统。 1hdfs dfs -getmerge hdfs文件1 hdfs文件2 hdfs文件3 输出本地文件名 这显然是低效的。 3.5.2 脚本实现当HDFS文件系统由于疏忽已经受到小文件系统侵扰时，就需要我们被动治理了。通常的做法是检查所有文件系统，并确认哪些文件夹中的小文件过年需要合并。可以通过自定义的脚本或程序。例如通过调用HDFS的sync()方法和append()方法，将小文件和目录每隔一定时间生成一个大文件，或者可以通过写程序来合并这些小文件。 这里推荐一个开源工具File Crush。 1https://github.com/edwardcapriolo/filecrush/ 另外还有文章《分析hdfs文件变化及监控小文件》可以参考。 3.5.3 hive分区文件合并当hive分区数据不再写入时，可以对历史数据进行合并。命令(关键字为：CONCATENATE)如下： 1alter table schema.table PARTITION(PartitionColumn=&#123;PartitionName&#125;) CONCATENATE; 上面的语句实现了表schema.table(其中schema为库名，table为表名)。命令将表中PartitionName分区名(其中PartitionColumn为分区列名)中小文件进行合并。 3.6 HDFS项目解决方案前文小文件治理方案虽然能够解决小文件的问题，但是这些方法都有不足或成本较高。那就需要在架构设计进行根本优化解决，目前 Hadoop 社区已经有很多相应的讨论和架构规划。例如下面的提案： HDFS-7836(将BlocksMap放在堆外) ，未实现； HDFS-8286 (将元数据保存在LevelDB)，已实现； HDFS-8998(一个Block对应多个文件)，未实现； 但是提案进度堪忧呀，大部分还处于讨论或推进一部分就没后续了…….，所以提出问题简单，解决问题难呀。扯远了，本文只挑两个简单看一下。 3.6.1 HDFS-8998目前HDFS版本中一个Block只能对应一个文件。社区和项目组考虑：能否一个Block能对应多个小文件。这就是解决提案：《Small files storage supported inside HDFS》，提案编号：HDFS-8998，提案参考说明文档。 主要的设计目标为： 为小文件设置单独的区域，称为Small file zone，用于小文件创建和写操作; NameNode 保存一个固定大小的 Block List，列表的大小是可以配置的； 当client1第一次向 NameNode 发出写请求时，NameNode 将为client1创建第一个 blockid，并锁定这个Block； 当其他客户端client2向 NameNode 发出写请求时，NameNode 将尝试为其分配未锁定的块（unlocked block），如果没有，并且现有的块数小于Block List，那么 NameNode 则为client2分配创建新的 blockid ，并且同时锁定。 其他客户端Client3...client N类似； 客户端如果获取不到未锁定的块资源，并且也不能新建（Block List资源上限）。这时候需要客户端等待其他客户端释放Block资源。 客户端写数据是将数据追加(appenging)到块上； 当客户端的读写（OutputStream）关闭，被客户端占用的Block将被释放； 当某个块被写满，也会分配新的一个块给客户端。这个写满的Block将从Block List中移除，同时会添加新的Block资源进行资源List。 新的设计中一个 Block 将包含多个文件。需要新的文件操作设计： 读取，读写小文件和平常读取hdfs文件类似。 删除，新的设计小文件是 Block 的一部分（segment），所以删除操作不能直接删除一个 Block。删除操作调整为：从 NameNode 中的 BlocksMap 删除 INode；然后当这个块中被删除的数据达到一定的阈值（可配置） ，对应的块将会被重写。 append 和 truncate，对小文件的 truncate 和 append 是不支持的，因为这些操作代价非常高，而且是不常用的。会增加后台进程对Block的小文件segment进行合并（segment合并触发数量可配置） 高可用：继承原架构的副本机制； 3.6.2 HDFS-8286提案：《Scaling out the namespace using KV store》，提案编号：HDFS-8286。该提案目标调整元数据的存储形式，从内存调整外置存储( KV存储系统)。现HDFS中以层次结构的形式来管理文件和目录，文件和目录表示为inode 对象。调整为KV存储，核心解决的问题是设计合适数据结构将元数据以KV格式存储。 详细设计就不展开了，可以参考提示设计说明文档。 3.6.3 Hadoop Ozone项目如果线上的业务数据是非结构化的小数据对象，例如海量图片（如银行业务保存的文件影像数据）、音频、小视频等。这种类型数据可以有适合的存储方式，对象存储。而Hadoop生态圈也正好有个项目。 Ozone 是 Hortonworks 基于 HDFS 实现的对象存储，OZone与HDFS有着很深的关系，在设计上也对HDFS存在的不足做了很多改进，使用HDFS的生态系统可以无缝切换到OZone，参考提案: HDFS-7240。 目前项目已经成为 Apache Hadoop的子项目。已经告别alpha版本阶段，最新的Release 1.1.0 版本已发布。 Ozone 和HDFS相同，也是采用 Master/Slave 架构，但是对管理服务namespace和BlockManager进行拆分，将元数据管理分成两个，一个是 Ozone Manager 作为对象存储元数据服务，另一个是 StorageContainerManager，作为存储容器管理服务。 另外Ozone Manager和StorageContainerManager的元数据都是使用RocksDB 进行单独存储，而不是放在NameNode内存中，架构上不再被堆内存限制，可以横向扩展。 第四部分 总结 技术没有银弹，只有合适的技术 实际生产环境面对HDFS小文件问题，需要提前管控业务写入，优化写入的客户端程序。为业务数据选择合适的数据存储方式，因地适宜。 追根溯源 当我们面多小文件的问题时，需要检查业务数据处理流，定位小文件产生的根因。 量力而行 需要评估企业大数据团队的技术能力。如果没有能力对原架构进行二次开发优化，就需要编写一些自定义程序来处理小文件。尽量摸透开源架构的特性，做好参数优化。 参考文献及资料[1] HDFS NameNode内存全景，链接：https://tech.meituan.com/2016/08/26/namenode.html [2] The Small Files Problem，链接：https://blog.cloudera.com/the-small-files-problem/ [3] HDFS Federation在美团点评的应用与改进，链接：https://tech.meituan.com/2017/04/14/hdfs-federation.html [4]Introducing Apache Hadoop Ozone: An Object Store for Apache Hadoop,链接：https://blog.cloudera.com/introducing-apache-hadoop-ozone-object-store-apache-hadoop/]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS小文件治理总结]]></title>
    <url>%2F2020%2F09%2F12%2F2020-09-13-HDFS%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 参考文献及资料 背景https://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html 参考文献及资料]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[布隆过滤算法总结]]></title>
    <url>%2F2020%2F09%2F12%2F2020-09-12-%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 参考文献及资料 背景在数据处理中，我们经常有这样的需求，判断某个元素是否在一个指定集合中。最朴素的处理方法是首先存储指定集合中数据，然后查找集合中数据，如有数据和查找元素相等即归属于该集合。 在数学中，如果只利用集合的定义属性，查找只能通过穷举遍历。如果集合元素较大，势必会影响查找的性能。 这时候对于集合数据类型使用特殊的数据架构实现，这就是Hash表（散列表）。集合数据结构由三个部分组成： 数据坐标集合B。 原始数据集合A。 Hash Function。hash函数是原始集合A到坐标集合的映射。 在这种数据结构下，判断元素是否属于指定集合，只需要计算该元素在hash函数作用下的坐标值。而这个坐标设置为数组的坐标，通过数组坐标就可以获取这个地址空间存储的值，最后判断是否和元素相等。 上面是最朴素的原理，在实际中由于hash函数的特点，存在集合A中不同值在hash函数映射下，坐标值可能是相同的，这就是hash冲突。具体细节后续介绍。 Hash表的实现本质思想是“空间换取时间”，对于特别大的集合，这种换取需要海量的内存存储空间。例如经常列举的案例就是垃圾邮件过滤场景。 引用自吴军的《数学之美》。Yahoo, Hotmail 和 Gmail 等公众电子邮件提供商，需要过滤垃圾邮件。一个办法就是记录下那些发垃圾邮件的 email 地址。由于那些发送者不停地在注册新的地址，全世界少说也有几十亿个发垃圾邮件的地址，将他们都存起来则需要大量的存储。如果用哈希表，每存储一亿 个 email 地址， 就需要 1.6 GB 的内存（用哈希表实现的具体办法是将每一个 email 地址对应成一个八字节的信息指纹， 然后将这些信息指纹存入哈希表，由于哈希表的存储效率一般只有 50%，因此一个 email 地址需要占用十六个字节。一亿个地址大约要 1.6 GB内存资源）。因此存储几十亿个邮件地址可能需要上百 GB 的内存。 那么我们重新分析一下我们业务场景，实际核心需求是：判断元素是否重复，并不需要存储集合中具体数据。 数据坐标集合B，在数学中我们有很多实现方式（空间坐标等）。但是在计算机科学中，我们需要使用基础数据结构和运算方式。布隆（Burton Howard Bloom）在1970年提出布隆过滤器算法，算法中使用位阵列（Bit Array）作为坐标集合。接下来我们将详细介绍。 第一部分 Hash函数1.1 Hash函数定义Hash函数，通常音译为哈希函数（也有翻译成：散列函数）。Hash函数首先是数学意义上的函数：$$Hash\ Function\ F：A-&gt;B$$其中定义域集合A是不等长的字符串集合，而值域集合B中字符串是固定长度。 理论上满足这样的函数的海量的，我们在挑选好的Hash函数时，通常有下面的标准： Hash函数在计算值域的时候是高效的（对于长度为n的字符串计算时间复杂度应为O(n)）。 确定性。对于任何给定的输入，哈希函数必须始终给出相同的结果。即函数值的确定性。 Hash碰撞概率低。由于值域集合中字符串是固定长度的，那么肯定是一个有限集合。例如SHA256算法值域大小（集合的势）为2^256。在我们进行2^256+1次输入时，必然会发生一次碰撞（$$x!=y,F(x)=F(y)$$）。所以选取的Hash函数针对具体场景，需要具有较低的碰撞概率。 隐蔽性。通俗的讲就是不能通过函数值F(x)，反向计算出x。这就是计算理论和密码学中单向函数概念。由于这个特性Hash函数大量应用于加密场景。 值域集合分布均匀。谈到分布那么值域空间就引入了距离的概念。通俗的讲就是点与点之间打散在空间中，没有聚集现象。 Hash函数将一个空间A中的数据映射到另一个空间B（坐标空间）中数据，通常集合B小于集合A（集合的势）。数据空间A定义成Hash表，在数据初始化和插入过程需要计算Hash坐标，并存储。这就完成了将计算时间（或计算消耗）转换成存储空间的思想。 1.2 Hash函数种类通常按照Hash函数的实现原理分为：加法Hash、位运算Hash、乘法Hash、除法Hash、查表Hash、混合Hash。 1.2.1 加法HashHash将字符串字符相加并处理后形成结果。下面案例同余质数。 123456789101112public static void main(String[] args) &#123; System.out.println(additiveHash("Secure Hash Algorit",19)); // 输出3&#125;public static int additiveHash(String key, int prime)&#123; int hash,i; for(hash=key.length(),i=0;i&lt;key.length();i++)&#123; hash+=key.charAt(i); &#125; return hash%prime;&#125; 1.2.2 位运算Hash这类型Hash函数通过利用各种位运算（常见的是移位和异或等）来充分的变换输入元素。 1234567891011public static void main(String[] args) &#123; System.out.println(rotatingHash("Secure Hash Algorit",19)); //6&#125;public static int rotatingHash(String key,int prime)&#123; int hash,i; for(hash = key.length(),i=0;i&lt;key.length();i++)&#123; hash=(hash&lt;&lt;1)^(hash&gt;&gt;10)^key.charAt(i)&amp;key.charAt(i); &#125; return hash%prime;&#125; 1.2.3 乘法Hash这种类型的Hash函数利用了乘法的不相关性. 123456789101112public static void main(String[] args) &#123; System.out.println(bernstein("Secure Hash Algorit")); //361558494&#125; public static int bernstein(String key)&#123; int hash=0; int i; for(i=0;i&lt;key.length();i++)&#123; hash=hash*10+key.charAt(i); &#125; return hash;&#125; 1.2.4 除法Hash因为除法太慢，这种方式几乎找不到真正的应用。 1.2.5 查表Hash查表Hash中有名的例子有：Universal Hashing和Zobrist Hashing。他们的表格都是随机生成的。 1.2.6 混合Hash混合Hash算法利用了以上各种方式。各种常见的Hash算法，比如MD5、Tiger都属于这个范围。它们一般很少在面向查找的Hash函数里面使用。 1.3 Hash函数应用Hash函数主要应用有： 安全加密 密钥加密通常使用MD5和SHA系列算法。SHA系列有五个算法，分别是 SHA-1、SHA-224、SHA-256、SHA-384，和SHA-512。后四者有时并称为 SHA-2。SHA-1在许多安全协定中广为使用，包括 TLS/SSL 等，是 MD5 的后继者。 SHA-256可能是所有加密哈希函数中最著名的，因为它已在区块链技术中广泛使用。中本聪的原始比特币协议中使用了它。 唯一标识 文件之类的二进制数据做 md5 处理，作为唯一标识，这样判定重复文件的时候更快捷。 数据校验 比如从网上下载的很多文件（尤其是P2P站点资源），都会包含一个 MD5 值，用于校验下载数据的完整性，避免数据在中途被劫持篡改。 分布式缓存 分布式缓存和其他机器或数据库的分布式不一样，因为每台机器存放的缓存数据不一致，每当缓存机器扩容时，需要对缓存存放机器进行重新索引（或者部分重新索引），这里应用到的也是哈希算法的思想。 负载均衡 对于同一个客户端上的请求，尤其是已登录用户的请求，需要将其会话请求都路由到同一台机器，以保证数据的一致性，这可以借助哈希算法来实现，通过用户 ID 尾号对总机器数取模（取多少位可以根据机器数定），将结果值作为机器编号。 第二部分 布隆过滤2.1 原理前文我们讨论了“判断某个元素是否在一个指定集合中”问题的实现思路。在数据量较小的情况下，我们可以使用经典的HashMap数据结构。对于大量数据场景下，布隆（Burton Howard Bloom）在1970年提出布隆过滤器算法。 2.2 Java实现2.2 布隆过滤的应用Google 著名的分布式数据库 Bigtable 使用了布隆过滤器来查找不存在的行或列，以减少磁盘查找的IO次数［3］。 Squid 网页代理缓存服务器在 cache digests 中使用了也布隆过滤器［4］。 Venti 文档存储系统也采用布隆过滤器来检测先前存储的数据［5］。 SPIN 模型检测器也使用布隆过滤器在大规模验证问题时跟踪可达状态空间［6］。 Google Chrome浏览器使用了布隆过滤器加速安全浏览服务［7］。 参考文献及资料1、数学之美二十一：布隆过滤器（Bloom Filter）：http://www.google.com.hk/ggblog/googlechinablog/2007/07/bloom-filter_7469.html]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[布隆过滤算法总结]]></title>
    <url>%2F2020%2F09%2F12%2F2020-09-13-Yarn%E9%9B%86%E7%BE%A4%E8%B0%83%E5%BA%A6%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 参考文献及资料 背景https://blog.csdn.net/zhanyuanlin/article/details/78799341 https://blog.csdn.net/zhanyuanlin/article/details/78799131 https://blog.csdn.net/u010770993/article/details/70312473 参考文献及资料1、数学之美二十一：布隆过滤器（Bloom Filter）：http://www.google.com.hk/ggblog/googlechinablog/2007/07/bloom-filter_7469.html]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[操作系统三种IP区别总结]]></title>
    <url>%2F2020%2F09%2F03%2F2020-10-01-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B8%89%E7%A7%8DIP%E5%8C%BA%E5%88%AB%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 三种IP介绍 第二部分 localhost 参考文献及资料 背景在我们配置服务监听地址的时候，会面对服务器上三种”IP“: 本机IP。例如：192.168.1.1、8.8.8.8； 0.0.0.0地址； 127.0.0.1地址； 那么这三种在使用上有哪些区别呢？本文将总结介绍。 第一部分 三种IP介绍1.1 本机IP对于linux系统，我们输入ifconfig命令会回显一个eth0网卡信息： 12345678eth0 Link encap:Ethernet HWaddr 98:3f:9f:18:25:97 inet addr:192.168.1.3 Bcast:192.168.1.255 Mask:255.255.255.0 inet6 addr: fe80::519e:af46:9443:4f0d/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:431456 errors:0 dropped:0 overruns:0 frame:0 TX packets:248421 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:405352090 (405.3 MB) TX bytes:35380000 (35.3 MB) 这就是有线网卡的信息。如果有无线网卡还会有一个wlan网卡信息。这是局域网给计算机分配的局域网唯一的IP。如果计算机直接接入互联网，会分配一个公网IP（互联网唯一IP）。 这个IP用于和局域网或互联网中其他机器通信的唯一IP，即网络中唯一标识。 1.2 0.0.0.0地址0.0.0.0表示“本地计算机上的所有IP地址”（所有IPv4地址）。因此，如果服务器有两个IP地址:192.168.1.1和8.1.2.1。如果这时候如果我们在服务器上开启一个http服务（端口8080），配置文件中监听地址为：0.0.0.0。那么我们可以通过下面两个url访问服务： http://192.168.1.1:8080 http://8.1.2.1:8080 但是我们如果监听地址配置成192.168.1.1，那么http://8.1.2.1:8080就无法访问服务了。 1.3 127.0.0.1地址对于linux系统，我们输入ifconfig命令会回显一个lo网卡信息： 12345678lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:415 errors:0 dropped:0 overruns:0 frame:0 TX packets:415 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:37794 (37.7 KB) TX bytes:37794 (37.7 KB) 127.0.0.1地址分配给 loopback 接口，也称为回环地址(loopback address)。用来测试本机的 TCP/IP 协议栈。loopback是一个特殊的网络接口(可理解成虚拟网卡)，用于本机中各个应用之间的网络交互。只要操作系统的网络组件是正常的，loopback 就能工作。 如果我们服务监听的地址配置成127.0.0.1，那么只能本机服务能访问了。 第三部分 localhostlocalhost是一个域名，而不是一个ip地址。localhost域名配置在本地DNS中，即/etc/hosts文件中定义，例如： 1127.0.0.1 localhost 例如我们ping localhost，就会解析成127.0.0.1地址。 12root@deeplearning:# ping localhostPING localhost (127.0.0.1) 56(84) bytes of data. 如果我们服务监听的地址配置成localhost，那么会根据/etc/hosts中的配置，解析成127.0.0.1地址。这时候也只能本机服务能访问了。 参考文献及资料1、0.0.0.0地址，链接：https://en.wikipedia.org/wiki/0.0.0.0 2、IP地址，链接：https://zh.wikipedia.org/zh/IP%E5%9C%B0%E5%9D%80]]></content>
      <categories>
        <category>IP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Maven项目中resources配置总结]]></title>
    <url>%2F2020%2F09%2F03%2F2020-09-03-Maven%E9%A1%B9%E7%9B%AE%E4%B8%ADresources%E9%85%8D%E7%BD%AE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 基本配置介绍 第二部分 具体配置和注意事项 第三部分 读取resources资源 参考文献及资料 背景通常Maven项目的文件目录结构如下： 1234567891011121314151617181920212223242526# Maven项目的标准目录结构src main java #源文件 resources #资源文件 filters #资源过滤文件 config #配置文件 scripts #脚本文件 webapp #web应用文件 test java #测试源文件 resources #测试资源文件 filters #测试资源过滤文件 it #集成测试 assembly #assembly descriptors site #Sitetarget generated-sources classes generated-test-sources test-classes xxx.jarpom.xmlLICENSE.txtNOTICE.txtREADME.txt 其中src/main/resources和src/test/resources是资源文件目录。本文将详细介绍资源文件相关的配置。 第一部分 基本配置介绍我们在使用Maven组件来构建项目的时候，通常将配置文件放在资源文件目录下。针对这个目录，在pom.xml文件进行了定义，我们首先看一个案例： 123456789101112131415161718&lt;build&gt;&lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;includes&gt; &lt;include&gt;application.properties&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;excludes&gt; &lt;exclude&gt;application.properties&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt;&lt;/resources&gt;&lt;/build&gt; 标签&lt;directory&gt;指定资源文件目录； 标签&lt;include&gt;指定资源文件目录中，哪些文件被打包。 标签&lt;excludes&gt;指定资源文件目录中，哪些文件不被打包。 特别的，标签&lt;filtering&gt;是一个bool值，默认值为false。在maven资源文件中，支持使用变量placeholder，例如资源文件： 123# application.propertiesapplication.user=$&#123;username&#125;application.password=$&#123;password&#125; 文件中使用${keyword}占位符来标识变量。这时候可以在pom.xml文件中定义变量的取值： 1234&lt;properties&gt; &lt;username&gt;mysql&lt;/username&gt; &lt;password&gt;password123&lt;/password&gt;&lt;/properties&gt; 如果需要对配置文件中变量进行替换实际值，就需要开启&lt;filtering&gt;，该值设置为true。 第二部分 具体配置和注意事项2.1 案例说明根据上面的介绍，最开始例子中有两段resource的配置描述，分别的含义为： 第一个配置的含义是：在配置文件目录src/main/resources过滤掉其他文件，只保留application.properties文件。并且开启filtering变量替换属性。 第二个配置的含义是：在配置文件目录src/main/resources过滤掉application.properties文件，其他文件均保留。并且关闭filtering变量替换属性。 需要特别注意的是，这里两个&lt;resources&gt;都是对资源目录&lt;src/main/resources&gt;的配置定义，一个是保留application.properties，一个是去除application.properties。这样两个配置会不会冲突？实际上两个配置是兼容。最后是取两个配置分别过滤的文件集合的并集。 可以看一下例子，资源目录src/main/resources里面有三个文件： 123application.ymlapplication.propertiesapplication.xml 编译后，target/classes路径中三个配置文件都是有的。第一配置文件过滤后文件集合为{application.properties}，第二个配置过滤后的集合为{application.yml,application.xml},最后取并集就得到了最后编译结果。 2.2 正则过滤在对资源目录中文件进行过滤时，还支持正则表达式。例如： 1&lt;include&gt;**/*.xml&lt;/include&gt; 这个表达式表示包含了资源目录下面所有xml文件（以及子目录下面）。 2.3 变量占位符这里主要指的是&lt;filtering&gt;的功能。例如下面的xml文件定义了一个研发&lt;profile&gt;。 12345678910111213&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;properties&gt; &lt;resource.delimiter&gt;$&#123;&#125;&lt;/resource.delimiter&gt; &lt;username&gt;mysql&lt;/username&gt; &lt;password&gt;password123&lt;/password&gt; &lt;/properties&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;/profile&gt; &lt;/profiles&gt; 配置中定义的username和password两个变量的值。使用package -P dev编译后，配置文件中占位符变量被替换： 12application.user=mysqlapplication.password=password123 需要注意的是这里增加了&lt;resource.delimiter&gt;标签配置，定义了占位符的格式。有些时候其他依赖包的pom文件也会指定占位符的格式，就会造成格式不统一。例如：spring boot把默认的占位符号${}改成了@var@。所以建议进行配置，否则容易环境”污染”。 2.4 关于一个错误观点的说明有很多关于这个主题的文章（例如CSND）中，认为同一个&lt;resource&gt;中，若是&lt;include&gt;和&lt;exclude&gt;都存在的话，那就发生冲突了，这时会以&lt;exclude&gt;为准。 关于这个论点，笔者实际做了实验，同一个&lt;resource&gt;中，同时配置了&lt;include&gt;和&lt;exclude&gt;。 1234567891011121314&lt;build&gt;&lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;includes&gt; &lt;include&gt;application.properties&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;exclude&gt;application.properties&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt;&lt;/resources&gt;&lt;/build&gt; 编译结果，配置文件没有打包进入target/classes。说明这个论点是有问题的。说明在同一个resource中两种配置是取交集的。 2.5 子目录资源目录也是支持子目录的。即可以在资源目录下面创建子目录，在打包过程中会保留子目录结构。例如： 123456resources -test --app.xml -application.yml -application.properties -application.xml 在项目编译后，如果子目录中资源文件被保留，那么子目录的结构也是保留的。例如： 1234567target -classes --test ---app.xml -application.yml -application.properties -application.xml 第二部分 读取resources资源例如我们的配置文件properties类型的配置文件，可以使用下面的语句进行读取： 方法1，从编译后的整个classes目录下去找； 1InputStream is = this.getClass().getResourceAsStream("/" +application.properties); 方法2，ClassLoader从整个classes目录找； 1InputStream is = this.getClass().getClassLoader().getResourceAsStream(application.properties); 读取使用Java的工具包java.util.Properties： 12345678import java.util.Properties;Properties properties = new Properties();InputStream is = this.getClass().getClassLoader().getResourceAsStream(application.properties);properties.load(is)//获取配置文件中name的配置值System.out.println(properties.get(getProperty("name"))) 其他类型的配置文件读取读者可以执行查找资料。 参考文献及资料1、Maven Resources Plugin，链接：https://maven.apache.org/components/plugins-archives/maven-resources-plugin-2.6/ 2、Maven资源过滤的配置，链接：http://c.biancheng.net/view/5285.html]]></content>
      <categories>
        <category>Maven</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring框架下Kafka交互总结]]></title>
    <url>%2F2020%2F09%2F03%2F2020-09-03-Spring%E6%A1%86%E6%9E%B6%E4%B8%8BKafka%E4%BA%A4%E4%BA%92%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 参考文献及资料 背景Java中与Kafka交互的API类包有： Kafka client Spring for Apache Kafka Spring Integration Kafka Spring Cloud stream binder Kafka 本文主要介绍Spring for Apache Kafka的使用。 第一部分 项目配置1.1 依赖将spring-kafka依赖项添加到项目中的pom.xml中： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;2.3.7.RELEASE&lt;/version&gt;&lt;/dependency&gt; 需要注意的是：Apache Kafka的Spring基于Java kafka-clientsjar，需要小心依赖兼容性。以下是兼容性对照表： Spring for Apache Kafka Version Spring Integration for Apache Kafka Version kafka-clients Spring Boot 2.6.0 5.3.x or 5.4.0-SNAPSHOT (pre-release) 2.6.0 2.3.x or 2.4.0-SNAPSHOT (pre-release) 2.5.x 3.3.x 2.5.0 2.3.x 2.4.x 3.2.x 2.4.1 2.2.x 2.3.x 3.2.x 2.3.1 2.2.x 2.2.x 3.1.x 2.0.1, 2.1.x, 2.2.x 2.1.x 2.1.x 3.0.x 1.0.2 2.0.x (End of Life) 1.3.x 2.3.x 0.11.0.x, 1.0.x 1.5.x (End of Life) 1.2 Kafka配置在src/mian/java/resources中创建配置文件application.yml: 12345678910111213server: port: 9000spring: kafka: consumer: bootstrap-servers: localhost:9092 group-id: group_id auto-offset-reset: earliest key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer producer: bootstrap-servers: localhost:9092 key-serializer: org.apache.kafka.common.serialization.StringSerializer value-serializer: org.apache.kafka.common.serialization.StringSerializer 第二部分 Kafka生产消费2.1 Kafka生产者Spring for Apache Kafka 2.2 Kafka消费者参考文献及资料1、Spring for Apache Kafka，链接：https://spring.io/projects/spring-kafka]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[提交Spark任务到kerberos安全集群]]></title>
    <url>%2F2020%2F09%2F03%2F2020-09-03-%E6%8F%90%E4%BA%A4Spark%E4%BB%BB%E5%8A%A1%E5%88%B0kerberos%E5%AE%89%E5%85%A8%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景在生产Hadoop集群上，通常配置了kerberos认证服务，需要通过认证后，才能和集群中其他服务进行交互。例如像安全集群提交Spark任务。 第一部分 实现方式1123UserGroupInformation.setConfiguration(SparkHadoopUtil.get().newConfiguration(sparkConfiguration)); Credentials credentials = UserGroupInformation.getLoginUser().getCredentials(); SparkHadoopUtil.get().addCurrentUserCredentials(credentials); 第二部分 实现方式212345SparkConf sparkConfiguration = new SparkConf();sparkConfiguration.set("spark.hadoop.hadoop.security.authentication", "kerberos");sparkConfiguration.set("spark.hadoop.hadoop.security.authorization", "true");sparkConfiguration.set("spark.hadoop.dfs.namenode.kerberos.principal","hdfs/_HOST@EXAMPLE.COM");sparkConfiguration.set("spark.hadoop.yarn.resourcemanager.principal", "yarn/&lt;resource-manager-host-name&gt;@EXAMPLE.COM"); 参考文献及资料]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring框架下Kafka交互总结]]></title>
    <url>%2F2020%2F09%2F03%2F2020-10-01-Java%E4%B8%AD%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 参考文献及资料 背景https://www.cnblogs.com/qlqwjy/p/9275415.html参考文献及资料1、Spring for Apache Kafka，链接：https://spring.io/projects/spring-kafka]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java中定时任务]]></title>
    <url>%2F2020%2F09%2F03%2F2020-09-17-Java%E4%B8%AD%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景参考文献及资料1、https://www.cnblogs.com/wenbronk/p/6433178.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Maven项目中读取properties配置文件总结]]></title>
    <url>%2F2020%2F09%2F03%2F2020-09-03-Maven%E9%A1%B9%E7%9B%AE%E4%B8%AD%E8%AF%BB%E5%8F%96properties%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 读取properties配置文件方法 参考文献及资料 背景在java开发中，通常将配置信息存储在特定的配置文件中，而不是内嵌在程序代码中。即代码可配置化。properties文件以key=value键值对形式表达，结构比较简单，但是难以表达层次，适合小型项目。本文总结汇总了读取properties配置文件方法。 第一部分 读取properties配置文件方法为了讲解方便，我们在maven项目的资源目录resources中创建配置文件application.properties。文件内容为： 12username=apppassword=password123 1.1 基于ClassLoder的getResourceAsStream方法读取配置文件本方法基于ClassLoder的getResourceAsStream方法，通过类加载器来定位资源，返回InputStream后用Properties对象进行加载。 123456789public class configRead &#123; public static void main(String[] args) throws IOException &#123; InputStream in = configRead.class .getClassLoader().getResourceAsStream("application.properties"); Properties properties = new Properties(); properties.load(in); System.out.println(properties.getProperty("username")); &#125;&#125; 1.2 基于getResourceAsStream()方法读取配置文件利用class的getResourceAsStream方法来定位资源文件，并且直接返回InputStream对象，然后通过Properties进行加载。 12345678910public class configRead &#123; public static void main(String[] args) throws IOException &#123; InputStream In =configRead.class .getResourceAsStream("application.properties"); Properties properties = new Properties(); properties.load(in); System.out.println(properties.getProperty("username")); //app &#125;&#125; 1.3 基于ClassLoader类的getSystemResourceAsStream()静态方法读取配置文件使用ClassLoader的getSystemResourceAsStream()静态方法来定位资源，并且返回InputStream，最后用Properties来加载。 123456789public class configRead &#123; public static void main(String[] args) throws IOException &#123; InputStream in = ClassLoader .getSystemResourceAsStream("application.properties"); Properties properties = new Properties(); properties.load(in); System.out.println(properties.getProperty("username")); &#125;&#125; 1.4 基于FileInputStream读取配置文件这种方法通过类的路径来定位properties文件资源的路径，然后通过FileInputStream读取流，最后通过java.util.Properties类的load()方法来加载数据。 1234567891011121314public class configRead &#123; public static void main(String[] args) throws IOException &#123; URL url = configRead.class.getClassLoader() .getResource("application.properties"); if (url != null) &#123; String fileName = url.getFile(); InputStream in = new BufferedInputStream( new FileInputStream(fileName)); Properties properties = new Properties(); properties.load(in); System.out.println(properties.getProperty("username")); &#125; &#125;&#125; 1.5 基于ResourceBundle读取配置文件利用ResourceBundle来读取properties文件。 12345678public class configRead &#123; public static void main(String[] args) throws IOException &#123; ResourceBundle resourceBundle = ResourceBundle .getBundle("application.properties", locale1); System.out.println(resourceBundle.getString("username")); &#125;&#125; 1.6 基于PropertyResourceBundle读取配置文件PropertyResourceBundle是ResourceBundle的子类，同样我们也可以利用PropertyResourceBundle来加载配置文件的数据，具体的示例如下： 12345678910public class configRead &#123; public static void main(String[] args) throws IOException &#123; URL url = configRead.class.getClassLoader().getResource("application.properties"); if (url != null) &#123; InputStream in = new BufferedInputStream(new FileInputStream(url.getFile())); ResourceBundle resourceBundle = new PropertyResourceBundle(in); System.out.println(resourceBundle.getString("username")); &#125; &#125;&#125; 参考文献及资料1、Properties files，链接：https://commons.apache.org/proper/commons-configuration/userguide/howto_properties.html]]></content>
      <categories>
        <category>Maven</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Maven项目中父子模块依赖包配置总结]]></title>
    <url>%2F2020%2F09%2F03%2F2021-07-23-Maven%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%88%B6%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BE%9D%E8%B5%96%E5%8C%85%E9%85%8D%E7%BD%AE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 读取properties配置文件方法 参考文献及资料 背景项目越来越趋向模块化开发，使用maven构建工程，必然涉及到父子pom的关联，父pom文件的父级又会继承springboot项目，就这样在开发中踩坑不少，简单记录一下。 看问题之前先了解maven中的两个标签和，明白的直接跳过。 maven标签1、这里其实是起到管理依赖jar版本号的作用，一般只会在项目的最顶层的pom.xml中使用到，所有子module如果想要使用到这里面声明的jar，只需要在子module中添加相应的groupId和artifactId即可，并不需要声明版本号，需要注意的是这里面只是声明一个依赖，并不是真实的下载jar，只有在子module中使用到，才会去下载依赖。 2、我们是这里引入了一个jar包之后，这里如果没有加上version版本号的话，那么maven就会去里找对应groupId和artifactId的jar,如果有就继承他，如果没有就会报错，这时候其实在我们配置的本地仓库中会真实的下载对应的jar包，这时候所有的子module都会默认继承这里面所有声明的jar。 总的来说，就是在中声明依赖和版本号，该标签中的依赖不会被子模块继承，仅仅是声明，子pom中直接引入依赖，具体的版本号会在父子中去找。 父pom的packaging都是pom，子项目pom的packaging都是jar。关于在父子配置pom的引用有两种方案，这里以springboot项目为例说明问题。 第一种pom配置我们希望在父pom中引入相关依赖，都记录在下，子模块直接继承父pom的依赖，在子模块中开发中就不必再去引入依赖，但在项目中有模块可能就是单一的工具包，它并不需要springboot的依赖，这时候启动就会冲突。可以这样解决，在父pom中定义springboot版本号，子模块作为项目启动的模块配置springboot插件依赖，普通的dao，serivce，common不必引入。如下配置文件，文件中只列举个别依赖包，重在说明问题： 父pom.xml配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.demo&lt;/groupId&gt; &lt;artifactId&gt;demo-api&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;modules&gt; &lt;module&gt;demo-web&lt;/module&gt; &lt;module&gt;demo-common&lt;/module&gt; &lt;module&gt;demo-service&lt;/module&gt; &lt;/modules&gt; &lt;properties&gt; &lt;spring-boot.version&gt;2.1.8.RELEASE&lt;/spring-boot.version&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;fastjson.version&gt;1.2.47&lt;/fastjson.version&gt; &lt;pagehelper.version&gt;5.1.6&lt;/pagehelper.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-boot.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;!--这两个依赖都将被子模块继承--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;$&#123;fastjson.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;$&#123;pagehelper.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;8&lt;/source&gt; &lt;target&gt;8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;properties&gt; &lt;package.environment&gt;dev&lt;/package.environment&gt; &lt;/properties&gt; &lt;!-- 是否默认 true表示默认--&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;/profile&gt; &lt;profile&gt; &lt;!-- 测试环境 --&gt; &lt;id&gt;test&lt;/id&gt; &lt;properties&gt; &lt;package.environment&gt;test&lt;/package.environment&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;profile&gt; &lt;!-- 生产环境 --&gt; &lt;id&gt;prod&lt;/id&gt; &lt;properties&gt; &lt;package.environment&gt;prod&lt;/package.environment&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/project&gt; 普通子模块pom.xml配置： 1234567891011121314151617&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;parent&gt; &lt;artifactId&gt;demo-api&lt;/artifactId&gt; &lt;groupId&gt;com.demo&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.demo.common&lt;/groupId&gt; &lt;artifactId&gt;demo-common&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;demo-common&lt;/name&gt; &lt;description&gt;demo-common project&lt;/description&gt; &lt;packaging&gt;jar&lt;/packaging&gt;&lt;/project&gt; 启动类子模块pom.xml配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;com.demo&lt;/groupId&gt; &lt;artifactId&gt;demo-api&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;com.demo.web&lt;/groupId&gt; &lt;artifactId&gt;demo-web&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;demo-web&lt;/name&gt; &lt;description&gt;demo-web project&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!--引入子模块相关jar --&gt; &lt;dependency&gt; &lt;groupId&gt;com.demo.common&lt;/groupId&gt; &lt;artifactId&gt;demo-common&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.myway.service&lt;/groupId&gt; &lt;artifactId&gt;share-read-service&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;!--重要 如果不设置resource 会导致application.yaml中的@@找不到pom文件中的配置--&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;plugins&gt; &lt;!--仅在启动项目中引入springboot插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 第二种pom配置将所有的依赖在父pom的中声明，子模块把需要的都引入一遍： 父pom.xml配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.demo&lt;/groupId&gt; &lt;artifactId&gt;demo&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;druid.version&gt;1.1.14&lt;/druid.version&gt; &lt;pagehelper.boot.version&gt;1.2.5&lt;/pagehelper.boot.version&gt; &lt;fastjson.version&gt;1.2.70&lt;/fastjson.version&gt; &lt;/properties&gt; &lt;!-- 依赖声明 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- SpringBoot的依赖配置--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.8.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!--阿里数据库连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;druid.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- pagehelper 分页插件 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;pagehelper.boot.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;modules&gt; &lt;module&gt;demo-web&lt;/module&gt; &lt;module&gt;demo-common&lt;/module&gt; &lt;/modules&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;dependencies&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;$&#123;java.version&#125;&lt;/source&gt; &lt;target&gt;$&#123;java.version&#125;&lt;/target&gt; &lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;public&lt;/id&gt; &lt;name&gt;aliyun nexus&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;public&lt;/id&gt; &lt;name&gt;aliyun nexus&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;properties&gt; &lt;package.environment&gt;dev&lt;/package.environment&gt; &lt;/properties&gt; &lt;!-- 是否默认 true表示默认--&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;/profile&gt; &lt;profile&gt; &lt;!-- 测试环境 --&gt; &lt;id&gt;test&lt;/id&gt; &lt;properties&gt; &lt;package.environment&gt;test&lt;/package.environment&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;profile&gt; &lt;!-- 生产环境 --&gt; &lt;id&gt;prod&lt;/id&gt; &lt;properties&gt; &lt;package.environment&gt;prod&lt;/package.environment&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/project&gt;普通子模块pom.xml配置：&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;parent&gt; &lt;artifactId&gt;demo&lt;/artifactId&gt; &lt;groupId&gt;com.demo&lt;/groupId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;demo-system&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.demo&lt;/groupId&gt; &lt;artifactId&gt;demo-common&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;启动类子模块pom.xml配置：&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;parent&gt; &lt;artifactId&gt;demo&lt;/artifactId&gt; &lt;groupId&gt;com.demo&lt;/groupId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;artifactId&gt;demo-admin&lt;/artifactId&gt; &lt;description&gt; web服务 &lt;/description&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;artifactId&gt;demo&lt;/artifactId&gt; &lt;groupId&gt;com.demo&lt;/groupId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.1.8.RELEASE&lt;/version&gt; &lt;configuration&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;!-- 如果没有该配置，devtools不会生效 --&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;configuration&gt; &lt;failOnMissingWebXml&gt;false&lt;/failOnMissingWebXml&gt; &lt;warName&gt;$&#123;project.artifactId&#125;&lt;/warName&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 参考文献及资料1、Properties files，链接：https://commons.apache.org/proper/commons-configuration/userguide/howto_properties.html]]></content>
      <categories>
        <category>Maven</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[配置kerberos认证的Kafka集群交互介绍]]></title>
    <url>%2F2020%2F09%2F03%2F2020-09-03-%E9%85%8D%E7%BD%AEkerberos%E8%AE%A4%E8%AF%81%E7%9A%84Kafka%E9%9B%86%E7%BE%A4%E4%BA%A4%E4%BA%92%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 参考文献及资料 背景https://docs.spring.io/spring-kafka/api/org/springframework/kafka/core/KafkaTemplate.html https://blog.csdn.net/justry_deng/article/details/88387898 Spark Kafka Consumer in secure( Kerberos) enviornment Raw SparkKafkaIntegration.md Sample Applicationusing direct stream1234567891011121314151617181920212223242526272829303132import kafka.serializer.StringDecoder;import org.apache.spark.SparkConfimport org.apache.spark.streaming._import org.apache.spark.streaming.kafka._object SparkKafkaConsumer2 &#123; def main(args: Array[String]) &#123; // TODO: Print out line in log of authenticated user val Array(brokerlist, group, topics, numThreads) = args var kafkaParams = Map( "bootstrap.servers"-&gt;"rks253secure.hdp.local:6667", "key.deserializer" -&gt;"org.apache.kafka.common.serialization.StringDeserializer", "value.deserializer" -&gt;"org.apache.kafka.common.serialization.StringDeserializer", "group.id"-&gt; "test", "security.protocol"-&gt;"PLAINTEXTSASL", "auto.offset.reset"-&gt; "smallest" ) val sparkConf = new SparkConf().setAppName("KafkaWordCount") val ssc = new StreamingContext(sparkConf, Seconds(100)) val kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder] (ssc, kafkaParams, Set(topics)) // TODO: change to be a variable kafkaStream.saveAsTextFiles("/tmp/streaming_output") ssc.start() ssc.awaitTermination() &#125;&#125; using createStream1234567891011121314151617181920212223242526272829303132333435import kafka.serializer.StringDecoderimport org.apache.spark.SparkConfimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.Secondsimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.kafka.KafkaUtilsobject Kafka_Word_Count &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName("KafkaWordCount") .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .set("spark.driver.allowMultipleContexts", "true") val ssc = new StreamingContext(conf, Seconds(3)) val groupID = "test" val numThreads = "2" val topic = "kafkatopic" val topicMap = topic.split(",").map((_, numThreads.toInt)).toMap val kafkaParams = Map[String, String]( "zookeeper.connect" -&gt; "rks253secure.hdp.local:2181", "group.id" -&gt; groupID, "zookeeper.connection.timeout.ms" -&gt; "10000", "security.protocol"-&gt;"PLAINTEXTSASL" ) val lines = KafkaUtils.createStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicMap, StorageLevel.MEMORY_AND_DISK_SER_2).map(_._2) lines.print() ssc.start() ssc.awaitTermination() &#125;&#125; kafka_jaas.conf (for spark local mode)123456789101112131415161718192021222324KafkaServer &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab="/etc/security/keytabs/kafka.service.keytab" storeKey=true useTicketCache=false serviceName="kafka" principal="kafka/rks253secure.hdp.local@EXAMPLE.COM"; &#125;; KafkaClient &#123; com.sun.security.auth.module.Krb5LoginModule required useTicketCache=true renewTicket=true serviceName="kafka"; &#125;; Client &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab="/etc/security/keytabs/kafka.service.keytab" storeKey=true useTicketCache=false serviceName="zookeeper" principal="kafka/rks253secure.hdp.local@EXAMPLE.COM"; &#125;; kafka_jaas.conf (for spark yarn client mode)12345678910111213141516KafkaClient &#123;com.sun.security.auth.module.Krb5LoginModule requireduseKeyTab=truekeyTab="/etc/security/keytabs/kafka.service.keytab"serviceName="kafka"principal="kafka/rks253secure.hdp.local@EXAMPLE.COM";&#125;;Client &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab="/etc/security/keytabs/kafka.service.keytab" storeKey=true useTicketCache=false serviceName="zookeeper" principal="kafka/rks253secure.hdp.local@EXAMPLE.COM";&#125;; Spark Submit commandkinit from kafka user.. 12spark-submit --files /etc/kafka/conf/kafka_jaas.conf,/etc/security/keytabs/kafka.service.keytab --conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=/etc/kafka/conf/kafka_jaas.conf" --driver-java-options "-Djava.security.auth.login.config=/etc/kafka/conf/kafka_jaas.conf" --class SparkKafkaConsumer2 --master local[2] /tmp/SparkKafkaSampleApp-1.0-SNAPSHOT-jar-with-dependencies.jar "rks253secure.hdp.local:6667" test kafkatopic 1 参考文献及资料1、]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[提升Intellij IDEA工具生产力]]></title>
    <url>%2F2020%2F08%2F15%2F2020-08-15-Intellij%20IDEA%E7%BC%96%E8%BE%91%E5%99%A8%E4%B8%ADJava%E4%BB%A3%E7%A0%81%E5%9D%97%E7%9A%84%E5%BF%AB%E6%8D%B7%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景记录常用的ideal快捷输入； 第一部分 常用快捷键1.1 mian主函数快捷键psvm 123public static void main(String[] args) &#123; &#125; 1.2 输出快捷键sout 1System.out.println(); 1.3 循环遍历快捷键iter 123for (String arg : args) &#123; &#125; 1.4 生成公共静态finalpsf 1public static final 1.5 生成公共静态 final intpsfi 1public static final int 1.6 生成公共静态final Stringpsfs 1public static final String 第二部分 自动配置注释创建文件时，idea自动生成注释 过程是，在设置中找到“File and Code Templates”，在类文件的模板中添加注释。具体如下图（图1）所示： （1）首先打开设置，点File，Settings即可打开下图界面，或者直接用快捷键Ctrl+Alt+S打开设计界面。 （2）在搜索框中输入“File and Code Templates”，打开“File and Code Templates”。 （3）在右侧的Files一栏中，找到Class，编辑注释。下面的Description介绍了可以使用的系统变量，按需索取。 123456789101112131415#if ($&#123;PACKAGE_NAME&#125; &amp;&amp; $&#123;PACKAGE_NAME&#125; != "")package $&#123;PACKAGE_NAME&#125;;#end#parse("File Header.java")/*** @program: $&#123;PROJECT_NAME&#125;** @description: $&#123;description&#125;** @author: rongxiang** @create: $&#123;YEAR&#125;-$&#123;MONTH&#125;-$&#123;DAY&#125; $&#123;HOUR&#125;:$&#123;MINUTE&#125;**/public class $&#123;NAME&#125; &#123;&#125; 参考文献及资料1、IntelliJ IDEA，链接：https://www.jetbrains.com/idea/]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pyspark系列文章-Pyspark日志实践介绍]]></title>
    <url>%2F2020%2F06%2F11%2F2022-06-11-Pyspark%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Pyspark%E4%B8%AD%E7%9A%84Dataframe%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Receiver接口模式 第二部分 Direct接口模式 第三部分 PySpark和Kafka交互 第四部分 任务提交 参考文献及资料 背景https://blog.csdn.net/hejp_123/article/details/88027866 https://cloud.tencent.com/developer/article/1435989]]></content>
      <categories>
        <category>pyspark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pyspark系列文章-Pyspark日志实践介绍]]></title>
    <url>%2F2020%2F06%2F11%2F2022-06-11-Pyspark%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Pyspark%E6%97%A5%E5%BF%97%E5%AE%9E%E8%B7%B5%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Receiver接口模式 第二部分 Direct接口模式 第三部分 PySpark和Kafka交互 第四部分 任务提交 参考文献及资料 背景## 1、Improvements to Kafka integration of Spark Streaming，]]></content>
      <categories>
        <category>pyspark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka系列文章(第三篇 Kafka可视化管理界面)]]></title>
    <url>%2F2020%2F05%2F12%2F2020-05-11-Kafka%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88%E7%AC%AC%E4%B8%89%E7%AF%87Kafka%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AE%A1%E7%90%86%E7%95%8C%E9%9D%A2%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 kafka-manager安装 第二部分 kafka-manager配置 第三部分 kafka-manager管理 第四部分 总结 参考文献及资料 背景在Kafka的监控系统中有很多优秀的开源监控系统。比如Kafka-manager，open-faclcon，zabbix等主流监控工具均可直接监控kafka。Kafka集群性能监控可以从消息网络传输，消息传输流量，请求次数等指标来衡量集群性能。这些指标数据可以通过访问kafka集群的JMX接口获取。Kafka-manager工具由Yahoo研发的Kafka管理和监控工具，并在github上开源。 对于非加密Kafka集群配置Kafka manager，目前互联网也有大量的资料。而对于加密集群（特别是云端集群还配置了域名方式），参考材料较为匮乏。本文针对云端加密Kafka集群配置Kafka Manager进行详细介绍，供大家参考。 第一部分 kafka-manager安装1.1 版本选择版本使用cmak-3.0.0.0版本，依赖java11（使用openjdk-11+28_linux-x64_bin.tar.gz）。使用已经编译好的介质包cmak-3.0.0.0.zip。假设安装目录为/dmqs。 1.2 介质部署1.2.1 部署cmak上传cmak-3.0.0.0.zip至安装目录/dmqs,使用命令解压： 1f-itdw-4c8g-100g-11:/dmqs # unzip cmak-3.0.0.0.zip 1.2.2 部署java上传openjdk-11+28_linux-x64_bin.tar.gz介质到/dmqs/cmak-3.0.0.0路径： 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # tar -zxvf openjdk-11+28_linux-x64_bin.tar.gz 重命名java路径名： 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # mv jdk11 jdk 1.3 配置文件准备1.3.1 配置application.conf文件备份文件并修改： 12f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # cp application.conf application.conf.bakf-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # vi application.conf 调整kafka-manager.zkhosts参数项的配置信息： 1kafka-manager.zkhosts="84.10.228.50:2181,84.10.228.55:2181,84.10.228.56:2181" 1.3.2 加密集群配置jaas文件如果是加密集群需要准备jaas文件，文件名为：kafka_server_jaas.conf。 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # touch kafka_server_jaas.conf 文件内容如下： 1234567891011KafkaClient &#123; org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret";&#125;;Client &#123; org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin-secret";&#125;; 上面配置中KafkaClient为和kafka通信配置；Client为和zookeeper通信配置。 1.3.3 配置consumer.properties 首先备份： 12f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # cp consumer.properties consumer.properties.bakf-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # vi consumer.properties 配置文件调整为： 12345678910111213#security.protocol=PLAINTEXT#key.deserializer=org.apache.kafka.common.serialization.ByteArrayDeserializer#value.deserializer=org.apache.kafka.common.serialization.ByteArrayDeseriazerbootstrap.servers=kafka.itdw.node1:9093,kafka.itdw.node2:9093,kafka.itdw.node3:9093security.protocol=SASL_SSLsasl.mechanism=SCRAM-SHA-512ssl.truststore.location=/usr/ca/trust/client.truststore.jksssl.truststore.password=itdw123 ssl.keystore.password=itdw123ssl.keystore.location=/usr/ca/client/client.keystore.jksssl.key.password=itdw123ssl.endpoint.identification.algorithm= 其中注释部分为源配置文件内容。 1.4 准备ca信任证书对于已经配置为域名方式的Kafka集群需要配置域名信任证书。 创建InstallCert.java，java程序文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185/* * Copyright 2006 Sun Microsystems, Inc. All Rights Reserved. * * Redistribution and use in source and binary forms, with or without * modification, are permitted provided that the following conditions * are met: * * - Redistributions of source code must retain the above copyright * notice, this list of conditions and the following disclaimer. * * - Redistributions in binary form must reproduce the above copyright * notice, this list of conditions and the following disclaimer in the * documentation and/or other materials provided with the distribution. * * - Neither the name of Sun Microsystems nor the names of its * contributors may be used to endorse or promote products derived * from this software without specific prior written permission. * * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS * IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. */ import java.io.*;import java.net.URL; import java.security.*;import java.security.cert.*; import javax.net.ssl.*; public class InstallCert &#123; public static void main(String[] args) throws Exception &#123; String host; int port; char[] passphrase; if ((args.length == 1) || (args.length == 2)) &#123; String[] c = args[0].split(":"); host = c[0]; port = (c.length == 1) ? 443 : Integer.parseInt(c[1]); String p = (args.length == 1) ? "changeit" : args[1]; passphrase = p.toCharArray(); &#125; else &#123; System.out.println("Usage: java InstallCert &lt;host&gt;[:port] [passphrase]"); return; &#125; File file = new File("jssecacerts"); if (file.isFile() == false) &#123; char SEP = File.separatorChar; File dir = new File(System.getProperty("java.home") + SEP + "lib" + SEP + "security"); file = new File(dir, "jssecacerts"); if (file.isFile() == false) &#123; file = new File(dir, "cacerts"); &#125; &#125; System.out.println("Loading KeyStore " + file + "..."); InputStream in = new FileInputStream(file); KeyStore ks = KeyStore.getInstance(KeyStore.getDefaultType()); ks.load(in, passphrase); in.close(); SSLContext context = SSLContext.getInstance("TLS"); TrustManagerFactory tmf = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm()); tmf.init(ks); X509TrustManager defaultTrustManager = (X509TrustManager)tmf.getTrustManagers()[0]; SavingTrustManager tm = new SavingTrustManager(defaultTrustManager); context.init(null, new TrustManager[] &#123;tm&#125;, null); SSLSocketFactory factory = context.getSocketFactory(); System.out.println("Opening connection to " + host + ":" + port + "..."); SSLSocket socket = (SSLSocket)factory.createSocket(host, port); socket.setSoTimeout(10000); try &#123; System.out.println("Starting SSL handshake..."); socket.startHandshake(); socket.close(); System.out.println(); System.out.println("No errors, certificate is already trusted"); &#125; catch (SSLException e) &#123; System.out.println(); e.printStackTrace(System.out); &#125; X509Certificate[] chain = tm.chain; if (chain == null) &#123; System.out.println("Could not obtain server certificate chain"); return; &#125; BufferedReader reader = new BufferedReader(new InputStreamReader(System.in)); System.out.println(); System.out.println("Server sent " + chain.length + " certificate(s):"); System.out.println(); MessageDigest sha1 = MessageDigest.getInstance("SHA1"); MessageDigest md5 = MessageDigest.getInstance("MD5"); for (int i = 0; i &lt; chain.length; i++) &#123; X509Certificate cert = chain[i]; System.out.println (" " + (i + 1) + " Subject " + cert.getSubjectDN()); System.out.println(" Issuer " + cert.getIssuerDN()); sha1.update(cert.getEncoded()); System.out.println(" sha1 " + toHexString(sha1.digest())); md5.update(cert.getEncoded()); System.out.println(" md5 " + toHexString(md5.digest())); System.out.println(); &#125; System.out.println("Enter certificate to add to trusted keystore or 'q' to quit: [1]"); String line = reader.readLine().trim(); int k; try &#123; k = (line.length() == 0) ? 0 : Integer.parseInt(line) - 1; &#125; catch (NumberFormatException e) &#123; System.out.println("KeyStore not changed"); return; &#125; X509Certificate cert = chain[k]; String alias = host + "-" + (k + 1); ks.setCertificateEntry(alias, cert); OutputStream out = new FileOutputStream("jssecacerts"); ks.store(out, passphrase); out.close(); System.out.println(); System.out.println(cert); System.out.println(); System.out.println ("Added certificate to keystore 'jssecacerts' using alias '" + alias + "'"); &#125; private static final char[] HEXDIGITS = "0123456789abcdef".toCharArray(); private static String toHexString(byte[] bytes) &#123; StringBuilder sb = new StringBuilder(bytes.length * 3); for (int b : bytes) &#123; b &amp;= 0xff; sb.append(HEXDIGITS[b &gt;&gt; 4]); sb.append(HEXDIGITS[b &amp; 15]); sb.append(' '); &#125; return sb.toString(); &#125; private static class SavingTrustManager implements X509TrustManager &#123; private final X509TrustManager tm; private X509Certificate[] chain; SavingTrustManager(X509TrustManager tm) &#123; this.tm = tm; &#125; public X509Certificate[] getAcceptedIssuers() &#123; throw new UnsupportedOperationException(); &#125; public void checkClientTrusted(X509Certificate[] chain, String authType) throws CertificateException &#123; throw new UnsupportedOperationException(); &#125; public void checkServerTrusted(X509Certificate[] chain, String authType) throws CertificateException &#123; this.chain = chain; tm.checkServerTrusted(chain, authType); &#125; &#125;&#125; 上传至目的目录,并编译： 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # /dmqs/cmak-3.0.0.0/jdk/bin/javac InstallCert.java 编译后生成下面的文件： 1234f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # ll-rw-r--r-- 1 dmqs dmqs 975 May 28 02:23 InstallCert$SavingTrustManager.class-rw-r--r-- 1 dmqs dmqs 6126 May 28 02:23 InstallCert.class-rw-r--r-- 1 dmqs dmqs 6884 May 28 02:21 InstallCert.java 添加域名（kafka集群配置为域名方式）到jssecacerts文件中： 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # /dmqs/cmak-3.0.0.0/jdk/bin/java InstallCert kafka.itdw.node1:9093 这时在当前目录就生成了jssecacerts文件。如果集群是多节点，需要将其他节点域名信息追加到这个文件中。执行命令即为： 12f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # /dmqs/cmak-3.0.0.0/jdk/bin/java InstallCert kafka.itdw.node2:9093f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # /dmqs/cmak-3.0.0.0/jdk/bin/java InstallCert kafka.itdw.node3:9093 这样就生成了集群所有的节点域名的信任证书。 最后将jssecacerts文件拷贝至jdk/lib/security： 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # cp jssecacerts jdk/lib/security 完成所有配置的准备。 1.5 服务启动完成配置文件准备后，使用下面的命令启动Kafka-manager服务： 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/bin # ./cmak -java-home ../jdk -Djava.security.auth.login.config=../conf/kafka_server_jaas.conf -Dapplication.home=/dmqs/cmak-3.0.0.0 &gt; /dev/null 2&gt;&amp;1 &amp; 其中参数命令说明如下： 参数-java-home指定服务启动的java依赖环境目录； 参数-Djava.security.auth.login.config指定和kafka和zookeeper交互的jaas文件路径； 参数-Dapplication.home指定了应用的主目录； 参数-Dhttp.port=8888指定了应用的监听端口，默认9000； 参数-Dconfig.file=../conf/application.conf指定了应用的应用配置文件； 启动后应用目录下面生成logs目录，作为日志存放目录。启动命令不指定端口的情况下，默认监听9000端口。 1.6 自动化脚本为了提高服务运维管理，对服务启停进行自动化管理。 12345678910111213141516171819202122232425262728293031323334353637383940414243#!/bin/bash -e RETVAL=0cmak="/dmqs/cmak-3.0.0.0/bin/cmak"start() &#123; $cmak -java-home ../jdk -Djava.security.auth.login.config=../conf/kafka_server_jaas.conf -Dapplication.home=/dmqs/cmak-3.0.0.0 &gt;/dev/null 2&gt;&amp;1 &amp; RETVAL=$? [ $RETVAL -eq 0 ] &amp;&amp; echo "Start Kafka Manager Success!" ||echo "Start Kafka Manager failed!" return $RETVAL&#125; stop() &#123; CMAKPID=$(ps -ef|grep cmak|grep -v grep| awk '&#123;print $2&#125;') if [[ -a /dmqs/cmak-3.0.0.0/RUNNING_PID ]] then rm /dmqs/cmak-3.0.0.0/RUNNING_PID &amp;&amp; echo -e "\n已删除文件:RUNNING_PID\n" &amp;&amp; kill -9 $CMAKPID &gt;/dev/null 2&gt;&amp;1 &amp; RETVAL=$? else kill -9 $CMAKPID &gt;/dev/null 2&gt;&amp;1 &amp; RETVAL=$? fi; [ $? -eq 0 ] &amp;&amp; echo "Stop Kafka Manager Success!" ||echo "Stop Kafka Manager failed!" return $RETVAL&#125;case "$1" in start) start ;; stop) stop ;; restart) sh $0 stop sh $0 start ;; *) echo "Format error!" echo $"Usage: $0 &#123;start|stop|restart&#125;" exit 1 ;;esacexit $RETVAL 对于启动命令，可以自定义修改。 第二部分 kafka-manager配置2.1 创建新集群管理创建新的管理集群，需要填入下面的信息： Cluster Name 集群名称； Cluster Zookeeper Hosts 配置kafka集群背后的zookeeper集群的信息。例如：192.168.1.1:2181； Kafka Version Kafka的版本信息； Enable JMX Polling (Set JMX_PORT env variable before starting kafka server) 是否启用集群的监控组件。 Security Protocol 安全协议。目前支持：SSL、SASL_PLAINTEXT、SASL_SSL、PLAINTEXT SASL Mechanism (only applies to SASL based security) SASL的权限管理协议：DEFAULT、PLAIN、GSSAPI、SCRAM-SHA-256、SCRAM-SHA-512 SASL JAAS Config (only applies to SASL based security) SASL的用户配置信息。例如： 1org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret"; 需要注意的是配置以分号结束，否则会报错。 第三部分 kafka-manager管理Kafka Manager服务启动后，默认监听9000端口，所以服务URL地址为：http://102.168.1.1:9000。目前组件支持的管理功能有： 管理多个集群 轻松检查集群状态（主题，使用者，偏移量，代理，副本分发，分区分发） 运行首选副本选择 生成带有选项的分区分配，以选择要使用的代理 运行分区的重新分配（基于生成的分配） 使用可选的主题配置创建主题（0.8.1.1与0.8.2+具有不同的配置） 删除主题（仅在0.8.2+上受支持，并记住在代理配置中设置delete.topic.enable = true） 现在，主题列表指示标记为删除的主题（仅在0.8.2+上受支持） 批量生成多个主题的分区分配，并可以选择要使用的代理 批量运行分区的多个主题的重新分配 将分区添加到现有主题 更新现有主题的配置 （可选）为代理级别和主题级别的度量启用JMX轮询。 （可选）过滤出在Zookeeper中没有id / owner /＆offsets /目录的使用者。 对于具体的组件使用，可以参文献中的[3]。 参考文献及资料1、kafka-manager项目地址，链接：https://github.com/yahoo/kafka-manager 2、kafka-manager项目下载地址，链接：https://blog.wolfogre.com/posts/kafka-manager-download/ 3、Apache Kafka集群管理工具CMAK(Cluster Manager for Apache Kafka)从安装启动到配置使用，链接：http://www.luyixian.cn/news_show_324464.aspx]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统远程执行shell命令]]></title>
    <url>%2F2020%2F05%2F07%2F2020-05-07-Linux%E7%B3%BB%E7%BB%9F%E8%BF%9C%E7%A8%8B%E6%89%A7%E8%A1%8Cshell%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 远程执行shell命令 参考文献及资料 背景第一部分 远程执行shell命令第二部分 Python实现远程执行shell命令参考文献及资料1、iptables命令详解，链接]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统中环境变量详解]]></title>
    <url>%2F2020%2F04%2F21%2F2020-04-21-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 参考文献及资料 背景第一部分 环境变量分类1.1 生命周期永久环境变量 临时环境变量 1.2 作用域系统级环境变量 用户环境变量 参考文献及资料1、iptables命令详解，链接：https://wangchujiang.com/linux-command/c/iptables.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统中的shell解释器介绍]]></title>
    <url>%2F2020%2F04%2F21%2F2020-04-21-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84shell%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 shell解释器种类 第二部分 shell执行命令模式 第三部分 shell命令的类型 第四部分 shell初始化和调用 第五部分 启动文件介绍 第六部分 生产运维注意事项 参考文献及资料 背景shell是用C语言编写的程序，用户用它来和Linux进行交互。我们通常说的 shell 都是指 shell 脚本，但 shell 和 shell script 是两个不同的概念，这是后话。 第一部分 shell解释器种类1.1 shell常见解释器类型Linux shell常用类型有：bash、ksh、csh、zsh、ash等。可以通过下面的命令查看本机操作系统支持的shell类型（下面是ubuntu系统）： 123456root@deeplearning:/# cat /etc/shells# /etc/shells: valid login shells/bin/sh/bin/dash/bin/bash/bin/rbash 使用下面的命令查看当前（ubuntu）正在使用的shell： 12root@deeplearning:/# echo $SHELL/bin/bash 这里SHELL是一个环境变量，用于记录当前用户使用的shell类型。在当前shell环境中，可以打开其他的shell环境（子shell）： 12root@deeplearning:/# /bin/dash# 上面命令我们打开了dash，如果要退出，直接使用exit命令即可退出子shell。 1.2 常用shell详细介绍1.2.1 ashash Shell是由Kenneth Almquist编写的，是Linux 中占用系统资源最少的一个小Shell。但是它只包含24个内部命令，因而使用起来很不方便。 1.2.2 bashbash是Linux系统默认使用的Shell，它由Brian Fox 和Chet Ramey共同完成，是BourneAgain Shell的缩写，内部命令一共有40 个。Linux 使用它作为默认的Shell是因为它具有以下特色： 可以使用类似DOS下面的doskey的功能，用上下方向键查阅和快速输入并修改命令。 自动通过查找匹配的方式，给出以某字串开头的命令。 包含了自身的帮助功能，你只要在提示符下面键入help就可以得到相关的帮助信息。 1.2.3 kshksh是Korn Shell的缩写，由Eric Gisin编写，共有42 条内部命令。该Shell最大的优点是几乎和商业发行版的ksh 完全相容，这样就可以在不用花钱购买商业版本的情况下尝试商业版本的性能了。 1.2.4 cshcsh 是Linux 比较大的内核，它由以William Joy 为代表的共计47 位作者编成，共有52个内部命令。该Shell其实是指向/bin/tcsh这样的一个Shell，也就是说，csh其实就是tcsh。 1.2.5 zchzch是Linux 最大的Shell之一，由Paul Falstad完成，共有84 个内部命令。如果只是一般的用途，没有必要安装这样的Shell 第二部分 Shell执行命令的模式Shell按照是否交互，分为交互式和非交互两种模式。这就像Python等解释型语言，有交互式和非交互式。 2.1 交互式(Interactive Shell)交互式（Interactive）：解释执行用户的命令，用户输入一条命令，Shell就解释执行一条。 2.2 非交互式(Non Interactive Shell)非交互式即批处理（batch）。用户提前编写好Shell脚本，文件中存储多条命令。Shell读取并执行文件中命令，直到读到文件的结束EOF，Shell终止。另外对于使用管道连接的多个命令也算批处理。 可以使用打印$-来判别当前交互模式。echo $-，输出himBH为交互式，输出hB表示非交互式。 另外按照是否需要登录（使用用户名/密钥），分为登录式和非登录式。 2.3 登录式(Login Shell)需要用户名、密码登录后才能进入的shell（或者通过--login选项生成的shell）。 2.4 非登录式(Non Login Shell)不需要输入用户名和密码即可打开的Shell。例如在当前shell交互命令行中，执行bash就会开启一个新的shell（子shell）环境，这就是一个非登录式的shell。 2.5 shell的退出执行exit命令，退出一个shell（登录或非登录shell）。执行logout命令，退出登录shell（不能退出非登录shell）。 第三部分 shell命令的类型3.1 内部命令内部命令内置于Shell源码中，即存在于内存中，一般比较简短，代码量很少，执行起来速度快，经常会使用，比如cd、echo。它与shell本身处在同一进程内（就写在Shell这个程序里面）,当打开Shell时，操作系统会将Shell程序放入内存 。 类似Python的程序中的内置函数（Build-in Function），Python解析器初始化化就会加载这些函数。 3.2 外部命令外部命令一般功能比较强大，包含的代码量也较大，所以在系统加载时并不随系统一起被加载到内存中，而是在需要时才调用，它们是存在于文件系统中某个目录下的单独的程序，当执行外部命令时，会到文件系统中文件的目录中寻找，例如 cp、rm、ifconfig。 3.3 查看命令类型对于一个命令是否是内部或者外部命令，可以使用type命令来检测。 1234root@vultr:~# type cd cd is a shell builtinroot@vultr:~# type cpcp is /bin/cp 其中builtin就是指是内部命令，类似Python中builtin包。另外type本身也是一个内部命令： 12root@vultr:~# type typetype is a shell builtin 第四部 shell的初始化和调用当shell被调用时，会读取一些初始化启动文件。主要作用是为shell本身或用户设定运行环境，包含一些函数、 变量、别名等等。 shell有两种类型的初始化文件： 系统级启动文件。这些包含适用于系统上所有用户的全局配置，通常位于/etc目录中。 包括：/etc/profiles和/etc/bashrc或/etc/bash.bashrc(不同操作系统差异) 。 用户级启动文件 。这些存储配置适用于系统上的单个用户，通常位于用户主目录中的点文件（使用la命令查看）。 包括： .profiles ， .bash_profile ， .bashrc和.bash_login。 shell可以以三种模式被调用，分别是：交互式登录、非登录交互式、非交互式。 4.1 交互式登录shell用户成功登录系统后，使用/bin/login登录，随后读取/etc/passwd文件，获取用户凭证后调用shell。/etc/passwd文件中配置了用户默认的shell类型（每行最后）。 123root@VM-0-5-ubuntu:~# cat /etc/passwdroot:x:0:0:root:/root:/bin/bash...... 然后这个登录shell 将查找几个不同的启动文件来处理其中的命令（它的作用是初始化linux系统相关配置）。例如 bash shell 处理文件的顺序如下： 系统登录后，shell首先执行/etc/profile文件中的命令(系统级)。设置这个文件后，可以为系统内所有的用户建立默认的特征（不同版本的Linux此文件放置路径有区别）。 如果是超级用户则提示符用#，如果是普通用户则提示符用$. 当某个用户登录后，shell依次查找~/.bash_profile、~/.bash_login、~/.profile这几个文件。 其中~/.bash_profile、~/.bash_login和 ~/.profile文件往往只存在一个，这与Linux的发行版本有关。ubuntu则为 ~/.profile 如果用户级有与系统级/etc/profile相同的环境变量，将会重新更新系统级的值。 对于Centos系统，加载的是.bash_profile。另外还会加载~/.bashrc，~/.bashrc文件中还会调用文件：/etc/bashrc。 当用户注销时，bash执行文件~/.bash_logout中的命令，这个文件包含了退出会话时执行的清理命令和退出等，如：exit退出。 4.2 交互式非登录Shell这种情况下调用时，它将拷贝父shell的环境，并读取相应用户级的~/.bashrc配置文件。交互式非登录shell 就是指你在当前图形界面中打开“终端”出来的那种窗口程序，和登录shell相比，它是“非登录”的，你并不需要输入用户名和码；和非交互式shell相比，这是“交互式”的，就像你说的那它：你输入什么，它就解释出什么。 4.3 非交互式Shell当执行脚本时，则调用非交互式shell。在这种模式下，它将处理所运行的脚本中的命令、函数等操作，不需要进行交互式输入（除非脚本需要交互式输入）。使用的环境继承自父shell。 第五部分 启动文件介绍5.1 系统级启动文件 /etc/profile，文件保存了登录时系统级环境配置和启动程序。如果你想配置对于所有用户的环境生效，可以加入此文件。 /etc/bashrc（ubuntu为 /etc/bash.bashrc），包含应用于所有用户的系统级函数、变量、别名等配置信息。 5.2 用户级启动文件在/home/用户名该目录下面一般有下面文件（存在操作系统差异）： 1234567.bash_logout # 用户登出shell是加载# 下面三个是用户级启动文件.profile.bash_profile.bash_login.bashrc # Centos操作系统用户级启动文件 第六部分 生产运维注意事项6.1 su命令注意事项在Linux系统使用中，很多用户无法区分su 用户名和su - 用户名两个命令的区别。甚至不懂差异，经常混用，非常危险，特别是生产运维中使用。两个命令的区别我们举个栗子说明一下。假如当前是root用户，su guest执行后，只是切换了用户身份由root切换成guest，但是shell环境仍然继承了root用户的shell环境。但是su - guest命令不仅切换了用户身份，而且shell环境也切换成guest登录后的shell环境。 事实上，下面三个命令形式是等价的： 1234# ubuntu用户切换到guest用户（使用登录方式）ubuntu@VM-0-5-ubuntu:~$ su - guestubuntu@VM-0-5-ubuntu:~$ su -l guestubuntu@VM-0-5-ubuntu:~$ su --login guest 另外这个差异还可以通过命令执行后的用户目录更为形象的感知：su guest执行后，工作目录并没有切换，而su - guest执行后工作目录切换为/home/guest。 对于生产环境，应该统一使用su - 用户名的方式，避免用户shell的继承，造成对切换用户后环境变量的变化（通常应用用户会有特殊的环境变量）。 6.2 crontab中的shell首先需要注意的是：crontab中的shell脚本，既不是交互式shell，也不是登录shell。所以不会加载启动配置文件。所以环境变量需要自行加载，不能想当然脚本在用户shell环境能执行，部署crontab也能执行。 另外下面是crontab的PATH值： 12ubuntu@VM-0-5-ubuntu:~$ grep PATH /etc/crontabPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin 而操作系统用户的PATH值如下，也是有差异的。 12root@VM-0-5-ubuntu:~# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin 可以按照下面的方式解决： 可以把shebang改为#!/bin/bash -l让脚本用登录Shell来解释执行，这个时候，执行脚本要采用路径执行的方式 调用Bash解释器，加-l参数，即 /bin/bash -l shell脚本 参考文献及资料1、GNU Bash，链接：https://www.gnu.org/software/bash/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何禁止云服务器端口被嗅探]]></title>
    <url>%2F2020%2F04%2F11%2F2020-04-14-%E5%A6%82%E4%BD%95%E7%A6%81%E6%AD%A2%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E5%8F%A3%E8%A2%AB%E5%97%85%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 实施 参考文献及资料 背景为了保证云服务器的安全，特别是一些匿名服务器嗅探你的服务端口，然后对你的端口进行封禁。为了保证云机器上服务安全，可以使用linux自带的IPTABLES工具进行防护，相当于操作系统级别的防火墙。 第一部分 实施使用下面的命令只允许222.69.213.94ip对40000端口进行访问，其他ip对该端口的访问全部拒绝。 1iptables -A INPUT -s 218.92.0.202 -p tcp --dport 11111 -j ACCEPT 避免外界对该端口进行嗅探，甄别该端口功能后，进行封禁端口。 参考文献及资料1、iptables命令详解，链接：https://wangchujiang.com/linux-command/c/iptables.html]]></content>
      <categories>
        <category>路由器</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[小米路由器MT工具箱配置]]></title>
    <url>%2F2020%2F03%2F22%2F2020-03-22-%E5%B0%8F%E7%B1%B3%E8%B7%AF%E7%94%B1%E5%99%A8MT%E5%B7%A5%E5%85%B7%E7%AE%B1%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 装备工作 第二部分 配置MT工具箱 参考文献及资料 背景家庭使用的小米路由器，需要折腾一下，开始全局小飞机。 第一部分 准备工作首先需要小米路由器准备好下面的配置前提： 安装了开发版的rom，获得root权限； 路由器开启了ssh； 路由器默认的IP地址为“192.168.31.1”； 具体可以参考文献中文章《小米路由器安装MT工具箱》。 第二部分 配置MT工具箱使用SSH登录小米路由器，执行下面的安装命令： 1curl -s -k https://beta.misstar.com/download/$(uname -m)/mtinstall -o /tmp/mtinstall &amp;&amp; chmod +x /tmp/mtinstall &amp;&amp; /tmp/mtinstall 选择网络安装（选项2），按照提示配置用户和密钥。提示安装Misstar tools 3.0beta版成功： 1http://192.168.31.1:1314/ 但是提示页面url并不能打开。事实上监听的端口是1024。 12root@XiaoQiang:~# netstat -ant|grep 1024tcp 0 0 :::1024 :::* LISTEN 0 0 使用下面的url进入MT工具箱： 1http://192.168.31.1:1024 然后部署你的小飞机吧。新建节点等不再赘述。 参考文献及资料1、小米路由器安装MT工具箱，链接：https://whrr.blog/2019/01/06/install-mt-tools-on-a-xiaomi-router/]]></content>
      <categories>
        <category>路由器</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch集群升级指引]]></title>
    <url>%2F2020%2F03%2F07%2F2020-03-07-Elasticsearch%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E9%9B%86%E7%BE%A4%E5%8D%87%E7%BA%A7%E6%8C%87%E5%BC%95%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 版本升级指引 第二部分 升级方法和具体步骤 总结 参考文献及资料 背景Elasticsearch集群的版本升级是一项重要的集群维护工作。本篇文章参考官方文档，将详细介绍相关细节。 第一部分 版本升级指引1.1 同步升级Elastic Stack组件对于Elasticsearch的生态圈组件需要同步升级，具体配套版本可以参考官方提供的升级指南。 https://www.elastic.co/cn/products/upgrade_guide 1.2 索引兼容性Elasticsearch对于老版本的索引（index）兼容性如下： Elasticsearch 6.x兼容Elasticsearch 5.x中创建的索引，但不兼容Elasticsearch 2.x或更旧版本的索引。 Elasticsearch 5.x兼容Elasticsearch 2.x中创建的索引，但不不兼容Elasticsearch1.x或更旧版本的索引。 如果升级过程中遇到索引不兼容场景，升级后集群将无法正常启动。 1.3 版本升级路线Elasticsearch版本升级具体路线总结如下： 序号 原版本 升级目标版本 支持的升级类型 1 5.x 5.y 滚动升级（其中 y &gt; x） 2 5.6 6.x 滚动升级 3 5.0-5.5 6.x 集群重启 4 &lt;5.x 6.x reindex升级 5 6.x 6.y 滚动升级（其中 y &gt; x） 6 1.x 5.x reindex升级 7 2.x 2.y 滚动升级（其中 y &gt; x） 8 2.x 5.x 集群重启 9 5.0.0 pre GA 5.x 集群重启 10 5.x 5.y 滚动升级（其中 y &gt; x） 关于Elasticsearch的版本序列需要特别说明一下。Elasticsearch版本序列不是连续递增的，从2.4.x版本后直接跳跃到5.0.x。所以对于5.x版本，如果按照严格顺序递增编号，应该是3.x。之所以没有连续编号，主要是为了保持ELK（Elasticsearch 、 Logstash 、 Kibana）整体版本的统一。 其中第4种情况，小于5.x其实就是2.x和1.x。由于6.x对于更低版本的索引不兼容，所以需要对原集群的中索引实施reindex。方案分别为： 1.3.1 2.x升级到6.x按照上面的升级路线有两种升级方案： 方案1：先由2.x升级到5.6版本（reindex升级索引版本），然后由5.6升级到6.x（滚动升级）； 方案2：创建全新的6.x集群，然后将旧集群中的索引数据远程reindex到新集群中； 1.3.2 1.x升级到6.x同样有两个方案： 方案1：先由1.x升级到2.4.x版本（reindex升级索引版本），最后按照上面2.x升级到6.x的方案实施； 方案2：创建全新的6.x集群，然后将旧集群中的索引reindex到新集群中； 第二部分 升级方法和具体步骤集群升级路线中，针对不同的版本之间升级，一共有三种升级方案：滚动升级、集群重启、reindex。下面将分别介绍。 2.1 滚动升级所谓滚动升级指的是集群中节点逐个将版本升级至目标（高）版本，升级期间集群保持对外服务不中断。这种升级方案都是针对同一个大版本内的升级，即x.y升级到x.z（z&gt;y）。特别的，5.6升级到6.x也是支持使用滚动升级方式的。 https://www.elastic.co/guide/en/elasticsearch/reference/current/rolling-upgrades.html 通常滚动升级的步骤如下： 第1步 禁用副本分片（shards）分配在下宕升级节点前，需要提前禁止副本分片的分配。 节点下宕后，副本分配进程会等待index.unassigned.node_left.delayed_timeout（默认情况下为1分钟），然后再开始将该节点上的分片复制到群集中的其他节点，这会导致大量I/O。由于节点很快将重新启动，所以并不需要重新分配。 API命令如下： 123456PUT _cluster/settings&#123; "persistent": &#123; "cluster.routing.allocation.enable": "primaries" &#125;&#125; 第2步 执行同步刷新 重启集群时如果translog过大，日志回放恢复数据耗时较长，建议手动同步刷新，减少translog。 注意：这个过程较为缓慢。 1POST _flush/synced 第3步 停止机器学习作业如果集群中运行了机器学习任务，需要停止任务运行。 参考：https://www.elastic.co/guide/en/elastic-stack-overview/6.8/stopping-ml.html 第4部 下宕待升级节点并安装主版本和插件对升级节点实施下宕，开始文件系统的升级。 第5步 启动节点启动节点，并用下面的API检查节点是否加入集群。 1GET _cat/nodes 第6步 重启分片分配节点加入集群后，设置启用分片分配开始使用该节点。 123456PUT _cluster/settings&#123; "persistent": &#123; "cluster.routing.allocation.enable": null &#125;&#125; 在升级下一个节点前，等待集群分片完成。可以通过下面的API检查集群状态： 1GET _cat/health?v 等待集群的状态由red变成yellow，再到green。说明集群完成所有主分片和副分片的分配。 第7步 重复升级其他节点重复滚动升级集群其他节点。 第8步 重启机器学习任务如果集群中有机器学习任务，需要从新启动。 2.2 集群整体重启集群整体重启指的是升级前将集群所有节点均下宕，集群停止对外服务，待所有节点完成升级后，整体启动集群，恢复对外服务。例如：5.6之前的版本升级到6.x需要重启集群实施升级。 https://www.elastic.co/guide/en/elasticsearch/reference/current/restart-upgrade.html 集群重启升级步骤和滚动方式相似，主要步骤如下： 第1步 禁用副本分片（shards）分配下宕升级节点前需要，提前禁止副本分片的分配。（参考滚动升级） 第2步 停止不必要的索引并执行同步刷新参考滚动升级。 第3步 停止机器学习作业参考滚动升级 第4部 下宕所有节点并安装主版本和插件对集群所有节点实施下宕，开始文件系统版本升级。 第5步 启动节点并等待集群状态为yellow启动所有节点，并用下面的API检查所有节点是否加入集群。 1GET _cat/nodes 第6步 重启分片分配节点加入集群后，设置启用分片分配开始使用该节点。 123456PUT _cluster/settings&#123; "persistent": &#123; "cluster.routing.allocation.enable": null &#125;&#125; 在升级下一个节点前，等待集群分片完成。可以通过下面的API检查集群状态： 1GET _cat/health?v 等待集群的状态由yellow变为green。说明集群完成所有主分片和副分片的分配。 第7步 重启机器学习任务参考滚动升级 2.3 reindexElasticsearch中相邻版本的index具有兼容性，但是跨度较大的版本不再向下兼容。在上文（1.2 索引兼容性）中已做介绍。而在ElasticSearch中，索引的field设置是不能被修改的，如果要修改一个field，那么应该重新按照新的mapping，建立一个index，然后将数据批量查询出来，重新用bulk api写入新index中。 批量查询的时候，建议采用scroll api，并且采用多线程并发的方式来reindex数据，每次scroll就查询指定日期的一段数据，交给一个线程即可。 第1步 搭建新版本集群申请服务器资源，搭建全新版本的ElasticSearch集群。将对外服务全部指向新集群。 第2步 将老集群中数据reindex到新集群在老集群上使用reindex API将老集群中index历史数据逐步迁移至新集群。 如果集群数据量较大，迁移过程是一个很缓慢的过程。 API案例（下面是简单的配置）： 12345678910111213141516171819202122POST _reindex&#123; "source": &#123; "remote": &#123; "host": "http://otherhost:9200", "username": "user", "password": "pass" &#125;, "index": "source", "query": &#123; "match": &#123; "test": "data" &#125; &#125; &#125;, "dest": &#123; "index": "dest" &#125;&#125;//host为远程集群（新集群）的地址。//username和password针对安全集群的密钥验证。//"index": "source"为旧集群中index名，dest的所对应的是新集群目标index名。 迁移完成后，可以对旧集群中数据实施清理。清理完成后根据情况需要，旧节点可以离线升级文件系统，最后作为全新的节点加入新集群。 如果旧集群中历史数据不重要，可以删除数据后，搭建全新的集群。 2.4 分步升级对于跨度较大的版本升级，如果不采用新建集群再实施reindex方式，那么就需要分步升级。例如A、B、C依次为三个版本，版本级别A&lt;B&lt;C，其中index数据B兼容A，C兼容B，但是C不兼容A。这种情况需要分步升级： A升级到B，使用滚动升级或者集群整体重启方式。 对于B版本的集群，将A版本的所有数据reindex到B版本。这个过程较为耗时。 等到集群中所有历史index（新建的index自然是B版本）均为B版本后，升级集群版本到C版本。 如果index数据是时间序列类的数据，可以不实施reindex，等到历史数据生命周期结束后（集群中不在有A版本的index数据），再从B版本升级到C版本。 总结（1）一般Elasticsearch大版本之间跨度升级需要重启整体集群。 （2）部分ElasticSearch大版本间index并不兼容，需要对数据重索引（reindex）。 （3）大版本中的小版本升级，通常只需要滚动重启方式即可。 参考材料1、Elasticsearch官网 链接：https://www.elastic.co/cn/]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pyspark系列文章-Spark SQL与Hive交互实践]]></title>
    <url>%2F2020%2F03%2F02%2F2022-05-23-Pyspark%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Spark%20SQL%E4%B8%8EHive%E4%BA%A4%E4%BA%92%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark SQL 第二部分 Spark SQL实践 第三部分 参考文献及资料 背景第一部分 Spark SQL1.1 Spark 2.0之前在Spark 1.6（Spark 2.0之前）中，我们通过Spark SQL用于处理结构化数据。Spark SQL的入口点是SQLContext类或者子类HiveContext。 例如下面的例子（Pyspark），可以访问Hive仓库中表（default库中的users表）： 注：需要在Spark conf目录（classpath）下配置hive相关配置（hive-site.xml），才能访问Hive MetaStore，实现读写Hive表。 12345from pyspark.sql import HiveContext# Pyspark on Jupyter,sc（sparkContext）已创建hiveContext = HiveContext(sc)tableExample = hiveContext.table("default.users")tableExample.show() 另外也支持将DataFrame注册成Table，通过SQL语言处理DataFrame数据。例如下面的例子： 12345678from pyspark.sql import SQLContextsqlContext = SQLContext(sc)# 从外部数据源创建DataFramejsonData = "file:///opt/spark-1.5.1/examples/src/main/resources/people.json"df = sqlContext.read.json(jsonData)# 注册表sqlContext.registerDataFrameAsTable(df, "tempTable")sqlContext.sql("SELECT * from tempTable limit 1").show() 注：SQLContext现在只支持sql语法解析器（SQL-92语法），而HiveContext现在既支持sql语法解析器又支持hivesql语法解析器，默认为hivesql语法解析器，用户可以通过配置参数（spark.sql.dialect）切换成sql语法解析器，来运行hiveql不支持的语法。 通常我们直接使用HiveContext （HiveContext is a super set of the SQLContext. Hortonworks and the Spark community suggest using the HiveContext.）。但是需要Hive启动MetaStore的Thrift接口服务。 1.2 Spark 2.0 之后Spark 2.0之前，SparkContext是Spark的唯一切入点（Entry Point），其他Context都是基于其实现。例如下面例子： 1234567891011121314from pyspark import SparkContextfrom pyspark.sql import SQLContextfrom pyspark.sql import HiveContextfrom pyspark.streaming import StreamingContextfrom pyspark import SparkConf# 实例化SparkContext，通常起名scconf = SparkConf()sc = SparkContext(conf=conf)# hivehiveContext = HiveContext(sc)# sqlsqlContext = SQLContext(sc)# 流处理streamingcontext = StreamingContext(sc) Spark2.0开始只需要创建一个SparkSession就够了，SparkConf、SparkContext和SQLContext都已经被封装在SparkSession当中，常用场景逐步替代SparkContext。 创建方法如下： 123456from pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName("Python Spark SQL basic example") \ .config("spark.some.config.option", "some-value") \ .getOrCreate() SparkSession接口代替Spark1.6 中的SQLcontext和HiveContext 来实现对数据的加载、转换、处理等工作，并且实现了SQLcontext和HiveContext的所有功能。 在新版本中并不需要之前那么繁琐的创建很多对象，只需要创建一个SparkSession对象即可。SparkSession支持从不同的数据源加载数据，并把数据转换成DataFrame，并支持把DataFrame转换成SQLContext自身中的表。然后使用SQL语句来操作数据，也提供了HiveQL以及其他依赖于Hive的功能支持。 第二部分 Spark SQL实践2.1 PySpark on Jupyter在测试研发环境我们通常使用Jupyter来研发和测试数据类Spark任务。启动命令： 1# /root/anaconda3/bin/python /root/anaconda3/bin/jupyter-notebook --ip=0.0.0.0 --no-browser --allow-root --notebook-dir=/opt/jupyter 需要提前配置相关Python环境变量： 123456"SPARK_HOME": "/opt/spark-2.3.2/","PYSPARK_PYTHON": "/root/anaconda3/bin/python","PYSPARK_DRIVER_PYTHON": "ipython3", "PYTHONPATH": "/opt/spark-2.3.2/python/:/opt/spark-2.3.2/python/lib/py4j-0.10.7-src.zip","PYTHONSTARTUP": "/opt/spark-2.3.2/python/pyspark/shell.py","PYSPARK_SUBMIT_ARGS": "--name pyspark --master local pyspark-shell" 启动Jupyter后已经预定义好了两个对象：sc和spark，类型分别是：SparkContext和SparkSession。 123Spark context Web UI available at http://hadoop01:4040Spark context available as 'sc' (master = local[*], app id = local-1653645196618).Spark session available as 'spark'. 用户在Jupyter上编写代码时候，直接使用两个对象即可（注意：如果重新定义，Spark部分配置是无法覆盖的）。 2.2 Two CatalogsSpark 2.0版本后，Spark有两个Catalogs，分别是Hive和in-memory。如果需要使用Hive需要配置Spark配置（参数默认值为：in-memory）： 1spark.sql.catalogImplementation：hive 可以在Spark UI上查看任务配置，显示如下： Spark 任务可以有如下功能： 读写Hive库的元数据库（读写Hive表数据）; 使用Hive的UDF函数； 使用Hive的SerDe序列化； 另外也可以基于内存实现Catalogs，从而和Hive解耦。配置如下： 1spark.sql.catalogImplementation：in-memory 2.3 连接HiveSparkSession连接Hive Metastore，需要指定配置文件hive-site.xml（文件从hive conf目录中拷贝至Spark conf目录）。 也可以通过在创建SparkSession对象是通过配置指定hive.metastore.uris，例如： 123456spark = SparkSession .builder .config("hive.metastore.uris", "thrift://172.25.21.2:9083") .enableHiveSupport() .appName('SparkByExamples') .getOrCreate() 接下来我们就可以通过SparkSession对Hive进行操作。 1234567891011121314151617import spark.implicits._ //展示hive中的表spark.sql("show tables").show(10) // 创建一个dataframeval df1 = Seq(("one", 1), ("two", 2), ("three", 3)).toDF("word", "count")df1.show()df1.write.mode("overwrite").saveAsTable("t4") //打印表结构val df2 = spark.sql("show create table t4")df2.foreach(println(_))//输出条数val df3 = spark.sql("select count(1) as total from t4")df3.show()spark.close() } 2.4 PySpark Catalog API PySpark中目前支持的访问Catalog的方法清单： SPARK CATALOG API IN PYSPARK DESCRIPTION currentDatabase() This API returns the current default database in this session setCurrentDatabase() You can use this API to sets the current default database in this session. listDatabases() Returns a list of databases available across all sessions listTables(dbName=None) Returns a list of tables/views in the specified database. API uses current database if no database is provided. listFunctions(dbName=None) Returns a list of functions registered in the specified database. API uses current database if no database is provided. listColumns(tableName, dbName=None) Returns a list of columns for the given table/view in the specified database.API uses current database if no database is provided. createTable(tableName, path=None, source=None, schema=None, **options) Creates a table based on the dataset in a data source and returns the DataFrame associated with the table. dropGlobalTempView(viewName) Drops the global temporary view with the given view name in the catalog. If the view has been cached before, then it will also be uncached. Returns true if this view is dropped successfully, false otherwise. dropTempView(viewName) Drops the local temporary view with the given view name in the catalog. If the view has been cached before, then it will also be uncached. Returns true if this view is dropped successfully, false otherwise. isCached(tableName) Returns true if the table is currently cached in-memory. recoverPartitions(tableName) Recovers all the partitions of the given table and update the catalog. Only works with a partitioned table, and not a view. refreshByPath(path) Invalidates and refreshes all the cached data for any DataFrame that contains the given data source path. refreshTable(tableName) Invalidates and refreshes all the cached data and metadata of the given table. cacheTable(tableName) Caches the specified table in-memory. isCached(tableName) Returns true if the table is currently cached in-memory. uncacheTable(tableName) Removes the specified table from the in-memory cache. clearCache() Removes all cached tables from the in-memory cache. 第三部分 Spark on Hive 和Hive on Spark3.1 Hive on SparkHive最开始底层使用MapReduce作为计算引擎，通常称为：Hive on MapReduce。为了提升效率出现了tez内存计算引擎，即Hive on tez。后续考虑到Spark的优秀性能，Hive项目也支持Spark作为底层引擎。这就是Hive on Spark。 在Hive CLI中可以临时调整计算引擎： 123456# 配置mapreduce计算引擎set hive.execution.engine=mr;# 配置tez计算引擎set hive.execution.engine=tez;# 配置spark计算引擎set hive.execution.engine=spark; 如果持久化配置，需要在配置文件hive-default.xml中调整计算引擎，如下： 12345678910&lt;property&gt; &lt;name&gt;hive.execution.engine&lt;/name&gt; &lt;value&gt;mr&lt;/value&gt; &lt;description&gt; Expects one of [mr, tez, spark]. Chooses execution engine. Options are: mr (Map reduce, default), tez, spark. While MR remains the default engine for historical reasons, it is itself a historical engine and is deprecated in Hive 2 line. It may be removed without further warning. &lt;/description&gt;&lt;/property&gt; 3.2 Spark on HiveSpark本身是一个计算引擎，并不负责数据的存储，计算过程需要从外部数据源读写数据。当然Hive也是一种外部数据源，所以这种模式通常被称为：Spark on Hive。 这时候用户可以通过Spark的提供的java/scala/pyhon/r等接口访问外部数据源。 3。spark + spark hive catalog。这是spark和hive结合的一种新形势，随着数据湖相关技术的进一步发展，这种模式现在在市场上受到了越来越多用户的青睐。其本质是，数据以orc/parquet/delta lake等格式存储在分布式文件系统如hdfs或对象存储系统如s3中，然后通过使用spark计算引擎提供的scala/java/python等api或spark 语法规范的sql来进行处理。由于在处理分析时针对的对象是table, 而table的底层对应的才是hdfs/s3上的文件/对象，所以我们需要维护这种table到文件/对象的映射关系，而spark自身就提供了 spark hive catalog来维护这种table到文件/对象的映射关系。注意这里的spark hive catalog，其本质是使用了hive 的 metasore 相关 api来读写表到文件/对象的映射关系（以及一起其他的元数据信息）到 metasore db如mysql, postgresql等数据库中。（由于spark编译时可以把hive metastore api等相关代码一并打包到spark的二进制安装包中，所以使用这种模式，我们并不需要额外单独安装hive）。 问题对于shell中重新定义spark 123scala&gt; val spark = SparkSession.builder().enableHiveSupport().getOrCreate()2022-05-23 08:47:52 WARN SparkSession$Builder:66 - Using an existing SparkSession; some configuration may not take effect.spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7ecca48 spark和hive版本兼容问题： https://blog.csdn.net/z1941563559/article/details/120764519 如果Windows启动部署Spark，需要注意Java部署的路径中不能有括号等特殊字符。 参考文献及资料1、Pyspark接口说明，链接：https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started]]></content>
      <categories>
        <category>pyspark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python系列文章-Python中的环境变量介绍]]></title>
    <url>%2F2020%2F03%2F02%2F2022-05-27-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Python%E4%B8%AD%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 变量说明 第二部分 总结 参考文献及资料 背景环境说明：Python 3.7.3 使用下面的命令显示相关版本的说明信息： 123456789101112131415161718192021222324# python --help# ......Other environment variables:PYTHONSTARTUP: file executed on interactive startup (no default)PYTHONPATH : ':'-separated list of directories prefixed to the default module search path. The result is sys.path.PYTHONHOME : alternate &lt;prefix&gt; directory (or &lt;prefix&gt;:&lt;exec_prefix&gt;). The default module search path uses &lt;prefix&gt;/lib/pythonX.X.PYTHONCASEOK : ignore case in 'import' statements (Windows).PYTHONIOENCODING: Encoding[:errors] used for stdin/stdout/stderr.PYTHONFAULTHANDLER: dump the Python traceback on fatal errors.PYTHONHASHSEED: if this variable is set to 'random', a random value is used to seed the hashes of str, bytes and datetime objects. It can also be set to an integer in the range [0,4294967295] to get hash values with a predictable seed.PYTHONMALLOC: set the Python memory allocators and/or install debug hooks on Python memory allocators. Use PYTHONMALLOC=debug to install debug hooks.PYTHONCOERCECLOCALE: if this variable is set to 0, it disables the locale coercion behavior. Use PYTHONCOERCECLOCALE=warn to request display of locale coercion and locale compatibility warnings on stderr.PYTHONBREAKPOINT: if this variable is set to 0, it disables the default debugger. It can be set to the callable of your debugger of choice.PYTHONDEVMODE: enable the development mode. 第一部分 变量说明1.1 PYTHONSTARTUP就是一个运行交互式解释器之前会自动调用的一个文件，我们可以在这个文件中放入一些我们想再解释器中事先运行的一些代码，比如导入一些经常会用到的一些模块等等。这个文件是在系统变量中用PYTHONSTARTUP指向的文件。就是在打开一个解释器之前要做的事情。 1.2 PYTHONPATHThe PYTHONPATH variable has a value that is a string with a list of directories that Python should add to the sys.path directory list. 增加模块文件默认搜索路径。 所用格式与终端的 PATH 相同：一个或多个由 os.pathsep 分隔的目录路径名称（例如 Unix 上用冒号而在 Windows 上用分号）。 默认忽略不存在的目录。 1.3 PYTHONHOME第二部分 总结对于zip包文件支持从 Python 2.3 开始，您可以从ZIP 文件中导入模块和包。此功能称为Zip 导入 参考文献及资料1、变量官网介绍：https://docs.python.org/zh-cn/3/using/cmdline.html#environment-variables]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pyspark系列文章-通过toree项目使用Pyspark]]></title>
    <url>%2F2020%2F03%2F02%2F2022-05-23-Pyspark%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E9%80%9A%E8%BF%87toree%E9%A1%B9%E7%9B%AE%E4%BD%BF%E7%94%A8Pyspark%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Receiver接口模式 第二部分 Direct接口模式 第三部分 PySpark和Kafka交互 第四部分 任务提交 参考文献及资料 背景第一部分 安装toree1# pip install toree 安装 1# jupyter toree install --spark_home=/opt/spark-2.3.2/ 参考文献及资料1、Improvements to Kafka integration of Spark Streaming，]]></content>
      <categories>
        <category>pyspark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python系列文章-Pandas使用碎碎念]]></title>
    <url>%2F2020%2F03%2F02%2F2022-06-09-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Pandas%E4%BD%BF%E7%94%A8%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 变量说明 第二部分 总结 参考文献及资料 背景第一部分 数据过滤1.1 按照字段值过滤1df1 = df[df['A']==1] 第二部分 数据合并第三部分 数据替换参考文献及资料1、变量官网介绍：]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pyspark系列文章-通过toree项目使用Pyspark]]></title>
    <url>%2F2020%2F03%2F02%2F2022-05-23-Pyspark%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Py4j%E5%8C%85%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Receiver接口模式 第二部分 Direct接口模式 第三部分 PySpark和Kafka交互 第四部分 任务提交 参考文献及资料 背景第一部分 安装toree1# pip install toree 安装 1# jupyter toree install --spark_home=/opt/spark-2.3.2/ 参考文献及资料1、Improvements to Kafka integration of Spark Streaming，]]></content>
      <categories>
        <category>pyspark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark源码编译实践]]></title>
    <url>%2F2020%2F03%2F02%2F2022-05-25-Spark%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Receiver接口模式 第二部分 Direct接口模式 第三部分 PySpark和Kafka交互 第四部分 任务提交 参考文献及资料 背景编译spark2.X源码这里我们使用源码包中自带的make-distribution.sh文件进行编译。当然在编译之前你可以试着修改一些源代码。在spark源码目录下运行 ./dev/make-distribution.sh –name “hadoop2-without-hive” –tgz “-Pyarn,hadoop-provided,hadoop-2.7,parquet-provided,-Dscala-2.11” -rf :spark-repl_2.11./dev/make-distribution.sh –name “hadoop2-hive” –tgz “-Pyarn,-Phive,hadoop-provided,hadoop-2.7,parquet-provided,-Dscala-2.11” -rf :spark-repl_2.11参数解释：-DskipTests，不执行测试用例，但编译测试用例类生成相应的class文件至target/test-classes下。-Dhadoop.version 和-Phadoop： Hadoop 版本号，不加此参数时hadoop 版本为1.0.4 。-Pyarn ：是否支持Hadoop YARN ，不加参数时为不支持yarn 。-Phive和-Phive-thriftserver：是否在Spark SQL 中支持hive ，不加此参数时为不支持hive 。–with-tachyon ：是否支持内存文件系统Tachyon ，不加此参数时不支持tachyon 。–tgz ：在根目录下生成 spark-$VERSION-bin.tgz ，不加此参数时不生成tgz 文件，只生成/dist 目录。 –name ：和–tgz结合可以生成spark-＄VERSION-bin-$NAME.tgz的部署包，不加此参数时NAME为hadoop的版本号。 这样大概要等二十分钟到一个多小时不等，主要取决于网络环境，因为要下载一些依赖包之类的。之后你就可以获得一个spark编译好的包了，解压之后就可以部署到机器上了。 执行以下命令，会在spark-2.0.2下生成文件 spark-2.0.2-bin-hadoop2-with-hive.tgz [root@master spark-2.0.2]# ./dev/change-scala-version.sh 2.11[root@master spark-2.0.2]# ./dev/make-distribution.sh –name “hadoop2-with-hive” –tgz “-Pyarn,-Phive,hadoop-provided,hadoop-2.7,parquet-provided” main:[INFO] Executed tasks[INFO] ————————————————————————[INFO] Reactor Summary:[INFO][INFO] Spark Project Parent POM ……………………… SUCCESS [ 19.119 s][INFO] Spark Project Tags …………………………… SUCCESS [ 7.630 s][INFO] Spark Project Sketch …………………………. SUCCESS [ 6.463 s][INFO] Spark Project Networking ……………………… SUCCESS [ 19.845 s][INFO] Spark Project Shuffle Streaming Service ………… SUCCESS [ 13.890 s][INFO] Spark Project Unsafe …………………………. SUCCESS [ 13.337 s][INFO] Spark Project Launcher ……………………….. SUCCESS [ 23.115 s][INFO] Spark Project Core …………………………… SUCCESS [03:42 min][INFO] Spark Project GraphX …………………………. SUCCESS [ 26.100 s][INFO] Spark Project Streaming ………………………. SUCCESS [01:07 min][INFO] Spark Project Catalyst ……………………….. SUCCESS [02:36 min][INFO] Spark Project SQL ……………………………. SUCCESS [03:26 min][INFO] Spark Project ML Local Library ………………… SUCCESS [ 14.402 s][INFO] Spark Project ML Library ……………………… SUCCESS [02:54 min][INFO] Spark Project Tools ………………………….. SUCCESS [ 3.691 s][INFO] Spark Project Hive …………………………… SUCCESS [01:32 min][INFO] Spark Project REPL …………………………… SUCCESS [ 11.372 s][INFO] Spark Project YARN Shuffle Service …………….. SUCCESS [ 16.772 s][INFO] Spark Project YARN …………………………… SUCCESS [ 27.160 s][INFO] Spark Project Assembly ……………………….. SUCCESS [ 5.484 s][INFO] Spark Project External Flume Sink ……………… SUCCESS [ 22.666 s][INFO] Spark Project External Flume ………………….. SUCCESS [ 22.288 s][INFO] Spark Project External Flume Assembly ………….. SUCCESS [ 5.101 s][INFO] Spark Integration for Kafka 0.8 ……………….. SUCCESS [ 21.637 s][INFO] Spark Project Examples ……………………….. SUCCESS [ 42.329 s][INFO] Spark Project External Kafka Assembly ………….. SUCCESS [ 8.713 s][INFO] Spark Integration for Kafka 0.10 ………………. SUCCESS [ 22.547 s][INFO] Spark Integration for Kafka 0.10 Assembly ………. SUCCESS [ 7.028 s][INFO] Kafka 0.10 Source for Structured Streaming ……… SUCCESS [ 18.807 s][INFO] ————————————————————————[INFO] BUILD SUCCESS[INFO] ————————————————————————[INFO] Total time: 21:43 min[INFO] Finished at: 2018-01-25T11:13:06+08:00[INFO] Final Memory: 76M/327M[INFO] ———————————————————————— rm -rf /opt/spark-2.0.2/dist mkdir -p /opt/spark-2.0.2/dist/jars echo ‘Spark 2.0.2 built for Hadoop 2.7.3’ ————————————————版权声明：本文为CSDN博主「xzg1109」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/xzg1109/article/details/79141179 参考文献及资料1、Pyspark接口说明，链接：https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Catalog.html https://dwgeek.com/pyspark-check-if-table-exists-in-database.html/ 2、https://www.cnblogs.com/cc11001100/p/9463578.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PySpark和Kafka交互总结]]></title>
    <url>%2F2020%2F03%2F02%2F2020-03-02-Pyspark%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Pyspark%E5%92%8CKafka%E4%BA%A4%E4%BA%92%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Receiver接口模式 第二部分 Direct接口模式 第三部分 PySpark和Kafka交互 第四部分 任务提交 参考文献及资料 背景Apache Kafka项目是大数据处理中重要的消息引擎。Spark Streaming作为重要的流处理计算框架通常和Kafka结合使用。Spark Streaming（新旧版本）支持Kafka的编程模型有两种：Receiver模式和Direct模式。Direct模式在Apache Spark 1.3中最新引入，下面是版本演进过程（截止Spark 2.1）。 本文主要介绍Spark Streaming和Kafka进行交互模式和PySpark实现。 0.0 Kafka的Consumer API在正式介绍前，我们首先介绍一下Kafka Consumer API。Kafka在0.9版本前提供了两种版本的Consumer：高级版和简单版。 高级版。消费者客户端不需要去管理偏移量（offset）状态，而是由Zookeeper管理。消费者中断重启后，可以根据上一次记录在Zookeeper中的offset状态信息，继续获取数据（默认为1分钟更新一次Zookeeper中的offset）。通俗的说就是全auto，最适合小白，哈哈。 简单版。消费者客户端需要自行管理offset状态信息（持久化为文件、数据库或者内存等）。 两个客户端的主要差异就是：谁来负责管理offset状态。但是高级的不一定就好，需要具体架构场景具体分析，适合才是最好的，哈哈（扯远了）。 Kafka项目组在0.8.1版本开始重写生产者API，在0.9版本完成新版本API的发版。新版的Consumer API使用原生Java编写（居然不用scala了，啧啧）。当然新版本API也不依赖于Zookeeper。详细细节可以参考官方文章：《Introducing the Kafka Consumer: Getting Started with the New Apache Kafka 0.9 Consumer Client》。 第一部分 Receiver模式1.1 原始方式最开始Kafka和Spark Streaming集成是通过Receiver。即在每个Spark Executor中实例化Apache Kafka”高级版” Consumer API，启动一个Receiver，实现异步消费Kafka消息，并存储在执行程序的内存中，同时更新在Zookeeper中偏移量（offset）。接着Spark Streaming Driver启动作业（jobs）处理已经接受到的缓存数据。 这种方式是朴素和高效的，但是不具备容错性，遇到故障或者Executor重启任务，均会导致缓存消息的丢失和offset状态的滞后。对于一些重要业务数据处理场景，这是无法容忍的架构缺陷。 1.2 预写日志（WAL）模式为了保证容错性，最简单的架构思路就是”存储状态”。Spark项目组引入了预写日志机制（WAL，Write Ahead Logs）。主要思想是：每个Spark Executor中Receiver接受到的数据持久化到分布式文件系统（HDFS）中，数据只有完成持久化，Receiver才会发起更新Zookeeper中的偏移量状态。当发生故障或者Executor重启时，从持久化的预写日志中恢复数据。 引入WAL后，保证了数据不会丢失。但是有这样的场景：Receiver接收到数据并持久化到WAL，但是系统在更新Zookeeper中相应的偏移量时出现故障，更新失败。当从故障中恢复后，由于offset滞后性，就会出现部分数据重复消费。 之所以出现这种场景，因为架构上无法保证状态信息在两个系统中保持数据一致（exactly-once）。为了避免这种情况，架构上需要只指定一个系统来维护这个状态信息。 Spark项目最后决定由Spark侧来管理offset状态，将offset信息持久化在Spark Streaming中（持久化在checkpoint中（hdfs文件系统）、数据库等，如果是client模式，可以存放在client本地文件系统等），当然与Kafka交互的API也要换成”简单版” Consumer API。这就是下面要介绍的Direct接口方式。 第二部分 Direct模式2.1 Direct模式介绍Apache Spark 1.3中项目组引入了Direct接口方式。Direct方式抛弃了Receiver，采取周期性（Batch Intervel）获取Kafka中每个topic的所有partition中的最新offsets信息，然后根据参数spark.streaming.kafka.maxRatePerPartition设置的速度来消费数据。这就避免了和Zookeeper中偏移量的不一致的问题。而且可以保证即使出现故障，每个记录仅仅被消费一次。 具体消费速度计算如下： 假设Spark window窗口设置为60s(60s拉取一次Kafka数据)；Kafka中该Topic有3个Partitions；maxRatePerPartition设置为100。那么每次拉取的最大数据量为： 60* 3 * 100 条数据。 2.2 两种方式的比较Direct模式相比Receiver模式的优势有： Receiver模式需要在内存中和预写日志中保存两份数据，如果数据量较大，特别是当任务的作业出现大量延迟（delay）的时候，会占用大量存储资源。而Direct模式只有在计算的时候才会去拉取Kafka侧数据，Kafka侧仍然要充当数据的缓冲角色（削峰填谷）。 Direct模式中，Kafka中的partition与RDD中的partition是一一对应的（即一个KafkaRDDIterator对应一个 KafkaRDDPartition）并行读取Kafka数据，即天然利用了并发处理优势。而Receiver模式需要创建多个Receiver之后，可以利用union方法合并成一个Dstream的方式提高数据传输的并行度（后面程序实现将详细介绍）。 Direct模式保证了流计算Spark Streaming和Kafka管道数据 at least once语义。 第三部分 PySpark和Kafka交互3.1 PySpark中Receiver接口3.1.1 接口说明pyspark.streaming.kafka文件中的KafkaUtils类： 123def createStream(ssc, zkQuorum, groupId, topics, kafkaParams=None, storageLevel=StorageLevel.MEMORY_AND_DISK_2, keyDecoder=utf8_decoder, valueDecoder=utf8_decoder) 参数说明： ssc：StreamingContext对象； zkQuorum：Zookeeper集群地址，格式为：hostname:port,hostname:port,..，逗号隔开； groupId：消费者群组名； topics：主题名，字典类型。例如：{“test”：1}，其中1表示线程数量（core）。 kafkaParams：可以补充的Kafka其他参数，字典数据类型；如果非空，其他Kafka的参数设置失效。 storageLevel：RDD的存储级别，StorageLevel参数决定如何存储RDD。在Spark中，StorageLevel决定RDD是应该存储在内存中还是存储在磁盘上，或两者都存储。它还决定是否序列化RDD以及是否复制RDD分区。参数类型有： 1234567891011DISK_ONLY = StorageLevel（True，False，False，False，1）DISK_ONLY_2 = StorageLevel（True，False，False，False，2）MEMORY_AND_DISK = StorageLevel（True，True，False，False，1）MEMORY_AND_DISK_2 = StorageLevel（True，True，False，False，2）MEMORY_AND_DISK_SER = StorageLevel（True，True，False，False，1）MEMORY_AND_DISK_SER_2 = StorageLevel（True，True，False，False，2）MEMORY_ONLY = StorageLevel（False，True，False，False，1）MEMORY_ONLY_2 = StorageLevel（False，True，False，False，2）MEMORY_ONLY_SER = StorageLevel（False，True，False，False，1）MEMORY_ONLY_SER_2 = StorageLevel（False，True，False，False，2）OFF_HEAP = StorageLevel（True，True，True，False，1） 从命名规范也很容易理解，MEMORY是内存，DISK是磁盘，SER表示是否序列化，数字是副本数量。那么默认参数：MEMORY_AND_DISK_2，含义是：数据同时存储内存和磁盘，并且副本数量为2（存储在不同节点）。即开启了WAL机制。 3.1.2 案例介绍下面是计算单词数的流处理任务代码案例： 123456789101112131415161718192021222324252627282930313233# -*- coding: utf-8 -*-from pyspark import SparkContext, SparkConffrom pyspark.streaming import StreamingContextfrom pyspark.streaming.kafka import KafkaUtilszkQuorum = "localhost:2181"topics = "test"groupid = "test"if __name__ == "__main__": SparkConf = SparkConf() SparkConf.set("spark.streaming.kafka.maxRatePerPartition",5000) # 开启WAL机制 #sparkConf.set("spark.streaming.receiver.writeAheadLog.enable", "true") sc = SparkContext(appName="PythonStreamingKafkaWordCount", conf=SparkConf) ssc = StreamingContext(sc, 20) # 偏移量模式：largest、smallest、none，默认largest kafkaParams = &#123;"auto.offset.reset": "smallest"&#125; # 提高并行度 numStreams = 3 kafkaStreams = [KafkaUtils.createStream(ssc, zkQuorum, groupid, &#123;topics：1&#125;, kafkaParams) for _ in range (numStreams)] unifiedStream = ssc.union(*kafkaStreams) # 统计单词数 lines = unifiedStream.map(lambda x: x[1]) counts = lines.flatMap(lambda line: line.split(" ")) \ .map(lambda word: (word, 1)) \ .reduceByKey(lambda a, b: a+b) # 打印 counts.pprint() ssc.start() ssc.awaitTermination() 关于auto.offset.reset参数补充说明一下。原生的API中这个参数有三个值： earliest：自动将偏移重置为最早的偏移量； latest（默认值）：自动将偏移量重置为最新偏移量； none：如果consumer group中没有发现先前的偏移量，则抛出异常； 与Spark Streaming整合后，有两个参数： smallest：从头开始消费，等价于上面的 earliest； largest（默认值）：从最新的开始消费，等价于上面的 latest； 在spark-streaming-kafka-0-10新客户端中，这个参数也有none值（offest保存在kafka的一个特殊的topic名为:__consumer_offsets里面）。程序具体判断逻辑是：如果存在已经提交的offest时,不管设置为earliest 或者latest 都会从已经提交的offest处开始消费。如果不存在已经提交的offest时,使用参数auto.offset.reset的值。当值为none时，topic各分区都存在已提交的offset时，从提交的offest处开始消费；只要有一个分区不存在已提交的offset，则抛出异常。 程序中我们将并发度设置为3，即实例化了3个DStreams，最后union合并。这个处理效果可以在作业的UI界面中看到差异： 补充UI界面。 https://www.cnblogs.com/juncaoit/p/9452333.html 3.2 PySpark中Direct接口Python API在Spark 1.4中引入了此功能。 3.2.1 接口说明pyspark.streaming.kafka文件中的KafkaUtils类： 123def createDirectStream(ssc, topics, kafkaParams, fromOffsets=None, keyDecoder=utf8_decoder, valueDecoder=utf8_decoder, messageHandler=None): 参数说明： ssc：StreamingContext对象； topics：主题名，List数据类型（支持多个topic同时消费）； kafkaParams：Kafka参数，字典格式； fromOffsets：offset状态信息（Per-topic/partition Kafka offsets defining the (inclusive) starting point of the stream.）。如果没有指定，则使用参数：{&quot;auto.offset.reset&quot;: &quot;largest&quot;}。 需要特别注意的是：createDirectStream在spark-streaming-kafka-0-8下不支持group id模式，因为它是使用“简单版”Kafka API。后续spark-streaming-kafka-0-10开始提供对group id模式的支持。在0.9.0.0中，引入了新的Java 客户端API，以替代旧的基于Scala的简单和高级API。Spark Streaming integration for Kafka 0.10是使用新的API，支持group id参数。 3.2.2 案例介绍下面的是接口的使用案例。 123456789101112131415161718192021222324252627282930313233# -*- coding: utf-8 -*-from pyspark import SparkContext, SparkConffrom pyspark.streaming import StreamingContextfrom pyspark.streaming.kafka import KafkaUtilsbrokers = "localhost:9092"topic = "test"offsetRanges = []def storeOffsetRanges(rdd): global offsetRanges offsetRanges = rdd.offsetRanges() return rdddef printOffsetRanges(rdd): for o in offsetRanges: print(o.topic,o.partition,o.fromOffset,o.untilOffset,o.untilOffset-o.fromOffset)if __name__ == "__main__": conf = SparkConf().set("spark.streaming.kafka.maxRatePerPartition", 5000) sc = SparkContext(appName="PythonStreamingDirectKafkaWordCount",conf=conf) ssc = StreamingContext(sc, 60) kafkaStreams = KafkaUtils.createDirectStream(ssc, [topic],&#123;"metadata.broker.list": brokers&#125;) lines = kafkaStreams.map(lambda x: x[1]) counts = lines.flatMap(lambda line: line.split(' ')) \ .map(lambda word: (word, 1)) \ .reduceByKey(lambda a, b: a+b) kafkaStreams.transform(storeOffsetRanges).foreachRDD(printOffsetRanges) counts.pprint() ssc.start() ssc.awaitTermination() 我们启动一个Kafka 生产者每秒发送一条信息hello world,截取回显： 1234567891011121314151617test 1 4599722 4599724 2 test 0 4596564 4596567 3 test 2 4605282 4605282 0 ------------------------------------------- Time: 2020-06-19 16:22:00 ------------------------------------------- ('world', 5) ('hello', 5) test 1 4599724 4599748 24 test 0 4596567 4596586 19 test 2 4605282 4605299 17 ------------------------------------------- Time: 2020-06-19 16:23:00 ------------------------------------------- ('world', 60) ('hello', 60) 回显中： 12test 1 4599724 4599748 24 分别对应下面的字段：o.topic(主题名) o.partition（分区名） o.fromOffset（起始位移）,o.untilOffset（终止位移）,o.untilOffset-o.fromOffset（终止位移和起始位移的差） 上面的程序我们只是打印了offset的状态信息（参考官方案例），但是并没有对offset状态进行持久化处理。如果任务故障或终止，重新启动时候，任务会重新开始消费。在实际生产环境中，一些高可用数据处理场景，这是不可容忍的。offset状态数据需要进行高可用持久化处理。这就是接下来我们介绍的checkpoint机制。 3.3 checkpoint机制3.3.1 持久化（persist）和Checkpoint机制Spark中对于RDD的容错性是通过storageLevel参数设置RDD存储级别（持久化级别）。persist()的默认参数为MEMORY_ONLY（即storageLevel=MEMORY_ONLY），即内存缓存，称为缓存（cache）。在这种存储级别下，Spark计算出的RDD结果将缓存在内存中，一旦计算任务中一个执行器（Executor）故障下宕，缓存在该执行器的RDD数据就会丢失，执行器重建后需要通过RDD依赖链重新计算。 例如Receiver模式，Pyspark中接口默认使用MEMORY_AND_DISK_2。在该存储级别下，RDD除了缓存在内存还会持久化到磁盘（2份），当执行器失败可以从磁盘中加载持久化数据。但是一旦Spark的Driver故障下宕或者任务正常结束，计算的所有存储资源将被集群回收。 而Checkpoint机制将RDD持久化到HDFS文件系统，天然的利用了HDFS的分布式高可用文件系统特性。 3.3.2 Checkpoint机制的实现前面的Receiver模式的例子中，我们开启了WAL机制。但是这种机制是执行器级别的高可用。这里我们提高高可用级别，增加checkpoint机制将RDD持久化到HDFS分布式文件系统中。这样即使Spark流任务的Driver重启依然能从checkpoint重启启动，继续消费数据。 1234567891011121314151617181920212223242526272829303132333435363738# -*- coding: utf-8 -*-import sysfrom pyspark import SparkContext, SparkConffrom pyspark.streaming import StreamingContextfrom pyspark.streaming.kafka import KafkaUtils# 定义checkpointcheckpointDirectory = "hdfs://user/python/kafka/checkpoint"zkQuorum = "localhost:2181"topics = "test"groupid = "test"def functionToCreateContext(): sparkConf = SparkConf() sparkConf.set("spark.streaming.receiver.writeAheadLog.enable", "true") sc = SparkContext(appName="PythonStreamingKafkaWordCount",conf=sparkConf) ssc = StreamingContext(sc, 60) global zkQuorum global topics kafkaStreams = KafkaUtils.createStream(ssc, zkQuorum, "spark-streaming-consumer", &#123;topic: 1&#125;) lines = kafkaStreams.map(lambda x: x[1]) counts = lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b) counts.pprint() ssc.checkpoint(checkpointDirectory) # set checkpoint directory return sscif __name__ == "__main__": zkQuorum = "localhost:2181" topics = "test" groupid = "test" #如果 checkpointDirectory 目录存在，则context对象会从检查点数据重新构建出来。如果该目录不存在（如：首 #次运行），则 functionToCreateContext 函数会被调用，创建一个新的StreamingContext对象并定义好 #DStream数据流。 ssc = StreamingContext.getOrCreate(checkpointDirectory, lambda: functionToCreateContext()) ssc.start() ssc.awaitTermination() 但启动一个具有checkpoint机制的spark任务的时候。通过函数getOrCreate实现： StreamingContext.getOrCreate(checkpointDirectory, lambda: functionToCreateContext()) 函数参数的具体含义就是：首先检查是否有checkpoint（即checkpointDirectory）。如果非空，StreamingContext从checkpointDirectory加载启动。如果没有执行函数functionToCreateContext()创建（函数中已经声明了创建逻辑）。具体数据流参考下图： 3.4 自行管理offset状态除了checkpoint机制，例如下面代码将offset信息持久化到本地文件系统。需要注意的是该持久方式需要任务为Client模式提交集群，否则保存在Drive中本地文件系统会被集群回收。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/usr/bin/python# coding=utf-8from pyspark import SparkContextfrom pyspark.streaming import StreamingContextfrom pyspark.streaming.kafka import KafkaUtils, TopicAndPartitionimport timeimport osimport jsonbroker_list = "xxxx"topic_name = "xxxx"timer = 60offsetRanges = []def store_offset_ranges(rdd): global offsetRanges offsetRanges = rdd.offsetRanges() return rdddef save_offset_ranges(rdd): root_path = os.path.dirname(os.path.realpath(__file__)) record_path = os.path.join(root_path, "offset.txt") data = dict() f = open(record_path, "w") for o in offsetRanges: data = &#123;"topic": o.topic, "partition": o.partition, "fromOffset": o.fromOffset, "untilOffset": o.untilOffset&#125; f.write(json.dumps(data)) f.close()def deal_data(rdd): data = rdd.collect() for d in data: # do something passdef save_by_spark_streaming(): # 定义本地文件系统 root_path = os.path.dirname(os.path.realpath(__file__)) record_path = os.path.join(root_path, "offset.txt") # from_offsets = &#123;&#125; # 获取已有的offset，没有记录文件时则用默认值即最大值 if os.path.exists(record_path): f = open(record_path, "r") offset_data = json.loads(f.read()) f.close() if offset_data["topic"] != topic_name: raise Exception("the topic name in offset.txt is incorrect") topic_partion = TopicAndPartition(offset_data["topic"], offset_data["partition"]) from_offsets = &#123;topic_partion: long(offset_data["untilOffset"])&#125; # 注意设置起始offset时的方法 print("start from offsets: %s" % from_offsets) sc = SparkContext(appName="Realtime-Analytics-Engine") ssc = StreamingContext(sc, int(timer)) kafkaStreams = KafkaUtils.createDirectStream(ssc=ssc, topics=[topic_name], fromOffsets=from_offsets,kafkaParams=&#123;"metadata.broker.list": broker_list&#125;) kafkaStreams.foreachRDD(lambda rec: deal_data(rec)) kafkaStreams.transform(store_offset_ranges).foreachRDD(save_offset_ranges) ssc.start() ssc.awaitTermination() #ssc.stop()if __name__ == '__main__': save_by_spark_streaming() 上面的例子中，将offset信息以json格式持久化到文件系统中。实际生产中建议以二维表形式存储在Mysql等数据库中进行持久化（不再详细举例）。 java 保存在mysql的例子 https://www.jianshu.com/p/2369a020e604 https://blog.csdn.net/lxb1022/article/details/78041168 其他交互接口： https://www.cnblogs.com/yanshw/p/11929180.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566from kafka import SimpleClientfrom kafka.structs import OffsetRequestPayloadfrom pyspark.streaming.kafka import KafkaUtils, TopicAndPartitionfrom pyspark.streaming import StreamingContextfrom pyspark import SparkContextfrom algo_core.utils.hbase_util import HappyBaseUtil # 自己封装的happybase包def save_offsets(topic_name, group_id, offset_ranges, hbase_table_name): happybase_util = HappyBaseUtil() for offset in offset_ranges: happybase_util.put(hbase_table_name, topic_name+"_"+group_id, &#123;"offsets:"+str(offset.partition): str(offset.untilOffset)&#125;)def get_last_committed_offsets(topic_name, group_id, hbase_table_name): # client = SimpleClient('localhost:9092') client = SimpleClient(["xxxxx:9092","xxxxx:9092","xxxxx:9092"]) # 获取zookeeper中kafka topic的partition topic_partition_ids = client.get_partition_ids_for_topic(topic_name) happybase_util = HappyBaseUtil() # 获取hbase存放的kafka topic的partition partition_offset_values = happybase_util.get_row(hbase_table_name, row=topic_name+"_"+group_id) if len(partition_offset_values) == 0: # 第一次运行处理 partitions = client.topic_partitions[topic_name] offset_requests = [OffsetRequestPayload(topic_name, p, -1, 1) for p in partitions.keys()] offsets_responses = client.send_offset_request(offset_requests) offsets = dict((TopicAndPartition(topic_name, r.partition), r.offsets[0]) for r in offsets_responses) elif len(partition_offset_values) &lt; len(topic_partition_ids): # 如果hbase中partition个数小于zookeeper中partition的个数，说明有新增的partition，新增的partition偏移量设为0 offsets = dict((TopicAndPartition(topic_name, int(k.decode("utf-8").split(":")[1])), int(v)) for k, v in partition_offset_values.items()) extra_partitions = dict((TopicAndPartition(topic_name, i), 0) for i in range(len(topic_partition_ids), len(partition_offset_values))) offsets.update(extra_partitions) else: offsets = dict((TopicAndPartition(topic_name, int(k.decode("utf-8").split(":")[1])), int(v)) for k, v in partition_offset_values.items()) return offsetsif __name__ == "__main__": sc = SparkContext(appName="test") sc.setLogLevel("WARN") ssc = StreamingContext(sc, 5) # kafka_params = &#123;"metadata.broker.list": "localhost:9092"&#125; kafka_params = &#123;"metadata.broker.list": "xxxxx:9092,xxxxx:9092,xxxxx:9092"&#125; # fromOffset = get_last_committed_offsets("test", "test-id", "stream_kafka_offsets") fromOffset = get_last_committed_offsets("mytopic", "test-group-2", "stream_kafka_offsets") # kafkaStream = KafkaUtils.createDirectStream(ssc, ["test"], kafka_params, fromOffsets=fromOffset) kafkaStream = KafkaUtils.createDirectStream(ssc, ["mytopic"], kafka_params, fromOffsets=fromOffset) def inner_func(rdd): rdd.foreach(lambda x: print(x)) save_offsets("mytopic", "test-group-2", rdd.offsetRanges(),"stream_kafka_offsets") kafkaStream.foreachRDD(inner_func) ssc.start() ssc.awaitTermination() 第四部分 任务的提交4.1 兼容性Spark 针对 Kafka 的不同版本（主要还是以0.8版本为重要分界线），提供了两种方案：spark-streaming-kafka-0-8 和 spark-streaming-kafka-0-10，其主要区别如下： spark-streaming-kafka-0-8 spark-streaming-kafka-0-10 Broker Version（Kafka版本） 0.8.2.1 or higher 0.10.0 or higher Api Stability Stable Experimental Language Support Scala, Java, Python Scala, Java Receiver DStream Yes No Direct DStream Yes Yes SSL / TLS Support No Yes Offset Commit Api No Yes Dynamic Topic Subscription No Yes 目前只有spark-streaming-kafka-0-8方案是支持python语言的，所以我们提交任务是需要指定相应的依赖jar包。高版本（spark-streaming-kafka-0-10）的方案已经抛弃了对Receiver方式的支持。 spark-streaming-kafka-0-8接口的jar下载路径： spark-streaming-kafka_2.10-1.5.1.jar 和 spark-streaming-kafka-assembly_2.10-1.5.1.jar 如果出现包冲突，提交任务时添加参数--conf spark.yarn.user.classpath.first=true,这样设置后yarn中优先使用用户传上去的jar包,避免包冲突。 4.2 任务提交案例与任何Spark应用程序一样，spark-submit用于启动应用程序。但是，Scala / Java应用程序和Python应用程序的细节略有不同。 对于Scala和Java应用程序，如果您使用SBT或Maven进行项目管理，则将spark-streaming-kafka_2.11其及其依赖项打包到应用程序JAR中。确保spark-core_2.10并spark-streaming_2.10标记为providedSpark安装中已存在的依赖项。然后使用spark-submit启动应用程序。 对于缺少SBT / Maven项目管理的Python应用程序，spark-streaming-kafka_2.11可以直接将其依赖项添加到spark-submit使用中--packages。那是， 1./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka_2.11:1.6.3 ... 另外，您也可以下载Maven构件的JAR spark-streaming-kafka-assembly从 Maven仓库，并将其添加到spark-submit用--jars。 4.3 参数调优建议Spark streaming+Kafka的使用中，当数据量较小，很多时候默认配置和使用便能够满足情况，但是当数据量大的时候，就需要进行一定的调整和优化，而这种调整和优化本身也是不同的场景需要不同的配置。 合理的批处理时间（batchDuration）：几乎所有的Spark Streaming调优文档都会提及批处理时间的调整，在StreamingContext初始化的时候，有一个参数便是批处理时间的设定。如果这个值设置的过短，即个batchDuration所产生的Job并不能在这期间完成处理，那么就会造成数据不断堆积，最终导致Spark Streaming发生阻塞。而且，一般对于batchDuration的设置不会小于500ms，因为过小会导致SparkStreaming频繁的提交作业，对整个streaming造成额外的负担。在平时的应用中，根据不同的应用场景和硬件配置，我设在1~10s之间，我们可以根据SparkStreaming的可视化监控界面，观察Total Delay来进行batchDuration的调整。 合理的Kafka拉取量（maxRatePerPartition重要）：对于Spark Streaming消费kafka中数据的应用场景，这个配置是非常关键的，配置参数为：spark.streaming.kafka.maxRatePerPartition。这个参数默认是没有上线的，即kafka当中有多少数据它就会直接全部拉出。而根据生产者写入Kafka的速率以及消费者本身处理数据的速度，同时这个参数需要结合上面的batchDuration，使得每个partition拉取在每个batchDuration期间拉取的数据能够顺利的处理完毕，做到尽可能高的吞吐量，而这个参数的调整可以参考可视化监控界面中的Input Rate和Processing Time 缓存反复使用的Dstream（RDD）：Spark中的RDD和SparkStreaming中的Dstream，如果被反复的使用，最好利用cache，将该数据流缓存起来，防止过度的调度资源造成的网络开销。可以参考观察Scheduling Delay参数 设置合理的GC：长期使用Java的小伙伴都知道，JVM中的垃圾回收机制，可以让我们不过多的关注与内存的分配回收，更加专注于业务逻辑，JVM都会为我们搞定。对JVM有些了解的小伙伴应该知道，在Java虚拟机中，将内存分为了初生代（edengeneration）、年轻代young generation）、老年代（oldgeneration）以及永久代（permanentgeneration），其中每次GC都是需要耗费一定时间的，尤其是老年代的GC回收，需要对内存碎片进行整理，通常采用标记-清楚的做法。同样的在Spark程序中，JVMGC的频率和时间也是影响整个Spark效率的关键因素。在通常的使用中建议：–conf “spark.executor.extraJavaOptions=-XX:+UseConcMarkSweepGC” 设置合理的CPU资源数：CPU的core数量，每个executor可以占用一个或多个core，可以通过观察CPU的使用率变化来了解计算资源的使用情况，例如，很常见的一种浪费是一个executor占用了多个core，但是总的CPU使用率却不高（因为一个executor并不总能充分利用多核的能力），这个时候可以考虑让么个executor占用更少的core，同时worker下面增加更多的executor，或者一台host上面增加更多的worker来增加并行执行的executor的数量，从而增加CPU利用率。但是增加executor的时候需要考虑好内存消耗，因为一台机器的内存分配给越多的executor，每个executor的内存就越小，以致出现过多的数据spill over甚至out of memory的情况 设置合理的parallelism：partition和parallelism，partition指的就是数据分片的数量，每一次task只能处理一个partition的数据，这个值太小了会导致每片数据量太大，导致内存压力，或者诸多executor的计算能力无法利用充分；但是如果太大了则会导致分片太多，执行效率降低。在执行action类型操作的时候（比如各种reduce操作），partition的数量会选择parent RDD中最大的那一个。而parallelism则指的是在RDD进行reduce类操作的时候，默认返回数据的paritition数量（而在进行map类操作的时候，partition数量通常取自parent RDD中较大的一个，而且也不会涉及shuffle，因此这个parallelism的参数没有影响）。所以说，这两个概念密切相关，都是涉及到数据分片的，作用方式其实是统一的。通过spark.default.parallelism可以设置默认的分片数量，而很多RDD的操作都可以指定一个partition参数来显式控制具体的分片数量。 在SparkStreaming+kafka的使用中，我们采用了Direct连接方式，前文阐述过Spark中的partition和Kafka中的Partition是一一对应的，我们一般默认设置为Kafka中Partition的数量。 使用高性能的算子：（1）使用reduceByKey/aggregateByKey替代groupByKe(2)使用mapPartitions替代普通map(3) 使用foreachPartitions替代foreach(4) 使用filter之后进行coalesce操作5 使用repartitionAndSortWithinPartitions替代repartition与sort类操作 使用Kryo优化序列化性能主要有三个地方涉及到了序列化在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）： // 创建SparkConf对象。 val conf = new SparkConf.setMaster(…).setAppName(…) //设置序列化器为KryoSerializer。conf.set(“spark.serializer”,”org.apache.spark.serializer.KryoSerializer”) //注册要序列化的自定义类型。conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) https://www.cnblogs.com/frankdeng/p/9308585.html 参考文献及资料1、Improvements to Kafka integration of Spark Streaming，链接：https://databricks.com/blog/2015/03/30/improvements-to-kafka-integration-of-spark-streaming.html 2、Source code for pyspark.streaming.kafka，链接：https://spark.apache.org/docs/2.2.0/api/python/_modules/pyspark/streaming/kafka.html 3、Introducing Spark Streaming，链接：https://engineering.billymob.com/introducing-spark-streaming-c1b8be36c775 4、Spark Streaming基于kafka的Direct详解，链接：https://blog.csdn.net/matrix_google/article/details/80033524 5、Enabling fault-tolerant processing in Spark Streaming，链接:https://docs.cloudera.com/runtime/7.0.2/developing-spark-applications/topics/spark-streaming-fault-tolerance.html 6、Introducing the Kafka Consumer: Getting Started with the New Apache Kafka 0.9 Consumer Client，链接：https://www.confluent.io/blog/tutorial-getting-started-with-the-new-apache-kafka-0-9-consumer-client/ 7、官网文档，链接：https://spark.apache.org/docs/1.6.3/streaming-kafka-integration.html 8、官网文档，链接：https://spark.apache.org/docs/2.3.1/streaming-kafka-0-10-integration.html]]></content>
      <categories>
        <category>pyspark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka系列文章（第五篇 Kafka安全集群）]]></title>
    <url>%2F2020%2F03%2F02%2F2020-01-01-Kafka%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88%E7%AC%AC%E4%BA%94%E7%AF%87Kafka%E5%AE%89%E5%85%A8%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Kafka集群加密传输 第二部分 Kafka集群权限认证 第三部分 加密认证集群的客户端 第四部分 加密认证集群的性能压测 第五部分 总结 参考文献及资料 背景Kafka在0.9.0.0版本前没有安全机制功能。Kafka Client程序可以直接获取到Kafka集群元数据信息和Kafka Broker地址后，连接到Kafka集群，然后完全操作集群上的所有topic数据资源。另外集群节点间通讯、broker和zookeeper通讯、客户端和集群的网络层通信都是无加密模式。集群的数据存在极大的安全风险。 自0.9.0.0版本开始，Kafka社区逐步添加了较多功能用于提高Kafka群集的安全性。目前Kafka安全集群安全机制主要有三个方面的设置：通信加密（encryption）、身份认证（authentication）和授权（authorization）。 本文重点介绍生产安全集群的一种配置方案。数据通讯传输配置SSL，认证配置SASL，授权通过ACL接口命令来完成的，即：SSL+SASL/SCRAM+ACL。 第一部分 Kafka集群加密传输1.1 背景知识介绍涉及的技术知识不做详细介绍。 1.1.1 密码学基础加密算法分为两类： 对称密钥算法（Symmetric Cryptography）：数据加密和解密时使用相同的密钥。例如常用的DES就是对称加密算法。 非对称密钥算法（Asymmetric Cryptography）：数据加密和解密时使用不同的密钥，分为：公开的公钥（public key）和用户保存的私钥（private key），私钥和公钥在数学上是相关的。利用公钥（或私钥）加密的数据只能用相应的私钥（或公钥）才能解密。举一个例子：客户在银行网银上做一笔交易，首先向银行申请公钥，银行分发公钥给用户，用户使用公钥对请求数据进行加密。银行收到加密数据后通过银行侧保存的私钥进行解密处理，并处理后更新后台数据库。这个通讯过程中银行不需要通过互联网分发私钥。因此保证了私钥的安全。目前最常用的非对称加密算法是RSA算法。 非对称密钥算法中，私钥来解密公钥加密的数据，公钥来解密私钥加密的数据。 两种加密算法的比较： 对称密钥的强度和密钥长度成正比，但是解密效率和密钥长度成反比。另外私钥的分发存在安全风险。 非对称加密保证了私钥的安全性，但是加密和解密的效率比对称加密低。 所以通常加密场景是两种密钥结合使用。使用数据主体使用对称秘钥算法，但是私钥的传输使用非对称算法在互联网环境分发非对称密钥。最常见的就是SSL/TLS。 1.1.2 CA数字证书对于非对称密钥算法存在一个安全风险点，那就是公钥的分发存在中间人攻击。还是以客户和银行的通信为例（例子简单化处理）。客户和银行分别有自己的公钥和私钥，私钥各自保留本地。公钥通过互联网分发给对方。那么公钥就是有安全风险的。存在被黑客截取风险。客户向银行申请银行公钥，结果被黑客截取，黑客伪装成银行，返回给用户自己的黑客公钥，用户收到黑客公钥后，将信息加密发给黑客。黑客用黑客私钥进行解密，获取到真实信息。这时候黑客伪装成客户用相同的方法完成和银行的数据交互。这就是中间人攻击的案例。 所以非对称加密算法的公钥传输同样存在风险。当然如果使用原始的离线方式交换密钥是安全的，但是随着互联网通信的爆炸式增长，这是落后低效的。为了保证公钥的真实性和安全性，这时候我们引入第三个角色：公开密钥认证（Public key certificate，简称CA），又称数字证书（digital certificate）或身份证书（identity certificate）。 通常CA是一家第三方权威机构。负责管理和签发证书。整个实现原理也是非对称加密算法： 机构将自己的公钥以及身份信息交给CA机构（安全的），CA使用自己的私钥对各机构的公钥进行加密。这个过程称为验签。输出的加密后的公钥及身份信息称为数字证书。 当其他机构请求A机构公钥的时候，返回的是A机构的数字证书。其他机构可以使用CA的公钥对该数字证书中加密公钥进行解密获取A机构的通信公钥。 那么新得安全问题又来了，如何保证CA机构的公钥不被伪造？通常CA的公钥是集成在浏览器或者操作系统中，并且被很好的保护起来。 当然CA证书还涉及更多的安全细节设计（Hash算法防篡改、信任链等大量细节），这里只是简单的介绍。详细介绍可以查看：维基（证书颁发机构） 对于企业内部的应用系统就没必要花钱购买CA机构的证书服务了，可以自建 Root CA，自己给自己颁发证书，充当内网的CA机构。当然这时候客户端就需要导入CA的证书了（浏览器和操作系统没有自建的CA证书）。 1.1.3 SSL/TLS加密协议SSL（Secure Sockets Layer）是一种安全协议，目的是为保障互联网上数据传输安全，利用数据加密技术，确保数据在网络上之传输过程中不会被截取。 从网络协议层看，SSL协议位于TCP/IP协议与应用层协议之间，为数据通讯提供安全支持。SSL协议自身可分为两层： SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。 SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。例如HTTPS就是在HTTP应用层上增加了SSL加密协议支持（HTTP over SSL）。 TLS(Transport Layer Security，传输层安全协议)，同样用于两个应用程序之间提供保密性和数据完整性。 TLS 1.0建立在SSL 3.0协议规范之上，是SSL 3.0的后续版本，可以理解为SSL 3.1，即是SSL的升级版。TLS的主要目标是使SSL更安全，并使协议的规范更精确和完善。另外,TLS版本号也与SSL的不同(TLS的版本1.0使用的版本号为SSLv3.1) SSL通过握手过程在client和server之间协商会话參数，并建立会话。一共有三种方式： 仅仅验证server的SSL握手过程（单向SSL） 验证server和client的SSL握手过程（双向SSL） 恢复原有会话的SSL握手过程 第一种：单向SSL通信过程如下（SSL 客户端和SSL 服务端通信）： (1)SSL客户端向SSL服务端发起请求，请求信息包括SSL版本号、加密算法、密钥交换算法、MAC算法等信息； (2)SSL服务端确定本次通话的SSL版本和加密套件后，将携带公钥信息的证书回给客户端。如果通话可从重用，还会返回会话ID； (3)SSL服务端发送Server Hello Done消息。通知SSL客户端版本号和加密套件协商结束，开始进行密钥交换； (4)SSL客户端对CA证书进行验证，证书合法则继续、不成功弹出选择页面； (5)SSL客户端生产随机私有对称密钥key，并使用服务端公开密钥进行加密后，发给服务端； (6)SSL服务端使用自己的私钥解密，获取对称密钥key； (7)最后SSL客户端与SSL服务端将使用该对称密钥key进行加密通信。 第二种：单向认证，仅仅是客户端需要检验服务端证书是否是正确的。双向SSL和单向认证几乎一样，只是在客户端认证完服务器证书后，客户端会将自己的证书传给服务器。服务器验证通过后，才开始秘钥协商。 第三种：协商会话参数、建立会话的过程中，需要使用非对称密钥算法来加密密钥、验证通信对端的身份，计算量较大，占用了大量的系统资源。为了简化SSL握手过程，SSL允许重用已经协商过的会话。即可以重用会话ID。这就是第三种建立会话方式。 1.1.4 Openssl工具对于企业内部（内部局域网）的应用系统通讯，如果需要CA证书服务，可以使用Openssl自建CA，并完成证书签发。 先说一下常用密钥类文件的规范： 后缀名规范 通常约定后缀含义：crt或者cert 表示证书, key表示私钥, req和csr表示请求文件。 文件格式 pem表示pem格式（经过加密的文本文件），der表示der格式（经过加密的二进制文件）。所有证书和私钥可以是pem,也可以是der格式，取决于需要。两个格式可以转换。 Openssl的配置文件（openssl.cnf）定义CA的默认参数，例如ubuntu系统中配置文件位置在/usr/lib/ssl/openssl.cnf。如果不适用默认参数需要在命令中重新指定。 CA证书的制作 首先生成CA的私钥，使用下面的命令： 1$ openssl genrsa -out private/ca.key.pem 2048 private/ca.key.pem是CA私钥,格式为pem，长度（加密位数）为2048。 前面密码学知识知道CA使用一对密钥的（私钥和公钥），并且两个密钥是数学相关的。公钥可以通过私钥算出来。 CA证书自签发 参考命令如下： 1$ openssl req -new -x509 -key private/ca.key.pem -out certs/ca.cert.pem certs/ca.cert.pem 即CA的自签证书。部署导入到客户端（例如浏览器）。 用户证书签发 用户证书的签发和CA自签相同，用户证书由CA私钥签发。用户需要提供请求文件。 1$ openssl ca -in app.csr -out app.crt -days 365 app.crt为签发的证书。部署在应用服务器上。 1.1.5 Keytool工具介绍在密钥证书管理时，通常使用JAVA的Keytool工具程序。Keytool 是一个JAVA数据证书的管理工具 ,Keytool 将密钥（key）和证书（certificates）存在一个称为keystore的文件中，通常称为密钥库文件。文件的扩展名通常使用：jks，全名java key store file。 Keytool是一个Java数据证书的管理工具，所以节点需要配置JAVA_HOME环境变量。 这里列举了命令支持的参数含义及注意点（供后续使用查阅）： keystore 参数指定保存证书的文件（密钥库二进制文件）。密钥库文件包含证书的私钥，必须对其进行安全保存。 validity 参数指定密钥有效期，单位是天。默认为90天。 keyalg 参数指定密钥使用的加密算法（例如RSA，如果不指定默认采用DSA）。 keysize 参数指定密钥的长度。该参数是选项参数，默认长度是1024位。为了保证密钥安全强度，建议密码长度设置为2048位。 keypass 参数指定生成密钥的密码（私钥密码）。 storepass 指定密钥库的密码(获取keystore信息所需的密码)。另外密钥库创建后，要对其做任何修改都必须提供该密码，以便访问密钥库。 alias 参数指定密钥别名。每个密钥文件有一个唯一的别名，别名不区分大小写。 dname 参数指定证书拥有者信息。例如： “CN=名字与姓氏,OU=组织单位名称,O=组织名称,L=城市或区域名称,ST=州或省份名称,C=单位的两字母国家代码”。 list 参数显示密钥库中的证书信息。keytool -list -v -keystore 指定keystore -storepass 密码 v 参数显示密钥库中的证书详细信息。 export 将别名指定的证书导出到文件。keytool -export -alias 需要导出的别名 -keystore 指定keystore -file 指定导出的证书位置及证书名称 -storepass 密码。 file 参数指定导出到文件的文件名。 delete 删除密钥库中某条目。keytool -delete -alias 指定需删除的别名 -keystore 指定keystore -storepass 密码 printcert 查看导出的证书信息。keytool -printcert -file yushan.crt keypasswd 修改密钥库中指定条目口令。keytool -keypasswd -alias 需修改的别名 -keypass 旧密码 -new 新密码 -storepass keystore密码 -keystore sage storepasswd 修改keystore口令。keytool -storepasswd -keystore e:/yushan.keystore(需修改口令的keystore) -storepass 123456(原始密码) -new newpasswd(新密码) import 将已签名数字证书导入密钥库。keytool -import -alias 指定导入条目的别名 -keystore 指定keystore -file 需导入的证书 关于Keytool工具的详细介绍，可以参考oracle的官网。 1.2 Kafka集群配置SSL加密Apache Kafka允许客户端通过SSL连接。默认情况下，SSL是禁用的，可以根据需要打开。 1.2.1 集群环境准备为了后文讲解方便，我们部署了Kafka集群（3节点）和Zookeeper集群（3节点）测试环境。其中zookeeper和kafka混合部署。 节点编号 hostname IP地址 1 kafka.app.node1 192.168.1.5 2 kafka.app.node2 192.168.1.6 3 kafka.app.node3 192.168.1.7 Kafka集群节点对外服务端口为：9092；Zookeeper集群节点对外服务端口为：2181。 1.2.2 配置主机名验证从Kafka 2.0.0版开始，默认会为客户端连接以及broker之间的连接启用服务器的主机名验证（SSL端点识别算法），以防止中间人攻击。可以通过设置参数ssl.endpoint.identification.algorithm为空字符串来禁用服务器主机名验证。例如: 1ssl.endpoint.identification.algorithm= 另外高版本支持不停集群服务下，进行动态配置，使用脚本kafka-configs.sh，参考命令如下： 1bin/kafka-configs.sh --bootstrap-server localhost:9093 --entity-type brokers --entity-name 0 --alter --add-config "listener.name.internal.ssl.endpoint.identification.algorithm=" 对于较旧的Kafka版本，ssl.endpoint.identification.algorithm默认情况下未定义，因此不会启用主机名验证。若该属性设置HTTPS，则启用主机名验证，例如： 1ssl.endpoint.identification.algorithm=HTTPS 需要注意的是，一旦启用主机名验证，客户端将根据以下两个字段之一验证服务器的完全限定域名（FQDN）： 通用名称（CN，Common Name） 主题备用名称（SAN，Subject Alternative Name） 两个字段都有效，但RFC-2818建议使用SAN。 SAN也更灵活，允许声明多个DNS条目。 另一个优点是，CN可以设置为更有意义的值用于授权。如要添加SAN字段，需要将以下参数-ext SAN = DNS：{FQDN}添加到keytool命令中，例如： 1$ keytool -keystore server.keystore.jks -alias localhost -validity &#123;validity&#125; -genkey -keyalg RSA -ext SAN=DNS:&#123;FQDN&#125; 更通俗一点讲，SSL 握手期间验证主机名时，它会检查服务器证书是否具有 SAN 集。如果检测到 SAN 集，那么只使用 SAN 集中的名称或 IP 地址。如果未检测到 SAN 集，那么只使用主题专有名称 (DN) 最重要的属性，通常是通用名称(CN)。将该值与客户端尝试连接的服务器启的主机名进行比较。如果它们相同，主机名验证成功，允许建立连接。 1.2.3 生成SSL密钥和证书（密钥库）为了方便管理证书密钥，我们使用统一的路径保存。例如统一放在/usr/ca作为文件目录。 1$ mkdir -p /usr/ca/&#123;root,server,client,trust&#125; 这里各文件夹的功能是：root：存储CA私钥和证书；server：存储服务端的私钥和证书；client：存储客户端私钥和证书；trust：存储信任库文件； 节点1（kafka.app.node1） 1$ keytool -keystore /usr/ca/server/server.keystore.jks -alias kafka.app -validity 3650 -genkey -keypass app123 -keyalg RSA -dname "CN=kafka.app.node1,OU=depart,O=org,L=shanghai,S=shanghai,C=cn" -storepass app123 -ext SAN=DNS:kafka.app.node1 其中dname参数的含义参考Keytool工具介绍，文件名为：server.keystore.jks，这是密钥库。 节点2（kafka.app.node2） 1$ keytool -keystore /usr/ca/server/server.keystore.jks -alias kafka.app -validity 3650 -genkey -keypass app123 -keyalg RSA -dname "CN=kafka.app.node1,OU=depart,O=org,L=shanghai,S=shanghai,C=cn" -storepass app123 -ext SAN=DNS:kafka.app.node2 节点3（kafka.app.node3） 1$ keytool -keystore /usr/ca/server/server.keystore.jks -alias kafka.app -validity 3650 -genkey -keypass app123 -keyalg RSA -dname "CN=kafka.app.node1,OU=depart,O=org,L=shanghai,S=shanghai,C=cn" -storepass app123 -ext SAN=DNS:kafka.app.node3 证书生成后可以通过下面的命令进行查询（需要输入密钥库管理密码，即keypass的参数）： 1$ keytool -list -v -keystore server.keystore.jks 1.2.4 创建Kafka集群CA证书集群中每个服务节点都有一对公钥和私钥，以及用于标识该节点的证书。但这个证书是未签名的，存在中间者攻击的风险。所以需要证书颁发机构（CA）负责签署颁发证书，使用openssl工具实现。 同一个集群的所有节点共用一个CA证书，所以只需要在集群的一个节点（集群外节点均可）生成CA证书，然后分发给集群其他节点。例如在kafka.app.node1节点上创建CA证书，命令如下： 1$ openssl req -new -x509 -keyout /usr/ca/root/ca.key.pem -out /usr/ca/root/ca.cert.pem -days 3650 -passout pass:app123 -subj "/C=cn/ST=shanghai/L=shanghai/O=org/OU=depart/CN=kafka.app.node1" 然后使用scp命令分发给其他节点： 12$ scp /usr/ca/root/* root@kafka.app.node2:/usr/ca/root/$ scp /usr/ca/root/* root@kafka.app.node3:/usr/ca/root/ 生成两个文件，分别是私钥（ca.key.pem）和证书（ca.cert.pem），它用来签署其他证书。 1.2.5 集群服务节点签署证书首先给集群各服务节点签发证书（即签名）。步骤如下： 第一步 从密钥容器中提取和导出服务端证书（输出文件：server.cert-file，未签名） 1$ keytool -keystore /usr/ca/server/server.keystore.jks -alias kafka.itdw -certreq -file /usr/ca/server/server.cert-file -storepass app123 第二步 给服务端证书签名（输出文件：server.cert-signed，已签名） 1$ openssl x509 -req -CA /usr/ca/root/ca.cert.pem -CAkey /usr/ca/root/ca.key.pem -in /usr/ca/server/server.cert-file -out /usr/ca/server/server.cert-signed -days 365 -CAcreateserial -passin pass:app123 第三步 将CA证书导入服务端密钥容器中 1$ keytool -keystore /usr/ca/server/server.keystore.jks -alias CARoot -import -file /usr/ca/root/ca.cert.pem -storepass app123 第四步 将已签名的证书导入密钥容器中 1$ keytool -keystore /usr/ca/server/server.keystore.jks -alias kafka.app -import -file /usr/ca/server/server.cert-signed -storepass app123 需要注意集群上每个服务节点均需要签署。 1.2.6 生成服务端信任库如果kafka集群中配置中的参数ssl.client.auth设置为： requested或required，需要为集群节点提供一个信任库，这个库中需要包含所有CA证书。 使用下面的命令将CA证书导入服务端信任库，输出为信任库文件：server.truststore.jks 1$ keytool -keystore /usr/ca/trust/server.truststore.jks -alias CARoot -import -file /usr/ca/root/ca.cert.pem -storepass app123 将CA证书导入服务端信任库，意味着信任该CA证书签名的所有证书。此属性称为信任链，在大型Kafka群集上部署SSL时特别有用。您可以使用单个CA对群集中的所有证书进行签名，并使所有计算机共享信任该CA的同一信任库。这样，所有计算机都可以对所有其他计算机进行身份验证。 1.2.7 配置Kafka BrokersKafka Broker节点支持侦听多个端口上的连接。在server.properties中配置，多个端口类型使用逗号分隔，我们以集群中kafka.app.node1为例： 1listeners=SSL://kafka.app.node1:9092 代理端需要以下SSL配置 12345ssl.keystore.location=/usr/ca/server/server.keystore.jksssl.keystore.password=app123ssl.key.password=app123ssl.truststore.location=/usr/ca/trust/server.truststore.jksssl.truststore.password=app123 其他可选配置设置： ssl.client.auth（可选） 参数控制SSL认证模式。默认参数值为requested，默认使用单向认证，即客户端认证Kafka brokers。此时，没有证书的客户端仍然可以连接集群。参数值为required，指定开启双向验证(2-way authentication)。Kafka服务器同时会验证客户端证书。生成集群建议开始双向认证。 ssl.cipher.suites（可选） 密码套件是认证，加密，MAC和密钥交换算法的命名组合，用于协商使用TLS或SSL网络协议的网络连接的安全设置。（默认为空列表） ssl.enabled.protocols 建议参数值为TLSv1.2,TLSv1.1,TLSv1。列出支持的SSL协议。生成环境不建议使用SSL，建议使用TLS。 ssl.keystore.type和ssl.truststore.type` 文件格式：JKS security.inter.broker.protocol参数 kafka集群节点（brokers）之间启用SSL通讯，需要配置该配置参数为：SSL。 最后我们总结合并一下所有的配置参数： 123456789101112listeners=SSL://kafka.app.node1:9092ssl.keystore.location=/usr/ca/server/server.keystore.jksssl.keystore.password=app123ssl.key.password=app123ssl.truststore.location=/usr/ca/trust/server.truststore.jksssl.truststore.password=app123ssl.client.auth=requiredssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1ssl.keystore.type=JKS ssl.truststore.type=JKS ssl.endpoint.identification.algorithm=HTTPSsecurity.inter.broker.protocol=SSL 1.2.8 初步验证正常启动集群的Zookeeper集群，然后依次启动集群的所有节点。使用下面的命令检查： 1$ openssl s_client -debug -connect kafka.app.node1:9092 -tls1 该命令检查服务器的密钥库和信任库是否正确设置。命令中tls1必须是集群配置参数ssl.enabled.protocols所支持的协议。 12345678910111213141516171819202122232425262728293031Certificate chain（省略）---Server certificate-----BEGIN CERTIFICATE-----（省略）-----END CERTIFICATE-----subject=（省略）issuer=（省略）---No client certificate CA names sent---SSL handshake has read 2029 bytes and written 264 bytes---New, TLSv1/SSLv3, Cipher is ECDHE-RSA-DES-CBC3-SHAServer public key is 2048 bitSecure Renegotiation IS supportedCompression: NONEExpansion: NONESSL-Session: Protocol : TLSv1 Cipher : ECDHE-RSA-DES-CBC3-SHA Session-ID: 5E580D610AEB5DDD8BCD0D31E88180F45391109792CA3CDD1E861EB87C704261 Session-ID-ctx: Master-Key: E544FF34B993B2C3B7F7CB28D8166213F8D3A9864A82247F6948E33B319CD1A8943127DDF9B528EA73435EBC73B0DD55 Key-Arg : None Start Time: 1582828897 Timeout : 7200 (sec) Verify return code: 7 (certificate signature failure)---（省略） 如果证书未显示或有其他错误消息，则说明设置不正确。 另外对于’OpenSSL 0.9.8j-fips 07 Jan 2009’版本的openssl版本，由于这个版本不能自己检测出ssl的版本。会报下面的错误信息。 1816:error:1408E0F4:SSL routines:SSL3_GET_MESSAGE:unexpected message:s3_both.c:463: 1.3 配置kafka客户端kafka集群需要支持集群内外的客户端交互访问。安全集群的客户端同样需要进行相关安全配置。这里客户端指的是Console客户端。 1.3.1 签发客户端证书类似集群内部服务端的证书签发步骤，客户端证书签发过程入下： 生成客户端SSL密钥和证书，输出密钥容器：client.keystore.jks 12$ keytool -keystore /usr/ca/client/client.keystore.jks -alias kafka.app.node1 -validity 365 -genkey -keypass app123 -keyalg RSA -dname "CN=kafka.app.node1,OU=dccsh,O=icbc,L=shanghai,S=shanghai,C=cn" -ext SAN=DNS:kafka.app.node1 -storepass app123 从密钥容器中提取和导出客户端证书（输出文件：client.cert-file，未签名） 1$ keytool -keystore /usr/ca/client/client.keystore.jks -alias kafka.app.node1 -certreq -file /usr/ca/client/client.cert-file -storepass app123 给客户端证书签名（输出文件：client.cert-signed，已签名） 1$ openssl x509 -req -CA /usr/ca/root/ca.cert.pem -CAkey /usr/ca/root/ca.key.pem -in /usr/ca/client/client.cert-file -out /usr/ca/client/client.cert-signed -days 365 -CAcreateserial -passin pass:app123 将CA证书导入客户端密钥容器中 1$ keytool -keystore /usr/ca/client/client.keystore.jks -alias CARoot -import -file /usr/ca/root/client.cert-file -storepass app123 将已签名的证书导入密钥容器中 1$ keytool -keystore /usr/ca/client/client.keystore.jks -alias kafka.app.node1 -import -file /usr/ca/client/client.cert-signed -storepass app123 1.3.2 生成客户端信任库使用下面的命令将CA证书导入客户端信任库，输出为信任库文件：client.truststore.jks 1$ keytool -keystore /usr/ca/trust/client.truststore.jks -alias CARoot -import -file /usr/ca/root/ca.cert.pem -storepass app123 1.3.3 配置客户端客户端的console-producer和console-consumer命令需要添加相关安全配置。 如果kafka集群不需要客户端身份验证，只需配置下面的配置： 123security.protocol=SSLssl.truststore.location=/usr/ca/trust/client.truststore.jksssl.truststore.password=app123 如果需要客户端身份验证，还需要补充下面的配置信息： 123ssl.keystore.location=/usr/ca/client/client.keystore.jksssl.keystore.password=app123ssl.key.password=app123 根据我们的要求和代理配置，可能还需要其他配置设置： ssl.provider（可选）。用于SSL连接的安全提供程序的名称。 ssl.cipher.suites（可选）。密码套件是认证，加密，MAC和密钥交换算法的命名组合，用于协商使用TLS或SSL网络协议的网络连接的安全设置。 ssl.enabled.protocols = TLSv1.2，TLSv1.1，TLSv1。它应列出在代理方配置的至少一种协议 ssl.truststore.type = JKS ssl.keystore.type = JKS 最后我们总结合并一下所有的配置参数（编辑文件名为：client-ssl.properties）： 123456security.protocol=SSLssl.truststore.location=/usr/ca/trust/client.truststore.jksssl.truststore.password=app123ssl.keystore.location=/usr/ca/client/client.keystore.jksssl.keystore.password=app123ssl.key.password=app123 1.3.4 消费者生产者使用console-producer的命令： 1kafka-console-producer.sh --broker-list kafka.app.node1:9092,kafka.app.node2:9092,kafka.app.node3:9092 --topic test --producer.config client-ssl.properties 使用console-consumer的命令： 1kafka-console-consumer.sh --bootstrap-server kafka.app.node1:9092,kafka.app.node2:9092,kafka.app.node3:9092 --topic test --new-consumer --consumer.config client-ssl.properties 这里test为topic名称，在只有SSL通信加密集群中，topic的创建、删除、生产、消费并没有权限管理，依然存在安全问题。所以kafka集群需要进一步配置权限管理。 第二部分 Kafka集群权限认证Kafka集群的权限认证管理主要涉及： 身份认证（Authentication）。对客户端与服务器的连接进行身份认证，brokers和zookeeper之间的连接进行Authentication（producer 和 consumer）、其他 brokers、tools与 brokers 之间连接的认证。 权限控制（Authorization）。实现对于消息级别的权限控制，客户端的读写操作进行Authorization（生产、消费）管理。 通俗的讲，身份认证解决的是证明你是谁，而权限控制解决的是你能干什么。在Kafka中身份认证和权限控制是两套独立的安全配置。 2.1 集群权限认证策略Kafka从0.9.0.0版本后开始支持下面的SASL安全策略管理。这些安全功能为Kafka通信安全、多租户管理、集群云化提供了安全保障。截止目前Kafka 2.3版本，一共支持5种SASL方式。 验证方式 版本 说明 SASL/PLAIN 0.10.0.0 不能动态增加用户 SASL/SCRAM 0.10.2.0 可以动态增加用户。有两种方式：SASL/SCRAM-SHA-256 和SASL/SCRAM-SHA-512 SASL/GSSAPI 0.9.0.0 需要独立部署验证服务（即Kerberos服务） SASL/OAUTHBEARER 2.0.0 需自己实现接口实现token的创建和验证，需要额外Oauth服务 SASL/Delegation Token 1.1.0 补充现有 SASL 机制的轻量级认证机制 对于生产环境，SASL/PLAIN方式有个缺点：只能在JAAS文件KafkaServer参数中配置用户，集群运行期间无法动态新增用户（需要重启重新加载JAAS文件），这对维护管理带来不便。而SASL/SCRAM方式，将认证数据存储在Zookeeper中，可以动态新增用户并分配权限。 SASL/GSSAPI方式需要依赖Kerberos服务。对于一些已经部署了集中式的Kerberos服务的大厂，只需要申请一个principal即可。如果生产Kerberos认证中出现TGT分发性能瓶颈，可以使用SASL/Delegation Token模式。使用 Kafka 提供的 API 去获取对应的 Delegation Token。Broker 和客户端在做认证的时候，可以直接使用这个 token，不用每次都去 KDC 获取对应的 ticket（Kerberos 认证），减少性能压力。 同样SASL/OAUTHBEARER方式需要Oauth服务。 各种方式引入版本不同，使用依赖各有差异，需要结合自身业务特点选择合适的架构方式。 2.2 SASL/SCRAM策略配置介绍SASL/SCRAM方式将身份认证和权限控制的凭证（credential）数据均存储在Zookeeper中，需要对Zookeeper进行安全配置。 2.2.1 Zookeeper集群侧配置对Zookeeper集群中所有节点更新下面的策略后，重启集群生效。 配置zoo.cfg文件 文件尾部追加下面的配置： 123authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProviderrequireClientAuthScheme=sasljaasLoginRenew=3600000 新增zk_server_jaas.conf文件 配置文件内容如下： 123456Server &#123;org.apache.kafka.common.security.plain.PlainLoginModule requiredusername="admin"password="admin-secret"user_admin="admin-secret";&#125;; 其中username和password定义的用户和密钥，用于Zookeeper与Kafka集群进行认证。配置项user_admin=&quot;admin-secret&quot; 中 admin为用户名，admin-secret为密码，用于Zookeeper集群外客户端和集群内进行认证。 拷贝依赖包 将kafka文件系统中kafka/libs目录下的jar包拷贝到zookeeper/lib目录。 12345kafka-clients-2.1.1.jarlz4-java-1.5.0.jarosgi-resource-locator-1.0.1.jarslf4j-api-1.7.25.jarsnappy-java-1.1.7.2.jar 若没有引入依赖包，启动时会报找不到org.apache.kafka.common.security.plain.PlainLoginModule包的错误。 修改zookeeper启动参数 修改bin/zkEnv.sh文件, 在文件尾追加下面的配置内容。该配置完成引入的包的加载。变量CLASSPATH和SERVER_JVMFLAGS都会在Zookeeper启动时传给JVM虚拟机。 下面的配置中$ZOOKEEPER_HOME是zookeeper的环境变量，如果没有配置，使用绝对路径即可。 1234for i in $ZOOKEEPER_HOME/lib/*.jar; do CLASSPATH="$i:$CLASSPATH"doneSERVER_JVMFLAGS=" -Djava.security.auth.login.config=$ZOOKEEPER_HOME/conf/zk_server_jaas.conf" 2.2.2 kafka集群侧配置kafka集群中每一台节点均需要更新下面的配置。 新增kafka_server_scram_jaas.conf文件（在config目录中） 12345678910KafkaServer &#123;org.apache.kafka.common.security.scram.ScramLoginModule requiredusername="admin"password="admin-secret";# 自定义用户：# user_admin="admin-secret"# user_alice="alice-secret"# user_reader="reader-secret"# user_writer="writer-secret";&#125;; 其中配置username和password为Kafka集群之间通讯的SCRAM凭证，用户名为admin，密码为admin-secret。 配置中类似user_XXX格式的配置项为自定义用户。如果是SASL/PLAIN方式，用户只能在该文件中定义，不能动态新增。我们使用SASL/SCRAM方式，可以后续动态声明admin用户，不再此处进行配置。 更新Kafka的配置文件server.properties（在config目录中）： 12345678910111213141516171819#SASL CONFIGlisteners=SASL_SSL://kafka.app.node1:9092sasl.enabled.mechanisms=SCRAM-SHA-512sasl.mechanism.inter.broker.protocol=SCRAM-SHA-512authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer#allow.everyone.if.no.acl.found=truesuper.users=User:admin#SSL CINFIGssl.keystore.location=/usr/ca/server/server.keystore.jksssl.keystore.password=app123ssl.key.password=app123ssl.truststore.location=/usr/ca/trust/server.truststore.jksssl.truststore.password=app123ssl.client.auth=requiredssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1ssl.keystore.type=JKSssl.truststore.type=JKSssl.endpoint.identification.algorithm=HTTPSsecurity.inter.broker.protocol=SASL_SSL 需要注意参数allow.everyone.if.no.acl.found，如果开启参数开关，当客户端和集群交互时候未找到ACL策略时，允许所有类型的访问操作。建议该参数关闭（false）。 参数security.inter.broker.protocol指定集群brokers之间的通讯协议。不加密协议有：SASL_SSL、SASL_PLAINTEXT、PLAINTEXT；加密协议有：SSL。为了提高节点之间的交互性能，内部网络环境建议使用非加密协议。这里使用加密的SASL_SSL协议。 参数super.users指定了集群的超级用户为：admin。注意如果指定多个超级用户，每个用户使用分号隔开，例如：super.users=User:admin;User:alice 参数sasl.enabled.mechanisms列出支持的认证方式。即可以支持多种。 参数sasl.mechanism.inter.broker.protocol指定集群内部的认证方式。Kafka仅支持最小迭代次数为4096的强哈希函数SHA-256和SHA-512。所以有SCRAM-SHA-512和SCRAM-SHA-256两种方式。 配置kafka启动环境变量（bin目录下面的kafka-run-class.sh） 为 Kafka 添加 java.security.auth.login.config 环境变量（配置文件路径）。并且在启动模式中添加KAFKA_SASL_OPTS。 12345678# 截取配置文件片段：KAFKA_SASL_OPTS='-Djava.security.auth.login.config=/opt/software/kafka/config/kafka_server_scram_jaas.conf'# Launch modeif [ "x$DAEMON_MODE" = "xtrue" ]; then nohup $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_SASL_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS "$@" &gt; "$CONSOLE_OUTPUT_FILE" 2&gt;&amp;1 &lt; /dev/null &amp;else exec $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_SASL_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS "$@"fi 2.2.3 SCRAM认证管理在集群的配置文件kafka_server_scram_jaas.conf中，定义了集群内部的认证用户。对于客户端和集群之间认证可以使用kafka-configs.sh来动态创建。 创建用户SCRAM凭证 例如集群中的超级用户admin用户，使用下面的命令创建： 1$ kafka-configs.sh --zookeeper kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --alter --add-config 'SCRAM-SHA-256=[password=admin-secret],SCRAM-SHA-512=[password=admin-secret]' --entity-type users --entity-name admin 创建自定义普通用户alice。 1$ kafka-configs.sh --zookeeper kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --alter --add-config 'SCRAM-SHA-256=[iterations=8192,password=alice-secret],SCRAM-SHA-512=[password=alice-secret]' --entity-type users --entity-name alice 查看SCARM凭证 1$ kafka-configs.sh --zookeeper kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --describe --entity-type users --entity-name admin 删除SCRAM凭证 1$ kafka-configs.sh --zookeeper kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --alter --delete-config 'SCRAM-SHA-512' --entity-type users --entity-name alice 2.3 Kafka客户端配置Kafka集群配置了认证，那么对于Console客户端访问集群自然需要配置认证信息。可集群节点内部通讯凭证的认知，同样需要定义JAAS文件。加入我们自定义了用户alice，JAAS文件名为：kafka_console_client_jaas.conf，配置内容如下： 12345KafkaClient &#123;org.apache.kafka.common.security.scram.ScramLoginModule requiredusername="alice"password="alice-secret";&#125;; 然后更新kafka-console-producer.sh脚本和kafka-console-consumer.sh脚本的启动参数。 12345# 文件截取更新部分：if [ "x$KAFKA_OPTS" ]; thenexport KAFKA_OPTS="-Djava.security.auth.login.config=/opt/software/kafka/config/kafka_write_jaas.conf"fi 在配置SSL时候，我们新建了client-ssl.properties配置文件，作为Console客户端启动配置。在集群启用SASL_SSL后，我们同步更新如下： 123456security.protocol=SASL_SSLssl.truststore.location=/usr/ca/trust/client.truststore.jksssl.truststore.password=app123ssl.keystore.location=/usr/ca/client/client.keystore.jksssl.keystore.password=app123ssl.key.password=app123 至此Console客户端已经配置完毕，但目前Console客户端还不能通过命令方式和集群进行交互，因为我们指定的用户对于集群的资源还没有任何权限。需要对用户进行集群资源的ACL控制设置，赋予相关权限。 2.4 ACL控制Kafka权限资源包含Topic、Group、Cluster、TransactionalId（事务id），每个资源涉及的权限内容如下： 资源类型 权限类型 Topic Read,Write,Describe,Delete,DescribeConfigs,AlterConfigs,All Group Read,Describe,All Cluster Create,ClusterAction,DescribeConfigs,AlterConfigs,IdempotentWrite,Alter,Describe,All TransactionalId Describe,Write,All 对于常用类型进行说明： 权限 说明 Read 读取topic、group信息 Write 写topic、TransactionalId（存储在内部topic） Delete 删除topic Create 创建topic ALTER 修改topic Describe 获取topic、group、TransactionalId信息 ALL 所有权限 Kafka提供ACL管理脚本：kafka-acls.sh。 2.4.1 更新脚本配置认证数据均存储在Zookeeper集群中，需要和Zookeeper交互自然需要配置相关认证信息。 首先需要新建JAAS文件，文件名为：zk_client_jaas.conf。这里的用户已经在Zookeeper集群中进行定义。 12345Client &#123;org.apache.kafka.common.security.plain.PlainLoginModule requiredusername="admin"password="admin-secret";&#125;; 最后更新kafka-acls.sh脚本： 12345# 截取更新部分if [ "x$KAFKA_OPTS" ]; thenexport KAFKA_OPTS="-Djava.security.auth.login.config=/opt/software/kafka/config/zk_client_jaas.conf"fi 当然Kafka集群的配置文件中已经开启了ACL： 1authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer 至此完成配置。 2.4.2 ACL配置根据官网的介绍，ACL的格式如下： “Principal P is [Allowed/Denied] Operation O From Host H On Resource R” 参数含义描述如下： principal：指定一个Kafka user； operation：指定一个具体的操作类型，例如：Read, Write, Delete等； Host：表示与集群交互的客户端IP地址，如果是通配符‘*’表示所有IP。目前不支持主机名（hostname）形式，只能是IP地址； Resource：指定一种Kafka资源类型（共有4种类型）； 例如下面的ACL命令： 1$ sh kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --add --allow-principal User:alice --allow-host '*' --operation ALL --topic test 赋权之后，用户alice对test具有全部权限，并且访问请求可以是来自任何IP的客户端。 常用参数的补充说明： 对主机IP的限制参数，allow-host指定允许的IP，deny-host指定禁用IP； 新增和删除一个赋权策略，分别使用：add和remove 2.4.3 ACL策略查看使用参数list参看ACL策略。例如： 1$ sh kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --list --topic test-topic 该查看命令显示test-topic资源相关的所有ACL策略。 2.4.4 超级用户kafka集群的配置文件server.properties中定义了超级用户（Super Users），超级用户不在ACL控制范围内，默认可以访问集群中所有资源，并具备所有权限。 2.5 权限认证数据访问集群的认证数据存储在Zookeeper，可以通过Zookeeper的console客户端访问认证数据。 使用zookeeper自带的命令行客户端： 12345/dmqs/zookeeper/bin&gt; ./zkCli.shConnecting to localhost:2181Welcome to ZooKeeper!JLine support is enabled[zk: localhost:2181(CONNECTING) 0] 查看zookeeper中的数据： 12[zk: localhost:2181(CONNECTED) 1] ls /[cluster, controller_epoch, controller, brokers, zookeeper, kafka-acl, kafka-acl-changes, admin, isr_change_notification, consumers, config] 其中kafka-acl中存储相关权限认证数据。 12[zk: localhost:2181(CONNECTED) 3] ls /kafka-acl[Cluster, Topic] 可以查看其中的权限信息。 2.6 SSL和SASL的说明SSL是传输层安全协议，是位于传输层（TCP/IP）和应用层（HTTP）的协议，SSL是对整个传输过程的加密，SSL是对客户端和服务器之间传输的所有数据进行加密。假如在配置的时候使用了SASL，但是没有使用SSL，那么除了账号密码外，所有的传输内容都是裸奔的。 所以生产集群采用SSL和SASL结合方式，即SSL_SASL方式。 第三部分 安全集群的客户端3.1 开发语言类3.1.1 Python客户端目前市面上kafka的python API常用的有三种： 第一种 kafka 该项目是kafka-python的老项目，2017年后调整为kafka-python项目。 第二种 kafka-python 最新版本为2.0，首先从客户端的密钥库中导出CA证书。 1$ keytool -exportcert -alias CARoot -keystore client.keystore.jks -rfc -file ca.cert.pem 生产者和消费者的案例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# -*- coding: utf-8 -*-from kafka import KafkaConsumer, KafkaProducerimport kafkaimport sslimport loggingimport time#logging.basicConfig(level=logging.DEBUG)try: bootstrap_servers = 'kafka.itdw.node1:9092,kafka.itdw.node2:9092,kafka.itdw.node3:9092' topic = "test" sasl_mechanism = "SCRAM-SHA-512" username = "alice" password = "alice-secret" security_protocol = "SASL_SSL" # CA 证书路径 ssl_cafile = 'ca.cert.pem' # SSL context = ssl.SSLContext(ssl.PROTOCOL_SSLv23) context.verify_mode = ssl.CERT_NONE context.check_hostname = False context.load_verify_locations(ssl_cafile) # 消费者 consumer = KafkaConsumer(topic, bootstrap_servers=bootstrap_servers, api_version=(0, 10), security_protocol=security_protocol, ssl_context=context, sasl_mechanism = sasl_mechanism, sasl_plain_username = username, sasl_plain_password = password ) # 生产者 producer = KafkaProducer(bootstrap_servers=bootstrap_servers, api_version=(0, 10), acks='all', retries=1, security_protocol=security_protocol, ssl_context=context, sasl_mechanism=sasl_mechanism, sasl_plain_username=username, sasl_plain_password=password ) # 生产数据 for i in range(10): producer.send(topic, bytes("测试",encoding='utf8')) producer.flush() # 消费数据 for msg in consumer: print(msg)except Exception as e: print(e) 需要的注意的事项有： Kafka集群启用主机名模式，所以应用程序运行节点的hosts文件需要配置Kafka集群节点的域名映射。 ssl_context参数为包装套接字连接的预配置SSLContext。如果非None，将忽略所有其他ssl_ *配置。 主机名验证问题。如果证书中域名和主机名不匹配，客户端侧需要配置需要调整如下： 12ssl_ctx.check_hostname=Falsessl_ctx.verify_mode = CERT_NONE 如果不提前预配置SSLContext，还需要客户端的证书。 1$ keytool -exportcert -alias localhost -keystore client.keystore.jks -rfc -file client.cert.pem 生产者的参数需要添加： 1234567891011121314ssl_certfile = "client.cert.pem"ssl_cafile = "ca.cert.pem"producer = KafkaProducer(bootstrap_servers=bootstrap_servers, api_version=(0, 10), acks='all', retries=1, security_protocol=security_protocol, ssl_context=context, sasl_mechanism=sasl_mechanism, sasl_plain_username=username, sasl_plain_password=password, ssl_check_hostname=False, ssl_certfile=ssl_certfile, ssl_cafile=ssl_cafile) 第三种 confluent-kafka confluent-kafka包由confluent公司开源，主要是对C/C++客户端包（librdkafka）的封装。案例代码如下： 12345678910111213141516171819202122232425# -*- coding: utf-8 -*-from confluent_kafka import Producer# 回调函数def delivery_report(err, msg): """ Called once for each message produced to indicate delivery result. Triggered by poll() or flush(). """ if err is not None: print(’Message delivery failed: &#123;&#125;‘.format(err)) else: print(‘Message delivered to &#123;&#125; [&#123;&#125;]‘.format(msg.topic(), msg.partition()))if __name__ == ‘__main__‘: producerConfing = &#123;"bootstrap.servers": 'kafka.itdw.node1:9092,kafka.itdw.node2:9092,kafka.itdw.node3:9092', "security.protocol": 'SASL_SSL', "sasl.mechanisms": 'SCRAM-SHA-256', "sasl.username": 'alice', "sasl.password": 'alice-secret', "ssl.ca.location": 'ca.cert.pem' &#125; ProducerTest = Producer(producerConfing) ProducerTest.poll(0) ProducerTest.produce(‘testTopic‘, ‘confluent kafka test‘.encode(‘utf-8‘),callback=delivery_report) ProducerTest.flush() 3.1.2 Go客户端我们的Go语言中常用的Kafka的客户端包有： 1234"github.com/Shopify/sarama""github.com/bsm/sarama-cluster""github.com/confluentinc/confluent-kafka-go/kafka""github.com/segmentio/ksuid" 其中最常用的是sarama，案例参考github项目。 3.1.3 Java客户端生产者： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.kafka.security;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import java.util.Properties;import java.util.Random;public class KafkaProducerWithSASL_SSL &#123; private static final String KAFKA_TOPIC = "topsec"; private static final String BOOTSTRAP_SERVER = "docker31:9092"; private static final String[] strs = new String[]&#123;"zhao", "qian", "sun", "li", "zhou", "wu", "zheng", "wang", "feng", "chen"&#125;; private static final Random r = new Random(); public static void main(String[] args) &#123; try &#123; producer(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private static void producer() throws InterruptedException &#123; Properties props = new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVER); //SASL_SSL加密 props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_SSL"); props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, "D:\\Download\\ca\\trust\\client.truststore.jks"); props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "hadoop"); // SSL用户认证 props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, "D:\\Download\\ca\\client\\client.keystore.jks"); props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "hadoop"); props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "hadoop"); //SASL用户认证 props.put(SaslConfigs.SASL_JAAS_CONFIG,"org.apache.kafka.common.security.scram.ScramLoginModule required username=\"admin\" password=\"admin-secret\";"); props.put(SaslConfigs.SASL_MECHANISM, "SCRAM-SHA-512"); props.put(ProducerConfig.ACKS_CONFIG, "all"); props.put(ProducerConfig.RETRIES_CONFIG, 0); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer"); props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer"); props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384); props.put(ProducerConfig.LINGER_MS_CONFIG, 1); props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); while (true) &#123; producer.send(new ProducerRecord&lt;&gt;(KAFKA_TOPIC, strs[r.nextInt(10)],strs[r.nextInt(10)])); Thread.sleep(2000); &#125; &#125;&#125; 消费者： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.topsec.kafka.security;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import java.util.Collections;import java.util.Properties;public class KafkaConsumerWithSASLAndSSL &#123; private static final String KAFKA_TOPIC = "topsec"; private static final String BOOTSTRAP_SERVER = "docker31:9092"; public static void main(String[] args) &#123; consumer(); &#125; private static void consumer() &#123; Properties props = new Properties(); //SASL_SSL加密配置 props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_SSL"); props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, "D:\\Download\\ca\\trust\\client.truststore.jks"); props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "hadoop"); //SSL身份验证配置 props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, "D:\\Download\\ca\\client\\client.keystore.jks"); props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "hadoop"); props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "hadoop"); //SASL身份验证 props.put(SaslConfigs.SASL_JAAS_CONFIG,"org.apache.kafka.common.security.scram.ScramLoginModule required username=\"admin\" password=\"admin-secret\";"); props.put(SaslConfigs.SASL_MECHANISM, "SCRAM-SHA-512"); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVER); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer"); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer"); props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true"); props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "1000"); props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, "6000"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Collections.singletonList(KAFKA_TOPIC)); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(2000); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s, partition = %d %n", record.offset(), record.key(), record.value(), record.partition()); &#125; &#125; &#125;&#125; 3.2 组件类3.2.1 Console客户端客户端节点部署kafka项目，在bin目录下面我们已经更新了kafka-console-consumer.sh和kafka-console-producer.sh两个脚本。并分别新增了加密访问的配置文件consumer.config和producer.config。 命令案例参考：1.3.4 章节内容。 3.2.2 Flume客户端目前Flume项目官网项目文档介绍支持下面三种方式： SASL_PLAINTEXT - 无数据加密的 Kerberos 或明文认证； SASL_SSL - 有数据加密的 Kerberos 或明文认证； SSL - 基于TLS的加密，可选的身份验证； 事实上对于SASL/SCRAM方式Flume也是支持的。具体配置如下（以Flume1.9版本为例）： 3.2.2.1 第一步 新增jaas配置文件在Flume的conf配置目录下面新增flume_jaas.conf文件，文件内容： 123456789101112Server &#123; org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin-secret" user_admin="admin-secret";&#125;;KafkaClient &#123; org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret";&#125;; 3.2.2.2 第二步 更新flume-env.sh文件Flume的conf配置目录下面flume-env.sh文件添加JAVA_OPTS配置更新： 1JAVA_OPTS="$JAVA_OPTS -Djava.security.auth.login.config=/dmqs/apache-flume-1.9.0-bin/conf/flume_jaas.conf" 其中路径为第一步中新增的flume_jaas.conf文件路径。 3.2.2.3 测试案例（sinks）我们使用一个简单的案例来测试，Flume的source为监控文件尾写入，Flume的sinks为加密kafka集群。具体配置如下： 123456789101112131415161718192021222324252627282930313233343536373839#definea1.sources = r1a1.sinks = k1a1.channels = c1# binda1.sources.r1.channels = c1a1.sinks.k1.channel = c1# sourcea1.sources.r1.type = execa1.sources.r1.command = tail -f /dmqs/apache-flume-1.9.0-bin/data/flume/flume.loga1.sources.r1.shell = /bin/bash -c# channela1.channels.c1.type = memorya1.channels.c1.capacity = 15000a1.channels.c1.transactionCapacity = 15000# sinka1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.bootstrap.servers = kafka.itdw.node1:9093a1.sinks.k1.kafka.topic = flumea1.sinks.k1.kafka.flumeBatchSize = 15000a1.sinks.k1.kafka.producer.acks = 1a1.sinks.k1.kafka.producer.linger.ms = 1000a1.sinks.k1.kafka.producer.security.protocol=SASL_SSLa1.sinks.c1.kafka.producer.sasl.mechanism =SCRAM-SHA-512a1.sinks.c1.kafka.producer.sasl.jaas.config =org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret"####a1.sinks.k1.kafka.producer.ssl.truststore.location =/usr/ca/trust/client.truststore.jksa1.sinks.k1.kafka.producer.sasl.mechanism =SCRAM-SHA-512a1.sinks.k1.kafka.producer.ssl.truststore.password=app123a1.sinks.k1.kafka.producer.ssl.keystore.location=/usr/ca/client/client.keystore.jksa1.sinks.k1.kafka.producer.ssl.keystore.password=app123a1.sinks.k1.kafka.producer.ssl.key.password=app123a1.sinks.k1.kafka.producer.timeout.ms = 100a1.sinks.k1.batchSize=15000a1.sinks.k1.batchDurationMillis=2000 配置保存为flume-sink-auth-kafka.conf,为了检查输出结果使用下面命令启动（在bin目录中）： 1./flume-ng agent --conf ../conf --conf-file ../conf/flume-sink-auth-kafka.conf --name a1 -Dflume.root.logger=INFO,console 向文件尾部追加信息： 1echo "test" &gt;&gt; /dmqs/apache-flume-1.9.0-bin/data/flume/flume.log 然后使用消费者客户端查看数据是否写入kafka的flume主题中。 3.2.2.3 测试案例（source）同样可以也可以将加密Kafka作为Flume的source，配置案例如下： 1234567891011121314151617181920212223242526272829303132333435#definea1.sources = r1a1.sinks = k1a1.channels = c1# binda1.sources.r1.channels = c1a1.sinks.k1.channel = c1# sourcea1.sources.r1.type = org.apache.flume.source.kafka.KafkaSourcea1.sources.r1.kafka.bootstrap.servers = kafka.app .node1:9093,kafka.app.node2:9093,kafka.app.node3:9093a1.sources.r1.kafka.topics = flumea1.sources.r1.kafka.consumer.group.id = flumea1.sources.r1.kafka.consumer.timeout.ms = 2000a1.sources.r1.batchSize=150a1.sources.r1.batchDurationMillis=1000#####a1.sources.r1.kafka.consumer.ssl.truststore.location =/usr/ca/trust/client.truststore.jksa1.sources.r1.kafka.consumer.sasl.mechanism =SCRAM-SHA-512a1.sources.r1.kafka.consumer.ssl.truststore.password=itdw123a1.sources.r1.kafka.consumer.ssl.keystore.location=/usr/ca/client/client.keystore.jksa1.sources.r1.kafka.consumer.ssl.keystore.password=itdw123a1.sources.r1.kafka.consumer.ssl.key.password=itdw123a1.sources.r1.kafka.consumer.security.protocol=SASL_SSLa1.sources.r1.kafka.consumer.sasl.mechanism =SCRAM-SHA-512a1.sources.r1.kafka.consumer.sasl.jaas.config =org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret";# channela1.channels.c1.type = memorya1.channels.c1.capacity = 15000a1.channels.c1.transactionCapacity = 15000# sinka1.sinks.k1.type = file_rolla1.sinks.k1.sink.directory = /dmqs/apache-flume-1.9.0-bin/data/flumea1.sinks.k1.sink.serializer = TEXT 案例中将加密Kafka中flume主题中的数据汇入到指定目录的文件中。 3.2.3 Logstash客户端Logstash和Kafka交互使用Kafka output plugin插件实现。其中配置文件中output部分如下： 123456789101112131415161718output &#123; kafka &#123; id =&gt; "kafkaSLL_SASL" codec =&gt; "json" ssl_endpoint_identification_algorithm =&gt; "" bootstrap_servers =&gt; "kafka.app.node1:9092,kafka.app.node2:9092,kafka.app.node3:9092" jaas_path =&gt;"/etc/logstash/certificates/kafka_servcer_jaas.conf" ssl_keystore_location =&gt; "/etc/logstash/certificates/client.keystore.jks" ssl_keystore_password =&gt; "app123" ssl_keystore_type =&gt; "JKS" ssl_truststore_location =&gt; "/etc/logstash/certificates/client.truststore.jks" ssl_truststore_password =&gt; "app123" ssl_truststore_type =&gt; "JKS" sasl_mechanism =&gt; "SCRAM-SHA-512" security_protocol =&gt; "SASL_SSL" topic_id =&gt; "test" &#125;&#125; 参考：https://www.elastic.co/guide/en/logstash/current/plugins-outputs-kafka.html 第四部分 加密认证集群的性能压测集群启用SSL后，数据交互中需要加密、解密。kafka集群的I/O性能会降低。我们使用Kafka自带的压侧工具对集群加密前和加密后性能进行评测。 4.1生产者压力测试客户端写入参数配置为acks=all（即Topic中Leader和fellow副本均写入成功）。每条消息大小为1M（消息体小吞吐量会大一些）。另外测试客户端为集群内部节点，忽略了数据网络传输的性能消耗。 4.1.1不加密集群1./kafka-consumer-perf-test.sh --topic topsec --throughput 50000 --num-records 1500000 --record-size 10000 --producer-props bootstrap.servers=kafka.itdw.node1:9093,kafka.itdw.node2:9093,kafka.itdw.node3:9093 acks=all 测试结果： 11500000 records sent, 38538.615693 records/sec (37.64 MB/sec), 748.44 ms avg latency, 5485.00 ms max latency, 227 ms 50th, 3194 ms 95th, 3789 ms 99th, 3992 ms 99.9th. 4.1.2加密集群1./kafka-producer-perf-test.sh --topic topsec --throughput 50000 --num-records 1500000 --record-size 10000 --producer-props bootstrap.servers=kafka.itdw.node1:9093,kafka.itdw.node2:9093,kafka.itdw.node3:9093 acks=all --producer.config producer.config 测试结果： 11500000 records sent, 16901.027582 records/sec (16.50 MB/sec), 1713.43 ms avg latency, 9345.00 ms max latency, 72 ms 50th, 1283 ms 95th, 2067 ms 99th, 2217 ms 99.9th. 4.2 压侧结论加密改造前，生产者的吞吐量为3.8w 条/秒，改造后1.7W 条/秒。整体吞吐性能降低50%左右，数据的加密、解密导致吞吐量性能降低。平均时延也增加了一倍多（改造前700ms，改造后1700ms）。在实际生产中可参考这个性能折扣基线配置集群资源。 参考文献及资料1、Kafka官网对安全类功能介绍，链接：http://kafka.apache.org/documentation/#security 2、Kafka ACLs in Practice – User Authentication and Authorization，链接：https://developer.ibm.com/opentech/2017/05/31/kafka-acls-in-practice/ 3、维基百科（数字证书），链接：https://zh.wikipedia.org/wiki/公開金鑰認證 4、SSL技术白皮书，链接：https://blog.51cto.com/xuding/1732723 5、Kafka权限管理，链接：https://www.jianshu.com/p/09129c9f4c80 5、Flume文档，链接：https://flume.apache.org/FlumeUserGuide.html#kafka-sinkorg/FlumeUserGuide.html#kafka-sink]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习系列(一)Go语言Win开发环境部署]]></title>
    <url>%2F2020%2F01%2F31%2F2020-01-31-Go%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97(%E4%B8%80)Go%E8%AF%AD%E8%A8%80Win%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%20-%20%E5%89%AF%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 windows下安装 第二部分 配置环境变量 第三部分 IDE配置 第四部分 HelloWorld案例 参考文献及资料 背景Go语言的官方网站：https://golang.org/ 由于防火墙原因，请大家在这个网站下载：https://studygolang.com/dl 第一部分 windows下安装下载的是msi包，直接执行安装即可。 go1.12.5.windows-amd64.msi 第二部分 配置环境变量Go 语言需要配置 GOROOT 和 Path 两个环境变量：GOROOT 和 GOPATH。如果使用msi包安装，那么会自动配置好两个环境变量。 可以使用下面的命令检查变量： 1234567891011121314151617181920212223242526272829C:\Users\rongxiang&gt;go envset GOARCH=amd64set GOBIN=set GOCACHE=C:\Users\rongxiang\AppData\Local\go-buildset GOEXE=.exeset GOFLAGS=set GOHOSTARCH=amd64set GOHOSTOS=windowsset GOOS=windowsset GOPATH=C:\Users\rongxiang\goset GOPROXY=set GORACE=set GOROOT=C:\Goset GOTMPDIR=set GOTOOLDIR=C:\Go\pkg\tool\windows_amd64set GCCGO=gccgoset CC=gccset CXX=g++set CGO_ENABLED=1set GOMOD=set CGO_CFLAGS=-g -O2set CGO_CPPFLAGS=set CGO_CXXFLAGS=-g -O2set CGO_FFLAGS=-g -O2set CGO_LDFLAGS=-g -O2set PKG_CONFIG=pkg-configset GOGCCFLAGS=-m64 -mthreads -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -fdebug-prefix-map=C:\Users\RONGXI~1\AppData\Local\Temp\go-build030961398=/tmp/go-build -gno-record-gcc-switches 查看版本： 12C:\Users\rongxiang&gt;go versiongo version go1.12.5 windows/amd64 默认情况下，GOROOT = C:\Go；GOPATH = C:\Users\用户名\go。如果需要调整，修改环境变量参数即可。 第三部分 IDE配置使用Jetbrain公司的GoLand IDE（https://www.jetbrains.com/go/）。 下载安装成功后，打来GoLand。菜单File–&gt;Settings–&gt;GO中有两个配置项：GOROOT、GOPATH。 第四部分 HelloWorld案例配置好IDE环境，我们新建第一个项目（project）。 4.1 创建HelloWorld项目菜单栏File–&gt;New–&gt;Project,打开新建项目对话框。配置项目的文件位置（Location），例如我们配置为： D:\golang\workspace\HelloWorld，然后确定就新建Go项目。 在项目中新建main.go文件： 12345package mianimport "fmt"func main() &#123; fmt.Println("Hello World!") 4.2 编译并运行选中main.go文件，邮件选择运行。IDE将编译，并运行： 123Hello World!Process finished with exit code 0 控制台上打印上面的信息，说明执行成功。 参考文献及资料1、Go入门指南，https://github.com/Unknwon/the-way-to-go_ZH_CN 2、a tour of go，https://tour.golang.org/welcome/1]]></content>
      <categories>
        <category>Go</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch系列文章-数据的写入]]></title>
    <url>%2F2020%2F01%2F27%2F2021-05-01-Elasticsearch%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%95%B0%E6%8D%AE%E7%9A%84%E5%86%99%E5%85%A5%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 温热集群整体架构 第二部分 架构配置 第三部分 架构维护相关 第四部分 基于hot-warm架构的读写分离实现 总结 参考文献及资料 背景参考文献和资料1、Elasticsearch 主节点和暖热节点 https://dongbo0737.github.io/2017/06/06/elasticsearch-hot-warm/]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark中的Watermark]]></title>
    <url>%2F2020%2F01%2F27%2F2021-05-01-Spark%E4%B8%AD%E7%9A%84Watermark%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 温热集群整体架构 第二部分 架构配置 第三部分 架构维护相关 第四部分 基于hot-warm架构的读写分离实现 总结 参考文献及资料 背景https://towardsdatascience.com/watermarking-in-spark-structured-streaming-9e164f373e9 参考文献和资料1、]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark中的Structured Streaming介绍]]></title>
    <url>%2F2020%2F01%2F27%2F2021-05-04-Spark%E4%B8%AD%E7%9A%84Structured%20Streaming%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 温热集群整体架构 第二部分 架构配置 第三部分 架构维护相关 第四部分 基于hot-warm架构的读写分离实现 总结 参考文献及资料 背景Structured Streaming接口在社区2.0版本发布测试接口，主要暴露最初的设计思路及基本接口，不具备在生产环境使用的能力；2.1版本中Structured Streaming作为主要功能发布，支持Kafka数据源、基于event_time的window及watermark功能，虽然还在Alapha阶段，但从实现的完备程度及反馈来看已具备初步的功能需求。 发展历程： 2.0版本发布测试接口 2.1版本中Structured Streaming作为主要功能发布，支持Kafka数据源、基于event_time的window及watermark功能，虽然还在Alapha阶段，但从实现的完备程度及反馈来看已具备初步的功能需求 spark-2.2.0 ，可用于生产环境 第一部分 设计原理1.1 Spark streaming存在的问题 第二部分 编程实践第三部分 总结https://www.iteblog.com/archives/2084.html http://slamke.github.io/2017/04/06/Structured-Streaming%E4%BB%8B%E7%BB%8D/ https://zhuanlan.zhihu.com/p/51883927 参考文献和资料1、Structured Streaming Programming Guide，链接：http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[应用可用性监控项目设计]]></title>
    <url>%2F2020%2F01%2F11%2F2021-08-22-%E5%BA%94%E7%94%A8%E5%8F%AF%E7%94%A8%E6%80%A7%E7%9B%91%E6%8E%A7%E9%A1%B9%E7%9B%AE%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景https://gitee.com/ecar_team/apimonitor https://gitee.com/wanghouhou/wgcloud 第一部分 整体架构设计监控类型： 支撑shell命令行方式或脚本 支持python脚本方式 支持http方式（含BASIC认证） 告警发送模式： 支持邮件方式 支持UDP报文方式 支持Kafka方式 支持 是否需要数据库持久化？ 第二部分 告警消息设计告警消息支持简单模式和模板模式； 自定义模板模式 需要用户按照规范编写数据模板，${para}方式placehold，由用户补充参数实现； 告警消息均转换成 string格式； 默认模式 使用默认的模板模式。 第三部分 告警发送设计第三部分 项目的组织目录参考文献及资料1、Spring官网，链接：https://spring.io/projects/spring-cloud-gateway]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot系列文章（注解总结）]]></title>
    <url>%2F2020%2F01%2F11%2F2021-08-01-SpringBoot%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88%E6%B3%A8%E8%A7%A3%E6%80%BB%E7%BB%93%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景Java中的注解 - @NotNull参考文献及资料1、Spring官网，链接：https://spring.io/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统中用户相关操作总结]]></title>
    <url>%2F2020%2F01%2F11%2F2021-08-18-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%94%A8%E6%88%B7%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景第一部分Linux修改用户所属组 1、设置某个用户所在组 1usermod -g 用户组 用户名 注：-g|–gid，修改用户的gid，该组一定存在 2、把用户添加进入某个组(s） 1usermod -a -G 用户组 用户名 注： -a|–append，把用户追加到某些组中，仅与-G选项一起使用 -G|–groups，把用户追加到某些组中，仅与-a选项一起使用 内容扩展 用户管理命令 useradd 注：添加用户adduser 注：添加用户passwd 注：为用户设置密码usermod 注：修改用户命令，可以通过usermod 来修改登录名、用户的家目录等等；pwcov 注：同步用户从/etc/passwd 到/etc/shadowpwck 注：pwck是校验用户配置文件/etc/passwd 和/etc/shadow 文件内容是否合法或完整；pwunconv 注：是pwcov 的立逆向操作，是从/etc/shadow和 /etc/passwd 创建/etc/passwd ，然后会删除 /etc/shadow 文件；finger 注：查看用户信息工具id 注：查看用户的UID、GID及所归属的用户组chfn 注：更改用户信息工具su 注：用户切换工具sudo 注：sudo 是通过另一个用户来执行命令（execute a command as another user），su 是用来切换用户，然后通过切换到的用户来完成相应的任务，但sudo 能后面直接执行命令，比如sudo 不需要root 密码就可以执行root 赋与的执行只有root才能执行相应的命令；但得通过visudo 来编辑/etc/sudoers来实现；visudo 注：visodo 是编辑 /etc/sudoers 的命令；也可以不用这个命令，直接用vi 来编辑 /etc/sudoers 的效果是一样的；sudoedit 注：和sudo 功能差不多； 以上就是本次介绍整理的全部内容，感谢大家的学习和对脚本之家的支持。 参考文献及资料1、Spring官网，链接：https://spring.io/projects/spring-cloud-gateway]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[谈信息通信和表达]]></title>
    <url>%2F2020%2F01%2F11%2F2021-08-23-%E8%B0%88%E4%BF%A1%E6%81%AF%E9%80%9A%E4%BF%A1%E5%92%8C%E8%A1%A8%E8%BE%BE%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景信息是事物运动状态或存在方式的不确定性的描述。 信息的基本概念在于它的不确定性，任何已确定的事物都不含有信息。信息是不确定量的减少。 第一部分参考文献及资料1、Spring官网，链接：https://spring.io/projects/spring-cloud-gateway]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud系列文章（Spring Cloud Gateway）]]></title>
    <url>%2F2020%2F01%2F11%2F2021-08-15-SpringCloud%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88Spring%20Cloud%20Gateway%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景Spring 官方最终还是按捺不住推出了自己的网关组件：Spring Cloud Gateway ，相比之前我们使用的 Zuul（1.x） 它有哪些优势呢？Zuul（1.x） 基于 Servlet，使用阻塞 API，它不支持任何长连接，如 WebSockets，Spring Cloud Gateway 使用非阻塞 API，支持 WebSockets，支持限流等新特性。 Spring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。 Spring Cloud Gateway 作为 Spring Cloud 生态系统中的网关，目标是替代 Netflix Zuul，其不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/指标，和限流。 网关的基础就是路由功能，通俗解释就是地址转发，将一个请求地址转发到实际的服务地址。网关可提供请求路由与组合、协议转换、安全认证、服务鉴权、流量控制与日志监控等服务。可选的网关有不少，比如 Nginx、高性能网关 OpenResty、Linkerd 以及 Spring Cloud Gateway。 SpringCloud Gateway 作为 Spring Cloud 生态系统中的网关，目标是替代 Zuul，在Spring Cloud 2.0以上版本中，没有对新版本的Zuul 2.0以上最新高性能版本进行集成，仍然还是使用的Zuul 2.0之前的非Reactor模式的老版本。而为了提升网关的性能，SpringCloud Gateway是基于WebFlux框架实现的，而WebFlux框架底层则使用了高性能的Reactor模式通信框架Netty。 Spring Cloud Gateway 底层使用了高性能的通信框架Netty 第一部分 SpringCloud Gateway 特征参考文献及资料1、Spring官网，链接：https://spring.io/projects/spring-cloud-gateway]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot系列文章（SpringBoot项目中配置文件总结）]]></title>
    <url>%2F2020%2F01%2F11%2F2020-10-03-SpringBoot%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88SpringBoot%E9%A1%B9%E7%9B%AE%E4%B8%AD%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E6%80%BB%E7%BB%93%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景第一部分 配置文件在使用Spring Initializer快速创建SpringBoot项目时，会在resources目录下给我们一个默认的空的全局配置文件 application.properties。配置文件的作用也正是用来修改SpringBoot自动配置的默认值。 配置文件名是固定的，SpringBoot使用一个全局配置文件application.properties或者application.yml(或者yaml)。这两个文件本质是一样的，区别只是其中的语法略微不同。 application.yml 配置文件使用YMAL语言，YMAL不是如XML般的标记语言，以数据为中心，更适合作为配置文件。 YAML： 12server: port: 8081 XML： 123&lt;server&gt; &lt;port&gt;8081&lt;/port&gt;&lt;/server&gt; 第二部分 YAML语法2.1 基本语法 使用缩进表示层级关系 缩进时不允许使用Tab键，只允许使用空格。 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 大小写敏感 k:(空格)v——表示一对键值对（空格必须有）。以空格的缩进来控制层级关系，只要是左对齐的一列数据，都是同一个层级的。 123server: port: 8081 path: /hello 格式校验可以使用在线工具：https://www.bejson.com/validators/yaml_editor/ 2.2 写法（1）字面量字面量：普通的值（数字，字符串，布尔，日期 ） k: v——字面直接书写 字符串默认不用加上单引号或者双引号 1name: zhangsan &quot;&quot;：双引号，不会转义字符串里面的特殊字符，特殊字符会作为本身想表示的意思 1name: "zhangsan \n lisi" 输出: 12zhangsanlisi &#39;&#39;：单引号，会转义特殊字符，特殊字符最终只是一个普通的字符串数据 1name: 'zhangsan \n lisi' 输出： 1zhangsan \n lisi （2）对象、Map（属性和值）（键值对） k: v——在下一行来写对象的属性和值的关系（注意缩进） 123friends: lastName: zhangsan age: 20 行内写法： 1friends: &#123;lastName: zhangsan,age: 18&#125; （3）数组（List、Set）用- 值表示数组中的一个元素 1234pets: - cat - dog - pig 行内写法 1pets: [cat,dog,pig] 第三部分 配置文件的注入3.1 配置文件书写（1）yaml配置文件配置文件application.yml 123456789101112person: lastName: zhangsan age: 20 boss: false birth: 2020/10/10 maps: &#123;k1: v1,k2: 15&#125; lists: - lisi - wangwu dog: name: 狗狗 age: 5 （2）properties配置文件或者application.properties 123456789person.last-name=张三person.age=20person.birth=2020/10/10person.boss=trueperson.maps.k1=v1person.maps.k2=15person.lists=lisi,wangwuperson.dog.name=狗狗person.dog.age=5 3.2 属性注入（1）@ConfigurationPropertiesJavaBean： 1234567891011121314151617181920212223/** * 将配置文件中配置的每一个属性的值，映射到这个组件中。 * @ConfigurationProperties：告诉SpringBoot将本类中的所有属性和配置文件中相关的配置进行绑定。 * prefix = "person"：与配置文件中person下面的所有属性一一映射 * * 只有这个组件是容器中的组件，才能容器提供的@ConfigurationProperties功能。 * */@Component@ConfigurationProperties(prefix = "person")@Validated@Datapublic class Person &#123; @NotNull //JSR303数据校验 private String LastName; private String age; private boolean boss; private Date birthday; private Map&lt;String,Object&gt; maps;//复杂类型封装 private List&lt;Object&gt; lists; private Dog dog;&#125; 导入配置文件处理器，配置文件进行绑定就会有提示： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 编写测试方法： 1234567891011@RunWith(SpringRunner.class)@SpringBootTestclass SpringbootConfigApplicationTests &#123; @Autowired Person person; @Test void test01() &#123; System.out.println(person); &#125;&#125; 输出结果： 1Person(LastName=zhangsan, age=20, boss=false, birthday=Sat Oct 10 00:00:00 CST 2020, maps=&#123;k1=v1, k2=15&#125;, lists=[lisi, wangwu], dog=Dog(name=狗狗, age=5)) （2）@Value123456789101112131415161718192021@Component@Datapublic class Person &#123; /** * &lt;bean class="Person"&gt; * &lt;property name="lastName" value="字面量/$&#123;key&#125;从环境变量、配置文件中获取值/#&#123;SpEL&#125;"&gt;&lt;/property&gt; * &lt;bean/&gt; */ @Value("$&#123;person.last-name&#125;") private String lastName; @Value("#&#123;11*2&#125;")//SpEL private Integer age; @Value("true") private Boolean boss; @Value("2020/10/10") private Date birth; private Map&lt;String,Object&gt; maps; private List&lt;Object&gt; lists; private Dog dog;&#125; 输出结果： 1Person(LastName=张三, age=22, boss=true, birthday=Fri Oct 10 00:00:00 CST 1997, maps=null, lists=null, dog=null) @Value获取值和@ConfigurationProperties获取值比较: 比较 @ConfigurationProperties @Value 功能 批量注入配置文件中的属性 一个个指定 松散绑定（松散语法） 支持 不支持 SpEL 不支持 支持 JSR303数据校验 支持 不支持 复杂类型封装 支持 不支持 不论配置文件yml还是properties他们都能获取到值。 在某个业务逻辑中需要获取配置文件中的某项值——使用@Value；专门编写了一个JavaBean来和配置文件进行映射——使用@ConfigurationProperties。 第四部分 配置文件占位符第六部分 多环境配置Profile以上都不是重点，这才是重点，这才是重点，这才是重点，重要的事情说3遍。我们在开发Spring Boot应用时，通常同一套程序会被应用和安装到几个不同的环境，比如：开发、测试、生产等。其中每个环境的数据库地址、服务器端口等等配置都会不同，如果在为不同环境打包时都要频繁修改配置文件的话，那必将是个非常繁琐且容易发生错误的事。 对于多环境的配置，各种项目构建工具或是框架的基本思路是一致的，通过配置多份不同环境的配置文件，再通过打包命令指定需要打包的内容之后进行区分打包，Spring Boot也不例外，或者说更加简单。 在Spring Boot中多环境配置文件名需要满足 application-{profile}.properties 的格式，其中 {profile} 对应你的环境标识，比如： application-dev.properties：开发环境 application-test.properties：测试环境 application-prod.properties：生产环境 至于哪个具体的配置文件会被加载，需要在 application.properties 文件中通过 spring.profiles.active 属性来设置，其值对应 {profile} 值。如： spring.profiles.active=test 就会加载 application-test.properties 配置文件内容下面，以不同环境配置不同的服务端口为例，进行样例实验。 第七部分 配置文件加载位置第八部分 外部配置加载顺序第九部分 自动配置原理参考文献及资料1、Spring官网，链接：https://juejin.cn/post/6844904186539147271]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot系列文章（第一个SpringBoot实践练习项目）]]></title>
    <url>%2F2020%2F01%2F11%2F2020-01-01-SpringBoot%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88%E7%AC%AC%E4%B8%80%E4%B8%AASpringBoot%E5%AE%9E%E8%B7%B5%E7%BB%83%E4%B9%A0%E9%A1%B9%E7%9B%AE%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景第一次使用Spring Boot构建测试项目，实现一个简单的Http请求处理，通过这个例子对Spring Boot有一个初步的了解。 第一部分 开发环境准备 Maven版本：Maven 3，version 3.3.9 Java版本：Java 1.8（Java8） 第二部分 使用Maven构建项目Spring官网提供Spring Initializr工具生成项目 第一步：登录Spring Initializr网站： https://start.spring.io/ 第二步：配置项目的参数： Group ID是项目组织唯一的标识符，实际对应项目中的package包。 Artifact ID是项目的唯一的标识符，实际对应项目的project name名称，Artifact不可包含大写字母。 然后点击生成项目压缩文件，并下载到本地。 第三步：使用IDE加载项目（使用IntelliJ IDEA） 菜单中选择File–&gt;New–&gt;Project from Existing Sources... 选择解压后的项目文件夹，点击OK 点击Import project from external model并选择Maven，点击Next到底为止。 第三部分 项目目录结构开发环境是Win7环境，导入后，使用tree /f命令查看项目结构目录： 123456789101112131415161718192021222324252627282930313233# tree /f│ .gitignore│ HELP.md│ mvnw│ mvnw.cmd│ pom.xml├─.idea│ │ azureSettings.xml│ │ compiler.xml│ │ misc.xml│ │ workspace.xml│ └─inspectionProfiles│ Project_Default.xml├─.mvn│ └─wrapper│ maven-wrapper.jar│ maven-wrapper.properties│ MavenWrapperDownloader.java└─src ├─main │ ├─java │ │ └─com │ │ └─example │ │ └─demo │ │ DemoApplication.java │ └─resources │ application.properties └─test └─java └─com └─example └─demo DemoApplicationTests.java .gitignore文件是git控制文件。 mvnw(linux shell)和mvnw.cmd(windows),还有.mvn文件夹(包含Maven Wrapper Java库及其属性文件)。mvnw全名是Maven Wrapper,它的原理是在maven-wrapper.properties文件中记录你要使用的Maven版本，当用户执行mvnw clean 命令时，发现当前用户的Maven版本和期望的版本不一致，那么就下载期望的版本，然后用期望的版本来执行mvn命令，比如刚才的mvn clean。带有mvnw文件项目，只要有java环境，仅仅通过使用本项目的mvnw脚本就可以完成编译，打包，发布等一系列操作。 HELP.mdMaven的帮助文件。 pom.xmlProject Object Model 的缩写，即项目对象模型。maven 的配置文件，用以描述项目的各种信息。 .idea/文件夹来存放项目的配置信息。其中包括版本控制信息、历史记录等等。 src/main/java下的程序入口：DemoApplication.java。 src/main/resources下的配置文件：application.properties。 src/test/下的测试入口：DemoApplicationTests.java。 第四部分 编写HelloWorld项目引入Web模块，在pom.xml文件添加下面的依赖包： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 新建controller程序(HelloSpringBootController)： 123456789101112package com.example.demo.controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class HelloSpringBootController &#123; @RequestMapping("/") public String hello()&#123; return "Hello, SpringBoot!"; &#125;&#125; 完成后项目的结构如下： 1234567891011121314151617181920├─src│ ├─main│ │ ├─java│ │ │ └─com│ │ │ └─example│ │ │ └─demo│ │ │ │ DemoApplication.java│ │ │ ││ │ │ └─controller│ │ │ HelloSpringBootController.java│ │ ││ │ └─resources│ │ application.properties│ ││ └─test│ └─java│ └─com └─example └─demo DemoApplicationTests.java 最后使用maven编译： 12mvn clean mvn package #编译项目 会生成编译结果，在项目根目录生成target文件目录。其中demo-0.0.1-SNAPSHOT.jar为编译后的入口程序。在IDEA中，或者使用java -jar demo-0.0.1-SNAPSHOT.jar命令在win CMD命令窗口中运行。 1234567891011121314151617181920"C:\Program Files\Java\jdk-10.0.2\bin\java.exe" -Dfile.encoding=GBK -jar C:\Users\rongxiang\Desktop\SpringBoot\demo\target\demo-0.0.1-SNAPSHOT.jar . ____ _ __ _ _ /\\ / ___'_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.2.2.RELEASE)2020-01-12 13:56:37.333 INFO 5060 --- [ main] com.example.demo.DemoApplication : Starting DemoApplication v0.0.1-SNAPSHOT on rongxiang-PC with PID 5060 (C:\Users\rongxiang\Desktop\SpringBoot\demo\target\demo-0.0.1-SNAPSHOT.jar started by rongxiang in C:\Users\rongxiang\Desktop\SpringBoot\demo)2020-01-12 13:56:37.338 INFO 5060 --- [ main] com.example.demo.DemoApplication : No active profile set, falling back to default profiles: default2020-01-12 13:56:40.479 INFO 5060 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http)2020-01-12 13:56:40.497 INFO 5060 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]2020-01-12 13:56:40.497 INFO 5060 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.29]2020-01-12 13:56:40.598 INFO 5060 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext2020-01-12 13:56:40.598 INFO 5060 --- [ main] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 3142 ms2020-01-12 13:56:40.945 INFO 5060 --- [ main] o.s.s.concurrent.ThreadPoolTaskExecutor : Initializing ExecutorService 'applicationTaskExecutor'2020-01-12 13:56:41.227 INFO 5060 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path ''2020-01-12 13:56:41.233 INFO 5060 --- [ main] com.example.demo.DemoApplication : Started DemoApplication in 4.773 seconds (JVM running for 5.475) 在本地启了一个web服务（Tomcat started on port(s): 8080 (http)），对外服务端口为8080。 浏览器中输入url地址（http://localhost:8080/），网页显示“Hello, SpringBoot!”，说明服务服务正常。 注意： 如果运行工程，出现这个报错信息：Failed to clean project: Failed to delete 由于之前编译的工程还在运行，无法clean，导致maven生命周期无法继续进行。即由于已启动了另一个tomcat 进程，导致报错,关闭tomcat进程即可。可以在程序控制台中终止该进程即可。 参考文献及资料1、Spring官网，链接：https://spring.io/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统中用户相关操作总结]]></title>
    <url>%2F2020%2F01%2F11%2F2021-08-22-%E5%AE%89%E5%BE%BD%E7%9C%81%E4%B8%9C%E6%B5%81%E9%95%87%E7%A8%A0%E6%9E%97%E6%9D%91%E6%88%98%E4%BA%89%E9%81%97%E5%9D%80%E8%80%83%E8%AF%81%EF%BC%88%E9%9D%9E%E6%8A%80%E6%9C%AF%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景第一部分 渡江战役介绍参考文章[1]的介绍，渡江战役二野（前身中原野战军）西集团的五兵团杨勇、苏振华指挥，由安庆以西至望江段渡江，十六军为第1梯队，十七、十八军为第2梯队。渡江成功后以一部控制滩头阵地，主力围歼东流、至德守军，接应四兵团渡江。而后沿浮梁、婺源、景德镇向浙赣线攻击前进，控制衢县、江山段铁路和衢县、歙县段公路，切断守军退路。 从地图标识上看，东流县是我军16军解放区域。 1948年7月，整编第211、213旅划归整编第2师，师长晏子凤；另在山东滕县重建整编第45师，下辖212、281两个旅，于兆龙任师长。同年9月，整编第45师恢复第96军番号，于兆龙任军长，萧续武任副军长；下辖：第212旅改番号为141师，甫绍武任师长；281旅改番号为第282师，章毓金任师长。该军恢复番号后调往安徽蚌埠。整编第2师则覆没于济南战役。 11月，该军参加了徐蚌会战，隶属李延年第六兵团。战后，该军调至安庆以南长江沿岸担任防御任务。 1949年4月下旬，人民解放军发起渡江战役后，该军第141师在南逃过程中被人民解放军全歼。军部率领第282师及残部进入福建后改隶福州绥靖公署。不久，该军余部在福州战役中被人民解放军歼灭大部，残部逃至厦门后在漳厦战役中又大部被歼，其余残部在福建永泰县向人民解放军投诚。 参考文献及资料1、渡江战役（二）：风雨起（下），链接：https://zhuanlan.zhihu.com/p/386784050 2、国民革命军第九十六军，链接：https://baike.baidu.com/item/%E5%9B%BD%E6%B0%91%E9%9D%A9%E5%91%BD%E5%86%9B%E7%AC%AC%E4%B9%9D%E5%8D%81%E5%85%AD%E5%86%9B/1773780]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java在内存中操作文件]]></title>
    <url>%2F2020%2F01%2F11%2F2021-08-18-Java%E5%9C%A8%E5%86%85%E5%AD%98%E4%B8%AD%E6%93%8D%E4%BD%9C%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景https://stackoverflow.com/questions/5677490/how-to-send-email-with-attachment-using-inputstream-and-spring 参考文献及资料1、Spring官网，链接：https://spring.io/projects/spring-cloud-gateway]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java的三种连接池（druid、c3p0、dbcp）]]></title>
    <url>%2F2020%2F01%2F11%2F2021-08-17-Java%E7%9A%84%E4%B8%89%E7%A7%8D%E8%BF%9E%E6%8E%A5%E6%B1%A0%EF%BC%88druid%E3%80%81c3p0%E3%80%81dbcp%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景https://blog.csdn.net/qq_42982169/article/details/82181631?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-12.base&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-12.base https://mryang.blog.csdn.net/article/details/82883616?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.base&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.base 参考文献及资料1、Spring官网，链接：https://spring.io/projects/spring-cloud-gateway]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[单台Linux服务器最多开多少个连接？]]></title>
    <url>%2F2020%2F01%2F11%2F2021-08-17-%E5%8D%95%E5%8F%B0Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9C%80%E5%A4%9A%E5%BC%80%E5%A4%9A%E5%B0%91%E4%B8%AA%E8%BF%9E%E6%8E%A5%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景参考文献及资料1、Spring官网，链接：https://spring.io/projects/spring-cloud-gateway]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tsfresh时间序列特征提取包介绍]]></title>
    <url>%2F2019%2F09%2F30%2F2019-09-30-Tsfresh%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%8C%85%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 tsfresh包安装 第二部分 特征提取及介绍 第三部分 特征参数设置 第四部分 特征选择和过滤 参考文献及资料 背景时间序列类型的数据是我们数据处理经常遇到的数据类型，这类数据主要特征是具备时间属性，即数据按照时间顺序先后产生。 tsfresh是一个时间序列数据特征提取和特征选取工具包，主要用于时间序列数据的特征工程。官网地址为：https://github.com/blue-yonder/tsfresh 。 ###第一部分 tsfresh包安装 安装tsfresh可以使用pip命令进行安装： 1# pip install tsfresh 包文件介绍： 第一部分 项目结构第二部分 特征提取及介绍tsfresh中的特征提取代码在tsfresh/feature_extraction/feature_calculators.py中。我们逐个介绍这些特征的包。 对于特征分为两类： 简单类：特征提取只输出有个单个数值。 组合类： 1、时间序列平方和函数：tsfresh.feature_extraction.feature_calculators.abs_energy(x)$$E=\sum_{i=1}^{n} x_{i}^{2}，其中{x_i}_{i=1}^n为时间序列；$$源码如下： 12345678910111213def abs_energy(x): """ Returns the absolute energy of the time series which is the sum over the squared values .. math:: E=\sum_&#123;i=1&#125;^&#123;n&#125; x_&#123;i&#125;^&#123;2&#125; :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) return np.dot(x, x) 2、时间序列一阶差分绝对和函数：tsfresh.feature_extraction.feature_calculators.absolute_sum_of_changes(x)$$\sum_{i=1}^{n-1}\left|x_{i+1}-x_{i}\right|$$源码如下： 1234567891011def absolute_sum_of_changes(x): """ Returns the sum over the absolute value of consecutive changes in the series x .. math:: \\sum_&#123;i=1, \ldots, n-1&#125; \\mid x_&#123;i+1&#125;- x_i \\mid :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ return np.sum(np.abs(np.diff(x))) 3、时间序列各阶自相关系数的聚合统计特征函数：tsfresh.feature_extraction.feature_calculators.agg_autocorrelation(x, param) 该特征为各阶自相关系数的聚合统计特征。$$R(l)=\frac{1}{(n-l) \sigma^{2}} \sum_{i=1}^{n-l}\left(x_{i}-\mu\right)\left(x_{i+l}-\mu\right)$$parma(list) 包含一个字典{“f_agg”: x, “maxlag”, n} 其中x为聚合函数名，n为最大差分阶数。函数返回时序数据的各阶差分值之间的聚合（方差、均值）统计特征。 1234567891011121314151617181920212223242526272829303132333435def agg_autocorrelation(x, param): r""" Calculates the value of an aggregation function :math:`f_&#123;agg&#125;` (e.g. the variance or the mean) over the autocorrelation :math:`R(l)` for different lags. The autocorrelation :math:`R(l)` for lag :math:`l` is defined as .. math:: R(l) = \frac&#123;1&#125;&#123;(n-l)\sigma^&#123;2&#125;&#125; \sum_&#123;t=1&#125;^&#123;n-l&#125;(X_&#123;t&#125;-\mu )(X_&#123;t+l&#125;-\mu) where :math:`X_i` are the values of the time series, :math:`n` its length. Finally, :math:`\sigma^2` and :math:`\mu` are estimators for its variance and mean (See `Estimation of the Autocorrelation function &lt;http://en.wikipedia.org/wiki/Autocorrelation#Estimation&gt;`_). The :math:`R(l)` for different lags :math:`l` form a vector. This feature calculator applies the aggregation function :math:`f_&#123;agg&#125;` to this vector and returns .. math:: f_&#123;agg&#125; \left( R(1), \ldots, R(m)\right) \quad \text&#123;for&#125; \quad m = max(n, maxlag). Here :math:`maxlag` is the second parameter passed to this function. :param x: the time series to calculate the feature of :type x: numpy.ndarray :param param: contains dictionaries &#123;"f_agg": x, "maxlag", n&#125; with x str, the name of a numpy function (e.g. "mean", "var", "std", "median"), its the name of the aggregator function that is applied to the autocorrelations. Further, n is an int and the maximal number of lags to consider. :type param: list :return: the value of this feature :return type: float """ # if the time series is longer than the following threshold, we use fft to calculate the acf THRESHOLD_TO_USE_FFT = 1250 var = np.var(x) n = len(x) max_maxlag = max([config["maxlag"] for config in param]) if np.abs(var) &lt; 10**-10 or n == 1: a = [0] * len(x) else: a = acf(x, unbiased=True, fft=n &gt; THRESHOLD_TO_USE_FFT, nlags=max_maxlag)[1:] return [("f_agg_\"&#123;&#125;\"__maxlag_&#123;&#125;".format(config["f_agg"], config["maxlag"]), getattr(np, config["f_agg"])(a[:int(config["maxlag"])])) for config in param] 4、时间序列基于分块时序聚合值的线性回归函数：tsfresh.feature_extraction.feature_calculators.agg_linear_trend(x, param) 源代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445def agg_linear_trend(x, param): """ Calculates a linear least-squares regression for values of the time series that were aggregated over chunks versus the sequence from 0 up to the number of chunks minus one. This feature assumes the signal to be uniformly sampled. It will not use the time stamps to fit the model. The parameters attr controls which of the characteristics are returned. Possible extracted attributes are "pvalue", "rvalue", "intercept", "slope", "stderr", see the documentation of linregress for more information. The chunksize is regulated by "chunk_len". It specifies how many time series values are in each chunk. Further, the aggregation function is controlled by "f_agg", which can use "max", "min" or , "mean", "median" :param x: the time series to calculate the feature of :type x: numpy.ndarray :param param: contains dictionaries &#123;"attr": x, "chunk_len": l, "f_agg": f&#125; with x, f an string and l an int :type param: list :return: the different feature values :return type: pandas.Series """ # todo: we could use the index of the DataFrame here calculated_agg = &#123;&#125; res_data = [] res_index = [] for parameter_combination in param: chunk_len = parameter_combination["chunk_len"] f_agg = parameter_combination["f_agg"] aggregate_result = _aggregate_on_chunks(x, f_agg, chunk_len) if f_agg not in calculated_agg or chunk_len not in calculated_agg[f_agg]: if chunk_len &gt;= len(x): calculated_agg[f_agg] = &#123;chunk_len: np.NaN&#125; else: lin_reg_result = linregress(range(len(aggregate_result)), aggregate_result) calculated_agg[f_agg] = &#123;chunk_len: lin_reg_result&#125; attr = parameter_combination["attr"] if chunk_len &gt;= len(x): res_data.append(np.NaN) else: res_data.append(getattr(calculated_agg[f_agg][chunk_len], attr)) res_index.append("f_agg_\"&#123;&#125;\"__chunk_len_&#123;&#125;__attr_\"&#123;&#125;\"".format(f_agg, chunk_len, attr)) return zip(res_index, res_data) 5、时间序列近似熵函数：tsfresh.feature_extraction.feature_calculators.approximate_entropy(x, m, r) 源代码如下： 12345678910111213141516171819202122232425262728293031323334353637def approximate_entropy(x, m, r): """ Implements a vectorized Approximate entropy algorithm. https://en.wikipedia.org/wiki/Approximate_entropy For short time-series this method is highly dependent on the parameters, but should be stable for N &gt; 2000, see: Yentes et al. (2012) - *The Appropriate Use of Approximate Entropy and Sample Entropy with Short Data Sets* Other shortcomings and alternatives discussed in: Richman &amp; Moorman (2000) - *Physiological time-series analysis using approximate entropy and sample entropy* :param x: the time series to calculate the feature of :type x: numpy.ndarray :param m: Length of compared run of data :type m: int :param r: Filtering level, must be positive :type r: float :return: Approximate entropy :return type: float """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) N = x.size r *= np.std(x) if r &lt; 0: raise ValueError("Parameter r must be positive.") if N &lt;= m+1: return 0 def _phi(m): x_re = np.array([x[i:i+m] for i in range(N - m + 1)]) C = np.sum(np.max(np.abs(x_re[:, np.newaxis] - x_re[np.newaxis, :]), axis=2) &lt;= r, axis=0) / (N-m+1) return np.sum(np.log(C)) / (N - m + 1.0) return np.abs(_phi(m) - _phi(m + 1)) 6、时间序列自回归系数函数：tsfresh.feature_extraction.feature_calculators.ar_coefficient(x, param) 衡量时序数据的的周期性、不可预测性和波动性。 源代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def ar_coefficient(x, param): """ This feature calculator fits the unconditional maximum likelihood of an autoregressive AR(k) process. The k parameter is the maximum lag of the process .. math:: X_&#123;t&#125;=\\varphi_0 +\\sum _&#123;&#123;i=1&#125;&#125;^&#123;k&#125;\\varphi_&#123;i&#125;X_&#123;&#123;t-i&#125;&#125;+\\varepsilon_&#123;t&#125; For the configurations from param which should contain the maxlag "k" and such an AR process is calculated. Then the coefficients :math:`\\varphi_&#123;i&#125;` whose index :math:`i` contained from "coeff" are returned. :param x: the time series to calculate the feature of :type x: numpy.ndarray :param param: contains dictionaries &#123;"coeff": x, "k": y&#125; with x,y int :type param: list :return x: the different feature values :return type: pandas.Series """ calculated_ar_params = &#123;&#125; x_as_list = list(x) calculated_AR = AR(x_as_list) res = &#123;&#125; for parameter_combination in param: k = parameter_combination["k"] p = parameter_combination["coeff"] column_name = "k_&#123;&#125;__coeff_&#123;&#125;".format(k, p) if k not in calculated_ar_params: try: calculated_ar_params[k] = calculated_AR.fit(maxlag=k, solver="mle").params except (LinAlgError, ValueError): calculated_ar_params[k] = [np.NaN]*k mod = calculated_ar_params[k] if p &lt;= k: try: res[column_name] = mod[p] except IndexError: res[column_name] = 0 else: res[column_name] = np.NaN return [(key, value) for key, value in res.items()] 7、时间序列ADF检验函数：tsfresh.feature_extraction.feature_calculators.augmented_dickey_fuller(x, param) 测试一个自回归模型是否存在单位根，衡量时序数据的平稳性。 源代码： 123456789101112131415161718192021222324252627def augmented_dickey_fuller(x, param): """ The Augmented Dickey-Fuller test is a hypothesis test which checks whether a unit root is present in a time series sample. This feature calculator returns the value of the respective test statistic. See the statsmodels implementation for references and more details. :param x: the time series to calculate the feature of :type x: numpy.ndarray :param param: contains dictionaries &#123;"attr": x&#125; with x str, either "teststat", "pvalue" or "usedlag" :type param: list :return: the value of this feature :return type: float """ res = None try: res = adfuller(x) except LinAlgError: res = np.NaN, np.NaN, np.NaN except ValueError: # occurs if sample size is too small res = np.NaN, np.NaN, np.NaN except MissingDataError: # is thrown for e.g. inf or nan in the data res = np.NaN, np.NaN, np.NaN return [('attr_"&#123;&#125;"'.format(config["attr"]), res[0] if config["attr"] == "teststat" else res[1] if config["attr"] == "pvalue" else res[2] if config["attr"] == "usedlag" else np.NaN) for config in param] 8、时间序列lag阶自相关性函数：tsfresh.feature_extraction.feature_calculators.autocorrelation(x, lag) 计算lag阶滞后时序数据的自相关性（浮点数） 源代码： 1234567891011121314151617181920212223242526272829303132333435def autocorrelation(x, lag): """ Calculates the autocorrelation of the specified lag, according to the formula [1] .. math:: \\frac&#123;1&#125;&#123;(n-l)\sigma^&#123;2&#125;&#125; \\sum_&#123;t=1&#125;^&#123;n-l&#125;(X_&#123;t&#125;-\\mu )(X_&#123;t+l&#125;-\\mu) where :math:`n` is the length of the time series :math:`X_i`, :math:`\sigma^2` its variance and :math:`\mu` its mean. `l` denotes the lag. .. rubric:: References [1] https://en.wikipedia.org/wiki/Autocorrelation#Estimation :param x: the time series to calculate the feature of :type x: numpy.ndarray :param lag: the lag :type lag: int :return: the value of this feature :return type: float """ # This is important: If a series is passed, the product below is calculated # based on the index, which corresponds to squaring the series. if type(x) is pd.Series: x = x.values if len(x) &lt; lag: return np.nan # Slice the relevant subseries based on the lag y1 = x[:(len(x)-lag)] y2 = x[lag:] # Subtract the mean of the whole series x x_mean = np.mean(x) # The result is sometimes referred to as "covariation" sum_product = np.sum((y1 - x_mean) * (y2 - x_mean)) # Return the normalized unbiased covariance v = np.var(x) if np.isclose(v, 0): return np.NaN else: return sum_product / ((len(x) - lag) * v) 9、时间序列分组熵函数：tsfresh.feature_extraction.feature_calculators.binned_entropy(x, max_bins) 12345678910111213141516171819def binned_entropy(x, max_bins): """ First bins the values of x into max_bins equidistant bins. Then calculates the value of .. math:: - \\sum_&#123;k=0&#125;^&#123;min(max\\_bins, len(x))&#125; p_k log(p_k) \\cdot \\mathbf&#123;1&#125;_&#123;(p_k &gt; 0)&#125; where :math:`p_k` is the percentage of samples in bin :math:`k`. :param x: the time series to calculate the feature of :type x: numpy.ndarray :param max_bins: the maximal number of bins :type max_bins: int :return: the value of this feature :return type: float """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) hist, bin_edges = np.histogram(x, bins=max_bins) probs = hist / x.size return - np.sum(p * np.math.log(p) for p in probs if p != 0) 10、时序数据非线性度量函数：tsfresh.feature_extraction.feature_calculators.c3(x, lag) 基于物理学的时序数据非线性度量（浮点数） 12345678910111213141516171819202122232425262728def c3(x, lag): """ This function calculates the value of .. math:: \\frac&#123;1&#125;&#123;n-2lag&#125; \sum_&#123;i=0&#125;^&#123;n-2lag&#125; x_&#123;i + 2 \cdot lag&#125;^2 \cdot x_&#123;i + lag&#125; \cdot x_&#123;i&#125; which is .. math:: \\mathbb&#123;E&#125;[L^2(X)^2 \cdot L(X) \cdot X] where :math:`\\mathbb&#123;E&#125;` is the mean and :math:`L` is the lag operator. It was proposed in [1] as a measure of non linearity in the time series. .. rubric:: References | [1] Schreiber, T. and Schmitz, A. (1997). | Discrimination power of measures for nonlinearity in a time series | PHYSICAL REVIEW E, VOLUME 55, NUMBER 5 :param x: the time series to calculate the feature of :type x: numpy.ndarray :param lag: the lag that should be used in the calculation of the feature :type lag: int :return: the value of this feature :return type: float """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) n = x.size if 2 * lag &gt;= n: return 0 else: return np.mean((_roll(x, 2 * -lag) * _roll(x, -lag) * x)[0:(n - 2 * lag)]) 11、时间序列给定区间的统计量函数：tsfresh.feature_extraction.feature_calculators.change_quantiles(x, ql, qh, isabs, f_agg) 12345678910111213141516171819202122232425262728293031323334353637383940def change_quantiles(x, ql, qh, isabs, f_agg): """ First fixes a corridor given by the quantiles ql and qh of the distribution of x. Then calculates the average, absolute value of consecutive changes of the series x inside this corridor. Think about selecting a corridor on the y-Axis and only calculating the mean of the absolute change of the time series inside this corridor. :param x: the time series to calculate the feature of :type x: numpy.ndarray :param ql: the lower quantile of the corridor :type ql: float :param qh: the higher quantile of the corridor :type qh: float :param isabs: should the absolute differences be taken? :type isabs: bool :param f_agg: the aggregator function that is applied to the differences in the bin :type f_agg: str, name of a numpy function (e.g. mean, var, std, median) :return: the value of this feature :return type: float """ if ql &gt;= qh: ValueError("ql=&#123;&#125; should be lower than qh=&#123;&#125;".format(ql, qh)) div = np.diff(x) if isabs: div = np.abs(div) # All values that originate from the corridor between the quantiles ql and qh will have the category 0, # other will be np.NaN try: bin_cat = pd.qcut(x, [ql, qh], labels=False) bin_cat_0 = bin_cat == 0 except ValueError: # Occurs when ql are qh effectively equal, e.g. x is not long enough or is too categorical return 0 # We only count changes that start and end inside the corridor ind = (bin_cat_0 &amp; _roll(bin_cat_0, 1))[1:] if sum(ind) == 0: return 0 else: ind_inside_corridor = np.where(ind == 1) aggregator = getattr(np, f_agg) return aggregator(div[ind_inside_corridor]) 12、时间序列复杂度函数：tsfresh.feature_extraction.feature_calculators.cid_ce(x, normalize) 用来评估时间序列的复杂度，越复杂的序列有越多的谷峰。 （浮点数） 12345678910111213141516171819202122232425262728def cid_ce(x, normalize): """ This function calculator is an estimate for a time series complexity [1] (A more complex time series has more peaks, valleys etc.). It calculates the value of .. math:: \\sqrt&#123; \\sum_&#123;i=0&#125;^&#123;n-2lag&#125; ( x_&#123;i&#125; - x_&#123;i+1&#125;)^2 &#125; .. rubric:: References | [1] Batista, Gustavo EAPA, et al (2014). | CID: an efficient complexity-invariant distance for time series. | Data Mining and Knowledge Discovery 28.3 (2014): 634-669. :param x: the time series to calculate the feature of :type x: numpy.ndarray :param normalize: should the time series be z-transformed? :type normalize: bool :return: the value of this feature :return type: float """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) if normalize: s = np.std(x) if s!=0: x = (x - np.mean(x))/s else: return 0.0 x = np.diff(x) return np.sqrt(np.dot(x, x)) 13、时间序列高于均值个数函数：tsfresh.feature_extraction.feature_calculators.count_above_mean(x) 源码如下： 12345678910def count_above_mean(x): """ Returns the number of values in x that are higher than the mean of x :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ m = np.mean(x) return np.where(x &gt; m)[0].size 14、时间序列低于均值个数函数：tsfresh.feature_extraction.feature_calculators.count_below_mean(x) 源代码： 12345678910def count_below_mean(x): """ Returns the number of values in x that are lower than the mean of x :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ m = np.mean(x) return np.where(x &lt; m)[0].size 15、时间序列Ricker小波分析函数：tsfresh.feature_extraction.feature_calculators.cwt_coefficients(x, param) 连续的小波分析，ricker子波是地震勘探中常用的子波类型，ricker子波是基于波动方程严格推导得到的。（pandas.Series） 16、时间序列分块局部熵比率tsfresh.feature_extraction.feature_calculators.energy_ratio_by_chunks(x, param) 将时序数据分块后，计算目标块数据的熵与全体的熵比率。当数据不够均分时，会将多余的数据在前面的块中散布。（浮点数） 17、时间序列绝对傅里叶变换的谱统计量tsfresh.feature_extraction.feature_calculators.fft_aggregated(x, param) 18、时间序列傅里叶变换系数tsfresh.feature_extraction.feature_calculators.fft_coefficient(x, param) 19、时间序列第一个最大值位置函数：tsfresh.feature_extraction.feature_calculators.first_location_of_maximum(x) 源代码： 123456789101112def first_location_of_maximum(x): """ Returns the first location of the maximum value of x. The position is calculated relatively to the length of x. :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) return np.argmax(x) / len(x) if len(x) &gt; 0 else np.NaN 20、时间序列第一个最小值位置函数：tsfresh.feature_extraction.feature_calculators.first_location_of_minimum(x) 源代码： 123456789101112def first_location_of_minimum(x): """ Returns the first location of the minimal value of x. The position is calculated relatively to the length of x. :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) return np.argmin(x) / len(x) if len(x) &gt; 0 else np.NaN 21、时间序列Langevin模型拟合的多项式系数tsfresh.feature_extraction.feature_calculators.friedrich_coefficients(x, param) 22、时间序列数值是否有重复函数：tsfresh.feature_extraction.feature_calculators.has_duplicate(x) 1234567891011def has_duplicate(x): """ Checks if any value in x occurs more than once :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: bool """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) return x.size != np.unique(x).size 23、时间序列最大值是否有重复函数：tsfresh.feature_extraction.feature_calculators.has_duplicate_max(x) 1234567891011def has_duplicate(x): """ Checks if any value in x occurs more than once :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: bool """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) return x.size != np.unique(x).size 24、时间序列最小值是否有重复函数：tsfresh.feature_extraction.feature_calculators.has_duplicate_min(x) 1234567891011def has_duplicate_min(x): """ Checks if the minimal value of x is observed more than once :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: bool """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) return np.sum(x == np.min(x)) &gt;= 2 25、时间序列分位数索引tsfresh.feature_extraction.feature_calculators.index_mass_quantile(x, param) 26、时间序列峰度tsfresh.feature_extraction.feature_calculators.kurtosis(x) 27、时间序列标准差是否大于r倍偏差函数：tsfresh.feature_extraction.feature_calculators.large_standard_deviation(x, r) 源代码： 123456789101112131415161718def large_standard_deviation(x, r): """ Boolean variable denoting if the standard dev of x is higher than 'r' times the range = difference between max and min of x. Hence it checks if .. math:: std(x) &gt; r * (max(X)-min(X)) According to a rule of the thumb, the standard deviation should be a forth of the range of the values. :param x: the time series to calculate the feature of :type x: numpy.ndarray :param r: the percentage of the range to compare with :type r: float :return: the value of this feature :return type: bool """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) return np.std(x) &gt; (r * (np.max(x) - np.min(x))) 28、时间序列最后一个最大值位置函数：tsfresh.feature_extraction.feature_calculators.last_location_of_maximum(x) 源代码如下： 1234567891011def last_location_of_maximum(x): """ Returns the relative last location of the maximum value of x. The position is calculated relatively to the length of x. :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ x = np.asarray(x) return 1.0 - np.argmax(x[::-1]) / len(x) if len(x) &gt; 0 else np.NaN 29、时间序列最后一个最大值位置函数：tsfresh.feature_extraction.feature_calculators.last_location_of_minimum(x) 源代码： 1234567891011def last_location_of_minimum(x): """ Returns the last location of the minimal value of x. The position is calculated relatively to the length of x. :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ x = np.asarray(x) return 1.0 - np.argmin(x[::-1]) / len(x) if len(x) &gt; 0 else np.NaN 30、时间序列的长度函数：tsfresh.feature_extraction.feature_calculators.length(x) 源代码： 123456789def length(x): """ Returns the length of x :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: int """ return len(x) 31、时间序列线性回归分析tsfresh.feature_extraction.feature_calculators.linear_trend(x, param) 32、时间序列均值上的最长连续自列长度tsfresh.feature_extraction.feature_calculators.longest_strike_above_mean(x) 33、时间序列均值下的最长连续自列长度tsfresh.feature_extraction.feature_calculators.longest_strike_below_mean(x) 34、时间序列最大langevin不动点tsfresh.feature_extraction.feature_calculators.max_langevin_fixed_point(x, r, m) 35、时间序列最大值函数：tsfresh.feature_extraction.feature_calculators.maximum(x) 该特征为时间序列最大值。$$MAX = max{x_{i}}_{i=1}^{n}$$源代码为： 123456789def maximum(x): """ Calculates the highest value of the time series x. :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ return np.max(x) 36、时间序列平均值函数：tsfresh.feature_extraction.feature_calculators.mean(x) 该特征为时间序列平均值。$$Mean = Mean{x_{i}}_{i=1}^{n}$$源代码为： 123456789def mean(x): """ Returns the mean of x :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ return np.mean(x) 37、时间序列一阶差分绝对平均值函数：tsfresh.feature_extraction.feature_calculators.mean_abs_change(x) $$\frac{1}{n}\sum_{i=1}^{n-1}\left|x_{i+1}-x_{i}\right|$$源代码如下： 1234567891011def mean_abs_change(x): """ Returns the mean over the absolute differences between subsequent time series values which is .. math:: \\frac&#123;1&#125;&#123;n&#125; \\sum_&#123;i=1,\ldots, n-1&#125; | x_&#123;i+1&#125; - x_&#123;i&#125;| :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ return np.mean(np.abs(np.diff(x))) 38、时间序列一阶差分平均值函数：tsfresh.feature_extraction.feature_calculators.mean_change(x) $$\frac{1}{n} \sum_{i=1}^{n-1}x_{i+1}-x_{i}$$源代码如下： 1234567891011def mean_change(x): """ Returns the mean over the differences between subsequent time series values which is .. math:: \\frac&#123;1&#125;&#123;n&#125; \\sum_&#123;i=1,\ldots, n-1&#125; x_&#123;i+1&#125; - x_&#123;i&#125; :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ return np.mean(np.diff(x)) 39、时间序列二阶导数的中心均值函数：tsfresh.feature_extraction.feature_calculators.mean_second_derivative_central(x)$$\frac{1}{n} \sum_{i=1}^{n-1}\frac{1}{2} (x_{i+2} - 2 \cdot x_{i+1} + x_i)$$源代码如下： 12345678910111213def mean_second_derivative_central(x): """ Returns the mean value of a central approximation of the second derivative .. math:: \\frac&#123;1&#125;&#123;n&#125; \\sum_&#123;i=1,\ldots, n-1&#125; \\frac&#123;1&#125;&#123;2&#125; (x_&#123;i+2&#125; - 2 \\cdot x_&#123;i+1&#125; + x_i) :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ diff = (_roll(x, 1) - 2 * np.array(x) + _roll(x, -1)) / 2.0 return np.mean(diff[1:-1]) 40、时间序列中位数函数：tsfresh.feature_extraction.feature_calculators.median(x)$$median = median{x_{i}}_{i=1}^{n}$$源代码如下： 123456789def median(x): """ Returns the median of x :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ return np.median(x) 41、时间序列最小值函数：tsfresh.feature_extraction.feature_calculators.minimum(x)$$min = min{x_{i}}_{i=1}^{n}$$源代码如下： 123456789def minimum(x): """ Calculates the lowest value of the time series x. :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ return np.min(x) 42、时间序列的交叉次数函数：tsfresh.feature_extraction.feature_calculators.number_crossing_m(x, m) 这个特征通俗的讲：给定阀值m，查找时间序列中任意两个连续值组成的数值区间是否涵盖m值。例如时间序列[1,2,1,2,3]，对于给定的m=1.5，那么交叉数为3。 需要注意的：如果连续数值为a&lt;b，数值区间为[a,b)。 123456789101112131415def number_crossing_m(x, m): """ Calculates the number of crossings of x on m. A crossing is defined as two sequential values where the first value is lower than m and the next is greater, or vice-versa. If you set m to zero, you will get the number of zero crossings. :param x: the time series to calculate the feature of :type x: numpy.ndarray :param m: the threshold for the crossing :type m: float :return: the value of this feature :return type: int """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) # From https://stackoverflow.com/questions/3843017/efficiently-detect-sign-changes-in-python positive = x &gt; m return np.where(np.bitwise_xor(positive[1:], positive[:-1]))[0].size 43、时间序列搜寻不同峰值函数：tsfresh.feature_extraction.feature_calculators.number_cwt_peaks(x, n) 使用连续小波变换寻找时间序列中的波峰。 12345678910111213from scipy.signal import cwt, find_peaks_cwt, ricker, welchdef number_cwt_peaks(x, n): """ This feature calculator searches for different peaks in x. To do so, x is smoothed by a ricker wavelet and for widths ranging from 1 to n. This feature calculator returns the number of peaks that occur at enough width scales and with sufficiently high Signal-to-Noise-Ratio (SNR) :param x: the time series to calculate the feature of :type x: numpy.ndarray :param n: maximum width to consider :type n: int :return: the value of this feature :return type: int """ return len(find_peaks_cwt(vector=x, widths=np.array(list(range(1, n + 1))), wavelet=ricker)) 44、时间序列领域支撑峰值数量函数：tsfresh.feature_extraction.feature_calculators.number_peaks(x, n) 对于给定的n值（整型）， 源代码： 12345678910111213141516171819202122232425262728293031def number_peaks(x, n): """ Calculates the number of peaks of at least support n in the time series x. A peak of support n is defined as a subsequence of x where a value occurs, which is bigger than its n neighbours to the left and to the right. Hence in the sequence &gt;&gt;&gt; x = [3, 0, 0, 4, 0, 0, 13] 4 is a peak of support 1 and 2 because in the subsequences &gt;&gt;&gt; [0, 4, 0] &gt;&gt;&gt; [0, 0, 4, 0, 0] 4 is still the highest value. Here, 4 is not a peak of support 3 because 13 is the 3th neighbour to the right of 4 and its bigger than 4. :param x: the time series to calculate the feature of :type x: numpy.ndarray :param n: the support of the peak :type n: int :return: the value of this feature :return type: float """ x_reduced = x[n:-n] res = None for i in range(1, n + 1): result_first = (x_reduced &gt; _roll(x, i)[n:-n]) if res is None: res = result_first else: res &amp;= result_first res &amp;= (x_reduced &gt; _roll(x, -i)[n:-n]) return np.sum(res) 45、tsfresh.feature_extraction.feature_calculators.partial_autocorrelation(x, param) 46、时间序列重复数字个数占比函数：tsfresh.feature_extraction.feature_calculators.percentage_of_reoccurring_datapoints_to_all_datapoints(x) 源代码如下： 123456789101112131415161718192021def percentage_of_reoccurring_datapoints_to_all_datapoints(x): """ Returns the percentage of unique values, that are present in the time series more than once. len(different values occurring more than once) / len(different values) This means the percentage is normalized to the number of unique values, in contrast to the percentage_of_reoccurring_values_to_all_values. :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ if len(x) == 0: return np.nan unique, counts = np.unique(x, return_counts=True) if counts.shape[0] == 0: return 0 return np.sum(counts &gt; 1) / float(counts.shape[0]) 47、时间序列重复数字占比tsfresh.feature_extraction.feature_calculators.percentage_of_reoccurring_values_to_all_values(x) 源代码如下： 12345678910111213141516171819202122232425def percentage_of_reoccurring_values_to_all_values(x): """ Returns the ratio of unique values, that are present in the time series more than once. # of data points occurring more than once / # of all data points This means the ratio is normalized to the number of data points in the time series, in contrast to the percentage_of_reoccurring_datapoints_to_all_datapoints. :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ if not isinstance(x, pd.Series): x = pd.Series(x) if x.size == 0: return np.nan value_counts = x.value_counts() reoccuring_values = value_counts[value_counts &gt; 1].sum() if np.isnan(reoccuring_values): return 0 return reoccuring_values / x.size 48、时间序列分数位函数：tsfresh.feature_extraction.feature_calculators.quantile(x, q) 源代码： 123456789101112def quantile(x, q): """ Calculates the q quantile of x. This is the value of x greater than q% of the ordered values from x. :param x: the time series to calculate the feature of :type x: numpy.ndarray :param q: the quantile to calculate :type q: float :return: the value of this feature :return type: float """ x = pd.Series(x) return pd.Series.quantile(x, q) 49、时间序列指定区间数值个数函数：tsfresh.feature_extraction.feature_calculators.range_count(x, min, max) 源代码如下： 12345678910111213def range_count(x, min, max): """ Count observed values within the interval [min, max). :param x: the time series to calculate the feature of :type x: numpy.ndarray :param min: the inclusive lower bound of the range :type min: int or float :param max: the exclusive upper bound of the range :type max: int or float :return: the count of values within the range :rtype: int """ return np.sum((x &gt;= min) &amp; (x &lt; max)) 50、时间序列sigma原则函数：tsfresh.feature_extraction.feature_calculators.ratio_beyond_r_sigma(x, r) 源码入下： 1234567891011def ratio_beyond_r_sigma(x, r): """ Ratio of values that are more than r*std(x) (so r sigma) away from the mean of x. :param x: the time series to calculate the feature of :type x: iterable :return: the value of this feature :return type: float """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) return np.sum(np.abs(x - np.mean(x)) &gt; r * np.std(x))/x.size 51、时间序列唯一值数量占整体的比例函数：tsfresh.feature_extraction.feature_calculators.ratio_value_number_to_time_series_length(x) 源代码如下： 1234567891011121314151617def ratio_value_number_to_time_series_length(x): """ Returns a factor which is 1 if all values in the time series occur only once, and below one if this is not the case. In principle, it just returns # unique values / # values :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) if x.size == 0: return np.nan return np.unique(x).size / x.size 52、时间序列样本熵函数：tsfresh.feature_extraction.feature_calculators.sample_entropy(x) 通过度量信号中产生新模式的概率大小来衡量时间序列复杂性，新模式产生的概率越大，序列的复杂性就越大。样本熵的值越低，序列自我相似性就越高；样本熵的值越大，样本序列就越复杂。 源代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def sample_entropy(x): """ Calculate and return sample entropy of x. .. rubric:: References | [1] http://en.wikipedia.org/wiki/Sample_Entropy | [2] https://www.ncbi.nlm.nih.gov/pubmed/10843903?dopt=Abstract :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ x = np.array(x) sample_length = 1 # number of sequential points of the time series tolerance = 0.2 * np.std(x) # 0.2 is a common value for r - why? n = len(x) prev = np.zeros(n) curr = np.zeros(n) A = np.zeros((1, 1)) # number of matches for m = [1,...,template_length - 1] B = np.zeros((1, 1)) # number of matches for m = [1,...,template_length] for i in range(n - 1): nj = n - i - 1 ts1 = x[i] for jj in range(nj): j = jj + i + 1 if abs(x[j] - ts1) &lt; tolerance: # distance between two vectors curr[jj] = prev[jj] + 1 temp_ts_length = min(sample_length, curr[jj]) for m in range(int(temp_ts_length)): A[m] += 1 if j &lt; n - 1: B[m] += 1 else: curr[jj] = 0 for j in range(nj): prev[j] = curr[j] N = n * (n - 1) / 2 B = np.vstack(([N], B[0])) # sample entropy = -1 * (log (A/B)) similarity_ratio = A / B se = -1 * np.log(similarity_ratio) se = np.reshape(se, -1) return se[0] 53、tsfresh.feature_extraction.feature_calculators.set_property(key, value) 54、时间序列分布偏度函数：tsfresh.feature_extraction.feature_calculators.skewness(x) 偏度是统计数据分布偏斜方向和程度的度量，是统计数据分布非对称程度的数字特征。 123456789101112def skewness(x): """ Returns the sample skewness of x (calculated with the adjusted Fisher-Pearson standardized moment coefficient G1). :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ if not isinstance(x, pd.Series): x = pd.Series(x) return pd.Series.skew(x) 55、tsfresh.feature_extraction.feature_calculators.spkt_welch_density(x, param) 56、时间序列的标准方差函数：tsfresh.feature_extraction.feature_calculators.standard_deviation(x) 源代码如下： 123456789def standard_deviation(x): """ Returns the standard deviation of x :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ return np.std(x) 57、时间序列重复数据的总个数函数：tsfresh.feature_extraction.feature_calculators.sum_of_reoccurring_data_points(x) 源代码如下： 123456789101112def sum_of_reoccurring_data_points(x): """ Returns the sum of all data points, that are present in the time series more than once. :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ unique, counts = np.unique(x, return_counts=True) counts[counts &lt; 2] = 0 return np.sum(counts * unique) 58、时间序列重复数据的和函数：tsfresh.feature_extraction.feature_calculators.sum_of_reoccurring_values(x) 源代码如下： 12345678910111213def sum_of_reoccurring_values(x): """ Returns the sum of all values, that are present in the time series more than once. :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ unique, counts = np.unique(x, return_counts=True) counts[counts &lt; 2] = 0 counts[counts &gt; 1] = 1 return np.sum(counts * unique) 59、时间序列和函数：tsfresh.feature_extraction.feature_calculators.sum_values(x) 该特征为时间序列和。$$SUM = \sum_{i=1}^{n} x_{i}^{2}$$源代码如下： 123456789101112def sum_values(x): """ Calculates the sum over the time series values :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ if len(x) == 0: return 0 return np.sum(x) 60、tsfresh.feature_extraction.feature_calculators.symmetry_looking(x, param) 61、tsfresh.feature_extraction.feature_calculators.time_reversal_asymmetry_statistic(x, lag) 62、时间序列某个值的个数函数：tsfresh.feature_extraction.feature_calculators.value_count(x, value) 源代码： 1234567891011121314151617def value_count(x, value): """ Count occurrences of `value` in time series x. :param x: the time series to calculate the feature of :type x: numpy.ndarray :param value: the value to be counted :type value: int or float :return: the count :rtype: int """ if not isinstance(x, (np.ndarray, pd.Series)): x = np.asarray(x) if np.isnan(value): return np.isnan(x).sum() else: return x[x == value].size 63、时间序列方差函数：tsfresh.feature_extraction.feature_calculators.variance(x) 源代码如下： 123456789def variance(x): """ Returns the variance of x :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float """ return np.var(x) 64、时间序列方差是否大于标准方差函数：tsfresh.feature_extraction.feature_calculators.variance_larger_than_standard_deviation(x) 代码如下： 1234567891011def variance_larger_than_standard_deviation(x): """ Boolean variable denoting if the variance of x is greater than its standard deviation. Is equal to variance of x being larger than 1 :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: bool """ y = np.var(x) return y &gt; np.sqrt(y) 65、tsfresh.feature_extraction.feature_calculators.linear_trend_timewise(x, param) 第三部分 特征参数设置第四部分 特征选择和过滤参考文献及资料1、]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[外网环境访问内网（NAT）Kafka集群介绍]]></title>
    <url>%2F2019%2F09%2F07%2F2019-09-07-%E5%A4%96%E7%BD%91%E7%8E%AF%E5%A2%83%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%EF%BC%88NAT%EF%BC%89Kafka%E9%9B%86%E7%BE%A4%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Kafka几个配置参数介绍 第二部分 外网环境访问内网（NAT）Kafka集群配置 第三部分 总结 参考文献及资料 背景其实这是一个踩坑笔记。首先介绍踩坑背景。生产环境我们有两个网络区域，记为网络区域A（内网）、网络区域B（外网），其中为了外网环境能访问内网环境，内网对内部IP实施了IP映射（NAT），将内网IP映射为外部IP。Kafka版本为：kafka_2.11-0.10.2.0。IP清单及网络数据流图如下： hostname 内网ip 外网Ip kafka.node1 192.168.1.1 10.0.0.1 kafka.node2 192.168.1.2 10.0.0.2 kafka.node3 192.168.1.3 10.0.0.3 整个架构图为： 外网网络区域的客户端开始使用NAT地址（10.0.0.1-3）地址访问内部kafka，发现无法生产和消费kafka数据（telnet netip 9092是通的），会报解析服务器hostname失败的错误。而内部网络的客户端使用内网地址（192.168.1.1-3）是可以正常生产和消费kafka数据。 原因：advertised.listeners配置的是内网实地址，这个地址注册到Zookeeper中，当消费者和生产者访问时，Zookeeper将该地址提供给消费者和生产者。由于是内网地址，外网根本看不到这个地址（路由寻址）。所以无法获取元数据信息，通信异常。 第一部分 Kafka几个配置参数介绍首先要了解一下几个配置： host.name已弃用。 仅当listeners属性未配置时被使用，已用listeners属性代替。表示broker的hostname。 advertised.host.name已弃用。仅当advertised.listeners或者listeners属性未配置时被使用。官网建议使用advertised.listeners。该配置的意思是注册到zookeeper上的broker的hostname或ip。是提供给客户端与kafka通信使用的。如果没有设置则使用host.name。 listeners监听列表，broker对外提供服务时绑定的IP和端口。多个以逗号隔开，如果监听器名称是一个安全的协议， listener.security.protocol.map也必须设置。主机名称设置0.0.0.0绑定所有的接口，主机名称为空则绑定默认的接口。如： 1listeners = PLAINTEXT://myhost:9092,SSL://:9091 CLIENT://0.0.0.0:9092,REPLICATION://localhost:9093 如果未指定该配置，则使用java.net.InetAddress.getCanonicalHostName()函数的的返回值。 advertised.listeners客户端使用。发布至zookeeper的监听，broker会上送此地址到zookeeper，zookeeper会将此地址提供给消费者和生产者，消费者和生产者根据此地址获取消息。如果和上面的listeners不同则以此为准，在IaaS环境，此配置项可能和 broker绑定的接口主机名称不同，如果此配置项没有配置则以上面的listeners为准。 第二部分 外网环境访问内网（NAT）Kafka集群配置2.1 配置hosts方式 Kafka集群节点配置 每一台Kafka节点的hosts节点配置内部地址映射： 192.168.1.1 kafka.node1192.168.1.2 kafka.node2192.168.1.3 kafka.node3 Kafka中的配置文件（config/server.properties配置文件） advertised.listeners=PLAINTEXT://kafka.node1:9092 advertised.listeners=PLAINTEXT://kafka.node2:9092 advertised.listeners=PLAINTEXT://kafka.node3:9092 客户端节点配置 客户端的hosts文件也需要配置外部地址映射： 10.0.0.1 kafka.node110.0.0.2 kafka.node210.0.0.3 kafka.node3 应用程序使用 bootstrap.servers: [‘kafka.node1:9092’,’kafka.node2:9092’,’kafka.node3:9092’] 配置完成后，重启Kafka集群，重新使用客户端链接，测试客户端可以正常向Topic生产和消费数据。 2.2 内外部流量分离通常对于外部网络访问内网安全区域，架构使用安全套接字层 (SSL) 来保护外部客户端与 Kafka 之间的流量。而使用明文进行内部网络的broker间的通信。当 Kafka 侦听器绑定到用于内部和外部通信的网络接口时，配置侦听器就非常简单了。但在许多情况下，例如在云原生环境上部署时，集群中 Kafka broker 的外部通告地址将与 Kafka 使用的内部网络接口不同。在此情况下，可以 server.properties 中的参数 advertised.listeners 进行如下配置： 1234567891011# Configure protocol maplistener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:SSL# Use plaintext for inter-broker communicationinter.broker.listener.name=INTERNAL# Specify that Kafka listeners should bind to all local interfaceslisteners=INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093# Separately, specify externally visible addressadvertised.listeners=INTERNAL://kafkabroker-n.mydomain.com:9092,EXTERNAL://kafkabroker-n.mydomain.com:9093 内部使用9092端口，而外部网络使用9093端口。 第三部分 总结Kafka集群在内外网网络环境下，需要关注地址映射。使用hosts 本地DNS进行主机名和内外地址的映射。至此爬出该坑。 参考文献及资料1、kafka - advertised.listeners和listeners，链接：https://www.cnblogs.com/fxjwind/p/6225909.html 2、Kafka从上手到实践-Kafka集群：Kafka Listeners，链接：http://www.devtalking.com/articles/kafka-practice-16/ 3、使用 Cloud Dataflow 处理来自 Kafka 的外部托管消息 链接：https://cloud.google.com/solutions/processing-messages-from-kafka-hosted-outside-gcp?hl=zh-cn 4、Kafka Listeners - Explained 链接：https://rmoff.net/2018/08/02/kafka-listeners-explained/]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[翻译]基于深度学习对系统日志进行异常检测和诊断]]></title>
    <url>%2F2019%2F08%2F27%2F2021-09-20-%5B%E7%BF%BB%E8%AF%91%5D%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AF%B9%E7%B3%BB%E7%BB%9F%E6%97%A5%E5%BF%97%E8%BF%9B%E8%A1%8C%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E5%92%8C%E8%AF%8A%E6%96%AD%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/ARPOSPF/article/details/103236241 https://zhuanlan.zhihu.com/p/76567213 摘要异常检测是构建安全可靠系统的关键步骤。系统日志的主要目的是记录系统状态和重大事件在不同的临界点来帮助调试系统故障和执行根源分析。这样的日志数据在几乎所有的计算机系统中都是通用的。日志数据是了解系统状态和性能问题的重要和有价值的资源;因此，各种系统日志自然是在线监视和异常检测的良好信息来源。我们提出了一种利用长短时记忆(LSTM)的深度神经网络模型DeepLog，将系统日志建模为自然语言序列。这使DeepLog可以自动地从正常执行中学习日志模式并在正常执行情况下日志模式偏离日志数据训练的模型时检测异常。此外,我们演示了如何以在线方式增量更新DeepLog模型，以便其可以随着时间的推移适应新的日志模式。此外，Deeplog从底层系统构建工作流，以便一旦检测到异常，用户就可以诊断出检测到的异常并有效地进行根本原因分析。对于大型日志数据进行的广泛实验评估表明，Deeplog的性能优于基于传统数据挖掘方法的其他现有的日志异常检测方法。 关键词异常检测；深度学习；日志数据分析 1 引言异常检测是构建安全可靠的计算系统的一个重要任务。随着系统和应用程序比以往任何时候都变得越来越复杂，它们会遇到更多的错误和漏洞，攻击者可能利用这些错误和漏洞来发起攻击。这些攻击也越来越复杂。因此，异常检测变得更具挑战性，并且许多基于标准数据挖掘的传统方法不再有效。 系统日志在各个关键点记录系统状态和重大事件，以帮助调试性能问题和故障，并进行根本原因分析。这样的日志数据几乎在所有计算机系统中普遍存在，并且是了解系统状态的宝贵资源。此外，由于系统日志记录了活跃运行过程中发生的重要事件，因此它们是用于在线监视和异常检测的极佳信息源。 利用系统日志数据进行异常检测的现有方法可以大致分为三类：基于PCA的日志消息计数器方法，基于不变量挖掘的方法来捕获不同日志键之间的共现模式和基于工作流的方法以识别程序逻辑流程中的异常执行。尽管它们在某些情况下是成功的，但它们都不能作为一种有效的通用异常检测方法，即能够以在线的方式防御各种攻击。 这项工作提出了DeepLog，这是一种数据驱动的异常检测方法，可以利用大量系统日志。DeepLog设计背后的主要灵感来自自然语言处理：我们将日志条目视为遵循某些模式和语法规则的序列元素。确实，系统日志是由遵循严格的逻辑和控制流程集的程序生成的，并且非常类似于自然语言（尽管在词汇方面结构更严格，受约束）。为此，DeepLog是一个深层神经网络，它使用长短期记忆（LSTM）对日志条码的序列进行建模。这使DeepLog可以从正常执行中自动学习日志模式的模型，并将与正常系统执行的偏差标记为异常。此外，由于它是一种学习驱动的方法，因此，可以增量地更新DeepLog模型，以便它可以适应随着时间而出现的新日志模式。 挑战：日志数据是非结构化的，并且它们的格式和语言在系统之间可能会显著不同。即使知道发生了错误，使用非结构化日志来诊断问题也已经很困难。从大量日志数据中进行在线异常检测更具挑战性。一些现有的方法使用基于规则的方法来解决此问题，这需要特定的领域知识。例如，使用“IP地址”之类的功能来解析日志。但是，这对于一般用途的异常检测不起作用，在这种情况下，几乎不可能先验地了解不同类型的日志中有哪些有趣的功能（并防范不同类型的攻击）。 为了及时发现异常，必须及时进行检测，以便用户可以干预正在进行的攻击或系统性能问题[10]。 决策将以流方式进行。其结果是需要对整个日志数据进行多次传递的offine方法不适用于我们的设定[22，39]。我们还希望能够检测未知类型的异常，而不是针对特定类型的异常。 因此，以前的工作[44]使用正常和异常（对于特定类型的异常）日志数据条目来训练二分类器以进行异常检测在这种情况下是没有用的。 并发是另一个挑战。 显然，日志中日志消息的顺序为诊断和分析提供了重要信息（例如，确定程序的执行路径）。但是，在许多系统日志中，日志消息是由几个不同的线程或同时运行的任务产生的。 这种并发性使得难以应用基于工作流的异常检测方法[42]，该方法将针对单个任务的工作流模型用作生成模型来与一系列日志消息进行匹配。 最后，每个日志消息都包含丰富的信息，例如日志键和一个或多个度量值，以及其时间戳。整合和利用这些不同信息的整体方法将更加有效。大多数现有方法[22、32、39、41、42、44]仅分析日志消息的一个特定部分（例如，日志key），这限制了它们可以检测到的异常的类型。 我们的贡献。循环神经网络（RNN）是一个人工神经网络，它使用循环将最后一个状态的输出转发到当前输入，从而跟踪历史以进行预测。长短期记忆（LSTM）网络[13、18、27]是RNN的一个实例，能够记住序列的长期依赖性。LSTM已证明在各种任务中都取得了成功，例如机器翻译[35]，情感分析[8]和医学自我诊断[20]。 受到观察的启发，系统日志中的条目是由执行结构化源代码（因此可以视为结构化语言）产生的一系列事件，因此，我们使用LSTM神经网络设计DeepLog框架，以通过系统日志在线检测异常。 DeepLog不仅在日志条目中使用日志键，还使用度量值进行异常检测，因此，它能够捕获不同类型的异常。 DeepLog仅取决于小的训练数据集，该数据集由一系列“正常日志条目”组成。 在训练阶段之后，DeepLog可以识别正常的日志序列，并可以以流方式用于对传入日志条目进行在线异常检测。 直观上，DeepLog隐式地从与常规系统执行路径相对应的训练数据中捕获日志条目之间潜在的非线性和高维依赖性。为帮助用户在发现异常后诊断问题，DeepLog还在训练阶段从日志条目构建工作流模型。DeepLog将并发任务或线程产生的日志条目分为不同的顺序，以便可以为每个单独的任务构建工作流模型。 我们的评估表明，在以前的工作中探索的大型HDFS日志数据集[22，39]上，仅对很小一部分（少于1％）的日志条目进行了训练，这些日志条目对应于正常的系统执行，DeepLog在其余99％的日志条目上可以达到几乎100％的检测精度。来自大型OpenStack日志的结果有类似的趋势。此外，DeepLog还提供了通过合并实时用户反馈来在检测阶段逐步更新其权重的功能。更具体地说，如果正常日志条目被错误地分类为异常，DeepLog提供了一种用于用户反馈的机制。 然后，DeepLog可以使用此类反馈随着时间的推移在线动态调整其权重，以使其适应新的系统执行（因此为新的日志）模式。 2 预备知识2.1 日志解析我们首先将非结构化的、无格式文本的日志条目解析为结构化的表示，这样我们就可以基于结构化的数据学习一个序列模型。例如文献中前期工作[9, 22, 39, 42, 45]，一种有效的方法是从每个日志条目中提取一个“日志键”(也称为“消息类型”)。日志条目$e$的日志键即为源码中打印语句中的字符串常量$k$，源码在执行后打印出$e$​。例如，对于日志条目e=”Took 10 seconds to build instance.”的日志键为k =Took * seconds to build instance，即打印语句printf(“Took %f seconds to build instance.)”, t)的字符串常量。注意到参数(s)在日志键中抽象为信号(s)。这些指标值反应了基础系统的状态和性能状态。某些参数的值可以作为特定的执行序列的标识符。例如HDFS日志中的block_id，OpenStack日志中的instance_id。这些标识符可以对日志条目进行聚类，也可以对多进程产生的日志条目进行拆解，以分离出单线程顺序序列[22, 39,42, 45]。目前最新的日志解析方法代码有Spell [9],这是一个无监督的流失解析器，基于LCS（最长公共子序列）原理在线解析输入的日志条目。 前期在日志分析上的工作[22, 39, 42, 44]，已经丢弃了日志条目中时间字段和（或）参数值，仅适用日志键来检测异常。DeepLog会存储每个日志条目e的参数值，以及日志条目e和前项日志条目的时间间隔，表示成一个向量$v_{e}$。这个向量除了日志键，也被DeepLog适用。Table1中给了一个案例，该案例展示了OpenStack多伦执行删除虚拟机（VM）的任务产生的日志条目序列的解析结果。 2.2 DeepLog结构和概述 DeepLog的系统结构如Figure1图展示，主要分为三个组件：日志键异常检测模型，参数值异常检测模型，诊断和检测异常的工作流模型。 2.2.1 训练阶段DeepLog的训练数据来源是正常系统执行路径的日志条目。每条日志条目被江西成日志键和参数值向量。日志键序列从训练日志文件中解析获取，DeepLog用其训练日志键异常检测模型，并且创建系统执行工作流模型用于诊断目的。对于每个不同的日志键k，DeepLog还会训练和维护模型用来检测系统的性能异常，这些度量值通过参数值向量k训练获得。 2.2.2 检测阶段一个新的输入日志条目被解析成日志键和参数值向量。DeepLog首先使用日志键异常检测模型检测输入日志键是否正常。如果正常，DeepLog进一步使用该日志键的参数值异常检测模型，检查参数值向量。如果新的日志条目的日志键或参数值向量被预测为异常，则其标记为异常。最后，如果被标记为异常，DeepLog的工作流模型为用户提供语义信息诊断该异常。 2.3 威胁模型3 异常检测3.1 执行路径异常3.2 参数值和性能异常3.3 异常检测模型的在线更新4 多任务执行的工作流构建5 评估DeepLog是使用Keras [6]和TensorFlow [2]作为后端实现的。 在本节中，我们将对DeepLog的每个组件和整体性能进行评估，以展示其在从大型系统日志数据中查找异常的有效性。 6 相关工作系统事件日志主要用于记录重要事件以简化调试，它具有丰富的信息，并且实际上存在于每个计算机系统上，从而使它们成为跟踪和调查系统状态的宝贵资源。但是，由于系统日志主要由各种自由格式的文本组成，因此分析具有挑战性。 已经针对不同的系统设计了许多日志挖掘工具。 许多使用基于规则的方法[7、15、28、29、31、32、40、41]，尽管准确，但仅限于特定的应用场景，并且还需要领域专业知识。 例如，Beehive [41]通过对数据特定特征进行无监督的聚类，然后手动标记异常值，从日志中识别出潜在的安全威胁。 Oprea [28]使用信念传播从DNS日志中检测早期企业感染。 PerfAugur [32]专门用于通过使用谓词组合等专门功能来挖掘服务日志来发现性能问题。 DeepLog是一种通用方法，不依赖于任何特定于域的知识。 使用系统日志进行异常检测的其他通用方法通常采用两步过程。首先，日志解析器[9、14、16、23、36、37]用于将日志条目解析为结构化形式，通常仅包含“日志键”（或“消息类型”）。除了用于分隔和分组日志条目的标识符外，参数值和时间戳都将被丢弃。然后，对日志键序列执行异常检测。一种典型的方法是通过计数唯一的日志键或使用更复杂的方法（例如TF-IDF）为每个会话或时间窗口生成数字矢量。然后，由这些向量组成的矩阵将适合基于矩阵的无监督异常检测方法，例如主成分分析（PCA）[38，39]和不变挖掘（IM）[22]。构造这样的矩阵通常是一个精妙的过程，并且这些方法不能提供日志条目级别的异常检测（相反，它们只能在会话级别运行）。我们向读者推荐[17]，以对这些方法进行概述和比较。 有监督方法[17，44]使用正常和异常矢量来训练可检测未来异常的二进制分类器。这种方法的缺点是，可能无法检测到训练数据中未发现的未知异常。此外，很难获得异常数据进行训练。我们在评估中显示，仅使用一小部分正常数据进行训练，DeepLog可以实现具有更好性能的在线异常检测。此外，DeepLog还使用时间戳和参数值进行异常检测，这是先前工作中所缺少的。 人们已经大量使用从精细日志文件中提取的日志键来研究工作流的构建[4，11，21，42]。已经表明，工作流在异常检测方面提供了有限的优势[11，42]。相反，工作流的主要用途是辅助系统诊断[4，21]。但是，所有过去的工作都假定要建模的日志文件仅包含一个任务的重复执行。在本文中，我们提出了自动将不同任务与日志文件分离的方法，以便为不同任务构建工作流模型。 除了工作流之外，其他使用系统日志执行异常诊断的系统包括DISTALYZER [26]，它通过将有问题的日志与正常日志进行比较来诊断系统性能问题； LogCluster [19]，它聚类并组织历史日志以帮助将来的问题识别；以及Stitch [45]从系统日志中提取不同级别的标识符，并构建一个Web界面供用户直观地监视每个进程的进度会话并查找性能问题。请注意，一旦检测到异常，它们便用于诊断目的，并且本身不能用于异常检测。 7 结论本文介绍了DeepLog，这是一种使用基于深度神经网络的方法进行在线日志异常检测和诊断的通用框架。 DeepLog学习并编码整个日志消息，包括时间戳，日志密钥和参数值。它在每个日志条目级别而不是在每个会话级别执行异常检测，因为许多以前的方法都受到限制。DeepLog可以从日志文件中分离出不同的任务，并使用深度学习（LSTM）和经典挖掘（密度聚类）方法为每个任务构建工作流模型。这样可以进行有效的异常诊断。通过整合用户反馈，DeepLog支持对其LSTM模型的在线更新/训练，因此能够整合并适应新的执行模式。对大型系统日志的广泛评估清楚地证明了DeepLog与以前的方法相比的优越性。 未来的工作包括但不限于将其他类型的RNN（递归神经网络）合并到DeepLog中以测试其效率，并集成来自不同应用程序和系统的日志数据以执行更全面的系统诊断（例如，MySQL数据库的故障可能是由磁盘故障引起，正如一个独立的系统日志中所示）。 8 致谢9 文献[1] VAST Challenge 2011. 2011. MC2 - Computer Networking Operations.(2011). hp://hcil2.cs.umd.edu/newvarepository/VAST%20Challenge%202011/challenges/MC2%20-%20Computer%20Networking%20Operations/ [Online; accessed08-May-2017].[2] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, JereyDean, Mahieu Devin, Sanjay Ghemawat, Georey Irving, Michael Isard, andothers. 2016. TensorFlow: A system for large-scale machine learning. In Proc.USENIX Symposium on Operating Systems Design and Implementation (OSDI).264–285.[3] Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. Aneural probabilistic language model. Journal of machine learning research 3, Feb(2003), 1137–1155.[4] Ivan Beschastnikh, Yuriy Brun, Michael D Ernst, and Arvind Krishnamurthy.2014.Inferring models of concurrent systems from logs of their behavior withCSight. In Proc. International Conference on Soware Engineering (ICSE ). 468–479.[5] Andrea Biau, Adam Belay, Ali Mashtizadeh, David Mazi`eres, and Dan Boneh.Hacking blind. In Security and Privacy (SP), 2014 IEEE Symposium on. IEEE,227–242.[6] Franois Chollet. 2015. keras. hps://github.com/fchollet/keras. (2015). [Online;accessed 08-May-2017].[7] Marcello Cinque, Domenico Cotroneo, and Antonio Pecchia. 2013. Event logsfor the analysis of soware failures: A rule-based approach. IEEE Transactionson Soware Engineering (TSE) (2013), 806–821. [8] Andrew M Dai and oc V Le. 2015. Semi-supervised sequence learning. In Proc.Neural Information Processing Systems Conference (NIPS). 3079–3087.[9] Min Du and Feifei Li. 2016. Spell: Streaming Parsing of System Event Logs. InProc. IEEE International Conference on Data Mining (ICDM). 859–864.[10] Min Du and Feifei Li. 2017. ATOM: Ecient Tracking, Monitoring, and Orchestrationof Cloud Resources. IEEE Transactions on Parallel and Distributed Systems(2017).[11] Qiang Fu, Jian-Guang Lou, Yi Wang, and Jiang Li. 2009. Execution anomalydetection in distributed systems through unstructured log analysis. In Proc. IEEEInternational Conference on Data Mining (ICDM). 149–158.[12] Yoav Goldberg. 2016. A primer on neural network models for natural languageprocessing. Journal of Articial Intelligence Research 57 (2016), 345–420.[13] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MITPress. hp://www.deeplearningbook.org.[14] Hossein Hamooni, Biplob Debnath, Jianwu Xu, Hui Zhang, Guofei Jiang, andAbdullah Mueen. 2016. LogMine: Fast Paern Recognition for Log Analytics. InProc. Conference on Information and Knowledge Management (CIKM). 1573–1582.[15] Stephen E Hansen and E Todd Atkins. 1993. Automated System Monitoringand Notication with Swatch.. In Proc. Large Installation System AdministrationConference (LISA). 145–152.[16] Pinjia He, Jieming Zhu, Shilin He, Jian Li, and Michael R Lyu. 2016. An evaluationstudy on log parsing and its use in log mining. In Proc. International Conferenceon Dependable Systems and Networks (DSN). 654–661.[17] Shilin He, Jieming Zhu, Pinjia He, and Michael R Lyu. 2016. Experience Report:System Log Analysis for Anomaly Detection. In Proc. International Symposiumon Soware Reliability Engineering (ISSRE). 207–218.[18] Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neuralcomputation (1997), 1735–1780.[19] Qingwei Lin, Hongyu Zhang, Jian-Guang Lou, Yu Zhang, and Xuewei Chen. Log clustering based problem identication for online service systems. InProc. International Conference on Soware Engineering (ICSE ). 102–111.[20] Chaochun Liu, Huan Sun, Nan Du, Shulong Tan, Hongliang Fei, Wei Fan, TaoYang, Hao Wu, Yaliang Li, and Chenwei Zhang. 2016. Augmented LSTM Frameworkto Construct Medical Self-diagnosis Android. In Proc. IEEE InternationalConference on Data Mining (ICDM). 251–260.[21] Jian-Guang Lou, Qiang Fu, Shengqi Yang, Jiang Li, and Bin Wu. 2010. Miningprogram workow from interleaved traces. In Proc. ACM SIGKDD InternationalConference on Knowledge Discovery and Data Mining (SIGKDD).[22] Jian-Guang Lou, Qiang Fu, Shengqi Yang, Ye Xu, and Jiang Li. 2010. MiningInvariants from Console Logs for System Problem Detection.. In Proc. USENIXAnnual Technical Conference (ATC). 231–244.[23] Adetokunbo AO Makanju, A Nur Zincir-Heywood, and Evangelos E Milios. Clustering event logs using iterative partitioning. In Proc. ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining (SIGKDD).1255–1264.[24] Christopher D Manning and Hinrich Sch¨utze. 1999. Foundations of statisticalnatural language processing. MIT Press.[25] Tomas Mikolov, Martin Kara´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recurrent neural network based language model.. In Interspeech,Vol. 2. 3.[26] Karthik Nagaraj, Charles Killian, and Jennifer Neville. 2012. Structured comparativeanalysis of systems logs to diagnose performance problems. In Proc. USENIXSymposium on Networked Systems Design and Implementation (NSDI). 26–26.[27] Christopher Olah. 2015. Understanding LSTM Networks. (2015). hp://colah.github.io/posts/2015-08-Understanding-LSTMs [Online; accessed 16-May-2017].[28] Alina Oprea, Zhou Li, Ting-Fang Yen, Sang H Chin, and Sumayah Alrwais. 2015.Detection of early-stage enterprise infection by mining large-scale log data. In Proc. International Conference on Dependable Systems and Networks (DSN). 45–56.[29] James E Prewe. 2003. Analyzing cluster log les using Logsurfer. In Proc. AnnualConference on Linux Clusters.[30] Robert Ricci, Eric Eide, and e CloudLab Team. 2014. Introducing CloudLab:Scientic Infrastructure for Advancing Cloud Architectures and Applications.USENIX ;login: 39, 6 (Dec. 2014). hps://www.usenix.org/publications/login/dec14/ricci[31] John P Rouillard. 2004. Real-time Log File Analysis Using the Simple EventCorrelator (SEC).. In Proc. Large Installation System Administration Conference(LISA). 133–150.[32] Sudip Roy, Arnd Christian K¨onig, Igor Dvorkin, and Manish Kumar. 2015. Perfaugur:Robust diagnostics for performance anomalies in cloud services. In Proc.IEEE International Conference on Data Engineering (ICDE). IEEE, 1167–1178.[33] Elastic Stack. 2017. e Open Source Elastic Stack. (2017). hps://www.elastic.co/products [Online; accessed 16-May-2017].[34] Martin Sundermeyer, Ralf Schl¨uter, and Hermann Ney. 2012. LSTM NeuralNetworks for Language Modeling.. In Interspeech. 194–197.[35] Ilya Sutskever, Oriol Vinyals, andoc V Le. 2014. Sequence to sequence learningwith neural networks. In Proc. Neural Information Processing Systems Conference(NIPS). 3104–3112.[36] Liang Tang and Tao Li. 2010. LogTree: A framework for generating systemevents from raw textual logs. In Proc. IEEE International Conference on DataMining (ICDM). 491–500.[37] Liang Tang, Tao Li, and Chang-Shing Perng. 2011. LogSig: Generating systemevents from raw textual logs. In Proc. Conference on Information and KnowledgeManagement (CIKM). 785–794.[38] Wei Xu, Ling Huang, Armando Fox, David Paerson, and Michael Jordan. 2009.Online system problem detection by mining paerns of console logs. In Proc.IEEE International Conference on Data Mining (ICDM). 588–597.[39] Wei Xu, Ling Huang, Armando Fox, David Paerson, and Michael I Jordan. 2009.Detecting large-scale system problems by mining console logs. In Proc. ACMSymposium on Operating Systems Principles (SOSP). 117–132.[40] Kenji Yamanishi and Yuko Maruyama. 2015. Dynamic syslog mining for networkfailure monitoring. In Proc. ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining (SIGKDD). 499–508.[41] Ting-Fang Yen, Alina Oprea, Kaan Onarlioglu, Todd Leetham, William Robertson,Ari Juels, and Engin Kirda. 2013. Beehive: Large-scale log analysis for detectingsuspicious activity in enterprise networks. In Proc. International Conference onDependable Systems and Networks (ACSAC). 199–208.[42] Xiao Yu, Pallavi Joshi, Jianwu Xu, Guoliang Jin, Hui Zhang, and Guofei Jiang. 2016.CloudSeer: Workow Monitoring of Cloud Infrastructures via Interleaved Logs.In Proc. ACM International Conference on Architectural Support for ProgrammingLanguages and Operating Systems (ASPLOS). 489–502.[43] Ding Yuan, Haohui Mai, Weiwei Xiong, Lin Tan, Yuanyuan Zhou, and ShankarPasupathy. 2010. SherLog: error diagnosis by connecting clues from run-timelogs. In ACM SIGARCH computer architecture news. ACM, 143–154.[44] Ke Zhang, Jianwu Xu, Martin Renqiang Min, Guofei Jiang, Konstantinos Pelechrinis,and Hui Zhang. 2016. Automated IT system failure prediction: A deep learningapproach. In Proc. IEEE International Conference on Big Data (IEEE BigData).1291–1300.[45] Xu Zhao, Kirk Rodrigues, Yu Luo, Ding Yuan, and Michael Stumm. 2016. Nonintrusiveperformance proling for entire soware stacks based on the owreconstruction principle. In Proc. USENIX Symposium on Operating Systems Designand Implementation (OSDI). 603–618.]]></content>
      <categories>
        <category>deeplog</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于Kerberos认证的Hadoop集群实践]]></title>
    <url>%2F2019%2F08%2F27%2F2021-09-01-%E5%9F%BA%E4%BA%8EKerberos%E8%AE%A4%E8%AF%81%E7%9A%84Hadoop%E9%9B%86%E7%BE%A4%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[目录 术语说明 背景 第一部分 Kerberos认证原理 第二部分 第三部分 第五部分 总结 参考文献及资料 背景对于Hadoop生产集群，通常需要开启安全模式，拦截来自集群外部或者内部的恶意攻击和破坏，保护集群数据和作业运行的安全。 通常企业级集群使用的安全组件为Kerberos。下面是维基百科的介绍： Kerberos是一种计算机网络认证协议，它允许某实体在非安全网络环境下通信，向另一个实体以一种安全的方式证明自己的身份。它的设计主要针对客户-服务器模型，并提供了一系列交互认证——用户和服务器都能验证对方的身份。Kerberos协议可以保护网络实体免受窃听和重复攻击。 这个协议以希腊神话中的人物Kerberos（或者Cerberus）命名，他在希腊神话中是Hades的一条凶猛的三头保卫神犬。在系统设计上Kerberos采用C/S架构，基于DEC加密技术，支持客户端和服务器端双向认证。 为什么Hadoop集群不基于Linux实现身份认证？ Hadoop是一个分布式系统，对象是集群级别的多节点。Linux认证管理限制于操作系统层面的单机。所以Hadoop需要基于集群网络的身份认证系统。 为什么选择Kerberos，而不是SSL？ Kerberos的性能优于SSL，而在Kerberos中管理用户要简单得多。要删除用户，我们只是从Kerberos中删除它，而撤销SSL证书是一件复杂的事情。 但是开源Hadoop中没有内置身份认证系统，所以对于生产集群，用户要么自研身份认证组件，要么使用专用的身份认证系统与Hadoop集群集成。 第一部分 Kerberos认证原理介绍第三部分 常见故障排除第四部分 总结1、 https://www.jianshu.com/p/aefd553050e6 参考文献及资料1、什么是Kerberos？，链接：https://steveloughran.gitbooks.io/kerberos_and_hadoop/content/sections/what_is_kerberos.html 2、维基百科：Kerberos，链接：https://zh.wikipedia.org/wiki/Kerberos 3、Kerberos Concepts - Principals, Keytabs and Delegation Tokens 链接：https://www.cloudera.com/documentation/enterprise/5-8-x/topics/cm_sg_principal_keytab.html]]></content>
      <categories>
        <category>Kerberos</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[DeepLog论文解读和实现]]></title>
    <url>%2F2019%2F08%2F27%2F2021-09-19-DeepLog%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E5%92%8C%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[目录 术语说明 背景 第一部分 机器学习日志分析的挑战 第二部分 第三部分 第五部分 总结 参考文献及资料 背景当线上生产系统出现问题时，运维人员（SRE）通常会第一时间查看分析系统的历史日志，然后对问题原因进行定位。这是一个繁杂且耗时的工作，所以自然会想到能否通过机器学习实现对日志的自动分析。 机器学习分析日志主要优点： 快速。缩短定位异常原因的时间，远大于手动效率；特别是面对云基础设施海量设备，自动化和智能化意义重大； 主动学习。会主动的发现异常模式，这些模式通常是人为分析的盲点； 提前发现隐患。可提前检测异常发生前兆日志模式，提前预警； 第一部分 机器学习日志分析的挑战1.1 监督vs无监督vs半监督机器学习按照是否需要标签数据，通常分为监督和无监督两种学习模式。对于海量日志数据，如果靠人工进行梳理、打标签，这个工作量是庞大的。另外信息系统的架构和代码也在不停演进更新，原来的标签会逐渐失效，即历史标签保鲜性。人工标签是长期持续的工作，成本较大。所以监督模式更适合用于通用软件产品的日志分析，例如Mysql、Elasticsearch、Redis等开源组件。通常线上产品升级周期较长，所以历史标签的时效性较长。 工业界的实践更建议采用无监督方式。特别是迭代较快的自研应用系统。当然可以适当配合小规模的人工标签数据，采用半监督方式进行落地。 1.2 传统机器学习vs深度学习近年来学界和工业界逐步有相关研究和落地案例。按照是否是神经类算法，分为传统机器学习和深度学习两个方向。 传统机器学习，对于日志数据的单词或语句模式进行概率统计，提取和人为构造统计特征。然后使用相关分类算法检测异常模式。但是通常传统机器学习分类算法都是监督模式，例如随机森林、Xgboost、SVM等，工业落地需要持续投入人力，成本较高。 深度学习方向，近年来最具影响力的论文是Min Du、李飞飞发表的论文：《DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning》。深度学习的特点就是模型需要看大量的日志数据，模型训练收敛需要较长的时间，整个训练需要提供海量日志数据的输入。工业界落地需要企业具备较大规模的分布式机器学习平台用来支撑模型参数的训练。 1.3 非结构化&amp;噪声信息系统的日志数据通常是非结构化的文本信息。这些日志都是源码中对系统运行状态进行文本记录，通常日志文本使用日志处理包进行处理，会具备一定的结构的。 例如下面Java程序的输出日志，每条日志由日志产生时间、日志级别（INFO）、业务类别、日志内容、设计Java程序的Class类名和源码函数。 122021-05-14 22:20:12,083 | INFO | ResourceManager Event Processor | container_e67_1620959157525_31531_01_000001 Container Transitioned from ACQUIRED to RUNNING | RMContainerImpl.java:420 2021-05-14 22:32:52,696 | INFO | ResourceManager Event Processor | Application attempt appattempt_1620959157525_31531_000001 released container container_e67_1620959157525_31531_01_000001 on node: host: pdccjitdwdbs050:26009 #containers=22 available=&lt;memory:116224, vCores:30&gt; used=&lt;memory:76288, vCores:26&gt; with event: KILL | CapacityScheduler.java:1512 其中非结构化文本内容主要是指日志内容。既然是文本类数据，自然会想到使用NLP技术进行处理。 通常日志文件由多个进程写入，这就涉及日志的分类。对于规范类日志数据可以在日志内容中标识进程名。而对于非规范的日志，通常使用算法来完成，例如最长公共子串（Longest common substring）和最长公共子序列。 日志分析中正常日志远远大于异常日志，很容易被一些噪声日志所影响。 1.4 性能容量数据&amp;日志&amp;Aiops近些年使用人工智能辅助运维的应用落地较多，通常称为AIOps (Artificial intelligence for IT operations)。企业级Aiops平台建设中,日志分析系统应当属于子系统。 当信息系统发生或即将发生异常时候，性能指标数据通常也会出现相应异常。所以对于系统异常检测可以结合性能容量异常检测子系统，进行综合考虑。 第二部分 DeepLog实现原理分析油管视频地址：https://www.youtube.com/watch?v=At19CBGpbMI 第三部分 代码实现第四部分 总结参考文献及资料1、DeepLog: Anomaly detection and diagnosis from system logs through deep learning，链接：https://www.cs.utah.edu/~lifeifei/papers/deeplog.pdf]]></content>
      <categories>
        <category>Kerberos</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群kerberos互信]]></title>
    <url>%2F2019%2F08%2F27%2F2021-09-01-Hadoop%E9%9B%86%E7%BE%A4kerberos%E4%BA%92%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[目录 术语说明 背景 第一部分 Kerberos认证原理 第二部分 第三部分 第五部分 总结 参考文献及资料 背景第一部分 Kerberos认证原理第三部分 常见故障排除第四部分 总结1、 https://www.jianshu.com/p/aefd553050e6 参考文献及资料1、什么是Kerberos？，链接：https://steveloughran.gitbooks.io/kerberos_and_hadoop/content/sections/what_is_kerberos.html 2、维基百科：Kerberos，链接：https://zh.wikipedia.org/wiki/Kerberos 3、Kerberos Concepts - Principals, Keytabs and Delegation Tokens 链接：https://www.cloudera.com/documentation/enterprise/5-8-x/topics/cm_sg_principal_keytab.html]]></content>
      <categories>
        <category>Kerberos</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Helm简介和安装部署]]></title>
    <url>%2F2019%2F08%2F08%2F2019-08-08-Helm%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Minikube集群启动 第一部分 Kubernetes中StatefulSet介绍 第三部分 部署Zookeeper集群 第四部分 部署Kafka集群 第五部分 总结 参考文献及资料 背景Helm 是 Kubernetes 的软件包管理工具(Application deployment management for Kubernetes)。 通常我们在k8s集群中部署一个可以使用的应用，一般会涉及多个组件资源的共同协作。例如需要安装部署一个web应用，包括 Deployment 用于部署应用、Service 提供服务发现、Secret 配置 应用的用户名和密码，可能还需要 pv 和 pvc 来提供持久化服务。web应用的后台数据是存储在mysql里面的，所以需要 mysql启动就绪后才能启动 web前台。涉及的资源较多还有前后项编排等管理，如果只是通过kubectl管理，这是一个复杂辛苦的工作。 helm的出现就是解决上面的痛点。主要解决的问题有： 统一管理、配置和更新应用资源文件； 分发和服用应用模板； 将应用的一系列资源整体作为一个软件包管理； 第一部分 Helm的部署Helm是由helm CLI和Tiller组成，即典型的C/S应用。helm运行与客户端，提供命令行界面，而Tiller应用运行在Kubernetes内部。 helm 是一个命令行工具，用于本地开发及管理chart，chart仓库管理等； Tiller 是 Helm 的服务端。Tiller 负责接收 Helm 的请求，与 k8s 的 apiserver 交互，根据chart 来生成一个 release 并管理 release； chart Helm的打包格式叫做chart，所谓chart就是一系列文件, 它描述了一组相关的 k8s 集群资源； release 使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release； Repoistory Helm chart 的仓库，Helm 客户端通过 HTTP 协议来访问存储库中 chart 的索引文件和压缩包； 1.1 部署Helm CLI客户端helm客户端是一个单纯的可执行文件，我们从github上直接下载压缩包（由于墙的原因可能很慢）： 1root@deeplearning:/data/helm# wget https://get.helm.sh/helm-v2.14.3-linux-amd64.tar.gz 解压缩介质文件： 123456root@deeplearning:/data/helm# tar -zxvf helm-v2.14.3-linux-amd64.tar.gz linux-amd64/linux-amd64/helmlinux-amd64/README.mdlinux-amd64/LICENSElinux-amd64/tiller 部署： 1root@deeplearning:/data/helm# mv linux-amd64/helm /usr/local/bin 检查： 123root@deeplearning:/data/helm# helm versionClient: &amp;version.Version&#123;SemVer:"v2.14.3", GitCommit:"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085", GitTreeState:"clean"&#125;Error: could not find tiller 提示未连接到服务端tiller。下面我们部署服务端。 1.2 部署服务端Helm 的服务器端部分 Tiller 通常运行在 Kubernetes 集群内部。但是对于开发，它也可以在本地运行，并配置为与远程 Kubernetes 群集通信。 安装 tiller 到群集中最简单的方法就是运行 helm init。这将验证 helm 本地环境设置是否正确（并在必要时进行设置）。然后它会连接到 kubectl 默认连接的任何集群（kubectl config view）。一旦连接，它将安装 tiller 到 kube-system 命名空间中。 helm init 以后，可以运行 kubectl get pods --namespace kube-system 并看到 Tiller 正在运行。 你可以通过参数运行 helm init: --canary-image 参数安装金丝雀版本 --tiller-image 安装特定的镜像（版本） --kube-context 使用安装到特定群集 --tiller-namespace 用一个特定的命名空间 (namespace) 安装 --service-account 使用 Service Account 安装 RBAC enabled clusters --automount-service-account false 不适用 service account 安装 一旦安装了 Tiller，运行 helm version 会显示客户端和服务器版本。（如果它仅显示客户端版本， helm 则无法连接到服务器, 使用 kubectl 查看是否有任何 tiller Pod 正在运行。） 除非设置 --tiller-namespace 或 TILLER_NAMESPACE 参数，否则 Helm 将在命名空间 kube-system 中查找 Tiller 。 在缺省配置下， Helm 会利用 “gcr.io/kubernetes-helm/tiller“ 镜像在Kubernetes集群上安装配置 Tiller；并且利用 “https://kubernetes-charts.storage.googleapis.com“ 作为缺省的 stable repository 的地址。由于在国内可能无法访问 “gcr.io“, “storage.googleapis.com“ 等域名，阿里云容器服务为此提供了镜像站点。 首先创建服务。创建rbac-config.yaml文件，文件内容为： 123456789101112131415161718apiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tillerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: tiller namespace: kube-system 通过yaml文件创建服务： 123root@deeplearning:/data/helm# kubectl create -f rbac-config.yamlserviceaccount/tiller createdclusterrolebinding.rbac.authorization.k8s.io/tiller created 启Helm pod，即安装tiller： 1234567891011121314151617root@deeplearning:/data/helm#helm init -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v3.0.2 \ --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts \ --service-account tiller # 回显Creating /root/.helm Creating /root/.helm/repository Creating /root/.helm/repository/cache Creating /root/.helm/repository/local Creating /root/.helm/plugins Creating /root/.helm/starters Creating /root/.helm/cache/archive Creating /root/.helm/repository/repositories.yaml Adding stable repo with URL: https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /root/.helm.Warning: Tiller is already installed in the cluster.(Use --client-only to suppress this message, or --upgrade to upgrade Tiller to the current version.) 这时候我们再次执行： 123root@deeplearning:/data/helm# helm versionClient: &amp;version.Version&#123;SemVer:"v2.14.3", GitCommit:"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085", GitTreeState:"clean"&#125;Server: &amp;version.Version&#123;SemVer:"v2.10.0", GitCommit:"9ad53aac42165a5fadc6c87be0dea6b115f93090", GitTreeState:"clean"&#125; 这样Helm的服务端tiller就部署完毕。 1.3 Helm CLI 命令简要汇总12345678910111213141516171819202122232425// 安装一个 Charthelm install stable/mysql// 列出 Kubernetes 中已部署的 Charthelm list --all// helm repo 的操作helm repo updatehelm repo listhelm repo add dev https://example.com/dev-charts// 创建一个 Chart，会产生一个 Chart 所需的目录结构helm create deis-workflow// 安装自定义 charthelm inspect values stable/mysql # 列出一个 chart 的可配置项helm install -f config.yaml stable/mysql # 可以将修改的配置项写到文件中通过 -f 指定并替换helm install --set name: value stable/mysql # 也可以通过 --set 方式替换// 当新版本 chart 发布时，或者当你需要更改 release 配置时，helm 必须根据现在已有的 release 进行升级helm upgrade -f panda.yaml happy-panda stable/mariadb// 删除 releasehelm delete happy-panda 参考文献及材料1、Helm User Guide - Helm 用户指南 https://whmzsu.github.io/helm-doc-zh-cn/ 2、Kubernetes 包管理工具 Helm 简介 https://www.jianshu.com/p/d55e91e28f94 3、Helm介绍 https://zhaohuabing.com/2018/04/16/using-helm-to-deploy-to-kubernetes/]]></content>
      <categories>
        <category>Helm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[从Spark on Yarn到Python on Yarn]]></title>
    <url>%2F2019%2F08%2F01%2F2019-01-28-%E4%BB%8ESpark%20on%20Yarn%E5%88%B0Python%20on%20Yarn%2F</url>
    <content type="text"><![CDATA[目录 术语说明 背景 第一部分 Apache Spark运行模式介绍 第二部分 Spark on Yarn 第三部分 Pyspark Application原理 第四部分 Python on Yarn配置及运行 第五部分 总结 参考文献及资料 背景Apache Spark属于重要的大数据计算框架，另外spark还提供了Python的原生API和机器学习组件Spark Ml，使的可以通过Python编写机器学习任务由Spark运行。本篇文件从Spark运行模式开始讲起，重点介绍Spark on Yarn运行模式，最后重点介绍Python on Yarn（即Pyspark on Yarn）上运行原理和案例。 第一部分 Apache Spark运行模式目前 Apache Spark已知支持5种运行模式。按照节点资源数量可以分为单节点模式（2种）和集群模式（3种）。 单节点模式：本地模式、本地伪集群模式 集群模式：Standalone模式、Spark on Yarn模式、Spark on Mesos模式 原生云模式：在Kubernetes上运行。随着Docker容器和原生云技术的兴起，Spark开始支持在Kubernetes上运行。 对于Spark on Kubernetes可以参考官方文档：https://spark.apache.org/docs/latest/running-on-kubernetes.html。另外可以参考我的另外一篇技术总结：《在Minikube上运行Spark集群》。 1.1 本地模式（单节点模式）本地模式又称为Loacl[N]模式。该模式只需要在单节点上解压spark包即可运行，使用多个线程模拟Spark分布式计算，Master和Worker运行在同一个JVM虚拟机中。这里参数N代表可以使用（预申请）N个线程资源，每个线程拥有一个Core（默认值N=1）。 如果参数为：Loacl[*]，表明：Run Spark locally with as many worker threads as logical cores on your machine。即线程数和物理核数相同。 例如下面的启动命令： 1# ./spark-submit –class org.apache.spark.examples.JavaWordCount –master local[*] spark-examples_2.11-2.3.1.jar file:///opt/README.md 该模式不依懒于HDFS分布式文件系统。例如上面的命令使用的本地文件系统。 1.2 本地伪集群模式（单节点模式）该模式和Local[N]类似，不同的是，它会在单机启动多个进程来模拟集群下的分布式场景，而不像Local[N]这种多个线程在一个进程下共享资源。通常用来测试和验证应用程序逻辑上有没有问题，或者想使用Spark的计算框架而而受限于没有太多资源。 作业提交命令中使用local-cluster[x,y,z]参数模式：x代表要生成的executor数，y和z分别代表每个executor所拥有的core和memory数值。例如下面的命令作业申请了2个executor 进程，每个进程分配3个core和1G的内存，来运行应用程序。 1# ./spark-submit –master local-cluster[2, 3, 1024] 1.3 Standalone模式（集群模式）1.3.1 构架部署Standalone为spark自带资源管理系统（即经典的Master/Slaves架构模式）。该模式下集群由Master和Worker节点组成，程序通过与Master节点交互申请资源，Worker节点启动Executor运行。具体数据流图如下： 另外考虑到Master节点存在单点故障。Spark支持使用Zookeeper实现HA高可用（high avalible）。Zookeeper提供一种领导选举的机制，通过该机制可以保证集群中只有一个Master节点处于RecoveryState.Active状态，其他Master节点处于RecoveryState.Standby状态。 1.3.2 作业运行模式在该模式下，用户提交任务有两种方式：Standalone-client和Standalone-cluster。 1.5.1 Client模式执行流程： (1)客户端启动Driver进程。 (2)Driver向Master申请启动Application启动需要的资源。 (3)资源申请成功后，Driver将task发送到相应的Worker节点执行，并负责监控task运行情况。 (4)Worker将task执行结果返回到客户端的Driver进程。 Client模式适用于调试程序。Driver进程在客户端侧启动，如果生产采用这种模式，当业务量较大时，客户端需要启动大量Driver进程，会消耗大量系统资源，导致资源枯竭。 1.5.2 Cluster模式执行流程： (1)客户端会想Master节点申请启动Driver。 (2)Master受理客户端的请求，分配一个Work节点，启动Driver进程。 (3)Driver启动后，重新想Master节点申请运行资源，Master分配资源，并在相应的Worker节点上启动Executor进程。 (4)Driver发送task到相应的Worker节点运行，并负责监控task。 (5)Worker将task执行结果返回到Driver进程。 Driver运行有Master在集群Worker节点上随机分配，相当于在集群上负载资源。 两种方式最大的区别就是Driver进程运行的位置。Cluster模式相对于Client模式更适合于生成环境的部署。 1.4 Spark on Yarn（集群模式）目前大部分企业级Spark都是跑在已有的Hadoop集群（hadoop 2.0系统）中，均使用Yarn来作为Spark的Cluster Manager，为Spark提供资源管理服务，Spark自身完成任务调度和计算。这部分内容会在后文中细致介绍。 1.5 Spark on Mesos（集群模式）参考官方文档介绍：https://spark.apache.org/docs/latest/running-on-mesos.html 第二部分 Spark on Yarn我们知道MapReduce任务是运行在Yarn上的，同样Spark Application也可以运行在Yarn上。这种模式下，资源的管理、协调、执行和监控交给Yarn集群完成。 Yarn集群上可以运行：MapReduce任务、Spark Application、Hbase集群、Storm集群、Flink集群等等，还有我们后续重点介绍的Python on Yarn。 从节点功能上看，Yarn也采用类似Standalone模式的Master/Slave结构。资源框架中RM（ResourceManager）对应Master，NM（NodeManager）对应Slave。RM负责各个NM资源的统一管理和调度，NM节点负责启动和执行任务以及各任务间的资源隔离。 当集群中存在多种计算框架时，架构上选用Yarn统一管理资源要比Standalone更合适。类似Standalone模式，Spark on Yarn也有两种运行方式：Yarn-Client模式和Yarn-Cluster模式。从适用场景上看，Yarn-Cluster模式适用于生产环境，而Yarn-Client模式更适用于开发（交互式调试）。 2.1 Client模式在Yarn-client模式下，Driver运行在本地Client上，通过AM（ApplicationMaster）向RM申请资源。本地Driver负责与所有的executor container进行交互，并将最后的结果汇总。结束掉Client，相当于kill掉这个spark应用。 Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend。 ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派。 Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）。 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task。 client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。 应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。 2.2 Cluster模式在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序： 第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动。 第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成。 应用的运行结果不能在客户端显示（可以在history server中查看），所以最好将结果保存在HDFS而非stdout输出，客户端的终端显示的是作为YARN的job的简单运行状况，下图是yarn-cluster模式： 执行过程： Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等。 ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化。 ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束。 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，而Executor对象的创建及维护是由。CoarseGrainedExecutorBackend负责的，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等。 ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。 应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。 2.3 两种模式的比较在client模式下，Spark Application运行的Driver会在提交程序的节点上，而该节点可以是YARN集群内部节点，也可以不是。一般来说提交Spark Application的客户端节点不是YARN集群内部的节点，那么在客户端节点上可以根据自己的需要安装各种需要的软件和环境，以支撑Spark Application正常运行。在cluster模式下，Spark Application运行时的所有进程都在YARN集群的NodeManager节点上，而且具体在哪些NodeManager上运行是由YARN的调度策略所决定的。 对比这两种模式，最关键的是Spark Application运行时Driver所在的节点不同，而且，如果想要对Driver所在节点的运行环境进行配置，区别很大，但这对于PySpark Application运行来说是非常关键的。 第三部分 Pyspark Application原理PySpark是Spark为使用Python程序编写Spark Application而实现的客户端库，通过PySpark也可以编写Spark Application并在Spark集群上运行。Python具有非常丰富的科学计算、机器学习处理库，如numpy、pandas、scipy等等。为了能够充分利用这些高效的Python模块，很多机器学习程序都会使用Python实现，同时也希望能够在Spark集群上运行。 理解PySpark Application的运行原理，有助于我们使用Python编写Spark Application，并能够对PySpark Application进行各种调优。PySpark构建于Spark的Java API之上，数据在Python脚本里面进行处理，而在JVM中缓存和Shuffle数据，数据处理流程如下图所示: Spark Application会在Driver中创建pyspark.SparkContext对象，后续通过pyspark.SparkContext对象来构建Job DAG并提交DAG运行。使用Python编写PySpark Application，在Python编写的Driver中也有一个pyspark.SparkContext对象，该pyspark.SparkContext对象会通过Py4J模块启动一个JVM实例，创建一个JavaSparkContext对象。PY4J只用在Driver上，后续在Python程序与JavaSparkContext对象之间的通信，都会通过PY4J模块来实现，而且都是本地通信。 PySpark Application中也有RDD，对Python RDD的Transformation操作，都会被映射到Java中的PythonRDD对象上。对于远程节点上的Python RDD操作，Java PythonRDD对象会创建一个Python子进程，并基于Pipe的方式与该Python子进程通信，将用户编写Python处理代码和数据发送到Python子进程中进行处理。 第四部分 Python on Yarn配置及运行4.1 Yarn节点配置Python环境该模式需要在Yarn集群上每个NM节点（Node Manager）上部署Python编译环境，即安装Python安装包、依赖模块。用户编写的Pyspark Application由集群中Yarn调度执行。 通常使用Anaconda安装包进行统一部署，简化环境的部署。 该模式存在下面缺点： 新增依赖包部署安装代价大。如果后续用户编写的Spark Application需要依赖新的Python模块或包，那么就需要依次在集群Node Manager上部署更新依赖包。 用户对于Python环境的依赖差异化无法满足。通常不同用户编写Spark Application会依赖不同的Python环境，比如Python2、Python3环境等等。该模式下只能支持一种环境，无法满足Python多环境的需求。 各节点的Python环境需要统一。由于用户提交的Spark Application具体在哪些Node Manager上执行，由YARN调度决定，所以必须保证每个节点的Python环境（基础环境+依赖环境）都是相同的，环境维护成本高。 4.2 Yarn节点不配置Python环境该模式不需要提前在集群Node Manager上预安装Python环境。 参考文章：http://quasiben.github.io/blog/2016/4/15/conda-spark/ 我们基于华为C60集群（开源集群相同）以及Anaconda环境对该模式进行了测试验证。具体实现思路如下所示： 在一台SUSE节点上部署Anaconda，并创建虚拟Python环境（如果需要可以部署安装部分依赖包）。 创建conda虚拟环境，并整体打包为zip文件。 用户提交PySpark Application时，使用--archives参数指定该zip文件路径。 详细操作步骤如下： 第一步 下载Anaconda3-4.2.0-Linux-x86_64.sh安装软件（基于python3.5），在SUSE服务器上部署安装。Anaconda的安装路径为/usr/anaconda3。查看客户端服务器的python环境清单： 1234dkfzxwma07app08:/usr/anaconda3 # conda env list# conda environments:#root * /usr/anaconda3 其中root环境为目前的主环境。为了便于环境版本管理我们新建一个专用环境（mlpy_env）。 1dkfzxwma07app08:/usr/anaconda3/envs # conda create -n mlpy --clone root 上述命令创建了一个名称为mlpy_env的Python环境，clone选项将对应的软件包都安装到该环境中，包括一些C的动态链接库文件。 接着，将该Python环境打包，执行如下命令： 12dkfzxwma07app08:/usr/anaconda3/envs # cd /root/anaconda2/envsdkfzxwma07app08:/usr/anaconda3/envs # zip -r mlpy_env.zip mlpy_env 将该zip压缩包拷贝到指定目录中（或者后续引用使用绝对路径），方便后续提交PySpark Application： 1dkfzxwma07app08:/usr/anaconda3/envs # cp mlpy_env.zip /tmp/ 最后，我们可以提交我们的PySpark Application，执行如下命令（或打包成shell脚本）： 123456PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python spark-submit \--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python \--master yarn-cluster \--archives /tmp/mlpy_env.zip#ANACONDA \/var/lib/hadoop-hdfs/pyspark/test_pyspark_dependencies.py 注意：下面命令指的是zip包将在ANACONDA的目录中展开，需要注意路径。 12&gt; --archives /tmp/mlpy_env.zip#ANACONDA&gt; 环境打包需要注意压缩路径。 上面的依赖zip压缩包将整个Python的运行环境都包含在里面，在提交PySpark Application时会将该环境zip包上传到运行Application的所在的每个节点上。解压缩后为Python代码提供运行时环境。如果不想每次都从客户端将该环境文件上传到集群中运行节点上，也可以提前将zip包上传到HDFS文件系统中，并修改–archives参数的值为hdfs:///tmp/mlpy_env.zip #ANACONDA（注意环境差异），也是可以的。 另外，需要说明的是，如果我们开发的/var/lib/hadoop-hdfs/pyspark /test_pyspark_dependencies.py文件中，依赖多个其他Python文件，想要通过上面的方式运行，必须将这些依赖的Python文件拷贝到我们创建的环境中，对应的目录为mlpy_env/lib/python2.7/site-packages/下面。 注意：pyspark不支持python3.6版本，所以python环境使用python3.5 否则程序执行回显会有这样的报错信息： TypeError: namedtuple() missing 3 required keyword-only arguments: ‘verbose’, ‘rename’, and ‘module’ https://issues.apache.org/jira/browse/SPARK-19019?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel 4.3 一个机器学习任务栗子举一个Kmeans无监督算法的Python案例： 123456789101112131415161718192021222324252627282930313233343536import osfrom pyspark import SparkContextfrom pyspark.mllib.clustering import KMeans, KMeansModelfrom numpy import arrayfrom math import sqrt# 创建spark contextsc = SparkContext(appName="kmeans")# 加载和解析数据文件stg_path = "hdfs://hacluster" + "/user/" + str(os.environ['USER']) + "/.sparkStaging/" + str(sc.applicationId) + "/" data = sc.textFile(os.path.join(stg_path,'kmeans_data.txt')) parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))# 创建模型clusters = KMeans.train(parsedData, 2, maxIterations=10,runs=10,initializationMode="random")# 模型训练def error(point): i = clusters.predict(point) center = clusters.centers[i] print("(" + str(point[0]) + "," + str(point[1]) + "," + str(point[2]) + ")" + "blongs to cluster " + str(i+1)) # print("Cluster Number:" + str(len(clusters.centers))) return sqrt(sum([x**2 for x in (point - center)]))WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)print("Within Set Sum of Squared Error = " + str(WSSSE))# 打印类心for mCenter in clusters.centers: print(mCenter)# 保存模型myModelPath = "hdfs://hacluster"+"/user/model/"+"KMeansModel.ml"clusters.save(sc, myModelPath)# 加载模型并测试loadModel = KMeansModel.load(sc, myModelPath)print(loadModel.predict(array([1,1,1]))) 整理成下面的提交命令，将作业提交到Yarn集群： 12345678910dkfzxwma07app08:/tmp/pyspark # cat run.shPYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python \/approot1/utility/hadoopclient/Spark/spark/bin/spark-submit \--master yarn \--deploy-mode cluster \--archives /tmp/pyspark/mlpy_env.zip#ANACONDA \--files /tmp/pyspark/kmeans_data.txt \--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python \--conf spark.yarn.executorEnv.PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python \/tmp/pyspark/kmeanTest.py 模型训练结果会写到路径下面：/user/model： 123dkfzxwma07app08:/tmp/pyspark/pythonpkg # hdfs dfs -ls /user/modelFound 1 itemsdrwxr-xr-x+ - itdw hadoop 0 2019-07-30 11:30 /user/model/KMeansModel.ml 模型加载的预测结果可以在Yarn日志中查询： 12345678910dkfzxwma07app08:/tmp/pyspark # yarn logs -applicationId application_1562162322775_150207# 提取部分回显LogType:stdoutLog Upload Time:星期二 七月 30 13:47:23 +0800 2019LogLength:120414Log Contents:Within Set Sum of Squared Error = 0.6928203230275529[ 9.1 9.1 9.1][ 0.1 0.1 0.1]1 当然对于输出可以选择其他输出源(表或者文件)。 第五部分 总结5.1 混合多语言数据流通常一个完整的机器学习应用的数据流设计中，可以将数据ETL准备阶段和算法计算分离出来。使用Java/scala/sql进行数据的预处理，输出算法计算要求的数据格式。这会极大降低算法计算的数据输入规模，降低算法计算的节点的IO。 机器学习的算法计算部分具有高迭代计算特性，对于非分布式的机器学习算法，我们通常部署在高性能的节点上，基于丰富、高性能的Python科学计算模块，使用Python语言实现。而对于数据准备阶段，更适合使用原生的Scala/java编程语言实现Spark Application来处理数据，包括转换、统计、压缩等等，将满足算法输入格式的数据输出到HDFS文件系统中。特别对于数据规模较大的情况，在Spark集群上处理数据，Scala/Java实现的Spark Application运行（多机并行分布式处理）性能要好一些。然后输出数据交给Python进行迭代计算训练。 当然对于分布式机器学习框架，将数据迭代部分分解到多个节点并行处理，由参数服务器管理迭代参数的汇总和更新。在这种计算框架下可以利用数据集群天然的计算资源，实现分布式部署。这就形成了一个高效的混合的多语言的数据处理流。 5.2 架构建议和总结1、对于Python on Yarn架构下，采用“Yarn节点不配置Python环境”模式，便于Python环境的管理。这时候可以将Python环境zip文件上传至集群HDFS文件系统，避免每次提交任务都需要上传zip文件，但是不可避免集群内部HDFS文件系统分发到运行节点产生的网络IO。但比集群外部的上传效率高。 2、对于机器学习任务数据流建议采用混合多语言数据流方式，发挥各计算组件的优势。 3、对于分布式机器学习框架，建议结合集群的计算资源，直接在集群上展开分布式计算（例如Tensorflow计算框架）。而不是单独新建新的机器学习分布式集群。减少两个集群的数据搬运，并且使得数据和计算更加贴近，最重要的提高机器学习任务端到端的效率。 参考文献及资料1、Running Spark Python Applications，链接：https://www.cloudera.com/documentation/enterprise/5-9-x/topics/spark_python.html 2、基于YARN集群构建运行PySpark Application，链接： http://shiyanjun.cn/archives/1738.html 3、Running Spark on YARN，链接： https://spark.apache.org/docs/latest/running-on-yarn.html 4、Running Spark on Kubernetes，链接：https://spark.apache.org/docs/latest/running-on-kubernetes.html 5、Apache Spark Resource Management and YARN App Models，链接：https://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/ 6、Spark On Yarn的两种模式yarn-cluster和yarn-client深度剖析，链接：https://www.cnblogs.com/ITtangtang/p/7967386.html 7、Introducing Skein: Deploy Python on Apache YARN the Easy Way，链接：https://jcrist.github.io/introducing-skein.html 8、当Spark遇上TensorFlow分布式深度学习框架原理和实践，链接：https://juejin.im/post/5ad4b620f265da23a04a0ad0 9、Spark On Yarn的优势，链接：https://www.cnblogs.com/ITtangtang/p/7967386.html 10、基于YARN集群构建运行PySpark Application，链接：http://www.uml.org.cn/bigdata/201711132.asp]]></content>
      <categories>
        <category>pyspark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TDengine时间序列数据库压力测试]]></title>
    <url>%2F2019%2F07%2F01%2F2019-07-01-TDengine%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 部署压测工具vegeta 第二部分 部署TDengine数据库 第三部分 TDengine数据库RESTful接口介绍 第四部分 压力测试 第五部分 压力测试结果 参考文献及资料 背景最近涛思数据团队开源了自己的时序数据库，关注度较高。按照官方介绍，性能较为强悍，所以使用压测工具对该数据库进行了性能压测。压测使用的工具Vegeta 是一个用 Go 语言编写的多功能的 HTTP 负载测试工具。Vegeta 提供了命令行工具和一个开发库。 第一部分 部署压测工具vegeta1.1 部署从github上下载vegeta安装介质： 1$ wget https://github.com/tsenart/vegeta/releases/download/cli%2Fv12.5.1/vegeta-12.5.1-linux-amd64.tar.gz 该工具开箱即用，解压tar包： 1$ tar -zxvf vegeta-12.5.1-linux-amd64.tar.gz 1.2 简单测试使用我们使用vegeta测试一下下面的压力场景（对百度主页发起每秒100次（-rate=100）的请求，持续10秒（-duration=10s）），测试结果重定向到文件（result/results.bin）： 1$ echo "GET https://www.baidu.com" |./vegeta attack -duration=10s -rate=100 &gt;result/results.bin 查看一下测试结果： 123456789root@deeplearning:/data/vegeta# ./vegeta report result/results.binRequests [total, rate] 1000, 100.10Duration [total, attack, wait] 10.002241136s, 9.990128173s, 12.112963msLatencies [mean, 50, 95, 99, max] 2.315958984s, 1.889487082s, 6.064678156s, 6.583899297s, 6.961230585sBytes In [total, mean] 227000, 227.00Bytes Out [total, mean] 0, 0.00Success [ratio] 100.00%Status Codes [code:count] 200:1000 Error Set: 另外可以生成html文件报告（可视化）： 1root@deeplearning:/data/vegeta# ./vegeta plot result/results.bin &gt; result/plot.html 第二部分 部署TDengine数据库2.1 制作docker镜像 由于墙的原因我们在VPS上打包镜像，然后本机拉取部署。 为了保证测试环境的隔离性，我们制作docker镜像，使用docker环境进行测试。首先拉取ubuntu的基础镜像： 1$ docker run -t -i ubuntu:16.04 /bin/bash 通过TDengine源码安装。在这过程有大量操作系统工具未安装，需要使用atp-get安装部署。 2.1.1 第一步 clone项目1$ git clone https://github.com/taosdata/TDengine.git 2.1.2 第二步 编译12$ mkdir build &amp;&amp; cd build$ cmake .. &amp;&amp; cmake --build . 2.1.3 第三步 安装1$ make install 2.2 生成镜像打包成镜像，推送到Docker Hub： 123root@vultr:~# docker commit a35c43242a8e rongxiang1986/tdengineroot@vultr:~# docker tag rongxiang1986/tdengine rongxiang1986/tdengine:1.0root@vultr:~# docker push rongxiang1986/tdengine:1.0 2.3 拉取镜像部署拉取镜像： 1root@deeplearning:/data/TDengine/TDengine# docker pull rongxiang1986/tdengine:1.0 启动一个docker容器： 1root@deeplearning:/data/TDengine/TDengine# docker run -t -i --name tdengine -d -p 6020:6020 rongxiang1986/tdengine:1.0 /bin/bash 查看正在运行的容器： 12root@deeplearning:/data/TDengine/TDengine# docker ps# bec2e166c29f 进入容器： 1root@deeplearning:/data/TDengine/TDengine# docker attach bec2e166c29f 最后启动数据库服务，下面是启动回显信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107root@bec2e166c29f:~/TDengine/TDengine/build# ./build/bin/taosd -c test/cfg &amp;[1] 23root@bec2e166c29f:~/TDengine/TDengine/build# TDengine:[23]: Starting TDengine service...07/17 14:48:21.615248 7ff90254e700 UTL timezone not configured, set to system default:Etc/UTC (Etc, +0000)07/17 14:48:21.615261 7ff90254e700 UTL locale not configured, set to system default:C07/17 14:48:21.616266 7ff90254e700 UTL taos config &amp; system info:07/17 14:48:21.616274 7ff90254e700 UTL ==================================07/17 14:48:21.616278 7ff90254e700 UTL internalIp: 172.17.0.3 07/17 14:48:21.616283 7ff90254e700 UTL localIp: 172.17.0.3 07/17 14:48:21.616288 7ff90254e700 UTL httpIp: 0.0.0.0 07/17 14:48:21.616294 7ff90254e700 UTL httpPort: 6020 07/17 14:48:21.616299 7ff90254e700 UTL mgmtShellPort: 6030 07/17 14:48:21.616304 7ff90254e700 UTL vnodeShellPort: 6035 07/17 14:48:21.616308 7ff90254e700 UTL configDir: test/cfg 07/17 14:48:21.616312 7ff90254e700 UTL dataDir: /root/TDengine/TDengine/build/test/data 07/17 14:48:21.616318 7ff90254e700 UTL logDir: /root/TDengine/TDengine/build/test/log 07/17 14:48:21.616325 7ff90254e700 UTL scriptDir: /etc/taos 07/17 14:48:21.616330 7ff90254e700 UTL numOfThreadsPerCore: 1.000000 07/17 14:48:21.616338 7ff90254e700 UTL ratioOfQueryThreads: 0.500000 07/17 14:48:21.616344 7ff90254e700 UTL numOfVnodesPerCore: 8 07/17 14:48:21.616349 7ff90254e700 UTL numOfTotalVnodes: 0 07/17 14:48:21.616355 7ff90254e700 UTL tables: 1000 07/17 14:48:21.616360 7ff90254e700 UTL cache: 16384(byte)07/17 14:48:21.616366 7ff90254e700 UTL rows: 4096 07/17 14:48:21.616371 7ff90254e700 UTL fileBlockMinPercent: 0.250000 07/17 14:48:21.616376 7ff90254e700 UTL ablocks: 4 07/17 14:48:21.616382 7ff90254e700 UTL tblocks: 100 07/17 14:48:21.616386 7ff90254e700 UTL monitorInterval: 30(s)07/17 14:48:21.616391 7ff90254e700 UTL rpcTimer: 300(ms)07/17 14:48:21.616396 7ff90254e700 UTL rpcMaxTime: 600(s)07/17 14:48:21.616402 7ff90254e700 UTL ctime: 3600(s)07/17 14:48:21.616407 7ff90254e700 UTL statusInterval: 1(s)07/17 14:48:21.616414 7ff90254e700 UTL shellActivityTimer: 3(s)07/17 14:48:21.616420 7ff90254e700 UTL meterMetaKeepTimer: 7200(s)07/17 14:48:21.616423 7ff90254e700 UTL metricMetaKeepTimer: 600(s)07/17 14:48:21.616428 7ff90254e700 UTL maxUsers: 1000 07/17 14:48:21.616432 7ff90254e700 UTL maxDbs: 1000 07/17 14:48:21.616439 7ff90254e700 UTL maxTables: 650000 07/17 14:48:21.616442 7ff90254e700 UTL maxVGroups: 1000 07/17 14:48:21.616445 7ff90254e700 UTL minSlidingTime: 10(ms)07/17 14:48:21.616454 7ff90254e700 UTL minIntervalTime: 10(ms)07/17 14:48:21.616460 7ff90254e700 UTL maxStreamCompDelay: 20000(ms)07/17 14:48:21.616463 7ff90254e700 UTL maxFirstStreamCompDelay:10000(ms)07/17 14:48:21.616467 7ff90254e700 UTL retryStreamCompDelay: 10(ms)07/17 14:48:21.616470 7ff90254e700 UTL clog: 1 07/17 14:48:21.616474 7ff90254e700 UTL comp: 2 07/17 14:48:21.616480 7ff90254e700 UTL days: 10 07/17 14:48:21.616483 7ff90254e700 UTL keep: 3650 07/17 14:48:21.616488 7ff90254e700 UTL defaultDB: 07/17 14:48:21.616494 7ff90254e700 UTL defaultUser: root 07/17 14:48:21.616503 7ff90254e700 UTL defaultPass: taosdata 07/17 14:48:21.616508 7ff90254e700 UTL timezone: Etc/UTC (Etc, +0000) 07/17 14:48:21.616515 7ff90254e700 UTL locale: C 07/17 14:48:21.616520 7ff90254e700 UTL charset: UTF-8 07/17 14:48:21.616525 7ff90254e700 UTL maxShellConns: 2000 07/17 14:48:21.616531 7ff90254e700 UTL maxMeterConnections: 10000 07/17 14:48:21.616535 7ff90254e700 UTL maxMgmtConnections: 2000 07/17 14:48:21.616540 7ff90254e700 UTL maxVnodeConnections: 10000 07/17 14:48:21.616543 7ff90254e700 UTL enableHttp: 1 07/17 14:48:21.616549 7ff90254e700 UTL enableMonitor: 1 07/17 14:48:21.616553 7ff90254e700 UTL httpCacheSessions: 2000 07/17 14:48:21.616556 7ff90254e700 UTL httpMaxThreads: 2 07/17 14:48:21.616561 7ff90254e700 UTL numOfLogLines: 10000000 07/17 14:48:21.616565 7ff90254e700 UTL asyncLog: 1 07/17 14:48:21.616570 7ff90254e700 UTL debugFlag: 131 07/17 14:48:21.616574 7ff90254e700 UTL mDebugFlag: 135 07/17 14:48:21.616579 7ff90254e700 UTL dDebugFlag: 131 07/17 14:48:21.616584 7ff90254e700 UTL sdbDebugFlag: 135 07/17 14:48:21.616590 7ff90254e700 UTL taosDebugFlag: 131 07/17 14:48:21.616595 7ff90254e700 UTL tmrDebugFlag: 131 07/17 14:48:21.616601 7ff90254e700 UTL cDebugFlag: 131 07/17 14:48:21.616605 7ff90254e700 UTL jniDebugFlag: 131 07/17 14:48:21.616612 7ff90254e700 UTL odbcDebugFlag: 131 07/17 14:48:21.616617 7ff90254e700 UTL uDebugFlag: 131 07/17 14:48:21.616621 7ff90254e700 UTL httpDebugFlag: 131 07/17 14:48:21.616629 7ff90254e700 UTL monitorDebugFlag: 131 07/17 14:48:21.616634 7ff90254e700 UTL qDebugFlag: 131 07/17 14:48:21.616637 7ff90254e700 UTL gitinfo: 82cbce3261d06ab37c3bd4786c7b2e3d2316c42a 07/17 14:48:21.616643 7ff90254e700 UTL buildinfo: Built by ubuntu at 2019-07-05 18:42 07/17 14:48:21.616648 7ff90254e700 UTL version: 1.6.0.0 07/17 14:48:21.616653 7ff90254e700 UTL os pageSize: 4096(KB)07/17 14:48:21.616658 7ff90254e700 UTL os openMax: 104857607/17 14:48:21.616662 7ff90254e700 UTL os streamMax: 1607/17 14:48:21.616666 7ff90254e700 UTL os numOfCores: 807/17 14:48:21.616670 7ff90254e700 UTL os totalDisk: 426(GB)07/17 14:48:21.616676 7ff90254e700 UTL os totalMemory: 32028(MB)07/17 14:48:21.616682 7ff90254e700 UTL os sysname: Linux07/17 14:48:21.616686 7ff90254e700 UTL os nodename: bec2e166c29f07/17 14:48:21.616690 7ff90254e700 UTL os release: 4.15.0-51-generic07/17 14:48:21.616694 7ff90254e700 UTL os version: #55~16.04.1-Ubuntu SMP Thu May 16 09:24:37 UTC 201907/17 14:48:21.616700 7ff90254e700 UTL os machine: x86_6407/17 14:48:21.616707 7ff90254e700 UTL ==================================07/17 14:48:21.616712 7ff90254e700 DND Server IP address is:172.17.0.307/17 14:48:21.616717 7ff90254e700 DND starting to initialize TDengine engine ...07/17 14:48:21.619440 7ff90254e700 HTP failed to open telegraf schema config file:test/cfg/taos.telegraf.cfg, use default schema07/17 14:48:21.869736 7ff90254e700 DND vnode is initialized successfully07/17 14:48:21.869780 7ff90254e700 MND starting to initialize TDengine mgmt ...07/17 14:48:21.872494 7ff90254e700 MND first access, set total vnodes:6407/17 14:48:21.917758 7ff90254e700 MND TDengine mgmt is initialized successfully07/17 14:48:21.917781 7ff90254e700 HTP starting to initialize http service ...07/17 14:48:21.918417 7ff90254e700 DND TDengine is initialized successfully07/17 14:48:21.918533 7ff8e4f41700 HTP http service init success at ip:0.0.0.0:6020TDengine:[23]: Started TDengine service successfully.07/17 14:48:22.022278 7ff8ff835700 MON starting to initialize monitor service ..07/17 14:48:22.022747 7ff8eb109700 MND user:monitor login from 172.17.0.3, code:007/17 14:48:22.024412 7ff901038700 MON dnode:172.17.0.3 is started07/17 14:48:22.026780 7ff901038700 MON monitor service init success 上面回显service init success说明服务service启动成功。 2.4 镜像使用说明TDengine项目地址：https://github.com/taosdata/TDengine。使用ubuntu16.04作为基础镜像，部署安装TDengine。拉取镜像后，启动容器后： 启动服务：To start the TDengine server, run the command below in terminal: 1$ /root/TDengine/TDengine/build# ./build/bin/taosd -c test/cfg 启动客户端：In another terminal, use the TDengine shell to connect the server: 1$ /root/TDengine/TDengine/build#./build/bin/taos -c test/cfg 第三部分 TDengine数据库RESTful接口介绍按照官方给的例子我们新建案例数据库和表，并且新增数据记录。首先进入shell交互： 1234567root@a35c43242a8e:~/TDengine/TDengine/build# ./build/bin/taos -c test/cfg07/17 04:30:19.818000 7fa154b0e700 MND user:root login from 172.17.0.2, code:0Welcome to the TDengine shell, server version:1.6.0.0 client version:1.6.0.0Copyright (c) 2017 by TAOS Data, Inc. All rights reserved.taos&gt; 创建案例数据： 12345678910111213141516171819202122taos&gt; create database db;07/17 04:31:46.970580 7fa14ffff700 MND DB:0.db is created by rootQuery OK, 1 row(s) affected (0.001848s)taos&gt; use db;Database changed.taos&gt; create table t (ts timestamp, cdata int);Query OK, 1 row(s) affected (0.334998s)taos&gt; insert into t values ('2019-07-15 10:00:00', 10);Query OK, 1 row(s) affected (0.001639s)taos&gt; insert into t values ('2019-07-15 10:01:05', 20);Query OK, 1 row(s) affected (0.000245s)taos&gt; select * from t; ts | cdata |=================================== 19-07-15 10:00:00.000| 10| 19-07-15 10:01:05.000| 20|Query OK, 2 row(s) in set (0.001408s) 退出shell交互后，我们使用Restfull接口与数据库交互。 注意：目前RESTfull接口认证方式使用Http Basic Authorization请求格式，token使用base64(username:password)，即base64(root:taosdata)=cm9vdDp0YW9zZGF0YQ== 可以在在线网站上：https://www.base64encode.org/ encode一下。 1234root@a35c43242a8e:~/TDengine/TDengine/build# curl -H 'Authorization: Basic cm9vdDp0YW9zZGF0YQ==' -d 'select * from db.t' localhost:6020/rest/sql#下面是回显：07/17 04:33:34.834283 7fa154b0e700 MND user:root login from 172.17.0.2, code:0&#123;"status":"succ","head":["ts","cdata"],"data":[["2019-07-15 10:00:00.000",10],["2019-07-15 10:01:05.000",20]],"rows":2&#125; 返回结果是一个JSON格式串，规范化一下： 123456789101112131415161718&#123; "status":"succ", "head":[ "ts", "cdata" ], "data":[ [ "2019-07-15 10:00:00.000", 10 ], [ "2019-07-15 10:01:05.000", 20 ] ], "rows":2&#125; 第四部分 对RESTful接口压力测试最后我们使用vegeta对restful接口进行压力测试： 4.1 查询压测使用目标文件的内容进行压力测试。 首先创建target.txt文件以及数据文件data.json，内容如下： 1234root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat target.txtPOST http://localhost:6020/rest/sqlAuthorization: Basic cm9vdDp0YW9zZGF0YQ==@data.json 12root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat data.jsonselect * from db.t 最后使用下面的命令开始压测（每秒15000次请求，持续5分钟）： 123456789101112root@bec2e166c29f:~/TDengine/vegeta/TDengineTest#../vegeta attack -rate 15000 -targets target.txt -duration 5m &gt; out.dat# 生成报告root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report out.dat# 回显：Requests [total, rate] 4500044, 15000.15Duration [total, attack, wait] 5m0.000202809s, 4m59.999911126s, 291.683µsLatencies [mean, 50, 95, 99, max] 314.712µs, 230.113µs, 869.502µs, 1.501018ms, 205.870168msBytes In [total, mean] 954009328, 212.00Bytes Out [total, mean] 90000880, 20.00Success [ratio] 100.00%Status Codes [code:count] 200:4500044 Error Set: 4.2 写入压测首先创建writetarget.txt和数据文件：writedata.json，内容如下： 1234root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat writetarget.txtPOST http://localhost:6020/rest/sqlAuthorization: Basic cm9vdDp0YW9zZGF0YQ==@writedata.json 12root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat writedata.jsoninsert into db.cpu values (NOW,20,12); 这里写入语句使用时间函数NOW，保证写入时间无重复。 最后使用命令开始压测（每秒30000次请求，持续1分钟）： 1234567891011root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta attack -rate 30000 -targets writetarget.txt -duration 1m &gt; writeout.dat# 生成报告root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report writeout30000.datRequests [total, rate] 1800018, 30000.31Duration [total, attack, wait] 1m0.000154842s, 59.99998565s, 169.192µsLatencies [mean, 50, 95, 99, max] 254.824µs, 137.651µs, 904.696µs, 1.687601ms, 12.528831msBytes In [total, mean] 115201152, 64.00Bytes Out [total, mean] 70200702, 39.00Success [ratio] 100.00%Status Codes [code:count] 200:1800018 Error Set: 第五部分 压力测试结果由于机器环境的差异，只是做了尝试性测试，不代表产品的实际性能。 官方测试报告参考：https://www.taosdata.com/downloads/TDengine_Testing_Report_cn.pdf 5.1 查询对于查询性能我们每秒15000的请求结果如下： 123456789root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report out15000.datRequests [total, rate] 4500044, 15000.15Duration [total, attack, wait] 5m0.000202809s, 4m59.999911126s, 291.683µsLatencies [mean, 50, 95, 99, max] 314.712µs, 230.113µs, 869.502µs, 1.501018ms, 205.870168msBytes In [total, mean] 954009328, 212.00Bytes Out [total, mean] 90000880, 20.00Success [ratio] 100.00%Status Codes [code:count] 200:4500044 Error Set: 当提高到每秒20000次时，数据库出现响应失败，成功率只有38.78%： 123456789101112131415161718192021root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report out20000.dat Requests [total, rate] 904794, 2824.86Duration [total, attack, wait] 6m19.120695351s, 5m20.296894938s, 58.823800413sLatencies [mean, 50, 95, 99, max] 38.726984614s, 43.82040722s, 1m13.377381823s, 1m25.843611324s, 1m52.393219406sBytes In [total, mean] 58181450, 64.30Bytes Out [total, mean] 8016960, 8.86Success [ratio] 38.78%Status Codes [code:count] 0:503946 200:350896 400:49952 Error Set:400 Bad RequestPost http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in usePost http://localhost:6020/rest/sql: net/http: request canceled (Client.Timeout exceeded while awaiting headers)Post http://localhost:6020/rest/sql: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in use (Client.Timeout exceeded while awaiting headers)Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:34037-&gt;127.0.0.1:6020: read: connection reset by peerPost http://localhost:6020/rest/sql: read tcp 127.0.0.1:53020-&gt;127.0.0.1:6020: read: connection reset by peerPost http://localhost:6020/rest/sql: read tcp 127.0.0.1:47081-&gt;127.0.0.1:6020: read: connection reset by peerPost http://localhost:6020/rest/sql: read tcp 127.0.0.1:35442-&gt;127.0.0.1:6020: read: connection reset by peerPost http://localhost:6020/rest/sql: read tcp 127.0.0.1:58175-&gt;127.0.0.1:6020: read: connection reset by peerPost http://localhost:6020/rest/sql: read tcp 127.0.0.1:41857-&gt;127.0.0.1:6020: read: connection reset by peerPost http://localhost:6020/rest/sql: read tcp 127.0.0.1:58943-&gt;127.0.0.1:6020: read: connection reset by peer 5.2 写入对于写入测试。对于查询性能我们每秒30000的请求结果如下： 123456789root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report writeout30000.datRequests [total, rate] 1800018, 30000.31Duration [total, attack, wait] 1m0.000154842s, 59.99998565s, 169.192µsLatencies [mean, 50, 95, 99, max] 254.824µs, 137.651µs, 904.696µs, 1.687601ms, 12.528831msBytes In [total, mean] 115201152, 64.00Bytes Out [total, mean] 70200702, 39.00Success [ratio] 100.00%Status Codes [code:count] 200:1800018 Error Set: 当提高到每秒50000次时，数据库性能开始恶化，成功率只有55.73%： 12345678910111213root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report writeout50000.datRequests [total, rate] 160712, 2676.70Duration [total, attack, wait] 1m47.789030336s, 1m0.041100866s, 47.74792947sLatencies [mean, 50, 95, 99, max] 36.064568863s, 39.103737526s, 50.372069878s, 54.866214748s, 1m12.470342472sBytes In [total, mean] 5731840, 35.67Bytes Out [total, mean] 3492840, 21.73Success [ratio] 55.73%Status Codes [code:count] 0:71152 200:89560 Error Set:Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in usePost http://localhost:6020/rest/sql: net/http: request canceled (Client.Timeout exceeded while awaiting headers)Post http://localhost:6020/rest/sql: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in use (Client.Timeout exceeded while awaiting headers) 结论：单机环境下同等场景（都是压测RESTfull接口）下，TDengine可以抗住每秒15000次的读请求和每秒30000次写请求。influxdb只能抗住每秒6000次持续读、写。按照官网介绍如果写入是批量形式会更快。 参考文献和材料1、推荐一款高性能 HTTP 负载测试工具 Vegeta 链接：https://www.hi-linux.com/posts/4650.html 2、TDengine官网 链接：https://www.taosdata.com/cn/ 3、比Hadoop快至少10倍的物联网大数据平台，我把它开源了 链接：https://weibo.com/ttarticle/p/show?id=2309404394278649462890]]></content>
      <categories>
        <category>TDengine Vegeta</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python系列文章-Python并发和并行编程总结]]></title>
    <url>%2F2019%2F05%2F23%2F2019-06-23-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Python%E5%B9%B6%E5%8F%91%E5%92%8C%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 CPython全局解释器锁问题 第二部分 Python中的多线程实现 第三部分 Python中的多进程实现 第四部分 高级应用 参考文献及资料 背景在讲解正文之前我们先回忆明确一些计算机操作系统的概念。 0.1 线程和进程进程（Process）和线程（Thread）是计算机操作系统中的重要概念。进程是资源分配的基本单位，一个进程可以包括至少一个数量的线程。线程是程序执行流的最小单位，它隶属一个进程。并与进程的其他线程共享分配资源。例如在操作系统中打开浏览器，就相当于启动了浏览器的进程。在浏览器界面中，我们可以看视频、看网页内容、刷弹幕、留言等，这些就是线程。 0.2 多线程和多进程现代操作系统都是多任务，这里的任务可以理解为进程。多进程即同时执行多个进程，多个线程即同时执行多个线程。 0.3 并发和并行由于CPU执行代码都是顺序执行的，为了实现多任务同时执行，操作系统将CPU使用时间进行切片，各任务交替执行（此时其他任务挂起）。实际上由于CPU运行速度较快，会让人类感觉（错觉）同时有多个线程在执行。这种交错执行程序的方式称为并发（concurrency）。 当操作系统资源有多个CPU时，这时候任务可以做到真正的同时执行，程序指令可以同时分别运行在每个CPU核心上，通常称为并行（parallelism）。 在程序设计中，一些需求如果实现分布式处理，将提高程序运行效率和系统整体性能。其实原理是将一个进程分成多个线程，然后让它们并发异步执行，从而提高运行效率。 Python语言既支持多线程，也支持多进程。 0.4 I/O密集和CPU密集计算机中任务可以分为计算密集型（也称为CPU密集型）和I/O密集型两类。计算密集型任务，显而易见的特点是要进行大量的计算，消耗CPU资源，比如科学计算、对视频进行高清解码等等，全靠CPU的运算能力。 而I/O密集型主要是网络、磁盘I/O较多的任务。这类任务的特点是CPU消耗较少，大部分时间都在等待I/O操作完成（因为I/O的速度远远低于CPU和内存的速度）。 第一部分 CPython全局解释器锁问题1.1 什么是GIL机制Python是一种编程语言，需要其他语言实现它的解释器。目前使用最广泛的解释器是C语言实现的，所以称为CPython。例如Linux、os等操作系统自带的均是CPython，另外机器学习生态圈使用较多的Anaconda也是CPython解释器。 注：除了CPython，其实还有java实现的Jyphon解释器、RPython实现的Pypy解释器。 可以使用sys包中implementation方法查看解释器的类型： 1234&gt; # print(sys.implementation)&gt; namespace(cache_tag='cpython-35', hexversion=50660336, name='cpython', version=s&gt; ys.version_info(major=3, minor=5, micro=3, releaselevel='final', serial=0))&gt; CPython有两个缺点： CPython不支持即时编译。 CPython编译器的GIL全局锁的机制。 GIL（Global Interpreter Lock）是一个互斥锁（mutex），保证同一时刻只有一个线程控制解释器（即任何时间点只能一个线程处于运行状态）。通俗的理解，GIL相当于线程运行在CPU上的通行证，一个Python进程（也称为Python虚拟机）同时只能容许一个线程在CPU上执行，没有通行证的线程只能挂起等待。对于单个核心的CPU，这就是正常的并发。但是对于多核心的CPU，GIL仍然限制不能并行执行，这样就不能发挥最大资源优势。 显然，由于GIL机制，CPython多线程程序在多核心CPU架构下运行效率低下。另外需要注意的是：仅CPython解释器上存在GIL。 1.2 为什么要引入GIL机制Python中垃圾回收有个“引用计数”机制。Python登记每个对象的引用次数，当引用次数为0时候，Python将这个对象（Python中一切皆对象）从内存中删除，释放资源。可以体会下面的例子： 123456789testlist = [1,2]print(sys.getrefcount(testlist))# 输出是：2。需要注意的是getrefcount函数的本身引用也算1次。a = testlistprint(sys.getrefcount(testlist))# 输出是：3del aprint(sys.getrefcount(testlist))# 输出是：2。删除了变量a，对象引用数减少1。 在这种机制下，如果同时存在两个线程（不通信状态）对同一个对象增加或者减少其引用值，就会存在一定概率导致内存泄漏和内存错误释放，表现为程序崩溃或出现未知异常错误。为了保证对象数据的一致性安全，于是考虑增加锁机制。 如果对每个对象添加锁机制，在多锁机制下，又会存在死锁现象。综合以上因素和权衡，Python直接对解释器整体加了一把锁，这就是GIL。 1.3 GIL机制对于多线程程序的影响CPython引入GIL机制后，到底对多线程程序运行有多大影响呢？下面分别对CPU和I/O密集型场景进行分析： CPU密集型代码(各种循环处理、计数、搜索、矩阵计算等) 在这种情况下，ticks计数（计步（ticks）可粗略看作Python虚拟机的指令）很快就会达到阈值，然后触发GIL的释放与再竞争（多个线程来回切换需要消耗资源的），所以对CPU密集型代码并不友好。在python 3.2中，GIL不再使用ticks计数，改为使用计时器（执行时间达到阈值后，当前线程释放GIL，阀值默认为5ms，可设置），这样对CPU密集型程序会更加友好。 I/O密集型代码(文件处理、网络爬虫等) 多线程能够有效提升效率(单线程下有I/O操作会进行I/O等待，造成不必要的时间浪费，而开启多线程能在线程A I/O等待时，自动切换到线程B，可以不浪费CPU的资源，从而能提升程序执行效率)。所以对I/O密集型代码比较友好。 具体对比测试数据可以参考文章：http://cenalulu.github.io/python/gil-in-python/ 1.4 为什么不根本解决GILGuido van Rossum 在创造Python的时候，时间是90年代（1990年代）。那时候硬件条件大多数是单核CPU。随着CPU多核硬件条件的实现，能否解决GIL机制呢？CPython的作者和BDFL，Guido van Rossum，在2007年9月的文章It isn’t Easy to Remove the GIL给出了社区回应： I’d welcome it if someone did another experiment along the lines of Greg’s patch (which I haven’t found online), and I’d welcome a set of patches into Py3k only ifthe performance for a single-threaded program (and for a multi-threaded but I/O-bound program) does not decrease. 此后的任何尝试都没有实现这一条件。 第二部分 Python中的多线程实现Python通过两个标准库thread和threading提供对线程编程的支持。从Python 1.5.2版本开始支持threading模块，另外需要注意的是在Python 3中thread重命名为_thread。Threading模块是基于thread（\_thread）基础上构建了更易用的多线程支持（threading模块对thread模块进行了封装，更便于使用），建议使用更强大的Threading模块。 thread（_thread）和threading均是CPython中的内置模块。 2.1 简单例子下面的例子是一个单线程的程序实现。多个work任务串行执行： 123456789101112131415161718192021222324import timeimport threadingdef work(): print(threading.currentThread().getName()+" "+"start") time.sleep(1) print(threading.currentThread().getName() + " " + "end")if __name__ == '__main__': startTime = time.time() for i in range(4): work() print(time.time()-startTime)"""MainThread startMainThread endMainThread startMainThread endMainThread startMainThread endMainThread startMainThread end4.0""" 如果我们使用多线程编程实现： 12345678910111213141516171819202122232425import timeimport threadingdef work(): print(threading.currentThread().getName()+" "+"start") time.sleep(1) print(threading.currentThread().getName() + " " + "end")if __name__ == '__main__': startTime = time.time() for i in range(4): my_thread = threading.Thread(target=work) my_thread.start() print(time.time()-startTime)"""Thread-1 startThread-2 startThread-3 startThread-4 start0.0009999275207519531Thread-3 endThread-2 endThread-1 endThread-4 end""" 上面的例子能看出线程并没有串行执行，在线程sleep等待时，就会顺序将print语句执行。 2.2 GIL机制下单线程和多线程的比较下面的简单的案例，我们设计了两个CPU和I/O繁忙的程序对比单线程和多线程效率的差异。使用timeit模块计时。测试机为个人笔记本多核心CPU（4核）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import threadingdef CpuWork(): temp = 0 for i in range(100): temp = temp + 1def IoWork(): fo = open("python.text", "w+") fo.write("testwrite\n") fo.readlines() fo.close()def CpuWorkSigle(): for i in range(100): CpuWork()def CpuWorkTread(): for i in range(100): t = threading.Thread(target=CpuWork()) t.start()def IoWorkSigle(): for i in range(100): IoWork()def IoWorkTread(): for i in range(100): t = threading.Thread(target=IoWork()) t.start()if __name__ == '__main__': from timeit import timeit CpuWorkSigleT = timeit('CpuWorkSigle()', 'from __main__ import CpuWorkSigle', number=10000) CpuWorkTreadT = timeit('CpuWorkTread()', 'from __main__ import CpuWorkTread', number=10000) print("CpuWorkSigleT", CpuWorkSigleT) print("CpuWorkTreadT", CpuWorkTreadT) IoWorkSigleT = timeit('IoWorkSigle()', 'from __main__ import IoWorkSigle', number=100) IoWorkTreadT = timeit('IoWorkTread()', 'from __main__ import IoWorkTread', number=100) print("IoWorkSigleT", IoWorkSigleT) print("IoWorkTreadT", IoWorkTreadT)"""CpuWorkSigleT 5.429784478290195CpuWorkTreadT 170.5910518324614IoWorkSigleT 24.924760018310337IoWorkTreadT 29.044745530459664""" 测试结果发现对于CPU和I/O繁忙的程序有两个测试现象： 单线程任务均比多线程任务快。 对于CPU密集型任务，多线程与单线程相差较大。 就算GIL机制下不能实现并行运行，为什么多线程任务比单线程任务还要慢呢？特别是CPU密集型任务差距特别大。那是因为多线程涉及到CPU上下文切换、锁机制处理（获取锁、释放锁等）。 CPU上下文切换（context switch），也称做进程切换或者任务切换，是指CPU从一个进程或线程切换到另一个进程或线程。上下文切换对系统来说意味着消耗大量的 CPU 时间。上下文切换过高，会导致CPU像个搬运工，频繁在寄存器和运行队列之间奔波，更多时间消耗在线程切换，而不是真正工作的线程上。直接的消耗包括CPU寄存器需要保存和加载，系统调度器的代码需要执行。间接消耗在于多核cache之间的共享数据。 而对于IO密集型任务，多线程忙于单线程，但差距不大。这应该是归功于IO等待时间（当线程遇到IO操作会主动释放GIL锁），可以运行其他线程。而单线程需要严格顺序等待。 2.3 threading.RLock和threading.Lock在介绍线程锁之前，需要介绍一个概念：线程安全。我们知道操作系统中进程是最小的资源单位，线程是最小的执行单位。同一个进程中不同线程共享资源。既然是共享，就需要管理资源状态。我们先看看一个例子： 1234567891011121314151617181920212223242526272829from threading import Threadtotal = 0def add_total(amount): global total #lock.acquire() for _ in range(100000): total += amount #lock.release()def reduce_total(amount): global total #lock.acquire() for _ in range(100000): total -= amount #lock.release()if __name__ == '__main__': # lock = threading.Lock() thread_01 = Thread(target=add_total, args=(100,)) thread_02 = Thread(target=reduce_total, args=(100,)) thread_01.start() thread_02.start() thread_01.join() thread_02.join() print(total) 例子中每次执行后，程序打印的total值是不同。如果按照朴素的理解，两个线程一个减100，一个加100，并且执行的次数相同。那么最后的结果应该是0。然而并不是这样的。例如当reduce_total线程拿到值是0，还没减100或者没有更新。同时add_total线程同时也拿到值是0，并且快速更新值为100。这时候A线程才开始更新，会把值更新为-100。多次发生这类场景后，值就是不确定的。 为了保证数据的一致性，引入了锁的概念。threading包使用中将上面代码中注释内容释放后，在测试运行，结果是始终为：0。 事实上，调整后的程序，线程会严格顺序（即串行执行）。每个线程执行完才会释放锁资源。 另外还有RLock锁（可重入锁），它允许在同一线程中被多次acquire。而Lock却不允许这种情况。注意：如果使用RLock，那么acquire和release必须成对出现，即调用了n次acquire，必须调用n次的release才能真正释放所占用的琐。 两个锁的区别，我们举个栗子来比较一下： 12345678910111213141516171819import threadingdef lockTest(): lock = threading.Lock() #Lock对象 lock.acquire() lock.acquire() #产生了死琐 lock.release() lock.release()def rlockTest(): rLock = threading.RLock() #RLock对象 rLock.acquire() rLock.acquire() #在同一线程内，程序不会堵塞。 rLock.release() rLock.release()if __name__ == '__main__': # lockTest() rlockTest() 2.4 threading.Semaphore 信号量对象信号量的概念是由计算机科学家艾兹赫尔·戴克斯特拉（Edsger W. Dijkstra）发明的，操作系统用来解决并发中的互斥和同步问题的一种方法。 信号量是一个更高级的锁机制。信号量内部有一个计数器而不像锁对象内部有锁标识，而且只有当占用信号量的线程数超过信号量时线程才阻塞。这允许了多个线程可以同时访问相同的代码区。 Semaphore管理一个内置的计数器，每当调用acquire()时内置计数器-1；调用release() 时内置计数器+1；计数器不能小于0；当计数器为0时，acquire()将阻塞线程直到其他线程调用release()。 举一个简单的栗子： 12345678910111213141516171819202122232425262728293031323334import timeimport threadingsemaphore = threading.Semaphore(3)def semaphoreTest(): if semaphore.acquire(): for i in range(3): time.sleep(1) print(threading.currentThread().getName() + '获取锁') semaphore.release() print(threading.currentThread().getName() + '-释放锁')if __name__ == '__main__': for i in range(4): t1 = threading.Thread(target=semaphoreTest) t1.start()"""Thread-1获取锁Thread-3获取锁Thread-2获取锁Thread-1获取锁Thread-3获取锁Thread-2获取锁Thread-1获取锁Thread-1-释放锁 #释放Thread-3获取锁Thread-3-释放锁 #释放Thread-2获取锁Thread-2-释放锁 #释放Thread-4获取锁Thread-4获取锁Thread-4获取锁Thread-4 释放锁""" 线程名Thread-4 在Thread-1、Thread-2、Thread-3释放锁资源后 2.5 Event事件2.6 threading.Condition2.7 Quences第三部分 Python中的多进程实现既然Python对于多线程支持有天生的缺陷，那么如何解决呢？Python 在2.6版本后引入了multiprocessing包，提供了多进程并发的接口。这样每个进程都有自己的GIL，避免了线程之间挣抢GIL。 https://anyisalin.github.io/2017/03/12/python-multithread/ 参考文献及资料1、Python的GIL是什么鬼，多线程性能究竟如何，链接：http://cenalulu.github.io/python/gil-in-python/ 2、It isn’t Easy to Remove the GIL，链接：https://www.artima.com/weblogs/viewpost.jsp?thread=214235 3、深入理解Linux的CPU上下文切换，链接：https://www.linuxblogs.cn/articles/linux-context-switch.html 4、Python 最难的问题，链接：https://zhuanlan.zhihu.com/p/32284710]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sqlflow初体验]]></title>
    <url>%2F2019%2F05%2F06%2F2019-05-06-Sqlflow%E5%88%9D%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Sqlflow安装部署 第二部分 机器学习例子 第三部分 系统架构 参考文献及资料 背景2019年5 月 6 日，在QCon 全球软件开发大会（北京站）上，蚂蚁金服副 CTO 胡喜正式宣布开源机器学习工具 SQLFlow。实际上3个月前sqkflow项目已经在github上开源了。 本篇文件主要参考sqlflow官网的案例和说明对sqlflow进行了体验，并记录下来。 sqlflow按照官网的定义，将SQL引擎（例如MySQL，Hive，SparkSQL或SQL Server）和tensorflow和其他机器学习的桥梁。扩展了SQL语言，支持对机器学习模型训练、预测和推理。 目前开源版本仅支持MySQL和TensorFlow 介绍文档中也提到，在sqlflow之前也有SQL引擎提供了支持机器学习功能的扩展。 Microsoft SQL Server：Microsoft SQL Server具有机器学习服务，可以将R或Python中的机器学习程序作为外部脚本运行。 Teradata SQL for DL：Teradata还提供RESTful服务，可以从扩展的SQL SELECT语法中调用。 Google BigQuery：Google BigQuery通过引入CREATE MODEL语句在SQL中实现机器学习。 第一部分 Sqlflow安装部署1.1 部署mysql做为数据源（1）构建镜像官网提供了一个dockerfile，可以git clone整个项目。 https://github.com/sql-machine-learning/sqlflow/tree/7c873780bd8a3a9ea4d39ed7d0fcf154b2f8821f/example/datasets 1234# 进入Dockerfile文件所在目录cd example/datasets# 使用Dockerfile构建镜像docker build -t sqlflow:data . 可以查看创建了一个docker images： 12docker images# 创建了REPOSITORY：sqlflow镜像，TAG为：data （2）启动mysql容器用镜像启mysql容器： 12345docker run --rm -d --name sqlflowdata \ -p 3306:3306 \ -e MYSQL_ROOT_PASSWORD=root \ -e MYSQL_ROOT_HOST=% \ sqlflow:data 使用镜像：sqlflow:data，启动一个名为：sqlflowdata的容器，并且把3306端口映射到宿主机。mysql的root用户的密码为root。 （3）生成测试数据进入容器： 1docker exec -it sqlflowdata bash 执行SQL语句： 12345# 建库建表，注意宿主机目录：datasetscat /popularize_churn.sql | mysql -uroot -prootcat /popularize_iris.sql | mysql -uroot -proot# 建库echo "CREATE DATABASE IF NOT EXISTS sqlflow_models;" | mysql -uroot -proot 至此完成mysql容器的启动和测试数据的生成。按Ctrl+P+Q，正常退出不关闭容器。 1.2 使用docker部署slqflow（1）拉取镜像并启动容器首先从docker Hub上拉取镜像： 1# docker pull sqlflow/sqlflow:latest 启动容器： 123# docker run --rm -it --name sqlflowServer -p 8888:8888 sqlflow/sqlflow:latest \bash -c "sqlflowserver --datasource='mysql://root:root@tcp(192.168.31.3:3306)/?maxAllowedPacket=0' &amp;SQLFLOW_SERVER=localhost:50051 jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root" 命令使用镜像：sqlflow/sqlflow:lates，启动了名为：sqlflowServer的容器。将8888端口映射到宿主机上。这里需要配置datasource，指向mysql使用套接字：192.168.31.3:3306。这里使用之前构建的mysql容器的连接信息，可以根据实际情况配置。 如果mysql套接字配置错误，报错信息：connect: connection refused 如果没有报错： 12345672019/05/06 14:47:30 Server Started at :50051[I 14:47:30.261 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret[I 14:47:30.874 NotebookApp] Serving notebooks from local directory: /[I 14:47:30.874 NotebookApp] The Jupyter Notebook is running at:[I 14:47:30.874 NotebookApp] http://(fd2b9b3f994b or 127.0.0.1):8888/?token=265b3fc832b5b48689fa9f88483125dc9335188dd7c1d863[I 14:47:30.874 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[W 14:47:30.877 NotebookApp] No web browser found: could not locate runnable browser. 这里启动了Jupyter Notebook服务，对外服务端口为8888，并且映射到宿主机。例如这里可以使用下面的url范围web界面：http://192.168.31.3:8888/?token=265b3fc832b5b48689fa9f88483125dc9335188dd7c1d863 （2）简单测试Jupyter Notebook 新建一个python3交互环境。测试一下： 12%%sqlflowselect * from iris.train limit 5; 第二部分 机器学习例子使用iris数据集体验机器学习的例子，使用Jupyter Notebook 完成： （1）训练模型：12%%sqlflowSELECT * FROM iris.train TRAIN DNNClassifier WITH n_classes = 3, hidden_units = [10, 20] COLUMN sepal_length, sepal_width, petal_length, petal_width LABEL class INTO sqlflow_models.my_dnn_model; 使用iris.train表中的数据训练神经网络。 模型训练结果输入到sqlflow_models.my_dnn_model，回显训练正确率为：0.97273 12Training set accuracy: 0.97273Done training （2）模型应用使用训练结果对数据进行预测应用： 12%%sqlflowSELECT * FROM iris.test PREDICT iris.predict.class USING sqlflow_models.my_dnn_model; 使用iris.test中的数据喂给训练好的模型，预测结果输出到表：iris.predict。 1Done predicting. Predict table : iris.predict 查看结果表中的数据案例： 12%%sqlflowselect * from iris.predict limit 2 123456+--------------+-------------+--------------+-------------+-------+| sepal_length | sepal_width | petal_length | petal_width | class |+--------------+-------------+--------------+-------------+-------+| 6.3 | 2.7 | 4.9 | 1.8 | 2 || 5.7 | 2.8 | 4.1 | 1.3 | 1 |+--------------+-------------+--------------+-------------+-------+ 第三部分 系统架构系统原型使用下面的架构： 12SQL statement -&gt; our SQL parser --standard SQL-&gt; MySQL \-extended SQL-&gt; code generator -&gt; execution engine 原型运行的数据流为： 它通过MySQL Connector Python API从MySQL检索数据 从MySQL检索模型 通过调用用户指定的TensorFlow估算器训练模型或使用训练模型进行预测 并将训练过的模型或预测结果写入表格 参考文献及资料1、sqlflow项目官网 链接：https://github.com/sql-machine-learning/sqlflow 2、会 SQL 就能搞定 AI！蚂蚁金服重磅开源机器学习工具 SQLFlow 链接：https://www.infoq.cn/article/vlVqC68h2MT-028lh68C]]></content>
      <categories>
        <category>sqlflow</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Sqoop组件详细介绍]]></title>
    <url>%2F2019%2F04%2F28%2F2019-04-28-Sqoop%E7%BB%84%E4%BB%B6%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Sqoop原理 第一部分 Sqoop安装 第二部分 第三部分 第四部分 总结 参考文献及资料 背景在信息系统中，应用程序背后通常是关系型数据库（RDBMS），这类数据成为大数据的重要来源，在这样的需求背景下，诞生了Sqoop项目。 Apache Sqoop（发音：skup）是一个用于Hadoop生态系统（HDFS、Hive、HBase）和关系型数据库（RDBMS）（例如：Mysql、Oracle、Postgres、Tidb等）之间传输数据的工具。Sqoop是Apache Software Foundation的开源软件产品，官网介绍如下： Apache Sqoop(TM) is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases. 另外sqoop的命名取自：SQl+hadOOP=SQOOP。Sqoop项目开始于2009年（最初由Cloudera开发和维护），最早作为Hadoop的一个第三方模块，后来为了方便用户快速部署使用、开发人员快速开发和迭代，Sqoop独立成为Apache的项目（时间轴上，2011年7月23日，开始由Apache孵化；到2012年4月，Sqoop项目被提升为Apache的顶级项目）。 Latest stable release is 1.4.7 (download, documentation). Latest cut of Sqoop2 is 1.99.7 (download, documentation). Note that 1.99.7 is not compatible with 1.4.7 and not feature complete, it is not intended for production deployment. Sqoop的两代版本Sqoop目前有两代版本，分别是是Sqoop1（1.4.x）和Sqoop2（1.99.x），两个版本不兼容，架构和用法上已经完全不同。Sqoop1最新稳定版为1.4.7，Sqoop2最新版本为1.99.7（1.99.7这个版本官网不建议部署生产系统）。 Sqoop2相对于Sqoop1主要改进有： 引入Sqoop Server，集中化管理Connector 支持多种访问方式：CLI、WEB UI、Rest Api、Java API 引入基于角色的安全机制 两代Sqoop主要功能差异如下： Feature（功能） Sqoop 1 Sqoop 2 Connectors for all major RDBMS（连接所有主流RDBMS数据库） Supported（支持） Not supported.Workaround: Use the generic JDBC Connector which has been tested on the following databases: Microsoft SQL Server, PostgreSQL, MySQL and Oracle.This connector should work on any other JDBC compliant database. However, performance might not be comparable to that of specialized connectors in Sqoop.不再支持。但是使用通用连接器（generic JDBC Connector），性能上无法和专用连接器相比。 Kerberos Security Integration（Kerberos安全集成） Supported（支持） Supported（支持） Data transfer from RDBMS to Hive or HBase（数据从RDBMS传输到hive或者Hbase） Supported（支持） Not supported.Workaround: Follow this two-step approach.Import data from RDBMS into HDFSLoad data into Hive or HBase manually using appropriate tools and commands such as the LOAD DATA statement in Hive。不支持。解决办法： 按照此两步方法操作。 将数据从 RDBMS 导入 HDFS 在 Hive 中使用相应的工具和命令（例如 LOAD DATA 语句），手动将数据载入 Hive 或 HBase。 Data transfer from Hive or HBase to RDBMS（数据从Hive或者Hbase传输到RDBMS） Not supported.Workaround: Follow this two-step approach.Extract data from Hive or HBase into HDFS (either as a text or Avro file)Use Sqoop to export output of previous step to RDBMS。解决办法： 按照此两步方法操作。 从 Hive 或 HBase 将数据提取至 HDFS （作为文本或 Avro 文件） 使用 Sqoop 将上一步的输出导出至 RDBMS Not supported.Follow the same workaround as for Sqoop 1.（参考Sqoop1） Sqoop1支持集成Kerberos认证,Sqoop2的1.99.3版本还不支持(Sqoop 1.99.6版本已经支持)。 两代Sqoop的架构数据流图如下： Sqoop1 Sqoop1属于单机模式部署形态，使用客户端（CLI控制台）直接提交命令给hadoop集群，命令和脚本需要显示指定用户数据库名和密码。主要数据流如下： Sqoop2 Sqoop2属于C/S模式部署的形态，架构上引进了Server框架，对Connector实现集中管理。主要数据流如下： Sqoop的metadata保存在Derby数据库中。 两代版本的优缺点比较：Sqoop1使用客户端架构，用户需要在客户端部署Sqoop和相关连接器，而Sqoop2使用服务框架，连接器部署在Sqoop服务器上。从MR角度看，Sqoop1只是提交maper only作业，而Sqoop2提交了MapReduce作业，map完成从源传输数据，reduce根据指定的源转换数据，这是一个完整的MR抽象。另外从安全角度，Sqoop1每次任务都需要输入用户名和密钥，Sqoop2中由管理员设置源和目标的连接，普通用户只需要使用已经创建的连接，无需知道连接的详细信息。 第一部分 Sqoop原理Sqoop将导入和导出命令翻译成Mapreduce程序来实现。 第二部分 Sqoop的安装Sqoop只是一个工具，只需要在一个节点上安装部署即可。 Sqoop只能在Linux系统上部署。 目前那部分 2.1 Sqoop1的部署2.2 Sqoop2的部署2.3 部署总结第三部分 Sqoop的使用3.1 Sqoop1的使用3.2 Sqoop2的使用3.3 使用总结第四部分 华为hadoop集群中Loader产品的使用华为Loader组件产品基于Sqoop开源组件进行封装和优化，并且Loader提供一个可视化的配置管理界面，有效的降低的用户使用门槛。 参考文献及资料1、Sqoop项目官网 链接：http://sqoop.apache.org 2、Sqoop User Guide (v1.4.6) 链接：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html 3、sqoop1vs sqoop2 链接：https://www.wikitechy.com/tutorials/sqoop/sqoop1-vs-sqoop2 4、Feature Differences - Sqoop 1 and Sqoop 2 https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cdh_ig_sqoop_vs_sqoop2.html 5、sqoop关系型数据迁移原理以及map端内存为何不会爆掉窥探 6、重拾初心——Sqoop1和Sqoop2的刨析对比 链接：https://blog.csdn.net/gamer_gyt/article/details/55225700 7、sqoop2基本架构、部署和个人使用感受 https://cloud.tencent.com/info/d865fe6ed0d92d03bdf8512a4c9e4212.html https://blog.csdn.net/qq_38776653/article/details/77802871]]></content>
      <categories>
        <category>Sqoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Avro详细介绍]]></title>
    <url>%2F2019%2F04%2F25%2F2020-10-25-Avro%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Avro中的Schema 第二部分 数据序列化和反序列化 第二部分 命令行读写Avro文件 第三部分 Avro的使用场景介绍 第四部分 总结 参考文献及资料 背景Avro（发音[ævrə]）是Apache基金会的一个独立项目，按照项目介绍Avro是一个数据序列化系统（Apache Avro™ is a data serialization system）。这个项目是Ddoug Cutting（Hadoop之父）创立的，Hadoop生态圈中其他项目如Hbase、Hive也采用Avro进行数据传输。Avro将数据转换成便于存储和传输的格式，实现数据密集型应用完成远程和本地大规模数据的交互和存储。 按照项目官网的介绍，Avro可以提供： 丰富的数据结构类型 快速可压缩的二进制数据形式 存储持久数据的文件容器 远程过程调用 RPC 简单的动态语言结合功能，Avro 和动态语言结合后，读写数据文件和使用 RPC 协议都不需要生成代码，而代码生成作为一种可选的优化只值得在静态类型语言中实现。 Avro 支持跨编程语言实现（C、C++、C#、Java、Python、Ruby、PHP），例如我们使用Java API写入Avro文件，然后通过Python API来读取该文件。这个特性有点像Thrift 和 Protocol Buffers 等系统，区别主要是： 动态类型：Avro 并不需要将生成代码，模式和数据存放在一起，而模式使得整个数据的处理过程并不生成代码、静态数据类型等等。这方便了数据处理系统和语言的构造。 未标记的数据：由于读取数据的时候模式是已知的，那么需要和数据一起编码的类型信息就很少了，这样序列化的规模也就小了。 不需要用户指定字段号：即使模式改变，处理数据时新旧模式都是已知的，所以通过使用字段名称可以解决差异问题。 第一部分 Avro中的SchemaAvro Schema（模式）使用JSON语言定义。通过Schema定义Avro数据结构，只有确定了Schema才能对数据进行解析。 1.1 数据类型数据类型分为：基本类型和复杂类型。 1.1.1 基本类型（Primitive Types）Schema定义了8种简单数据类型，主要有： 类型 说明 null no value boolean a binary value int 32-bit signed integer long 64-bit signed integer float single precision (32-bit) IEEE 754 floating-point number double double precision (64-bit) IEEE 754 floating-point number bytes sequence of 8-bit unsigned bytes string unicode character sequence 基本类型没有子属性，基本类型的名字也就是类型的名字，例如： 1&#123;"type": "string"&#125; 1.1.2 复杂类型（Complex Types）Avro提供了6种复杂类型，分别是Record（记录类型）、Enum（枚举类型）、Array（数组类型）、Map（映射类型）、Union（组合类型）和Fixed（混合类型）。 Record（记录类型）record类型支持属性有： name：record类型的名字(必填) namespace：命名空间(可选) doc：这个类型的文档说明(可选) aliases：record类型的别名，是个字符串数组(可选) fields：record类型中的字段，是个对象数组(必填)。每个字段有以下属性： 序号 属性名 介绍 1 name 字段名字(必填) 2 doc 字段说明文档(可选) 3 type 一个schema的json对象或者一个类型名字(必填) 4 default 默认值(可选) 5 order 排序(可选)，只有3个值ascending(默认)，descending或ignore 6 aliases 别名，字符串数组(可选) 我们来看一个record类型例子。下面的例子定义了一个名为test的Schema，test有两个属性：name和value。 123456789&#123; "type": "record", "name": "test", "aliases": ["JustTest"], "fields" : [ &#123;"name": "name", "type": "string","default": "name"&#125;, &#123;"name": "value", "type": "long"&#125; ]&#125; 对于default属性意义是：当Schema实例数据中某个field属性没有提供实例数据时候，由默认值替代。 Enum（枚举类型）Enum为枚举类型，类型名字为”enum”，还支持其它属性的设置： name：枚举类型的名字(必填) namespace：命名空间(可选) aliases：字符串数组，别名(可选) doc：说明文档(可选) symbols：字符串数组，所有的枚举值(必填)，不允许重复数据。 下面定义一个名为language的Schema： 12345&#123; "name": "language", "type": "enum", "symbols" : ["python", "java", "go", "c++","C","R"]&#125; Array（数据类型）数组类型的类型名为”array”并且只支持一个属性：items。下面的Schema定义了一个字符串数组： 1234&#123; "type": "array", "items": "string" &#125; Map（映射类型）Map类型的类型名字是”map”并且只支持一个属性：values（Map的key必须是字符串）。例子： 1234&#123; "type": "map", "values": "long" &#125; Union（组合类型）组合类型，表示各种类型的组合，使用数组进行组合。比如[“null”, “string”]表示类型可以为null或者string。 组合类型的默认值是看组合类型的第一个元素，因此如果一个组合类型包括null类型，那么null类型一般都会放在第一个位置，这样子的话这个组合类型的默认值就是null。 组合类型中不允许同一种类型的元素的个数不会超过1个，除了record，fixed和enum。比如组合类中有2个array类型或者2个map类型，这是不允许的。 组合类型不允许嵌套组合类型。 Fixed（混合类型）混合类型的类型名为：fixed，支持以下属性： name：名字(必填) namespace：命名空间(可选) aliases：字符串数组，别名(可选) size：一个整数，表示每个值的字节数(必填) 比如16个字节数的fixed类型例子如下： 12345&#123; "name": "md5"， "type": "fixed", "size": 16&#125; 1.2 一个Schema例子下面是一个Schema的例子使用的record类型、array类型、string类型、enum类型。 123456789101112131415&#123;"name":"Parent", "type":"record", "fields":[ &#123;"name":"children", "type":&#123; "type":"array", "items":&#123;"name":"Child","type":"record", "fields":[&#123;"name":"name","type":"string"&#125;] &#125; &#125; &#125;,&#123;"name":"language", "type":&#123;"type":"enum","symbols":["python","java","go","c++","C","R"]&#125; &#125; ]&#125; 数据案例： 1&#123;"children":[&#123;"name": "xiaoming"&#125;,&#123;"name": "xaioqiang"&#125;&#125;],"language": "python"&#125; 下面网站提供根据json数据案例生成Schema： https://toolslick.com/generation/metadata/avro-schema-from-json? 第二部分 数据序列化和反序列化Avro支持两种数据序列化编码方式：二进制格式（Binary encoding） 和JSON格式（json encoding）。 二进制格式。使用二进制编码会高效序列化，并且序列化后得到的结果会比较小。用于生产环境。 JSON格式。而JSON一般用于调试系统或是基于WEB的应用。 下面将详细介绍使用Python和Java实现Avro的数据序列化和反序列化。重点介绍二进制格式的序列化。 2.1 Python API2.1.1 部署avro-python3环境针对python2和python3分别安装： 1234# python3pip install avro-python3# python2pip install avro 在解析Schema的时候python2和python3有个方法名的差异需要注意： python2：avro.schema.parse python3：avro.schema.Parse 2.1.2 用python实现avro的文件读写案例使用二进制序列化数据并反序列化解析打印： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import avro.schemafrom avro.io import DatumReader,DatumWriter,BinaryEncoder,BinaryDecoderimport ioimport json# 数据案例data = &#123;"name": "Virutal conference", "date": 25612345, "location":"New York", "speakers":["Speaker1","Speaker2"], "participants":["Participant1","Participant2","Participant3","Participant4", "Participant5"], "seatingArrangement":&#123;"Participant1":1,"Participant2":2, "Participant3":3, "Participant4":4, "Participant5":5&#125;&#125;# print(data)# 定义Schema模式，python中使用字典格式存储Schema。schema_dict = &#123; "namespace": "demo.avro", "type": "record", "name": "Conference", "fields": [ &#123;"name": "name", "type": "string"&#125;, &#123;"name": "date", "type": "long"&#125;, &#123;"name": "location", "type": "string"&#125;, &#123;"name": "speakers", "type": &#123;"type":"array","items":"string"&#125;&#125;, &#123;"name": "participants", "type": &#123;"type": "array", "items": "string"&#125;&#125;, &#123;"name": "seatingArrangement", "type": &#123;"type": "map", "values": "int"&#125;&#125; ] &#125;# print(schema_demo)# 解析Schema文件schema = avro.schema.Parse(json.dumps(schema_demo))############################### 定义writewriter = DatumWriter(schema)bytes_writer = io.BytesIO()# 使用二进制序列化BinaryEncoder数据encoder = BinaryEncoder(bytes_writer)# 写入数据writer.write(data,encoder)raw_bytes = bytes_writer.getvalue()print(len(raw_bytes))print(type(raw_bytes))############################## 定义read，使用二进制反序列化BinaryDecoder数据bytes_reader = io.BytesIO(raw_bytes)decoder = BinaryDecoder(bytes_reader)reader = DatumReader(schema)# 读取数据并打印item = reader.read(decoder)print(item)#回显："""382&lt;class 'bytes'&gt;&#123;'name': 'Virutal conference', 'date': 25612345, 'location': 'New York', 'speakers': ['Speaker1', 'Speaker2'], 'participants': ['Participant1', 'Participant2', 'Participant3', 'Participant4', 'Participant5'], 'seatingArrangement': &#123;'Participant1': 1, 'Participant2': 2, 'Participant3': 3, 'Participant4': 4, 'Participant5': 5&#125;&#125;""" json序列化：https://pythonhosted.org/avro_json_serializer/ https://wp.huangshiyang.com/avro-schema-python-serialize-and-deseralize 2.2 Java API2.2.1 Maven依赖新建一个Maven项目，在pom.xml中添加两个依赖项：Apache Avro库和Maven插件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;avro.version&gt;1.8.2&lt;/avro.version&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro&lt;/artifactId&gt; &lt;version&gt;$&#123;avro.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;avro.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro-compiler&lt;/artifactId&gt; &lt;version&gt;$&#123;avro.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro-ipc&lt;/artifactId&gt; &lt;version&gt;$&#123;avro.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;avro.version&#125;&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;schemas&lt;/id&gt; &lt;phase&gt;generate-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;schema&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;includes&gt; &lt;include&gt;StringPair.avsc&lt;/include&gt; &lt;/includes&gt; &lt;stringType&gt;String&lt;/stringType&gt; &lt;sourceDirectory&gt;src/main/avro/&lt;/sourceDirectory&gt; &lt;outputDirectory&gt;src/main/java/&lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 我们正在配置插件以在src/main/avro中查找规范文件并将生成的Java类放到src/main/java中 2.2.2 定义Schame文件https://www.hascode.com/2014/03/using-apache-avro-with-java-and-maven/ https://my.oschina.net/weiwei02/blog/877979 https://yanbin.blog/java-python-communicate-via-apache-avro/ http://cloudurable.com/blog/avro/index.html https://fangjian0423.github.io/2016/02/21/avro-intro/ 简单： https://juejin.im/post/5c36018ee51d4552475fb347 第三部分 命令行读写Avro文件Avro项目提供Avro Tools支持用户使用命令行方式快速读取一个二进制的Avro文件。这里案例使用下面github的项目文件： https://github.com/miguno/avro-cli-examples 3.1 Avro Tools获取目前Avro最新版本为1.8.2，我们获取最新的版本： 1wget http://ftp.wayne.edu/apache/avro/avro-1.8.2/java/avro-tools-1.8.2.jar 可以使用下面的命名查看工具支持的功能清单： 12345678910111213141516171819202122232425262728293031323334# java -jar avro-tools-1.8.2.jar --helpVersion 1.8.2 of Apache AvroCopyright 2010-2015 The Apache Software FoundationThis product includes software developed atThe Apache Software Foundation (http://www.apache.org/).----------------Available tools: cat extracts samples from files compile Generates Java code for the given schema. concat Concatenates avro files without re-compressing. fragtojson Renders a binary-encoded Avro datum as JSON. fromjson Reads JSON records and writes an Avro data file. fromtext Imports a text file into an avro data file. getmeta Prints out the metadata of an Avro data file. getschema Prints out schema of an Avro data file. idl Generates a JSON schema from an Avro IDL file idl2schemata Extract JSON schemata of the types from an Avro IDL file induce Induce schema/protocol from Java class/interface via reflection. jsontofrag Renders a JSON-encoded Avro datum as binary. random Creates a file with randomly generated instances of a schema. recodec Alters the codec of a data file. repair Recovers data from a corrupt Avro Data file rpcprotocol Output the protocol of a RPC service rpcreceive Opens an RPC Server and listens for one message. rpcsend Sends a single RPC message. tether Run a tethered mapreduce job. tojson Dumps an Avro data file as JSON, record per line or pretty. totext Converts an Avro data file to a text file. totrevni Converts an Avro data file to a Trevni file. trevni_meta Dumps a Trevni file's metadata as JSON.trevni_random Create a Trevni file filled with random instances of a schema.trevni_tojson Dumps a Trevni file as JSON. 3.2 JSON文件到二进制Avro文件将Json数据文件通过schema转换成Avro文件。使用fromjson命令，分为不压缩和压缩： 不压缩 1# java -jar avro-tools-1.8.2.jar fromjson --schema-file twitter.avsc twitter.json &gt; twitterTest.avro 使用twitter.avsc最为schema，转换结果重定向到文件twitterTest.avro。 压缩 1# java -jar avro-tools-1.8.2.jar fromjson --codec snappy --schema-file twitter.avsc twitter.json &gt; twitter.snappy.avro 使用snappy方式压缩 3.3 二进制Avro文件转换成json文件将Avro文件转换成json文件，使用tojson命令。 1# java -jar avro-tools-1.8.2.jar tojson twitter.snappy.avro &gt; twitter_tojson.json 注意这里不区分avro文件是否是压缩过的。另外可以使用-pretty参数打印出json数据： 1234567891011121314root@deeplearning:# java -jar avro-tools-1.8.2.jar tojson twitterTest.avro -prettylog4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.&#123; "username" : "miguno", "tweet" : "Rock: Nerf paper, scissors is fine.", "timestamp" : 1366150681&#125;&#123; "username" : "BlizzardCS", "tweet" : "Works as intended. Terran is IMBA.", "timestamp" : 1366154481&#125; 3.4 从Avro文件中获取Schema模式使用getschema 命令可以获取到Avro文件中Schema的模式信息（不区分是否压缩）： 1# java -jar avro-tools-1.8.2.jar getschema twitterTest.avro 3.5 将Schema编译成Java类使用下面的命令将Schema编译成java类文件： 1# java -jar avro-tools-1.8.2.jar compile schema twitter.avsc . 在当前目录会生成一个com目录。 第四部分 Avro文件结构https://www.cnblogs.com/duanxz/p/3799256.html https://blog.csdn.net/xyw_blog/article/details/8967362 第五部分 Avro的使用场景介绍kafka 使用：https://sonnguyen.ws/simple-example-python-kafka-avro/ 总结参考文献及资料1、Avro项目官网 链接：https://avro.apache.org/docs/current/ 2、Avro Tools API官方文档 链接：http://avro.apache.org/docs/1.7.4/api/java/org/apache/avro/tool/package-summary.html socket http://layer0.authentise.com/getting-started-with-avro-and-python-3.html]]></content>
      <categories>
        <category>Avro</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Avro详细介绍]]></title>
    <url>%2F2019%2F04%2F25%2F2019-07-13-%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Avro中的Schema 第二部分 数据序列化和反序列化 第二部分 命令行读写Avro文件 第三部分 Avro的使用场景介绍 第四部分 总结 参考文献及资料 背景同样的数据不同的表达，会直接影响后续任务的难易程度和有效性，因此寻找好的数据表示是机器学习的核心任务。 根据数据表示的提取方式（人工和自动）分为： 表示学习（representation learning）：自动学习数据的表示。 特征工程（feature engineering）：人工提起数据特征和表示。 比较：数据结构和算法紧密相连。数据表示和学习算法。 特征表达学习追溯到始祖象阶段，主要是无监督特征表达PCA和有监督特征表达LDA 但放在统计学习框架下，我们会碰到模型复杂性问题。这一问题的来源在于，设计的每个模型离真实的模型之间总会有偏差的存在，同时，模型的参数会导致其模型自身在寻优时存在波动，即会产生方差。因此，从统计意义来讲，一个好的模型需要在偏差和方差之间寻找平衡，如图1所示。在深度学习未包打天下之前的年代，这种平衡往往是通过控制模型的复杂性来获得的。对于复杂性的认识，这几十年来一直在变迁中。有通过控制模型的参数数量来实现的，如贝叶斯信息准则、Akaike信息准则；有从信息论的编码长度角度出发的，如Kolmogrov引出的最小描述长度，面向聚类的最小信息长度；有从数据几何结构出发的，如限束空间光滑性的流形约束；有从稀疏性角度出发的，如惩罚模型系数总量的L1范数；还有从模型结构的推广能力进行惩罚的，如统计机器学习中曾经盛行一时的VC维、最大边缘等约束。 那么，深度学习又是怎么玩的呢？不管采用什么样的结构，深度学习最明显的特点就是模型深，参数多。自2006年深度伯兹曼机的提出至今，残差网、稠密网、Inception网等各种深度学习模型的可调整参数的数量都在百万级甚至百万级的百倍以上。这带来一个好处，即他能表达一个远大于原有空间的空间，学术上称之为过完备空间。一般来说，在这个过完备空间上寻找不符合统计规律、但却具有优良品质的个例的机会就显著增大了。 那么为什么以前不做呢？一方面之前没有那么大规模的数据量，另一方面以前的工程技术也不支持考虑这么大规模的模型。目前多数已知的传感器成本降了不少、各种类型的数据获取成本也下来了，所以能看到PB级甚至ZB级的数据，如图像、语音、文本等。实在找不到数据的领域，还可以通过14年提出的生成式对抗网络来生成足够逼真的、海量的大数据。这两者都使得训练好的模型在刻画这个过完备空间的能力上增强了不少。 其次，工程技术上的革新也推动了深度学习的成功。深度学习的前身如多层感知器或其它神经网络模型在利用经典的反向传播算法调整模型的参数时，往往会陷入局部极小、过度拟合、会存在调参停滞的梯度消失、梯度爆炸等问题，还缺乏处理大规模数据需要的并行计算能力。这些问题，在近10年的深度学习发展中或多或少都得到了部分解决，比如通过规一化来防止梯度消失的Batch Normalization(批规范化)技术，考虑增强网络的稳定性、对网络层进行百分比随机采样的Drop Out技术，还有数据增广技术等。这使得深度学习在这个过完备空间搜索具有优良品质的个例的算力得到了显著增强。 总结参考文献及资料1、Avro项目官网 链接：https://avro.apache.org/docs/current/ 2、Avro Tools API官方文档 链接：http://avro.apache.org/docs/1.7.4/api/java/org/apache/avro/tool/package-summary.html socket http://layer0.authentise.com/getting-started-with-avro-and-python-3.html]]></content>
      <categories>
        <category>Avro，hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch性能调优总结]]></title>
    <url>%2F2019%2F01%2F27%2F2019-01-27-Elasticsearch%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录[TOC] 背景Elasticseach是一个基于Apache Lucene的开源检索引擎，允许用户近实时的存储、检索和分析数据。虽然 Elasticsearch 专为快速查询而设计，但其性能在很大程度上取决于应用使用的场景、索引的数据量以及应用用户查询数据的速度。本文主要参考Elasticseach官方文档和社区在集群调优上的经验，根据个人经验主要分为：集群的性能调优、集群稳定性调优、业务使用调优。 ##第一部分 性能调优 ###1.1 操作系统侧基线调优建议 ####1.1.1 关闭交换分区（或降低Swappiness值） swap空间是一块物理磁盘空间，操作系统使用这块空间保存从内存中换出的不常用内存数据，这样可以分配出更多的内存给当前使用的应用进程。 然而技术都是有适用场景的。相对于内存，磁盘的读写较慢（内存读写速度为纳秒级、磁盘的速度只能达到毫秒级），当系统发生大量的内存数据交换时，将影响系统运行性能。而elasticsearch对读写有较高的性能要求，如果进程使用的内存数据被写入swap磁盘，接着又换出读入内存，将极大影响性能，所以通常建议关闭或减少swap分区。 关于Linux系统swap交换分区的介绍可以参考下面的文章： https://www.linux.com/news/all-about-linux-swap-space 临时禁用swap（操作系统重启失效）： 1swapoff -a 永久禁用swap： 将/etc/fstab 文件中包含swap的行注释掉 1sed -i '/swap/s/^/#/' /etc/fstabswapoff -a 对于应用混用部署环境中，如果不能关闭swap分区的场景，Linux提供swappiness参数（0-100）控制。swappiness=0的时候表示最大限度使用物理内存，swappiness=100表示积极使用swap分区，及时将不用的内存数据搬运到swap中。操作系统默认值为60，即物理内存使用率操作40%（100-60），开始使用swap分区。这个值决定操作系统交换内存的频率。可以预防正常情况下发生交换，但仍允许操作系统在紧急情况下发生交换。 临时调整 1sysctl vm.swappiness=10 永久调整 12345# root用户vi /etc/sysctl.conf# 添加 vm.swappiness = 10# 生效sysctl -p 另外ElasticSearch 自身也提供相关优化参数。打开配置文件中的mlockall开关，它的作用就是运行JVM虚拟机时锁住内存，禁止操作系统将其内存数据交换出去。在elasticsearch.yml配置中修改如下： 1bootstrap.mlockall: true ElasticSearch 提供下面的API可以查询集群是否配置该参数： 12curl -XGET localhost:9200/_nodes?filter_path=**.mlockall##回显：&#123;"nodes":&#123;"VyKDGurkQiygV-of4B1ZAQ":&#123;"process":&#123;"mlockall":true&#125;&#125;&#125;&#125; ####1.1.2 其他操作系统资源限制调整 调整操作系统部分资源限制配置，提高性能。 1234567891011121314151617# 修改系统资源限制# 单用户可以打开的最大文件数量，可以设置为官方推荐的65536或更大些echo "* - nofile 655360" &gt;&gt;/etc/security/limits.conf# 单用户内存地址空间echo "* - as unlimited" &gt;&gt;/etc/security/limits.conf# 单用户线程数echo "* - nproc 2056474" &gt;&gt;/etc/security/limits.conf# 单用户文件大小echo "* - fsize unlimited" &gt;&gt;/etc/security/limits.conf# 单用户锁定内存echo "* - memlock unlimited" &gt;&gt;/etc/security/limits.conf# 单进程可以使用的最大map内存区域数量echo "vm.max_map_count = 655300" &gt;&gt;/etc/sysctl.conf# TCP全连接队列参数设置， 这样设置的目的是防止节点数较多（比如超过100）的ES集群中，节点异常重启时全连接队列在启动瞬间打满，造成节点hang住，整个集群响应迟滞的情况echo "net.ipv4.tcp_abort_on_overflow = 1" &gt;&gt;/etc/sysctl.confecho "net.core.somaxconn = 2048" &gt;&gt;/etc/sysctl.conf# 降低tcp alive time，防止无效链接占用链接数echo 300 &gt;/proc/sys/net/ipv4/tcp_keepalive_time ####1.1.3 多个path.data路径的设置 使用多个IO设备（设置多个path.data）存储shards，能够增加总的存储空间并提升IO性能。另外查询时shards并行检索，IO会分散负载到各个磁盘，提高查询效率。 ElasticSearch 2.0之前的版本，可以配置多个path.data路径，但是其相当于RAID 0，每个分片（shards）的数据会分布在所有的磁盘上。如果集群中一个节点上有一块盘坏了损坏，该节点上所有的shards都会损坏了。需要恢复该节点上的所有shards。2.0.0版本之后，这个功能得到了优化：每个shards所有的数据只会在一块磁盘上面。这样即使一个节点的一块磁盘损坏了，也只是损失了该磁盘上的shards，其它磁盘上的shards安然无事。只需要恢复该块盘上的shards即可。提高了数据分布式存储的高可用。 集群升级到2.0.0版本时，旧版本一个shard分布到所有磁盘上的数据，会调整拷贝到一块盘上。 参考官方文档： https://www.elastic.co/guide/en/elasticsearch/reference/master/path-settings.html ####1.1.4 磁盘挂载选项 服务器挂载磁盘时，可以做如下性能参数调优： noatime：禁止记录访问时间戳，提高文件系统读写性能 data=writeback： 不记录data journal，提高文件系统写入性能 barrier=0：barrier保证journal先于data刷到磁盘，上面关闭了journal，这里的barrier也就没必要开启了 nobh：关闭buffer_head，防止内核打断大块数据的IO操作 1mount -o noatime,data=writeback,barrier=0,nobh /dev/sda /es_data_dir ####1.1.5 磁盘设备优化 elasticsearch对读写性能要求较高，当然磁盘性能越高越好，生产建议使用固态SSD。读写速度媲美于内存，另外SSD磁盘采用电梯调度算法，提供了更智能的请求调度算法，不需要内核去做多余的调整 。 使用SSD（PCI-E接口SSD卡/SATA接口SSD盘）通常比机械硬盘（SATA盘/SAS盘）查询速度快5~10倍。但是写入性能提升不明显。 当然一分钱一分货，SSD比SAS贵不少。 ###1.2 elasticsearch 配置文件优化 ####1.2.1 增加写入buffer和bulk队列长度 elasticsearch在处理索引的时候会存在一个索引缓冲buffer，可以为其设置需要分配的缓冲buffer内存大小，这是一个全局设置。 123# 这意味着分配给一个节点的总存储器的10％将被用作索引的缓冲区大小indices.memory.index_buffer_size: 10%thread_pool.bulk.queue_size: 1024 ####1.2.2 计算disk使用量时，不考虑正在搬迁的shard 在规模比较大的集群中，可以防止新建shard时扫描所有的shard的元数据，提升shard的分配速度。 1cluster.routing.allocation.disk.include_relocations: false ##第二部分 集群稳定性调优 ###2.1 操作系统参数调优 主要为资源限制配置，参考1.1.2章节。 ###2.2 Elasticsearch 配置 ####2.2.1 JVM虚拟机参数 Elasticsearch由java语言编写开发。在 Java 中，所有的对象都分配在堆上，并通过一个指针进行引用。 普通对象指针（OOP）指向这些对象，通常为 CPU 字长的大小：32 位或 64 位，取决于你的处理器。指针引用的就是这个 OOP 值的字节位置。 对于 32 位的系统，意味着堆内存大小最大为 4 GB。对于 64 位的系统， 可以使用更大的内存，但是 64 位的指针意味着更大的浪费，因为你的指针本身大了。更大的指针在主内存和各级缓存（例如 LLC，L1 等）之间移动数据的时候，会占用更多的带宽. 配置文件默认堆内存的大小为1G，官方建议将堆大小保持在接近32 GB（最佳建议30.5 GB阈值）。超过32G，jvm会禁用内存对象指针压缩技术，造成内存浪费。 关于堆大小的设置，elasticsearh官网有比较详细的说明： https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html -Xms和-Xmx设置为相同的值，推荐设置为：min（机器内存的一半左右，30.5G)。剩余留给系统cache使用。 另外jvm堆内存建议不要低于2G，否则有可能因为内存不足导致elasticsearch进程无法正常启动或内存溢出（OOM）。 假设你是土豪，机器有 128 GB 的内存，你可以创建两个节点，每个节点内存分配不超过 32 GB。 也就是说不超过 64 GB 内存给 elasticsearch的堆内存，剩下的超过 64 GB 的内存给 Lucene。 ####2.2.2 节点配置参数 设置内存熔断参数，防止写入或查询压力过高导致OOM，具体数值可根据使用场景调整。 123indices.breaker.total.limit: 30% indices.breaker.request.limit: 6% indices.breaker.fielddata.limit: 3% 减少查询使用的cache，避免cache占用过多的jvm内存，具体数值可根据使用场景调整。 12indices.queries.cache.count: 500 indices.queries.cache.size: 5% 单机多节点时，主从shard分配以ip为依据，分配到不同的机器上，避免单机挂掉导致数据丢失。 12cluster.routing.allocation.awareness.attributes: ip node.attr.ip: 1.1.1.1 ###2.3 集群使用方式 ####2.3.1 设置专用master节点Elasticsearch集群的元数据管理、Index的新建、删除操作、节点的加入和隔离、定期将整个集群的健康状态信息广播给各节点等管理任务，均由master节点角色负责。当集群的节点数较大时，集群的管理任务需要消耗大量资源。建议设置专用master节点，master只负责集群管理工作，不负责存储数据（无数据读写压力），提高集群管理的稳定性。 123456789# 专有master节点的配置文件（elasticsearch.yml）增加如下属性：node.master: true node.data: false node.ingest: false# 数据节点的配置文件增加如下属性（与上面的属性相反）：node.master: false node.data: true node.ingest: true 2.3.2 大内存物理机多实例部署之前章节也提到，假如你的机器有128G内存，可以创建两个Data Node实例，使用32G内存。也就是说64G内存给ES的堆内存，剩下的64G给Lucene。如果你选择第二种，你需要配置（默认是false）： 1cluster.routing.allocation.same_shard.host:true 这会防止同一个shard的主副本存在同一个物理机上（因为如果都在一个机器上，副本的高可用性就没有了） ####2.3.3 控制集群中index（索引）和shard（分片）总量 集群中master节点负责集群元数据管理，定期会同步给各节点（集群每个节点都会存储一份）。 Elasticsearch元数据存储在clusterstate中。例如所有节点元信息（indices、节点各种统计参数）、所有index/shard的元信息（mapping, location, size）、元数据ingest等。 elasticsearch在创建新shard时，要根据现有的分片分布情况指定分片分配策略，从而使各个节点上的分片数基本一致，此过程中就需要深入遍历clusterstate。当集群中的index/shard过多时，clusterstate结构会变得过于复杂，导致遍历clusterstate效率低下，集群响应迟滞。 当index/shard数量过多时，可以考虑从以下几方面优化： 降低数据量较小的index的shard数量 把一些有关联的index合并成一个index 数据按某个维度做拆分，写入多个集群 ####2.3.4 Segment Memory优化 elasticsearch底层使用Lucene实现存储，Lucene的一个index由若干segment组成，每个segment都会建立自己的倒排索引用于数据查询。Lucene为了加速查询，为每个segment的倒排做了一层前缀索引，这个索引在Lucene4.0以后采用的数据结构是FST (Finite State Transducer)。Lucene加载segment的时候将其全量装载到内存中，加快查询速度。 这部分内存被称为Segment Memory， 常驻内存，占用heap，无法被GC。 前面提到，为利用JVM的对象指针压缩技术来节约内存，通常建议JVM内存分配不要超过32G。当集群的数据量过大时，Segment Memory会吃掉大量的堆内存，而JVM内存空间有限，此时就需要想办法降低Segment Memory的使用量了。 常用方法有： 定期清理删除不使用的index 对于不常访问的index，可以通过close接口将其关闭，用到时再打开（此时数据仍然保存在磁盘，只是释放掉了内存，无法检索，需要时可以重启open） 通过force_merge api接口强制合并segment，降低segment数量，从而减少Segment Memory 12&gt; curl -XPOST "http://localhost:9200/library/_forcemerge?max_num_segments=1&gt; ###2.4 温热数据分离架构 当Elasticsearch集群用于大量实时数据分析的场景时，elasticsearch官方推荐使用基于时间的索引，并且集群使用三种不同类型的节点：Master、Hot-Node、Warm-Node，进行结构分层。即所谓的“Hot-Warm”集群架构。 参考官方文章： https://www.elastic.co/blog/hot-warm-architecture-in-elasticsearch-5-x 第三部分 业务使用调优###3.1 控制字段的存储选项 elasticsearch底层使用Lucene存储数据，主要包括行存（StoreFiled）、列存（DocValues）和倒排索引（InvertIndex）三部分。 大多数使用场景中，没有必要同时存储这三个部分，可以通过下面的参数来做适当调整： StoreFiled： 行存，其中占比最大的是source字段，它控制doc原始数据的存储。在写入数据时，elasticsearch把doc原始数据的整个json结构体当做一个string，存储为source字段。查询时，可以通过source字段拿到当初写入时的整个json结构体。 所以，如果没有取出整个原始json结构体的需求，可以通过下面的命令，在mapping中关闭source字段或者只在source中存储部分字段，数据查询时仍可通过elasticsearch的docvaluefields获取所有字段的值。 注意：关闭source后， update, updatebyquery, reindex等接口将无法正常使用，所以有update等需求的index不能关闭source。 1234# 关闭 _sourcePUT my_index &#123;"mappings": &#123;"my_type": &#123;"_source": &#123;"enabled": false&#125;&#125;&#125;&#125;# _source只存储部分字段，通过includes指定要存储的字段或者通过excludes滤除不需要的字段，例如PUT my_index&#123;"mappings": &#123;"_doc": &#123;"_source": &#123;"includes": ["*.count","meta.*"], "excludes": ["meta.description","meta.other.*"]&#125;&#125;&#125;&#125; docvalues：控制列存。 ES主要使用列存来支持sorting, aggregations和scripts功能，对于没有上述需求的字段，可以通过下面的命令关闭docvalues，降低存储成本。 1PUT my_index&#123;"mappings": &#123;"my_type": &#123;"properties": &#123;"session_id": &#123;"type": "keyword", "doc_values": false&#125;&#125;&#125;&#125;&#125; index：控制倒排索引。 ES默认对于所有字段都开启了倒排索引，用于查询。对于没有查询需求的字段，可以通过下面的命令关闭倒排索引。 1PUT my_index&#123;"mappings": &#123;"my_type": &#123;"properties": &#123;"session_id": &#123;"type": "keyword", "index": false&#125;&#125;&#125;&#125;&#125; all：ES的一个特殊的字段，ES把用户写入json的所有字段值拼接成一个字符串后，做分词，然后保存倒排索引，用于支持整个json的全文检索。 这种需求适用的场景较少，可以通过下面的命令将all字段关闭，节约存储成本和cpu开销。（ES 6.0+以上的版本不再支持_all字段，不需要设置） 1PUT /my_index&#123;"mapping": &#123;"my_type": &#123;"_all": &#123;"enabled": false&#125;&#125;&#125;&#125; fieldnames：该字段用于exists查询，来确认某个doc里面有无一个字段存在。若没有这种需求，可以将其关闭。 1PUT /my_index&#123;"mapping": &#123;"my_type": &#123;"_field_names": &#123;"enabled": false&#125;&#125;&#125;&#125; ###3.2 开启最佳压缩 对于打开了上述_source字段的index，可以通过下面的命令来把Lucene适用的压缩算法替换成 DEFLATE，提高数据压缩率。 1234PUT /my_index/_settings&#123; "index.codec": "best_compression"&#125; ###3.3 bulk批量写入 在向elasticsearch集群写入数据时，建议尽量使用bulk接口批量写入，提高写入效率。每个bulk请求的doc数量设定区间推荐为1k~1w，具体可根据业务场景选取一个适当的数量。 例如Logstash向elasticsearch集群写数时，其中batch size是一个重要的调优参数（优化的大小需要根据doc的大小和服务器性能来确定）。如果写入过程中遇到大量EsRejectedExecutionException说明elasticsearch集群索引性能已经达到极限了。 12345POST _bulk&#123; "index" : &#123; "_index" : "test", "_type" : "type1" &#125; &#125;&#123; "field1" : "value1" &#125;&#123; "index" : &#123; "_index" : "test", "_type" : "type1" &#125; &#125;&#123; "field1" : "value2" &#125; ###3.4 调整translog同步策略 默认情况下，translog的持久化策略是，对于每个写入请求都做一次flush，刷新translog数据到磁盘上。这种频繁的磁盘IO操作是严重影响写入性能的，如果可以接受一定概率的数据丢失（这种硬件故障的概率很小），可以通过下面的命令调整 translog 持久化策略为异步周期性执行，并适当调整translog的刷盘周期。 1234567891011PUT my_index&#123; "settings": &#123; "index": &#123; "translog": &#123; "sync_interval": "5s", "durability": "async" &#125; &#125; &#125;&#125; ###3.5 适当增加refresh时间间隔elasticsearch为了提高索引性能，数据的写入采用延迟写入机制。数据首先写入内存，当操作一定时间间隔（index.refresh_interval）会统一发起一次写入磁盘的操作。即将内存中的segment数据写入磁盘，也就是这时候我们才能对数据进行访问。所以严格的说elasticsearch提供的近实时检索功能，这个延时大小就是index.refresh_interval参数（默认为1秒）决定的。 默认情况下，elasticsearch每一秒会refresh一次，产生一个新的segment，这样会导致产生的segment较多，从而segment merge较为频繁，系统开销较大。如果对数据的实时检索要求较低，可以提高refresh的时间间隔，降低系统开销。 12345678PUT my_index&#123; "settings": &#123; "index": &#123; "refresh_interval" : "30s" &#125; &#125;&#125; ###3.6 merge并发控制 elasticsearch的一个index由多个shard组成，而一个shard其实就是一个Lucene的index，它又由多个segment组成，且Lucene会不断地把一些小的segment合并成一个大的segment，这个过程被称为merge。 默认值是Math.max(1, Math.min(4, Runtime.getRuntime().availableProcessors() / 2))，当节点配置的cpu核数较高时，merge占用的资源可能会偏高，影响集群的性能，可以通过下面的命令调整某个index的merge过程的并发度： 1234PUT /my_index/_settings&#123; "index.merge.scheduler.max_thread_count": 2&#125; ###3.7 写入数据不指定_id，让elasticsearch自动产生 文档唯一标识由四个元数据字段组成：_id：文档的字符串 ID;_type：文档的类型名;_index：文档所在的索引 _uid：_type 和 _id 连接成的 type#id。 当用户指定_id写入数据时，elasticsearch会先发起查询来确定index中是否已经有相同id的doc存在，若有则先删除原有doc再写入新doc。这样每次写入时，都会耗费一定的资源做查询。如果用户写入数据时不指定id，则通过内部算法产生一个随机的id，并且保证id的唯一性，这样就可以跳过前面查询id的步骤，提高写入效率。 所以，在不需要通过id字段去重、update的使用场景中，写入不指定id可以提升写入速率。 自动生成的id，长度为20个字符，URL安全，base64编码，GUID，分布式系统并行生成时不可能会发生冲突。 12345678# 写入时指定_id，这里_id=1POST _bulk&#123; "index" : &#123; "_index" : "test", "_type" : "type1", "_id" : "1" &#125; &#125;&#123; "field1" : "value1" &#125;# 写入时不指定_idPOST _bulk&#123; "index" : &#123; "_index" : "test", "_type" : "type1" &#125; &#125;&#123; "field1" : "value1" &#125; ###3.8 使用routing Elasticsearch的路由机制与其分片机制直接相关。路由即通过哈希算法，将具有相同哈希值的文档放置到同一个主分片中。这里机制原理和通过哈希算法实现数据库负载均衡原理是相同的。 Elasticsearch有一个默认的路由算法：它会将文档的ID值作为依据将其哈希到相应的主分片上，这种算法基本上会保持所有数据在所有分片上的一个平均分布，而不会产生数据热点。 默认的Routing模式在很多情况下都是能满足需求的。但是在我们更深入地理解我们的数据的特征之后，使用自定义的Routing模式可能会带来更好的性能。 所有的文档API（get，index，delete，update和mget）都能接收一个routing参数，可以用来形成个性化文档分片映射。一个个性化的routing值可以确保相关的文档存储到同样的分片上。 参考官方文档： https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-routing-field.html 对于数据量较大的index，一般会配置多个shard来分摊压力。这种场景下，一个查询会同时搜索所有的shard，然后再将各个shard的结果合并后，返回给用户。具体过程如下： 查询的请求会被发送到一个节点 接收到这个请求的节点，将这个查询广播到这个索引的每个分片上（可能是主分片，也可能是复制分片） 每个分片执行这个搜索查询并返回结果 结果在通道节点上合并、排序并返回给用户 对于高并发的小查询场景，每个分片通常仅抓取极少量数据，此时查询过程中的调度开销远大于实际读取数据的开销，且查询速度取决于最慢的一个分片。 开启routing功能后，会将routing相同的数据写入到同一个分片中（也可以是多个，由index.routingpartitionsize参数控制）。如果查询时指定routing，那么只会查询routing指向的那个分片，可显著降低调度开销，提升查询效率。 routing的使用方式如下： 123456789101112131415# 写入PUT my_index/my_type/1?routing=user1&#123; "title": "This is a document"&#125;# 查询GET my_index/_search?routing=user1,user2 &#123; "query": &#123; "match": &#123; "title": "document" &#125; &#125;&#125; ###3.9 为string类型的字段选取合适的存储方式 存为text类型的字段（string字段默认类型为text）： 做分词后存储倒排索引，支持全文检索，可以通过下面几个参数优化其存储方式： norms：用于在搜索时计算该doc的_score（代表这条数据与搜索条件的相关度），如果不需要评分，可以将其关闭。 indexoptions：控制倒排索引中包括哪些信息（docs、freqs、positions、offsets）。对于不太注重score/highlighting的使用场景，可以设为 docs来降低内存/磁盘资源消耗。 fields: 用于添加子字段。对于有sort和聚合查询需求的场景，可以添加一个keyword子字段以支持这两种功能。 12345678910111213141516171819PUT my_index&#123; "mappings": &#123; "my_type": &#123; "properties": &#123; "title": &#123; "type": "text", "norms": false, "index_options": "docs", "fields": &#123; "raw": &#123; "type": "keyword" &#125; &#125; &#125; &#125; &#125; &#125;&#125; 存为keyword类型的字段： 不做分词，不支持全文检索。text分词消耗CPU资源，冗余存储keyword子字段占用存储空间。如果没有全文索引需求，只是要通过整个字段做搜索，可以设置该字段的类型为keyword，提升写入速率，降低存储成本。 设置字段类型的方法有两种：一是创建一个具体的index时，指定字段的类型；二是通过创建template，控制某一类index的字段类型。 123456789101112131415161718192021222324252627282930313233343536# 1. 通过mapping指定 tags 字段为keyword类型PUT my_index&#123; "mappings": &#123; "my_type": &#123; "properties": &#123; "tags": &#123; "type": "keyword" &#125; &#125; &#125; &#125;&#125;# 2. 通过template，指定my_index*类的index，其所有string字段默认为keyword类型PUT _template/my_template&#123; "order": 0, "template": "my_index*", "mappings": &#123; "_default_": &#123; "dynamic_templates": [ &#123; "strings": &#123; "match_mapping_type": "string", "mapping": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125; ] &#125; &#125;, "aliases": &#123;&#125; &#125; ###3.10 查询时，使用query-bool-filter组合取代普通query 默认情况下，ES通过一定的算法计算返回的每条数据与查询语句的相关度，并通过score字段来表征。但对于非全文索引的使用场景，用户并不care查询结果与查询条件的相关度，只是想精确的查找目标数据。此时，可以通过query-bool-filter组合来让ES不计算score，并且尽可能的缓存filter的结果集，供后续包含相同filter的查询使用，提高查询效率。 12345678910111213141516171819# 普通查询POST my_index/_search&#123; "query": &#123; "term" : &#123; "user" : "Kimchy" &#125; &#125;&#125;# query-bool-filter 加速查询POST my_index/_search&#123; "query": &#123; "bool": &#123; "filter": &#123; "term": &#123; "user": "Kimchy" &#125; &#125; &#125; &#125;&#125; ###3.11 index命名按日期滚动 写入elasticsearch的数据最好通过某种方式分割，存入不同的index。常见的做法是将数据按模块、功能分类，写入不同的index，然后按照时间去滚动生成index。这样做的好处是各种数据分开管理不会混淆，也易于提高查询效率。同时index按时间滚动，数据过期时删除整个index，要比一条条删除数据效率高很多，因为删除整个index是直接删除底层文件，而delete by query是查询-标记-删除。 举例说明，假如有两个source1和source2两个数据生产者产生的数据，那么index规划可以是这样的：一类index名称是source1 + {日期}，另一类index名称是source2+ {日期}。 对于名字中的日期，可以在写入数据时自己指定精确的日期，也可以通过elasticsearch的ingest pipeline中的index-name-processor实现（会有写入性能损耗）。参考官方文档： https://www.elastic.co/guide/en/elasticsearch/reference/5.6/date-index-name-processor.html 123456789101112131415161718192021222324# module_a 类index- 创建index：PUT module_a@2018_01_01&#123; "settings" : &#123; "index" : &#123; "number_of_shards" : 3, "number_of_replicas" : 2 &#125; &#125;&#125;PUT module_a@2018_01_02&#123; "settings" : &#123; "index" : &#123; "number_of_shards" : 3, "number_of_replicas" : 2 &#125; &#125;&#125;...- 查询数据：GET module_a@*/_search ###3.12 按需控制index的分片数和副本数 分片（shard）：Elasticsearch中的一个索引（index）由多个分片（shard）组成，每个shard承载index的一部分数据。 副本（replica）：index也可以设定副本数（numberofreplicas），也就是同一个shard有多少个备份。对于查询压力较大的index，可以考虑提高副本数（numberofreplicas），通过多个副本均摊查询压力。 Elasticsearch默认副本数量为3，能提高集群的业务检索性能，但是会影响索引的写入性能（写入过程需要等待所有副本写完，才算完成一次更新写入）。 shard数量设置过多或过低都会引发一些问题： shard数量过多，则批量写入、查询请求被分割为过多的子写入、查询，导致该index的写入、查询拒绝率上升。 对于数据量较大的index，当其shard数量过小时，无法充分利用节点资源，造成机器资源利用率不高或不均衡，影响写入、查询的效率。 对于每个index的shard数量，可以根据数据总量、写入压力、节点数量等综合考量后设定，然后根据数据增长状态定期检测下shard数量是否合理，推荐方案是： 对于数据量较小（100GB以下）的index，往往写入压力查询压力相对较低，一般设置3~5个shard，副本设置为1（也就是一主一从） 。 对于数据量较大（100GB以上）的index： 一般把单个shard的数据量控制在（20GB~50GB） 让index压力分摊至多个节点：可通过index.routing.allocation.totalshardsper_node参数，强制限定一个节点上该index的shard数量，让shard尽量分配到不同节点上 综合考虑整个index的shard数量，如果shard数量（不包括副本）超过50个，就很可能引发拒绝率上升的问题，此时可考虑把该index拆分为多个独立的index，分摊数据量，同时配合routing使用，降低每个查询需要访问的shard数量。 所以有一种优化建议：在index创建写入过程中将副本数设为0，待index完成后将副本数按需量改回来，这样也可以提高效率。 3.13 增加负载均衡（查询）节点负载均衡节点需要如下配置： 12node.master: falsenode.data: false 该节点服务器即不会被选作主节点，也不会存储任何索引数据，节点只能处理路由请求，处理搜索，分发索引操作等，从本质上来说该客户节点主要作用是负载均衡。 在查询的时候，通常会涉及到从多个节点服务器上查询数据，并请求分发到多个指定的节点服务器，并对各个节点服务器返回的结果进行一个汇总处理，最终返回给客户端。 性能方面对该节点要求是内存越大越好。 ###3.14 关闭data节点服务器中的http功能 针对ElasticSearch集群中的所有data节点，不用开启http服务。将其中的配置参数这样设置： 1http.enabled: false 同时不安装head, marvel等监控插件，这样保证data节点服务器只需处理创建/更新/删除/查询索引数据等操作。http功能可以在非数据节点服务器上开启，上述相关的监控插件也安装到这些服务器上，用于监控ElasticSearch集群状态等数据信息。这样做一来出于数据安全考虑，二来出于服务性能考虑。 ###3.15 调整集群热数据分布 集群在分配数据分片的时候，如果不指定路由，分片是均匀分派到各个节点上。当节点上存在太多热点数据分片时候，我们可以通过API手动进行调整分片的存储位置。 下面的API中，indexname：需要移动的索引名称，shard：分片编号，from_node:分片的原归属节点名称，to_node:分片的移动目的节点名称。 123456curl -XPOST 'http://localhost:9200/_cluster/reroute' -d '&#123;"commands" : [&#123;"move" :&#123;"index" : "indexname", "shard" : 1,"from_node" : "nodename", "to_node" : "nodename"&#125;&#125;]&#125;' ##总结 Elasticsearch 的性能取决于很多因素，包括文档结构、文档大小、索引设置 / 映射、请求率、数据集大小和查询命中次数等等。每个优化策略都自身的优化需求场景，没有技术银弹。因此，针对需求场景进行性能测试、收集数据，然后调优非常重要。 ##参考文献 1、A Heap of Trouble: Managing Elasticsearch’s Managed Heap 链接：https://www.elastic.co/blog/a-heap-of-trouble 2、elasticsearch官方网站 链接：https://www.elastic.co/ 3、《深入理解Elasticsearch》]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker Compose使用介绍]]></title>
    <url>%2F2019%2F01%2F27%2F2020-10-03-Docker%E7%B3%BB%E7%BB%9F%E6%96%87%E7%AB%A0-Docker%20compose%E7%BC%96%E6%8E%92Docker%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Docker Compose的安装 第二部分 Docker Compose的命令介绍 第三部分 模板文件 总结 背景Docker Compose是Docker开源项目（项目在github上的地址为： https://github.com/docker/compose）。功能上实现对Docker容器集群的资源定义和快速编排（官网上对项目功能的简述：Defining and running multi-container Docker applications）。 对于单独一个容器，我们通常使用Dockerfile模板文件进行定义，而实际应用场景中我们需要定义和管理多个容器。Compose提供用户使用docker-compose.yml文件定义一组相关联的容器集群。项目使用Python语言编写，实现上，调用Docker提供的API对容器进行管理。 Docker Compose中有两个重要的概念： 服务（service）：一个应用容器； 项目（project）：一组相关联的应用容器组成的整体； Docker Compose默认管理的对象是项目。通过命令对项目中的容器进行生命周期管理。 第一部分 Docker Compose的安装Docker Compose支持Linux、MAC、WIN10三个平台。由于项目是Python编写，所以安装方式上支持： pip安装工具安装 使用编译好的二进制文件安装 其中Docker for Mac 、Docker for Windows 自带 docker-compose 二进制文件，无需单独安装，安装 Docker 之后可以直接使用。所以主要介绍Linux系统安装部署。 1.1 pip安装这种安装方式将Compose当作一个Python应用使用pip命令安装，需要提前部署Python环境。安装命令： 1# pip install -U docker-compose 建议ARM架构（比如树莓派）的Linux使用pip源安装，对于X86_64架构的Linux建议使用二进制包安装方式。 1.2 二进制包安装从Compose在Github上项目网站下载编译好的二进制文件到本地文件系统即可。 1234567# curl -L https://github.com/docker/compose/releases/download/1.17.1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 617 0 617 0 0 617 0 --:--:-- --:--:-- --:--:-- 5016100 8648k 100 8648k 0 0 8648k 0 0:00:01 --:--:-- 0:00:01 8648k# chmod +x /usr/local/bin/docker-compose 对于卸载，如果是pip安装直接通过pip uninstall卸载，对于二进制文件安装，直接删除相关文件即可。 验证安装（查看版本号）： 12# docker-compose --versiondocker-compose version 1.16.1, build 6d1ac21 第二部分 Docker Compose的模板Docker Compose的核心思想是文件定义资源。默认的模板文件名称为 docker-compose.yml，格式为 YAML 格式。下面是一个模板案例： 12345678version: "3"services: webapp: image: examples/web ports: - "80:80" volumes: - "/data" 注意每个服务都必须通过 image 指令指定镜像或 build 指令（需要 Dockerfile）等来自动构建生成镜像。下面分别介绍各个指令的用法。 2.1 镜像容器类image指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试从远端镜像服务拉取这个镜像。 1234image: ubuntuimage: orchardup/postgresql# 支持使用images idimage: a4bc65fd build服务除了可以基于现有的镜像，还可以基于指定的Dockerfile。指定 Dockerfile 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）。 Compose 将会利用它自动构建这个镜像，然后使用这个镜像。 1234version: '3'services: webapp: build: ./dir 另外支持使用 context 指令指定 Dockerfile 所在文件夹的路径。使用 dockerfile 指令指定 Dockerfile 文件名。使用 arg 指令指定构建镜像时的变量。 123456789version: '3'services: webapp: build: context: ./dir dockerfile: Dockerfile-alternate args: buildno: 1 使用 cache_from 指定构建镜像的缓存： 12345build: context: . cache_from: - alpine:latest - corp/web_app:3.14 container_name指定容器名称。 1container_name: docker-web-container 注意: 指定容器名称后，该服务将无法进行扩展（scale），因为 Docker 不允许多个容器具有相同的名称。 labels为容器添加 Docker 元数据（metadata）信息。例如可以为容器添加辅助说明信息。 1234labels: com.startupteam.description: "webapp for a startup team" com.startupteam.department: "devops department" com.startupteam.release: "rc3 for v1.0" cap_add, cap_drop指定容器的内核能力（capacity）分配。例如，让容器拥有所有能力可以指定为： 12cap_add: - ALL 去掉 NET_ADMIN 能力可以指定为： 12cap_drop: - NET_ADMIN 2.2 网络资源类命令expose暴露端口，但不映射到宿主机，只被连接的服务访问。仅可以指定内部端口为参数。 123expose: - "3000" - "8000" network_mode设置网络模式。使用和 docker run 的 --network 参数一样的值。 12345network_mode: "bridge"network_mode: "host"network_mode: "none"network_mode: "service:[service name]"network_mode: "container:[container name/id]" networks配置容器连接的网络。 1234567891011version: "3"services: some-service: networks: - some-network - other-networknetworks: some-network: other-network: dns自定义 DNS 服务器。可以是一个值，也可以是一个列表。 12345dns: 8.8.8.8dns: - 8.8.8.8 - 114.114.114.114 dns_search配置 DNS 搜索域。可以是一个值，也可以是一个列表。 12345dns_search: example.comdns_search: - domain1.example.com - domain2.example.com ports暴露端口信息。使用宿主端口：容器端口 (HOST:CONTAINER) 格式，或者仅仅指定容器的端口（宿主将会随机选择端口）都可以。 12345ports: - "3000" - "8000:8000" - "49100:22" - "127.0.0.1:8001:8001" *注意：当使用 HOST:CONTAINER 格式来映射端口时，如果你使用的容器端口小于 60 并且没放到引号里，可能会得到错误结果，因为 YAML 会自动解析 xx:yy 这种数字格式为 60 进制。为避免出现这种问题，建议数字串都采用引号包括起来的字符串格式。 extra_hosts类似 Docker 中的 --add-host 参数，指定额外的 host 名称映射信息。 123extra_hosts: - "googledns:8.8.8.8" - "dockerhub:52.1.157.61" 会在启动后的服务容器中 /etc/hosts 文件中添加如下两条条目。 128.8.8.8 googledns52.1.157.61 dockerhub 2.3 系统资源类命令devices指定设备映射关系。 12devices: - "/dev/ttyUSB1:/dev/ttyUSB0" tmpfs挂载一个 tmpfs 文件系统到容器。 12345tmpfs: /runtmpfs: - /run - /tmp sysctls配置容器内核参数。 1234567sysctls: net.core.somaxconn: 1024 net.ipv4.tcp_syncookies: 0sysctls: - net.core.somaxconn=1024 - net.ipv4.tcp_syncookies=0 ulimits指定容器的 ulimits 限制值。例如，指定最大进程数为 65535，指定文件句柄数为 20000（软限制，应用可以随时修改，不能超过硬限制） 和 40000（系统硬限制，只能 root 用户提高）。 12345ulimits: nproc: 65535 nofile: soft: 20000 hard: 40000 volumes数据卷所挂载路径设置。可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）。该指令中路径支持相对路径。 1234volumes: - /var/lib/mysql - cache/:/tmp/cache - ~/configs:/etc/configs/:ro 2.4 调度管理类depends_on解决容器的依赖、启动先后的问题，比如让数据库内容器先运行起来。以下例子中会先启动 redis db 再启动 web 1234567891011121314version: '3'services: web: build: . depends_on: - db - redis redis: image: redis db: image: postgres 注意：web 服务不会等待 redis db 「完全启动」之后才启动。 pid跟主机系统共享进程命名空间。打开该选项的容器之间，以及容器和宿主机系统之间可以通过进程 ID 来相互访问和操作。 1pid: "host" command覆盖容器启动后默认执行的命令。 1command: echo "hello world" cgroup_parent指定父 cgroup 组，意味着将继承该组的资源限制。例如，创建了一个 cgroup 组名称为 cgroups_1。 1cgroup_parent: cgroups_1 stop_signal设置另一个信号来停止容器。在默认情况下使用的是 SIGTERM 停止容器。 1stop_signal: SIGUSR1 2.5 安全类secrets存储敏感数据，例如 mysql 服务密码。 12345678910111213141516version: "3.1"services:mysql: image: mysql environment: MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password secrets: - db_root_password - my_other_secretsecrets: my_secret: file: ./my_secret.txt my_other_secret: external: true security_opt指定容器模板标签（label）机制的默认属性（用户、角色、类型、级别等）。例如配置标签的用户名和角色名。 123security_opt: - label:user:USER - label:role:ROLE 2.6 日志和监控类healthcheck通过命令检查容器是否健康运行。 12345healthcheck: test: ["CMD", "curl", "-f", "http://localhost"] interval: 1m30s timeout: 10s retries: 3 logging配置日志选项。 1234logging: driver: syslog options: syslog-address: "tcp://192.168.0.42:123" 目前支持三种日志驱动类型。 123driver: "json-file"driver: "syslog"driver: "none" options 配置日志驱动的相关参数。 123options: max-size: "200k" max-file: "10" configs仅用于 Swarm mode，详细内容请查看 Swarm mode 一节。 deploy仅用于 Swarm mode，详细内容请查看 Swarm mode 一节 env_file从文件中获取环境变量，可以为单独的文件路径或列表。 如果通过 docker-compose -f FILE 方式来指定 Compose 模板文件，则 env_file中变量的路径会基于模板文件路径。 如果有变量名称与 environment 指令冲突，则按照惯例，以后者为准。 123456env_file: .envenv_file: - ./common.env - ./apps/web.env - /opt/secrets.env 环境变量文件中每一行必须符合格式，支持 # 开头的注释行。 12# common.env: Set development environmentPROG_ENV=development environment设置环境变量。你可以使用数组或字典两种格式。只给定名称的变量会自动获取运行 Compose 主机上对应变量的值，可以用来防止泄露不必要的数据。 1234567environment: RACK_ENV: development SESSION_SECRET:environment: - RACK_ENV=development - SESSION_SECRET 如果变量名称或者值中用到 true|false，yes|no 等表达布尔含义的词汇，最好放到引号里，避免 YAML 自动解析某些内容为对应的布尔语义。这些特定词汇，包括 1y|Y|yes|Yes|YES|n|N|no|No|NO|true|True|TRUE|false|False|FALSE|on|On|ON|off|Off|OFF external_links 注意：不建议使用该指令。 链接到 docker-compose.yml 外部的容器，甚至并非 Compose 管理的外部容器。 1234external_links: - redis_1 - project_db_1:mysql - project_db_1:postgresql links 注意：不推荐使用该指令。 其它指令此外，还有包括 domainname, entrypoint, hostname, ipc, mac_address, privileged, read_only, shm_size, restart, stdin_open, tty, user, working_dir等指令，基本跟 docker run 中对应参数的功能一致。 指定服务容器启动后执行的入口文件。 1entrypoint: /code/entrypoint.sh 指定容器中运行应用的用户名。 1user: nginx 指定容器中工作目录。 1working_dir: /code 指定容器中搜索域名、主机名、mac 地址等。 123domainname: your_website.comhostname: testmac_address: 08-00-27-00-0C-0A 允许容器中运行一些特权命令。 1privileged: true 指定容器退出后的重启策略为始终重启。该命令对保持服务始终运行十分有效，在生产环境中推荐配置为 always 或者 unless-stopped。 1restart: always 以只读模式挂载容器的 root 文件系统，意味着不能对容器内容进行修改。 1read_only: true 打开标准输入，可以接受外部输入。 1stdin_open: true 模拟一个伪终端。 1tty: true 读取变量Compose 模板文件支持动态读取主机的系统环境变量和当前目录下的 .env 文件中的变量。 例如，下面的 Compose 文件将从运行它的环境中读取变量 ${MONGO_VERSION} 的值，并写入执行的指令中。 12345version: "3"services:db: image: "mongo:$&#123;MONGO_VERSION&#125;" 如果执行 MONGO_VERSION=3.2 docker-compose up 则会启动一个 mongo:3.2 镜像的容器；如果执行 MONGO_VERSION=2.8 docker-compose up 则会启动一个 mongo:2.8镜像的容器。 若当前目录存在 .env 文件，执行 docker-compose 命令时将从该文件中读取变量。 在当前目录新建 .env 文件并写入以下内容。 12# 支持 # 号注释MONGO_VERSION=3.6 执行 docker-compose up 则会启动一个 mongo:3.6 镜像的容器。 第三部分 案例下面是一个启动Elasticsearch集群的docker compose资源定义文件，我们使用注释进行介绍。 一共定义了5个服务容器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495version: '3'services: cerebro: image: lmenezes/cerebro:0.8.3 container_name: hwc_cerebro ports: - "9000:9000" command: - -Dhosts.0.host=http://elasticsearch:9200 networks: - hwc_es7net kibana: image: docker.elastic.co/kibana/kibana:7.1.0 container_name: hwc_kibana7 environment: #- I18N_LOCALE=zh-CN - XPACK_GRAPH_ENABLED=true - TIMELION_ENABLED=true - XPACK_MONITORING_COLLECTION_ENABLED="true" ports: - "5601:5601" networks: - hwc_es7net elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.1.0 container_name: es7_hot environment: - cluster.name=elasticsearch - node.name=es7_hot - node.attr.box_type=hot - bootstrap.memory_lock=true - "ES_JAVA_OPTS=-Xms1g -Xmx1g" - discovery.seed_hosts=es7_hot,es7_warm,es7_cold - cluster.initial_master_nodes=es7_hot,es7_warm,es7_cold ulimits: memlock: soft: -1 hard: -1 volumes: - hwc_es7data_hot:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - hwc_es7net elasticsearch2: image: docker.elastic.co/elasticsearch/elasticsearch:7.1.0 container_name: es7_warm environment: - cluster.name=elasticsearch - node.name=es7_warm - node.attr.box_type=warm - bootstrap.memory_lock=true - "ES_JAVA_OPTS=-Xms1g -Xmx1g" - discovery.seed_hosts=es7_hot,es7_warm,es7_cold - cluster.initial_master_nodes=es7_hot,es7_warm,es7_cold ulimits: memlock: soft: -1 hard: -1 volumes: - hwc_es7data_warm:/usr/share/elasticsearch/data networks: - hwc_es7net elasticsearch3: image: docker.elastic.co/elasticsearch/elasticsearch:7.1.0 container_name: es7_cold environment: - cluster.name=elasticsearch - node.name=es7_cold - node.attr.box_type=cold - bootstrap.memory_lock=true - "ES_JAVA_OPTS=-Xms1g -Xmx1g" - discovery.seed_hosts=es7_hot,es7_warm,es7_cold - cluster.initial_master_nodes=es7_hot,es7_warm,es7_cold ulimits: memlock: soft: -1 hard: -1 volumes: - hwc_es7data_cold:/usr/share/elasticsearch/data networks: - hwc_es7netvolumes: hwc_es7data_hot: driver: local hwc_es7data_warm: driver: local hwc_es7data_cold: driver: localnetworks: hwc_es7net: driver: bridge 参考文献项目文档：https://yeasy.gitbooks.io/docker_practice/compose/compose_file.html Docker Compose 配置文件详解 https://www.jianshu.com/p/2217cfed29d7 Docker：Docker Compose 详解 https://www.jianshu.com/p/658911a8cff3]]></content>
      <categories>
        <category>docker compose</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch温热集群架构]]></title>
    <url>%2F2019%2F01%2F27%2F2019-01-27-Elasticsearch%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E5%86%B7%E7%83%AD%E8%8A%82%E7%82%B9%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 温热集群整体架构 第二部分 架构配置 第三部分 架构维护相关 第四部分 基于hot-warm架构的读写分离实现 总结 参考文献及资料 背景Elasticsearch集群通常有下面两种常见组织架构： 同质集群架构。即所有数据节点会负载所有的index存储和查询，节点配置均相同。 温热集群架构。此架构中数据节点有两种类型：温节点、热节点。 特别针对日志型数据和时间序列指标类数据处理，温热架构较为常见。使用该架构通常有这样的前提：数据通常是不可更改的。每个index仅包含特定时间段的数据，用户可通过删除整个index来管理数据生命周期。 当Elasticsearch用于大量实时数据分析的场景时，推荐使用基于时间的索引然后使用三种不同类型的节点（Master, Hot-Node 和 Warm-Node）进行结构分层。 第一部分 温热集群整体架构1.1 Master 节点通常推荐每个集群运行三个专用的Master节点来提供最好的弹性。三个专用的Master节点，专门负责处理集群的管理以及加强状态的整体稳定性。Master节点不存储实际数据、不实际参与搜索以及索引操作。因此不太可能会因为垃圾回收而导致停顿。所以在服务器资源规划上，Master节点的CPU，内存以及磁盘配置可以比Data节点少很多。 建议将 discovery.zen.minimum_master_nodes setting 参数设置为2，避免集群出现脑裂。 1.2 Hot 节点Hot节点会完成集群内所有的索引工作。这些节点同时还会保存近期的一些频繁被查询的索引。由于进行索引非常耗费CPU和I/O，因此这些节点的服务器建议配置SSD存储来支撑。 1.3 Warm 节点这种类型的节点是为了处理大量的而且不经常访问的只读索引而设计的。由于这些索引是只读的，Warm节点倾向于挂载大量磁盘（普通磁盘）来替代SSD。但是CPU和内存配置建议和Hot节点保持一致。 第二部分 架构配置Hot-Warm架构实现主要有以下几点： 有3台机器作为master节点 有若干台实例tag设置为hot（hot节点），有若干实例tag设置为warm（warm节点） hot节点中只存最近查询需求较高的索引 设定定时任务每天将前一天的索引标记为warm elasticsearch根据tag将hot数据迁移到warm 2.1 实例配置2.1.1 索引级路由配置实例标签设置在 elasticsearch.yml 配置文件中，参数名称为： node.attr.box_type 1234#设置该节点的tag为hotnode.attr.box_type：hot# 设置该节点的tag为warmnode.attr.box_type：warm 也可以在启动实例时使用 ./bin/elasticsearch -Enode.attr.box_type=hot 参数指定 另外box_type名称也是自定义的，例如也可以为：zone 通过以下配置（索引路由分布策略）创建索引，索引分片只会分配写入hot实例节点。 123456PUT /logs_2016-12-26&#123; "settings": &#123; "index.routing.allocation.require.box_type": "hot" &#125;&#125; 当索引不再需要大量检索时，可以将索引迁移到warm实例节点。通过API更新索引配置如下： 123456PUT /logs_2016-12-26/_settings &#123; "settings": &#123; "index.routing.allocation.require.box_type": "warm" &#125; &#125; 2.1.2 集群路由分布策略设置对于一个物理机上多个elasticsearch实例的场景，多个实例可能是hot和warm节点。这是就会存在索引主从分片分配在同一个物理机上，这不满足数据的高可用。所以需要设置集群路由分布策略。 12"cluster.routing.allocation.awareness.attributes": "box_type""cluster.routing.allocation.awareness.force.box_type.values": "hot,warm" 集群路由分布策略高于索引级路由策略 2.2 模板设置在hot-warm架构下，需要对索引模板（index template）进行设置。如果索引模板在logstash或者beats中管理，那么索引模板需要做一些更新，包括分配过滤器。”index.routing.allocation.require.box_type” : “hot” 这个配置会使新的索引创建在Hot节点上。例如： 12345&#123; "template" : "indexname-*", "version" : 50001, "settings" : &#123; "index.routing.allocation.require.box_type": "hot" 或者给集群中的所有索引设置一个普通模板，所有新增索引首先写入hot实例节点： 12345&#123; "template" : "*", "version" : 50001, "settings" : &#123; "index.routing.allocation.require.box_type": "hot" 后续可以通过更新它的索引配置：”index.routing.allocation.require.box_type” : “warm”，完成迁移。 2.3 其他优化设置warm节点开启压缩配置。在elasticsearch.yml配置文件中： 1index.codec: best_compression warm节点合并分段。当数据迁移到Warm节点后，可以调用 _forcemerge API 来合并分段。可以节约内存, 磁盘空间以及更少的文件句柄。 第三部分 架构维护相关在日常集群维护中，我们希望重复的维护工作，能通过工具或自动化手段完成。例如如何使用Curator这个工具来自动处理这些事情。在Elasticsearch6.8版本开始，可以通过索引生命周期管理（ILM）模块完成这些自动化管理。 下面的例子中我们使用curator 4.2从Hot节点移动三天前的索引到Warm节点： 12345678910111213141516171819202122actions: 1: action: allocation description: "Apply shard allocation filtering rules to the specified indices" options: key: box_type value: warm allocation_type: require wait_for_completion: true timeout_override: continue_if_exception: false disable_action: false filters: - filtertype: pattern kind: prefix value: logstash- - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 3 最后我们可以使用curator来强制合并索引。执行优化之前要确保等待足够长的时间进行索引重新分配。你可以设置操作1中 wait_for_completion，或者修改操作2中的 unit_count 来选择4天前的索引.这样就有机会在强制合并之前完全合并。 123456789101112131415161718192: action: forcemerge description: "Perform a forceMerge on selected indices to 'max_num_segments' per shard" options: max_num_segments: 1 delay: timeout_override: 21600 continue_if_exception: false disable_action: false filters: - filtertype: pattern kind: prefix value: logstash- - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 3 注意 timeout_override 要比默认值 21600 秒大，不过它可能会更快或者慢一点，这取决于你的配置。 从Elasticsearch 5.0开始我们还可以使用 Rollover 和 shrink api 来减少分片数量，可以以更简单高效的方式来管理基于时间的索引。你可以在这个博客中找到更多细节。 3.2 数据温热迁移shell脚本日常运维中可以定时执行shell脚本完成hot数据的迁移。 12345678910111213#!/bin/bashTime=$(date -d "1 week ago" +"%Y.%m.%d")Hostname=$(hostname)arr=("cron" "messages" "secure" "tomcat" "nginx-access" "nginxtcp" "nginxerror" "windows" ".monitoring-es-6" ".monitoring-beats-6" ".monitoring-kibana-6" ".monitoring-logstash-6" "metricbeat-6.5.3")for var in $&#123;arr[@]&#125;do curl -H "Content-Type: application/json" -XPUT http://$Hostname:9200/$var-$Time/_settings?pretty -d' &#123; "settings": &#123; "index.routing.allocation.require.box_type": "cold" &#125; &#125;'done 3.3 warm数据的清理对于warm节点的数据，可以用下面的shell脚本完成历史数据的定期清理。 123456#! /bin/bash# 清理7天前的indexecho "Begin @ `date +%Y%m%d%H%M%S`"d7=$(date +%Y.%m.%d --date '7 days ago')curl -XDELETE "http://192.168.1.30:9200/logstash-$d7?pretty"echo "End @ `date +%Y%m%d%H%M%S`" 第四部分 基于hot-warm架构的读写分离实现目标：使主分片分配在SSD磁盘上，副本落在SATA磁盘上，读取时优先从副本中查询数据，SSD节点只负责写入数据。 实现步骤： 修改集群路由分配策略配置 增加集群路由配置 12"allocation.awareness.attributes": "box_type","allocation.awareness.force.box_type.values": "hot,cool" 提前创建索引 提前创建下一天的索引，索引配置如下（可写入模板中）： 1234567PUT log4x_trace_2018_08_11&#123; "settings": &#123; "index.routing.allocation.require.box_type": "hot", "number_of_replicas": 0&#125;&#125; 此操作可使索引所有分片都分配在SSD磁盘中。 修改索引路由分配策略配置 索引创建好后，动态修改索引配置 12345PUT log4x_trace_2018_08_11/_settings&#123; "index.routing.allocation.require.box_type": null, "number_of_replicas": 1&#125; 4、转为冷数据 动态修改索引配置，并取消副本数 12345PUT log4x_trace_2018_08_11/_settings&#123; "index.routing.allocation.require.box_type": "cool", "number_of_replicas": 0&#125; 总结参考文献和资料1、Elasticsearch 主节点和暖热节点 https://dongbo0737.github.io/2017/06/06/elasticsearch-hot-warm/]]></content>
      <categories>
        <category>ElasticSearch，hot-warm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hive介绍及部署]]></title>
    <url>%2F2018%2F08%2F14%2F2018-08-13-Hive%E4%BB%8B%E7%BB%8D%E5%8F%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[背景Hive（蜂巢）是Hadoop的组件，官方介绍为： Hive™: A data warehouse infrastructure that provides data summarization and ad hoc querying. Hive有三种部署方式（本质是Hive Metastore的三种部署方式）： Embedded Metastore Database (Derby) 内嵌模式 内嵌模式使用的是内嵌的Derby数据库来存储元数据，也不需要额外起Metastore服务。这个是默认的，配置简单，但是一次只能一个客户端连接（Derby只提供单进程存储），适用于用来实验，不适用于生产环境。 Local Metastore Server 本地元存储 采用外部数据库来存储元数据 。本地元存储不需要单独起metastore服务，用的是跟hive在同一个进程里的metastore服务 。 目前支持：Derby，Mysql，微软SQLServer，Oracle和Postgres Remote Metastore Server 远程元存储 采用外部数据库来存储元数据 。远程元存储需要单独起metastore服务，然后每个客户端都在配置文件里配置连接到该metastore服务。远程元存储的metastore服务和hive运行在不同的进程里。 远程元存储是生产环境部署方式。 本地部署过程 由于设备资源限制，没有太多机器配置类似生产环境的集群环境。所以通过docker搭建大集群环境。 搭建目标： 集群中hadoop集群由3台构成（1台master，2台slaves） Hive的元数据库使用Mysql，并且单独包裹在一个docker环境中。 环境准备准备hadoop集群环境。启docker集群： 1234CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc27312e13270 kiwenlau/hadoop:1.0 "sh -c 'service ssh …" 2 hours ago Up 2 hours hadoop-slave2f8b69885f3ef kiwenlau/hadoop:1.0 "sh -c 'service ssh …" 2 hours ago Up 2 hours hadoop-slave1439b359d230e kiwenlau/hadoop:1.0 "sh -c 'service ssh …" 2 hours ago Up 2 hours 0.0.0.0:8088-&gt;8088/tcp, 0.0.0.0:50070-&gt;50070/tcp hadoop-master Hive部署下载安装包：1234# 进入hadoop-master主机，进入hadoop目录：/use/local/hadoopwget http://apache.claz.org/hive/hive-2.3.3/apache-hive-2.3.3-bin.tar.gztar -zxvf apache-hive-2.3.3-bin.tar.gzmv apache-hive-2.3.3-bin hive 配置Hive环境变量：123456vi /etc/profile#hiveexport HIVE_HOME=/usr/local/hadoop/hivePATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$PATH# 生效环境变量source /etc/profile 调整Hive的配置文件：12345# 进入hive 配置文件目录：cd confcp hive-default.xml.template hive-site.xml# 修改配置文件vim hive-site.xml 新建HDFS分布式文件目录：12345678# hadoop已经设置好环境变量，新建下面目录hadoop fs -mkdir -p /user/hive/warehouse hadoop fs -mkdir -p /user/hive/tmp hadoop fs -mkdir -p /user/hive/log # 设置目录权限hadoop fs -chmod -R 777 /user/hive/warehouse hadoop fs -chmod -R 777 /user/hive/tmp hadoop fs -chmod -R 777 /user/hive/log 可以用下面命令进行检查： 12345root@hadoop-master:/usr/local/hadoop/hive/conf# hadoop fs -ls /user/hiveFound 3 itemsdrwxrwxrwx - root supergroup 0 2018-08-14 07:34 /user/hive/logdrwxrwxrwx - root supergroup 0 2018-08-14 07:34 /user/hive/tmpdrwxrwxrwx - root supergroup 0 2018-08-14 07:34 /user/hive/warehouse 修改配置文件（hive-site.xml）：hive数据仓库数据路径：/user/hive/warehouse 需要使用hdfs新建文件目录。 12345&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt; 配置查询日志存放目录： 12345&lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/user/hive/log/hadoop&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;&lt;/property&gt; 数据库JDBC连接配置（172.18.0.5为mysql的ip地址，暴露3306端口）： 12345&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://172.18.0.5:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;description&gt;&lt;/property&gt; 数据库驱动： 12345&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt; 数据库用户名： 12345&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt;&lt;/property&gt; 数据库密码： 12345&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt; 配置Hive临时目录： 1mkdir /usr/local/hadoop/hive/tmp 并在 hive-site.xml 中修改: 把${system:java.io.tmpdir} 改成真实物理绝对路径 /usr/local/hadoop/hive/tmp 把 ${system:user.name} 改成 ${user.name} 可以在外面编辑好配置文件，拷贝进docke： 12&gt; docker cp hive-site.xml 439b359d230e:/usr/local/hadoop/hive/conf/hive-site.xml&gt; 配置hive-env.sh文件：尾部加上下面的配置（或者修改注释部分的配置亦可）： 123HADOOP_HOME=/usr/local/hadoopexport HIVE_CONF_DIR=/usr/local/hadoop/hive/confexport HIVE_AUX_JARS_PATH=/usr/local/hadoop/hive/lib 配置Mysql启mysql容器，容器名：first-mysql，使用和hadoop一个桥接网络hadoop，密码为123456 1docker run --name first-mysql --net=hadoop -p 3306:3306 -e MYSQL\_ROOT\_PASSWORD=123456 -d mysql:5.7 回显： 12CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES84ae224cee53 mysql:5.7 "docker-entrypoint.s…" 32 minutes ago Up 32 minutes 0.0.0.0:3306-&gt;3306/tcp first-mysql 在Hadoop-master中配置mysql客户端（用来访问mysql服务器）： 1apt-get install mysql-client-core-5.6 测试远程连接： 1mysql -h172.18.0.5 -P3306 -uroot -p123456 新建数据库，数据库名为hive： 12mysql&gt; CREATE DATABASE hive;Query OK, 1 row affected (0.00 sec) 初始化（Hive主机上）： 12cd /usr/local/hadoop/hive/bin./schematool -initSchema -dbType mysql 回显： 12345root@hadoop-master:/usr/local/hadoop/hive/bin# ./schematool -initSchema -dbType mysqlSLF4J: Class path contains multiple SLF4J bindings.。。。（略）schemaTool completed# 初始化成功 下载配置mysql驱动包，放在Hive的lib路径下面： 12cd /usr/local/hadoop/hive/libwget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.38/mysql-connector-java-5.1.38.jar 启动Hive做完上面准备工作后，开始启动hive： 12345678910root@hadoop-master:/usr/local/hadoop/hive/bin# ./hiveSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hadoop/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Logging initialized using configuration in jar:file:/usr/local/hadoop/hive/lib/hive-common-2.3.3.jar!/hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.hive&gt; 最后进入hive的命令界面。 踩坑备注1、Hive提示SSL连接警告 1Tue Aug 14 10:53:12 UTC 2018 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 虽然Hive SQL执行成功，但是报上面的错误。产生的原因是使用JDBC连接MySQL服务器时为设置useSSL参数 。 解决办法：javax.jdo.option.ConnectionURL 配置的value值进行调整，设置useSSL=false ，注意xml中的语法。 12345678&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://172.18.0.5:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;description&gt; JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. &lt;/description&gt; 重启Hive，不再有警告。 远程部署对于远程部署需要单独启metastore服务，具体需要调整下面的配置文件（hive-site.xml）： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://hadoop-master:9083&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动metastore服务： 1nohup hive --service metastore &amp; 当然这属于简单方式将Hive都扎堆部署在一个容器中。可以在集群其他几点启metastore服务，提升架构的高可用性，避免单点问题。 参考文献1、Apache Hive-2.3.0 快速搭建与使用，https://segmentfault.com/a/1190000011303459 2、Hive提示警告SSL，https://blog.csdn.net/u012922838/article/details/73291524]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-聚类算法总结]]></title>
    <url>%2F2018%2F06%2F25%2F2018-06-26-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景根据事物的特征差异，对事物进行分类是一个基本问题。这个问题在数据科学中进行抽象：（1）“特征”指的是事物对应的特征向量，（2）“差异”对应的为特征向量之间的距离（度量）。 对于这个基本问题，根据已知数据是否具有标签，数据科学中使用两种不同的学习方法来处理： 分类问题。样本抽样数据具有标签信息。监督学习。 聚类问题。样本抽样数据不具有标签信息。无监督学习。 形式上，抽样训练数据集$D={x_1,x_2,…,x_N}$ ，包含了$N$ 个样本点，每个样本点具有$m$维的特征向量$x_i=(x_{i,1},x_{i,2},…,x_{i,m})$ 。聚类算法目标将训练集$D$ 划分为若干个子集${C_i}_{i=1}^K$ 。 那么很自然有下面两个问题： （1）怎样分类能区别样本点的差异？使得相似样本点属于同一个分类子集。 （2）怎样的分类方法是最优的？即量化评价分类方法。 对于第一个问题，首先我们要量化“相似”这个概念。这样我们需要对数据集（点集）嵌入到带有度量的空间中，用特征向量的空间度量来刻画相似，比如常用的度量有欧几里得距离。 那么又引出另外一个问题：空间度量有很多，什么度量是最优的？ 对于数据集的划分 聚类算法 SKLearn数据包数据工程实现中，常用python包有：sklearn，对于聚类算法有很好的封装。接下来介绍的聚类算法，工程实现上主要使用sklearn。 算法集原型聚类K-means算法K-mearn算法应该是聚类算法中名气最大的，几乎是聚类算法的同义词。 二分K均值算法学习向量量化LVQ 高斯混合聚类 密度聚类层次聚类### 参考文献1、数据科学家需要了解的5种聚类算法。https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68 2、sklearn 聚类算法。http://sklearn.apachecn.org/cn/0.19.0/modules/clustering.html]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Minikube上运行Flink集群]]></title>
    <url>%2F2018%2F06%2F25%2F2019-04-23-%E5%9C%A8Minikube%E4%B8%8A%E8%BF%90%E8%A1%8CFlink%E9%9B%86%E7%BE%A4%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 部署准备 第二部分 验证 总结 参考文献及资料 背景第一部分 部署准备首先当然需要部署minikube集群。启动minikube集群： 1234567891011# minikube startStarting local Kubernetes v1.10.0 cluster...Starting VM...Getting VM IP address...Moving files into cluster...Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster.Loading cached images from config file. 上面的回显表明minikube已经启动成功。执行下面网络配置： 1# minikube ssh 'sudo ip link set docker0 promisc on' 第二部分 部署Flink集群一个基本的Flink集群运行在minikube需要三个组件： Deployment/Job：运行 JobManager Deployment for a pool of TaskManagers Service exposing the JobManager’s REST and UI ports 1.1 创建命名空间12# kubectl create -f namespace.yamlnamespace/flink created 其中namespace.yaml文件为： 123456kind: NamespaceapiVersion: v1metadata: name: flink labels: name: flink 查询minikube集群的的命名空间： 12345# kubectl get namespacesNAME STATUS AGEflink Active 1mkube-public Active 254dkube-system Active 254d 1.2 集群组件资源定义1.2.1 启动flink-jobmanager组件Job Manager 服务是Flink集群的主服务，使用jobmanager-deployment.yaml创建。 123456789101112131415161718192021222324252627282930apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: flink-jobmanager namespace: flinkspec: replicas: 1 template: metadata: labels: app: flink component: jobmanager spec: containers: - name: jobmanager image: flink:latest args: - jobmanager ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob - containerPort: 6125 name: query - containerPort: 8081 name: ui env: - name: JOB_MANAGER_RPC_ADDRESS value: flink-jobmanager 1.2.2 启动flink-taskmanager组件使用taskmanager-deployment.yaml创建。 12345678910111213141516171819202122232425262728apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: flink-taskmanager namespace: flinkspec: replicas: 2 template: metadata: labels: app: flink component: taskmanager spec: containers: - name: taskmanager image: flink:latest args: - taskmanager ports: - containerPort: 6121 name: data - containerPort: 6122 name: rpc - containerPort: 6125 name: query env: - name: JOB_MANAGER_RPC_ADDRESS value: flink-jobmanager 1.2.3 启用flink服务使用jobmanager-service.yaml创建服务，并且将端口映射到minikube主机响应端口。 1234567891011121314151617181920212223apiVersion: v1kind: Servicemetadata: name: flink-jobmanager namespace: flinkspec: type: NodePort ports: - name: rpc port: 6123 nodePort: 30123 - name: blob port: 6124 nodePort: 30124 - name: query port: 6125 nodePort: 30125 - name: ui port: 8081 nodePort: 30081 selector: app: flink component: jobmanager 1.3 端口映射到虚拟机主机minikube虚拟机停止的情况下的端口转发命令如下： 1234# VBoxManage modifyvm "minikube" --natpf1 "30123_6123,tcp,,6123,,30123"# VBoxManage modifyvm "minikube" --natpf1 "30124_6123,tcp,,6124,,30124"# VBoxManage modifyvm "minikube" --natpf1 "30125_6125,tcp,,6125,,30125"# VBoxManage modifyvm "minikube" --natpf1 "30081_8081,tcp,,8081,,30081" 格式说明：vboxmanage modifyvm 宿主机名称 natpf “映射别名,tcp,,本机端口,,虚拟机端口” minikube虚拟机运行的情况下的端口转发命令如下： 1234# VBoxManage controlvm "minikube" --natpf1 "30123_6123,tcp,,6123,,30123"# VBoxManage controlvm "minikube" --natpf1 "30124_6123,tcp,,6124,,30124"# VBoxManage controlvm "minikube" --natpf1 "30125_6125,tcp,,6125,,30125"# VBoxManage controlvm "minikube" --natpf1 "30081_8081,tcp,,8081,,30081" 格式说明：vboxmanage controlvm 宿主机名称 natpf “映射别名,tcp,,本机端口,,宿主机端口” 另外如果要删除上面转发规则： vboxmanage controlvm 宿主机名称 natpf delete 映射别名 vboxmanage modifyvm 宿主机名称 natpf delete 映射别名 第三部分 验证3.1 minikube控制台界面为了是主机局域网类服务器都能访问minikube控制台，需要将端口映射出去。 1# VBoxManage modifyvm "minikube" --natpf1 "kubedashboard,tcp,,30000,,30000" 3.2 Flink控制台 总结（1）部署前提前拉取镜像到本地镜像库。 （2）需要将服务端口映射到本地机器端口，供局域网服务访问，为后续访问Flink提供方便。 参考文献1、Kubernetes Setup ：https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/kubernetes.html 2、How to Deploy Flink Cluster &amp; Flink-exporter in Kubernetes Cluster：https://medium.com/pharos-production/how-to-deploy-flink-cluster-flink-exporter-in-kubernetes-cluster-48e24b440446 3、melentye/flink-kubernetes https://github.com/melentye/flink-kubernetes 4、Set up Ingress on Minikube with the NGINX Ingress Controller https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用Cloudera Quickstart Docker镜像快速部署hadoop集群]]></title>
    <url>%2F2018%2F06%2F25%2F2019-04-30-%E4%BD%BF%E7%94%A8Cloudera%20Quickstart%20Docker%E9%95%9C%E5%83%8F%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2hadoop%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Docker镜像准备 第二部分 运行容器 第三部分 cloudera-manager管理 第四部分 组件使用测试 第五部分 总结 参考文献及资料 背景通常在个人笔记本上部署Hadoop测试集群（含生态圈各组件）是个很耗时的工作。Cloudera公司提供一个快速部署的Docker镜像，可以快速启动一个测试集群。 测试环境为Ubuntu服务器 第一部分 Docker镜像准备首先本机需要部署有docker环境，如果没有需要提前部署。 1.1 拉取Docker镜像可以从DockerHub上拉取cloudera/quickstart镜像。 镜像项目地址为：https://hub.docker.com/r/cloudera/quickstart 1# docker pull cloudera/quickstart:latest 如果不具备联网环境，可以通过镜像介质包安装。介质可以在官网（需要注册用户）下载：https://www.cloudera.com/downloads/quickstart_vms/5-13.html 由于墙的原因下载会很慢 123# wget https://downloads.cloudera.com/demo_vm/docker/cloudera-quickstart-vm-5.13.0-0-beta-docker.tar.gz# tar xzf cloudera-quickstart-vm-5.13.0-0-beta-docker.tar.gz# docker import - cloudera/quickstart:latest &lt; cloudera-quickstart-vm-5.13.0-0-beta-docker/*.tar 1.2 检查镜像库12# docker images|grep clouderacloudera/quickstart latest 4239cd2958c6 3 years ago 6.34GB 说明镜像准备好了，下面基于镜像启动容器。 第二部分 运行容器2.1 使用镜像启动容器启动CDH集群的命令格式为： 1# docker run --hostname=quickstart.cloudera --privileged=true -t -i [OPTIONS] [IMAGE] /usr/bin/docker-quickstart 官方提示的参数介绍如下： Option Description –hostname=quickstart.cloudera Required: Pseudo-distributed configuration assumes this hostname.容器主机名（/etc/hosts中指定hostname）。 –privileged=true Required: For HBase, MySQL-backed Hive metastore, Hue, Oozie, Sentry, and Cloudera Manager.这是Hbase组件需要的模式。 -t Required: Allocate a pseudoterminal. Once services are started, a Bash shell takes over. This switch starts a terminal emulator to run the services. -i Required: If you want to use the terminal, either immediately or connect to the terminal later. -p 8888 Recommended: Map the Hue port in the guest to another port on the host.端口映射参数。 -p [PORT] Optional: Map any other ports (for example, 7180 for Cloudera Manager, 80 for a guided tutorial). -d Optional: Run the container in the background.容器后台启动。 –name 容器的名字 -v host_path:container_path 主机上目录挂载到容器中目录上，主机上该放入任何东西，Docker容器中对于目录可以直接访问。 当然还可以自定义其他docker启动参数。最后启动命令整理为： 1234567docker run -t -i -d \--name cdh \--hostname=quickstart.cloudera \--privileged=true \-v /data/CDH:/src \-p 8020:8020 -p 8042:8042 -p 8022:8022 -p 7180:7180 -p 21050:21050 -p 50070:50070 -p 50075:50075 -p 50010:50010 -p 50020:50020 -p 8890:8890 -p 60010:60010 -p 10002:10002 -p 25010:25010 -p 25020:25020 -p 18088:18088 -p 8088:8088 -p 19888:19888 -p 7187:7187 -p 11000:11000 -p 10000:10000 -p 88:88 -p 8888:8888 cloudera/quickstart \/bin/bash -c '/usr/bin/docker-quickstart' Cloudera 本身的 manager 是 7180 端口，提前配置端口映射。 启动容器我们使用了-d后台启动参数，如果没有指定后台启动，终端将自动连接到容器，退出shell后容器会中止运行（可以通过使用Ctrl + P + Q命令退出，这样容器会继续保持运行）。 对于已经后台运行的容器，我们使用下面的命令进入容器shell： 1# docker attach [CONTAINER HASH] 建议使用下面的命令： 1# docker exec -it 容器名 /bin/bash 2.2 时钟同步问题容器内部使用的时间时区为UTC，和主机（宿主机通常为CST（东八区））不同时区，会提示时钟同步问题。解决的办法是： 在环境变量中添加时区变量,并source生效: 123$ vi /etc/profile文件末尾添加一行：TZ='Asia/Shanghai'; export TZ$ source /etc/profile 最后启动时钟同步服务： 12345[root@quickstart home]# service ntpd startStarting ntpd: [ OK ]# 检查服务状态[root@quickstart home]# service ntpd statusntpd (pid 13536) is running... 这样完成时钟同步。 2.3 使用集群服务这样集群的大部分服务组件均可使用。 第三部分 cloudera-manager管理3.1 启动管理服务CDH在该镜像中提供cloudera-manager组件，用户集群web管理界面，可以通过下面的命令启动。 需要注意的是，启动后CDH会停止其他组件服务。 12345678910111213141516171819202122232425#./home/cloudera/cloudera-manager --express [--force][QuickStart] Shutting down CDH services via init scripts...kafka-server: unrecognized serviceJMX enabled by defaultUsing config: /etc/zookeeper/conf/zoo.cfg[QuickStart] Disabling CDH services on boot...error reading information on service kafka-server: No such file or directory[QuickStart] Starting Cloudera Manager server...[QuickStart] Waiting for Cloudera Manager API...[QuickStart] Starting Cloudera Manager agent...[QuickStart] Configuring deployment...Submitted jobs: 14[QuickStart] Deploying client configuration...Submitted jobs: 16[QuickStart] Starting Cloudera Management Service...Submitted jobs: 24[QuickStart] Enabling Cloudera Manager daemons on boot...________________________________________________________________________________Success! You can now log into Cloudera Manager from the QuickStart VM's browser: http://quickstart.cloudera:7180 Username: cloudera Password: cloudera 集群控制台的地址为：http://quickstart.cloudera:7180，需要注意的是这里quickstart.cloudera是主机名，需要客户端hosts中配置，否则使用实IP或者容器端口映射后使用宿主机IP（例如：192.168.31.3）。 用户名和密码为：cloudera/cloudera,登录界面如下： 登录后，下图是集群控制台： 从管理界面上可以看到除了主机和 manager ，其他服务组件均未启动。 3.2 启动集群组件服务在控制台上，我们按照顺序启动HDFS、Hive、Hue、Yarn服务。 如果服务启动异常，可以尝试重启服务组件。注意需要先启动HDFS后启动Hive，否则需要重启Hive。 第四部分 组件使用测试4.1 HDFS组件使用我们使用HDFS的Python API与集群hdfs文件系统进行交互测试： 4.1.1 查看文件系统123456from hdfs.client import Clientclient = Client("http://192.168.31.3:50070", root="/", timeout=100)print(client.list("/"))# ['benchmarks', 'hbase', 'tmp', 'user', 'var']# 返回一个list记录主目录 4.1.2 上传新增文件注意这里需要在宿主机（客户端机器）配置hosts文件： 1192.168.31.3 quickstart.cloudera 然后执行： 123client.upload("/tmp", "/root/jupyter/nohup.out")# '/tmp/nohup.out'# 返回路径信息 4.1.3 下载hdfs文件12client.download("/tmp/nohup.out", "/tmp")# 返回路径'/tmp/nohup.out' 第五部分 启用安全模式（Kerberos）默认情况下集群是非安全模式，如果测试需要安全集群模式，可以在菜单：管理-安全中开启Kerberos。需要注意的是需要在容器中安装Kerberos。安装步骤如下： 注释到无关源（docker容器执行）； 12# cd /etc/yum.repos.d# mv cloudera-manager.repo cloudera-manager.repo.bak 部署yum源（宿主机执行） 12# wget https://www.xmpan.com/Centos-6-Vault-Aliyun.repo# docker cp Centos-6-Vault-Aliyun.repo 容器名:/etc/yum.repos.d 然后安装Kerberos（docker容器执行） 1./home/cloudera/kerberos 等待安装完成即可。 接下来就是配置kerberos，主要涉及的配置文件有： 123/etc/krb5.conf/var/kerberos/krb5kdc/kdc.conf/var/kerberos/krb5kdc/kdc.conf 主要调整是将EXAMPLE.COM调整为HADOOP.COM。另外配置文件krb5.conf需要个性化指定KDC服务器： 1234HADOOP.COM = &#123;kdc = 服务器hostname或者IPadmin_server = 服务器hostname或者IP&#125; 然后创建Kerberos数据库： 1$ kdb5_util create –r HADOOP.COM -s 创建Kerberos管理账户： 12$ kadmin.localkadmin.local: addprinc admin/admin@HADOOP.COM 最后启动服务： 1234[root@quickstart cloudera]# service krb5kdc startStarting Kerberos 5 KDC: [ OK ][root@quickstart cloudera]# service kadmin startStarting Kerberos 5 Admin Server: [ OK ] 查看票据： 123456789[root@quickstart cloudera]# kinit admin/admin@HADOOP.COMPassword for admin/admin@HADOOP.COM: (admin123)[root@quickstart cloudera]# klistTicket cache: FILE:/tmp/krb5cc_0Default principal: admin/admin@HADOOP.COMValid starting Expires Service principal03/16/21 16:30:57 03/17/21 16:30:57 krbtgt/HADOOP.COM@HADOOP.COM renew until 03/16/21 16:30:57 生成keytab文件： 123[root@quickstart keytab]# kadmin.localAuthenticating as principal admin/admin@HADOOP.COM with password.kadmin.local: xst -norandkey -k admin.keytab admin/admin@HADOOP.COM 这样就生成了admin.keytab文件。然后使用下面的命令测试一下： 1kinit -kt admin.keytab admin/admin@HADOOP.COM 第六部分 测试运行Spark我们只测试Spark on Yarn场景。 6.1 运行Spark 1.6.0CDH 5.7.0 版本自带Spark版本是1.6.0的。所以我们不需要部署相关运行依赖。例如下面的命令运行一个pyspark的任务。 123456/usr/lib/spark/bin/spark-submit \--master yarn \--deploy-mode cluster \--archives hdfs:///user/admin/python/python3.5.2.zip \--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./python3.5.2.zip/conda/bin/python \test.py 6.2 运行Spark 2.3.0对于Spark 2.3.0 需要下载官网相关的版本的介质包。并且由于Spark 2.3.0的编译是java 8，所以需要配置JAVA_HOME环境变量为1.8。 具体命令如下： 12345678export YARN_CONF_DIR=/etc/hadoop/conf/home/spark-2.3.0/bin/spark-submit \--conf "spark.executorEnv.JAVA_HOME=/home/openjdk" \--conf "spark.yarn.appMasterEnv.JAVA_HOME=/home/openjdk" \--master yarn \--deploy-mode cluster \--class org.apache.spark.examples.SparkPi \/home/spark-2.3.0/examples/jars/spark-examples_2.11-2.3.0.jar 需要主要点有： 配置YARN_CONF_DIR环境变量 需要指定Driver和Excutor运行的JDK环境。需要注意的是对于分布式集群前提就是所有节点上均有相同路径jdk，或者需要配置一致的JAVA_HOME。 另外一种方法还没验证： 某些特殊的场景下，我们对集群没有管理权限，只能通过YARN提交Application，并且集群里没有部署我们需要的JDK版本，这种情形就需要将JDK的安装包也一并提交了。 这里要求我们的JDK安装包必须为gz格式的，和你代码打包后的jar包放在同一目录下，假设我们下载的JDK的安装包为：jdk-8u141-linux-x64.tar.gz。 关键配置如下： 12345$SPARK_HOME/bin/spark-submit \ --conf "spark.yarn.dist.archives=jdk-8u141-linux-x64.tar.gz" \ --conf "spark.executorEnv.JAVA_HOME=./jdk-8u141-linux-x64.tar.gz/jdk1.8.0_141" \ --conf "spark.yarn.appMasterEnv.JAVA_HOME=./jdk-8u141-linux-x64.tar.gz/jdk1.8.0_141" \ ... 我们可以通过指定spark.yarn.dist.archives配置，将JDK的安装包分发到所有Executor的工作目录下（包括Application Master的Executor），另外tar.gz的压缩包也会被自动解压，假设jdk-8u141-linux-x64.tar.gz解压后的目录为jdk1.8.0_141，那么我们特定的JDK的目录就是：./jdk-8u141-linux-x64.tar.gz/jdk1.8.0_141，不同的JDK版本以此类推即可。 注意：由于Spark Standalone没有提供分发JDK安装包并自动解压的功能，所以，这种方式只能用在YARN下。 第七部分 测试运行Flink我们测试使用Flink 1.7.2和Flink 1.12.7两个版本进行测试。两个版本类似。 7.1 运行Flink 1.7.2部署好Flink介质后，使用下面的命令运行： 1./bin/flink run -m yarn-cluster examples/batch/WordCount.jar 结果报错了，报错如下： 12345#org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Could not deploy Yarn job cluster.# 参看yarn日志Unrecognized VM option 'MaxMetaspaceSize=268435456'Error: Could not create the Java Virtual Machine.Error: A fatal exception has occurred. Program will exit. 上面的报错是JDK的兼容性报错。Flink运行需要在JDK 1.8版本下运行，但是集群的JDK为1.7版本。 这个报错Spark同样会出现这个问题。但是Spark可以通过spark.executorEnv.JAVA_HOME和spark.yarn.appMasterEnv.JAVA_HOME两个参数设置执行器和AM容器的java运行环境。 但是对于Flink查看了官方各种资料没有找到规避方法。其中有方案说通过flink-conf.yaml配置下面的参数。笔者实践后，证实是无效的。 1env.java.home: /home/openjdk 所以最后的解决方案是调整集群的JDK的版本为1.8版本。 第八部分 总结1、Cloudera 的 docker 版本分成两部分启动。(1)启动各组件启动,使用命令为： /usr/bin/docker-quickstart，(2) 启动Cloudera manager 管理服务，启动命令为：/home/cloudera/cloudera-manager。docker启动时选择启动一项。 参考文献及材料1、cloudera/quickstart镜像地址：https://hub.docker.com/r/cloudera/quickstart 2、cloudera/quickstart镜像部署指引：https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html 3、利用 Docker 搭建单机的 Cloudera CDH 以及使用实践]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Minikube上运行Kafka集群]]></title>
    <url>%2F2018%2F06%2F25%2F2019-07-27-%E5%9C%A8Minikube%E4%B8%8A%E8%BF%90%E8%A1%8CKafka%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Minikube集群启动 第一部分 Kubernetes中StatefulSet介绍 第三部分 部署Zookeeper集群 第四部分 部署Kafka集群 第五部分 总结 参考文献及资料 背景Kafka和zookeeper是在两种典型的有状态的集群服务。首先kafka和zookeeper都需要存储盘来保存有状态信息，其次kafka和zookeeper每一个实例都需要有对应的实例Id(Kafka需要broker.id,zookeeper需要my.id)来作为集群内部每个成员的标识，集群内节点之间进行内部通信时需要用到这些标识。 对于这类服务的部署，需要解决两个大的问题，一个是状态保存，另一个是集群管理(多服务实例管理)。kubernetes中提的StatefulSet(1.5版本之前称为Petset)方便了有状态集群服务在上的部署和管理。具体来说是通过Init Container来做集群的初始化工 作，用 Headless Service来维持集群成员的稳定关系，用Persistent Volume和Persistent Volume Claim提供网络存储来持久化数据，从而支持有状态集群服务的部署。 StatefulSet 是Kubernetes1.9版本中稳定的特性，本文使用的环境为 Kubernetes 1.10.0。 第一部分 Minikube集群启动12345678910111213141516171819202122root@deeplearning:~# minikube startThere is a newer version of minikube available (v1.2.0). Download it here:https://github.com/kubernetes/minikube/releases/tag/v1.2.0To disable this notification, run the following:minikube config set WantUpdateNotification falseStarting local Kubernetes v1.10.0 cluster...Starting VM...Downloading Minikube ISO 153.08 MB / 153.08 MB [============================================] 100.00% 0sGetting VM IP address...Moving files into cluster...Downloading kubeadm v1.10.0Downloading kubelet v1.10.0Finished Downloading kubeadm v1.10.0Finished Downloading kubelet v1.10.0Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster.Loading cached images from config file. 第二部分 Kubernetes中StatefulSet介绍使用Kubernetes来调度无状态的应用较为简单 StatefulSet 这个对象是专门用来部署用状态应用的，可以为Pod提供稳定的身份标识，包括hostname、启动顺序、DNS名称等。 在最新发布的 Kubernetes 1.5 我们将过去的 PetSet 功能升级到了 Beta 版本，并重新命名为StatefulSet 第三部分 部署Zookeeper集群第四部分 部署Kafka集群参考文献及材料1、kubernetes 中 kafka 和 zookeeper 有状态集群服务部署实践 (一) https://cloud.tencent.com/developer/article/1005492 2、https://cloud.tencent.com/developer/article/1005491 3、https://www.bogotobogo.com/DevOps/Docker/Docker_Kubernetes_StatefulSet.php https://technology.amis.nl/2018/04/19/15-minutes-to-get-a-kafka-cluster-running-on-kubernetes-and-start-producing-and-consuming-from-a-node-application/ 4、https://kubernetes.io/zh/docs/tutorials/stateful-application/basic-stateful-set/ 5、https://jimmysong.io/kubernetes-handbook/guide/using-statefulset.html 6、Kubernetes部署Kafka集群 https://blog.usejournal.com/kafka-on-kubernetes-a-good-fit-95251da55837 https://www.cnblogs.com/cocowool/p/kubernetes_statefulset.html]]></content>
      <categories>
        <category>Minikube Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Ubuntu上部署Minikube]]></title>
    <url>%2F2018%2F06%2F25%2F2018-06-25-%E5%9C%A8Ubuntu%E4%B8%8A%E9%83%A8%E7%BD%B2Minikube%2F</url>
    <content type="text"><![CDATA[[TOC] 背景Kubernetes是Google推出的容器编排工具，这是Google保密十几年的强大武器Borg的开源版本。Kubernetes这个名字源于古希腊，意思是舵手。既然docker被比喻成大海上驮着集装箱的鲸鱼，那么Kubernetes就是舵手，掌握鲸鱼的游弋方向，寓意深刻。 Kubernetes第一个正式版本于2015年7月发布。从Kubernetes 1.3开始提供了一个叫Minikube的强大测试工具，可以在主流操作系统平台（win、os、linux）上运行单节点的小型集群，这个工具默认安装和配置了一个Linux VM，Docker和Kubernetes的相关组件，并且提供Dashboard。 本篇主要介绍Ubuntu平台上部署Minikube。Minikube利用本地虚拟机环境部署Kubernetes，其基本架构如下图所示。 Minitube项目地址：https://github.com/kubernetes/minikube 第一部分 准备Minikube在OS X和Windows上部署需要安装虚拟机实现（用虚拟机来初始化Kubernetes环境），但是Linux例外可以使用自己的环境。参见：https://github.com/kubernetes/minikube#quickstart 1.1 准备工作检查CPU是否支持虚拟化，即BIOS中参数（VT-x/AMD-v ）设置为enable。 1.2 安装虚拟机Minikube在不同操作系统上支持不同的虚拟驱动： macOS xhyve driver, VirtualBox 或 VMware Fusion Linux VirtualBox 或 KVM 注意: Minikube 也支持 --vm-driver=none 选项来在本机运行 Kubernetes 组件，这时候需要本机安装了 Docker。 Windows VirtualBox 或 Hyper-V 本篇在Ubuntu部署VirtualBox虚拟驱动。 123# wget https://download.virtualbox.org/virtualbox/5.1.38/virtualbox-5.1_5.1.38-122592~Ubuntu~xenial_i386.deb#dpkg -i virtualbox-5.1_5.1.38-122592~Ubuntu~xenial_i386.deb 第二部分 安装minikube从阿里云下载最新版本的minikube： 最新版本在这个网址获取：https://github.com/AliyunContainerService/minikube 1234# curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v1.20.0/minikube-linux-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/ % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 40.8M 100 40.8M 0 0 4671k 0 0:00:08 0:00:08 --:--:-- 7574k 第三部分 安装Kubectlkubectl即kubernetes的客户端，通过他可以进行类似docker run等容器管理操作 。 下载： 1234567# curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 52.5M 100 52.5M 0 0 6654k 0 0:00:08 0:00:08 --:--:-- 10.3M# chmod +x kubectl# mv kubectl /usr/local/bin/ 第四部分 启集群1234567891011121314151617root@deeplearning:~# minikube start --vm-driver=virtualbox --registry-mirror=https://registry.docker-cn.com --image-mirror-country=cn --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --force=true --kubernetes-version="v1.18.3"Starting local Kubernetes v1.10.0 cluster...Starting VM...Downloading Minikube ISO 153.08 MB / 153.08 MB [============================================] 100.00% 0sGetting VM IP address...Moving files into cluster...Downloading kubeadm v1.10.0Downloading kubelet v1.10.0Finished Downloading kubelet v1.10.0Finished Downloading kubeadm v1.10.0Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster.Loading cached images from config file. 为了访问海外的资源，阿里云提供了一系列基础设施，请按照如下参数进行配置。其中常见参数 --driver=*** 从1.5.0版本开始，Minikube缺省使用本地最好的驱动来创建Kubernetes本地环境，测试过的版本 docker, kvm --image-mirror-country cn 将缺省利用 registry.cn-hangzhou.aliyuncs.com/google_containers 作为安装Kubernetes的容器镜像仓库 （阿里云版本可选） --iso-url=*** 利用阿里云的镜像地址下载相应的 .iso 文件 （阿里云版本可选） --registry-mirror=***为了拉取Docker Hub镜像，需要为 Docker daemon 配置镜像加速，参考阿里云镜像服务 --cpus=2: 为minikube虚拟机分配CPU核数 --memory=2048mb: 为minikube虚拟机分配内存数 --kubernetes-version=***: minikube 虚拟机将使用的 kubernetes 版本 进入minikube虚拟机： 123456789101112root@deeplearning:~# minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\| |/' _ `\| || , &lt; ( ) ( )| '_`\ /'__`\| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )( ___/(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)$ # 通过exit退出集群$ exitlogout 虚拟机地址： 12# minikube ip192.168.99.100 启停集群： 1# minikube start/stop 获取集群信息： 123root@deeplearning:/home/rongxiang# kubectl get node NAME STATUS ROLES AGE VERSIONminikube Ready master 7h v1.10.0 删除集群： 123# minikube delete # rm -rf ~/.minikube # kubeadm reset 第五部分 心酸踩坑如果在启集群时遇到下面类似的错误，不要慌。国内环境99%的原因是GFW的原因，集群在抓取Google站点docker镜像时候被墙咔嚓了，然后time out。 WTF！我开始不知道呀。网上的部署指引都那么轻松！！然后重复删除集群，重新装，配参数，给docker配代理。尼玛，最后代理都被咔嚓了。终于撞了南墙，去阿里云拉取镜像，几秒钟搞定。 1234567891011121314151617# minikube startStarting local Kubernetes v1.10.0 cluster...Starting VM...Getting VM IP address...Moving files into cluster...Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...E0626 12:57:46.868961 26526 start.go:299] Error restarting cluster: restarting kube-proxy: waiting for kube-proxy to be up for configmap update: timed out waiting for the condition================================================================================An error has occurred. Would you like to opt in to sending anonymized crashinformation to minikube to help prevent future errors?To opt out of these messages, run the command: minikube config set WantReportErrorPrompt false================================================================================Please enter your response [Y/n]: 第六部分 远程访问 minikube dashboard6.1旧版本dashboard 1.0在虚拟机启动前，设置端口转发。注意这里使用tcp而不是http。 1# VBoxManage modifyvm "minikube" --natpf1 "kubedashboard,tcp,,30000,,30000" 然后启动虚拟机，这时候局域网上的其他服务器就可以通过宿主机的IP:30000访问web UI。或者： 1# kubectl proxy --address='0.0.0.0' --disable-filter=true 然后启动： 12# minikube dashboard --urlhttp://192.168.99.102:30000 6.2 新版本dashboard 2.0在新版本中，dashboard 2.0 默认会启用 https 的认证，具体认证方式有：TLS、token 和 username/passwd 首先地址映射： 12root@deeplearning:~# kubectl proxy &amp;Starting to serve on 127.0.0.1:8001 然后生成token： 1234567891011121314151617181920212223242526272829303132root@deeplearning:~# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yamlroot@deeplearning:~# kubectl create serviceaccount dashboard-admin -n kube-system#创建用于登录dashborad的serviceaccount账号serviceaccount "dashboard-admin" createdroot@deeplearning:~# kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminclusterrolebinding.rbac.authorization.k8s.io/dashboard-cluster-admin created #创建一个clusterrolebingding，将名称为cluster-admin的clusterrole绑定到我们刚刚从的serviceaccount上，名称空间和sa使用:作为间隔root@deeplearning:~# kubectl get secret -n kube-system#创建完成后系统会自动创建一个secret，名称以serviceaccount名称开头NAME TYPE DATA AGE......dashboard-admin-token-twwfl kubernetes.io/service-account-token 3 46s.....#使用describe查看该secret的详细信息，主要是token一段root@deeplearning:~# kubectl describe secret dashboard-admin-token-twwfl -n kube-systemName: dashboard-admin-token-twwflNamespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: dashboard-admin kubernetes.io/service-account.uid: ae20b337-3729-4c97-9852-19e160b37427Type: kubernetes.io/service-account-tokenData====token: eyJhbGciOiJSUzI1NiIsImtpZCI6IjZGSnZhZ2FiTUhlMTBoN1dYWFB6cmwzSkphNWVNQ0ZLWGZ0NEhOOUZST1UifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tdHd3ZmwiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWUyMGIzMzctMzcyOS00Yzk3LTk4NTItMTllMTYwYjM3NDI3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.lOL6l1uWwEBfw4bbsdRyhvRnLzeOakz-kf38rc2RXPzBgMHqeLgsLw2o7TwRRp84XVNPoDaIa6HwW_6StxYZG5dfiCdUlClWHgepl-z3dq9r49IPqh-ZJLA56D1BZP-iRptmFjJHy5uAXPOyRRg-a43FwM0VZa4aaSe-NKdh7eceJxig0t_mJJbSYdIG_PqZS-JiBsJPb8KYl_GEWSNB4jzQ0SA8CZrB9Yl_ifhTL3vAie6FgLawPXENjz1puufene2Kymo-fVEmGK6KedRdIm4gCpg5NGmuEdJB3ikEzObN_Mv5JbK9wTmGW0s1-6m5ogdshpvKmSBQZ9Vuik_Qfwca.crt: 1066 bytesnamespace: 11 bytes 打开下面url，使用token方式登录： http://127.0.0.1:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login 6.3 新版本dashboard 2.0也可以使用下面命名启动： 1minikube dashboard &amp; 注意：需要后台启动，否则代理启动在前台，shell断开就链接失败了。 附录 补充VBoxManage管理查询虚拟机： 123# VBoxManage list vms"&lt;inaccessible&gt;" &#123;4a3cefe1-11d1-45d2-91c5-1e39fccb6a8d&#125;"minikube" &#123;dfcd1bdf-afc1-49e6-a270-9d8ff14bf167&#125; 删除虚拟机： 1# VBoxManage unregistervm --delete 4a3cefe1-11d1-45d2-91c5-1e39fccb6a8d 参考文献及材料1、Minitube项目地址：https://github.com/kubernetes/minikube 2、kubernetes学习笔记之十一：kubernetes dashboard认证及分级授权]]></content>
      <categories>
        <category>Minikube Ubuntu</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Minikube上运行Spark集群]]></title>
    <url>%2F2018%2F06%2F25%2F2018-08-06-%E5%9C%A8Minikube%E4%B8%8A%E8%BF%90%E8%A1%8CSpark%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[[TOC] 背景​ Spark2.3版本开始支持使用spark-submit直接提交任务给Kubernetes集群。执行机制原理： Spark创建一个在Kubernetes pod中运行的Spark驱动程序。 驱动程序创建执行程序，这些执行程序也在Kubernetes pod中运行并连接到它们，并执行应用程序代码。 当应用程序完成时，执行程序窗格会终止并清理，但驱动程序窗格会保留日志并在Kubernetes API中保持“已完成”状态，直到它最终被垃圾收集或手动清理。 第一部分 环境准备1.1 minikube虚拟机准备由于spark集群对内存和cpu资源要求较高，在minikube启动前，提前配置较多的资源给虚拟机。 当minikube启动时，它以单节点配置开始，默认情况下占用1Gb内存和2CPU内核，但是，为了运行spark集群，这个资源配置是不够的，而且作业会失败。 12345# minikube config set memory 8192These changes will take effect upon a minikube delete and then a minikube start# minikube config set cpus 2These changes will take effect upon a minikube delete and then a minikube start 或者用下面的命令启集群 1# minikube start --cpus 2 --memory 8192 1.2 Spark环境准备第一步 下载saprk2.3 1# wget http://apache.mirrors.hoobly.com/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz 解压缩： 1# tar xvf spark-2.3.0-bin-hadoop2.7.tgz 制作docker镜像 12# cd spark-2.3.0-bin-hadoop2.7# docker build -t rongxiang/spark:2.3.0 -f kubernetes/dockerfiles/spark/Dockerfile . 查看镜像情况： 123# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZErongxiang1986/spark 2.3.0 c5c806314f25 5 days ago 346MB 登录docker 账户： 12345# docker loginLogin with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.Username: Password: Login Succeeded 将之前build好的镜像pull到docker hub上： 1# docker push rongxiang1986/spark:2.3.0 注意这里的格式要求（我踩坑了）：docker push 注册用户名/镜像名 在https://hub.docker.com/上查看，镜像确实push上去了。 第二部分 提交Spark作业2.1 作业提交提前配置serviceaccount信息。 1234# kubectl create serviceaccount sparkserviceaccount/spark created# kubectl create clusterrolebinding spark-role --clusterrole=admin --serviceaccount=default:spark --namespace=defaultclusterrolebinding.rbac.authorization.k8s.io/spark-role created 提交作业： 12345678910# ./spark-submit \--master k8s://https://192.168.99.100:8443 \--deploy-mode cluster \--name spark-pi \--class org.apache.spark.examples.SparkPi \--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \--conf spark.kubernetes.authenticate.executor.serviceAccountName=spark \--conf spark.executor.instances=2 \--conf spark.kubernetes.container.image=rongxiang1986/spark:2.3.0 \local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar 提交命令的参数含义分别是： --class：应用程序的入口点（命令中使用：org.apache.spark.examples.SparkPi）； --master：Kubernetes集群的URL（k8s://https://192.168.99.100:8443）； --deploy-mode：驱动程序部署位置（默认值：客户端），这里部署在集群中； --conf spark.executor.instances=2：运行作业启动的executor个数； --conf spark.kubernetes.container.image=rongxiang1986/spark:2.3.0：使用的docker镜像名称； local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar：应用程序依赖jar包路径； 注意：目前deploy-mode只支持cluster模式，不支持client模式。 Error: Client mode is currently not supported for Kubernetes. 作业运行回显如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677782018-08-12 15:51:17 WARN Utils:66 - Your hostname, deeplearning resolves to a loopback address: 127.0.1.1; using 192.168.31.3 instead (on interface enp0s31f6)2018-08-12 15:51:17 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address2018-08-12 15:51:18 INFO LoggingPodStatusWatcherImpl:54 - State changed, new state: pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver namespace: default labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0 creation time: 2018-08-12T07:51:18Z service account name: spark volumes: spark-token-rzrgk node name: N/A start time: N/A container images: N/A phase: Pending status: []2018-08-12 15:51:18 INFO LoggingPodStatusWatcherImpl:54 - State changed, new state: pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver namespace: default labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0 creation time: 2018-08-12T07:51:18Z service account name: spark volumes: spark-token-rzrgk node name: minikube start time: N/A container images: N/A phase: Pending status: []2018-08-12 15:51:18 INFO LoggingPodStatusWatcherImpl:54 - State changed, new state: pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver namespace: default labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0 creation time: 2018-08-12T07:51:18Z service account name: spark volumes: spark-token-rzrgk node name: minikube start time: 2018-08-12T07:51:18Z container images: rongxiang1986/spark:2.3.0 phase: Pending status: [ContainerStatus(containerID=null, image=rongxiang1986/spark:2.3.0, imageID=, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties=&#123;&#125;), name=spark-kubernetes-driver, ready=false, restartCount=0, state=ContainerState(running=null, terminated=null, waiting=ContainerStateWaiting(message=null, reason=ContainerCreating, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;)]2018-08-12 15:51:18 INFO Client:54 - Waiting for application spark-pi to finish...2018-08-12 15:51:51 INFO LoggingPodStatusWatcherImpl:54 - State changed, new state: pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver namespace: default labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0 creation time: 2018-08-12T07:51:18Z service account name: spark volumes: spark-token-rzrgk node name: minikube start time: 2018-08-12T07:51:18Z container images: rongxiang1986/spark:2.3.0 phase: Running status: [ContainerStatus(containerID=docker://d43089c8340affc4534f796b94a90ae080670c36c095176575fbeebacaab648e, image=rongxiang1986/spark:2.3.0, imageID=docker-pullable://rongxiang1986/spark@sha256:3e93a2d462679015a9fb7d723f53ab1d62c5e3619e3f1564d182c3d297ddf75d, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties=&#123;&#125;), name=spark-kubernetes-driver, ready=true, restartCount=0, state=ContainerState(running=ContainerStateRunning(startedAt=Time(time=2018-08-12T07:51:51Z, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;), terminated=null, waiting=null, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;)]2018-08-12 15:51:57 INFO LoggingPodStatusWatcherImpl:54 - State changed, new state: pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver namespace: default labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0 creation time: 2018-08-12T07:51:18Z service account name: spark volumes: spark-token-rzrgk node name: minikube start time: 2018-08-12T07:51:18Z container images: rongxiang1986/spark:2.3.0 phase: Succeeded status: [ContainerStatus(containerID=docker://d43089c8340affc4534f796b94a90ae080670c36c095176575fbeebacaab648e, image=rongxiang1986/spark:2.3.0, imageID=docker-pullable://rongxiang1986/spark@sha256:3e93a2d462679015a9fb7d723f53ab1d62c5e3619e3f1564d182c3d297ddf75d, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties=&#123;&#125;), name=spark-kubernetes-driver, ready=false, restartCount=0, state=ContainerState(running=null, terminated=ContainerStateTerminated(containerID=docker://d43089c8340affc4534f796b94a90ae080670c36c095176575fbeebacaab648e, exitCode=0, finishedAt=Time(time=2018-08-12T07:51:57Z, additionalProperties=&#123;&#125;), message=null, reason=Completed, signal=null, startedAt=Time(time=2018-08-12T07:51:51Z, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;), waiting=null, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;)]2018-08-12 15:51:57 INFO LoggingPodStatusWatcherImpl:54 - Container final statuses: Container name: spark-kubernetes-driver Container image: rongxiang1986/spark:2.3.0 Container state: Terminated Exit code: 02018-08-12 15:51:57 INFO Client:54 - Application spark-pi finished.2018-08-12 15:51:57 INFO ShutdownHookManager:54 - Shutdown hook called2018-08-12 15:51:57 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-6dd1c204-4ad7-40c4-b47f-a34f18e1995d 2.2 日志查询可以通过命令查看容器执行日志，或者通过kubernetes-dashboard提供web界面查看。 1# kubectl logs spark-pi-709e1c1b19813e7cbc1aeff45200c64e-driver 1234567891011121314152018-08-12 07:51:57 INFO DAGScheduler:54 - Job 0 finished: reduce at SparkPi.scala:38, took 0.576528 sPi is roughly 3.13367566837834182018-08-12 07:51:57 INFO AbstractConnector:318 - Stopped Spark@9635fa&#123;HTTP/1.1,[http/1.1]&#125;&#123;0.0.0.0:4040&#125;2018-08-12 07:51:57 INFO SparkUI:54 - Stopped Spark web UI at http://spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver-svc.default.svc:40402018-08-12 07:51:57 INFO KubernetesClusterSchedulerBackend:54 - Shutting down all executors2018-08-12 07:51:57 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint:54 - Asking each executor to shut down2018-08-12 07:51:57 INFO KubernetesClusterSchedulerBackend:54 - Closing kubernetes client2018-08-12 07:51:57 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!2018-08-12 07:51:57 INFO MemoryStore:54 - MemoryStore cleared2018-08-12 07:51:57 INFO BlockManager:54 - BlockManager stopped2018-08-12 07:51:57 INFO BlockManagerMaster:54 - BlockManagerMaster stopped2018-08-12 07:51:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!2018-08-12 07:51:57 INFO SparkContext:54 - Successfully stopped SparkContext2018-08-12 07:51:57 INFO ShutdownHookManager:54 - Shutdown hook called2018-08-12 07:51:57 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-435d5ab2-f7b4-45d0-a00f-0bd9f162f9db 执行结束后executor pod被自动清除。计算得到pi的值为： 1Pi is roughly 3.1336756683783418 如果作业通过cluster提交，driver容器会被保留，可以查看： 123456789# minikube service list|-------------|------------------------------------------------------|-----------------------------|| NAMESPACE | NAME | URL ||-------------|------------------------------------------------------|-----------------------------|| default | kubernetes | No node port || default | spark-pi-27fcc168740e372292b27185d124ad7b-driver-svc | No node port || kube-system | kube-dns | No node port || kube-system | kubernetes-dashboard | http://192.168.99.100:30000 ||-------------|------------------------------------------------------|-----------------------------| 第三部分 常见报错异常处理1、如果遇到下面的报错信息，可能是Spark版本太低，建议升级大于2.4.5+以上版本后重试。 1234567891011121314151617181920212020-08-09 10:13:14 WARN KubernetesClusterManager:66 - The executor's init-container config map is not specified. Executors will therefore not attempt to fetch remote or submitted dependencies.2020-08-09 10:13:14 WARN KubernetesClusterManager:66 - The executor's init-container config map key is not specified. Executors will therefore not attempt to fetch remote or submitted dependencies.2020-08-09 10:13:15 WARN WatchConnectionManager:185 - Exec Failure: HTTP 403, Status: 403 -java.net.ProtocolException: Expected HTTP 101 response but was '403 Forbidden'at okhttp3.internal.ws.RealWebSocket.checkResponse(RealWebSocket.java:216)at okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:183)at okhttp3.RealCall$AsyncCall.execute(RealCall.java:141)at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)2020-08-09 10:13:15 ERROR SparkContext:91 - Error initializing SparkContext.io.fabric8.kubernetes.client.KubernetesClientException: at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager$2.onFailure(WatchConnectionManager.java:188)at okhttp3.internal.ws.RealWebSocket.failWebSocket(RealWebSocket.java:543)at okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:185)at okhttp3.RealCall$AsyncCall.execute(RealCall.java:141)at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748) 2、如果发现拉取镜像的比较慢，或者任务状态一直停留也pulling。这是由于minikube主机中本地是没有这个私有镜像，需要进入minikube组件提前将dockerhub中的远程镜像拉取到本地。否则每次提交任务，都会从远程拉取，由于网络条件的限制（你懂的），导致每次拉取都很慢或者超时。 1234567891011root@deeplearning:~# minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\| |/' _ `\| || , &lt; ( ) ( )| '_`\ /'__`\| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )( ___/(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)$ docker images#显示本地没有，然后手动提前拉取到本地$ docker pull rongxiang1986/spark:2.4.6 2、spark 任务每个容器至少需要一个CPU，如果启动的minikube集群的资源是默认的2CPU，如果单个任务申请多个执行器就会报资源不足。所以在创建minikube集群时，提前分配足够的资源。 参考文献1、Running Spark on Kubernetes ：https://spark.apache.org/docs/latest/running-on-kubernetes.html 2、在Minikube Kubernetes集群上运行Spark工作：https://iamninad.com/running-spark-job-on-kubernetes-minikube/]]></content>
      <categories>
        <category>Minikube spark Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于正则表达式使用和总结]]></title>
    <url>%2F2018%2F06%2F25%2F2018-08-06-%E5%85%B3%E4%BA%8E%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%BD%BF%E7%94%A8%E5%92%8C%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景正则表达式（regular Expression）是计算机中的概念。regular这里是规则、规律的意思，字面翻译指的是：规则的表达式。正则表达式主要用来处理字符串的工具，拥有自己独特的语法。 计算机常用语言均支持正则表达式，语法都是相同的，区别在于不同的语言支持的语法略有差异。 在数据科学实践中，特别是文本数据分析中，由于实际文本数据的纷繁复杂，正则表达式就成为文本检索分析的重要工具。本篇介绍以python语言背景来介绍正则表达式的实现。 https://jex.im/regulex/#!flags=&amp;re=%5E(a%7Cb)*%3F%24 第一章 正则表达式字符匹配正则表达式可以理解为一种字符串模式识别，识别对象有两个：子字符串、子字符串位置。 python中内置正则表达式包，直接import re。比如下面实现一个精确匹配： 1234import repattern = re.compile('hello')print(pattern.findall('hello'))# ['hello'] 1.1 两种模糊匹配正则表达式的强大主要实现模糊匹配，主要有：横向模糊匹配、纵向模糊匹配。 1、横向模糊匹配定义：正则匹配的字符串的长度不是固定的。 举一个栗子： 我们需要匹配字符串中具有特有的模式子串：第一个字符是’a’，最后一个字符是‘c’，中间部分长度可变，但是均为’b’，数量范围为[2,3]。具体代码实现如下： 1234import repattern = re.compile('ab&#123;2,3&#125;c')print(pattern.findall('abbcabbbbcaaabcaabbbc'))# ['abbc', 'abbbc'] 注意findall函数是全局匹配，即按顺序遍历字符串进行匹配。 2、纵向模糊匹配定义：正则匹配的字符串，存在一些位置的值不固定。 举个栗子： 我们需要匹配字符串具有的模式为：第一个和最后一个字符为‘a’、’b’，中间部分长度为1，但是字符可选，备选集合为：{b,c,d} 。代码实现如下： 1234import repattern = re.compile('a[bcd]e')print(pattern.findall('abeaaacdeade'))# ['abe', 'ade'] 正则表达式匹配的主要模式就是横向模糊、纵向模糊及两种模式的组合。 1.2 字符组 这里字符组其实匹配的是单个字符，并不是“组”哈。 范围表示方法例如在纵向模糊中我们举的例子，用[b,c,d]表示这三个字符的其中一个。在实际中如果备选集合比较大怎么办？有简略的表达式，比如： 123# [0,1,2,3,4,5,6,7,8,9] 等价表示为[0-9]# [a,b,c,d,e,f,g,h] 等价表示为[a-h]# 还可以各种类型字符合并写：[0-9a-h] 上面范围表达式，会被自动解析为连续字符。 有些杠精要问了假如备选集合中有横杆字符’-‘字符咋办，比如{‘a’,’-‘,’c’}，这时候我们不能写写成这样了。要这样写：[-ac]、[ac-]、[a\-c]，注意这里的转义符: ‘\’，后续我们再介绍。 排除字符组（反义字符组）在纵向模糊匹配中，还有一种匹配模式：某个字符除了在排除集中的字符，可以是任意字符。 1# 例如[^abc]，表示一个除"a"、"b"、"c"之外的任意一个字符。字符组的第一位放^（脱字符），表示求反的概念。 最后我们介绍简写形式，即解析语义约定俗成的简短写法。 \d就是[0-9]。表示是一位数字。记忆方式：其英文是digit（数字）。 \D就是[^0-9]。表示除数字外的任意字符。 \w就是[0-9a-zA-Z_]。表示数字、大小写字母和下划线。记忆方式：w是word的简写，也称单词字符。 \W是[^0-9a-zA-Z_]。非单词字符。 \s是[ \t\v\n\r\f]。表示空白符，包括空格、水平制表符、垂直制表符、换行符、回车符、换页符。记忆方式：s是space character的首字母。 \S是[^ \t\v\n\r\f]。 非空白符。 .就是[^\n\r\u2028\u2029]。通配符，表示几乎任意字符。换行符、回车符、行分隔符和段分隔符除外。 1.3 量词简写形式量词即连续重复的字符模式。 {m,n} 表示至少出现次数范围为：大于等于m，小于等于n。 {m,} 表示至少出现m次，即{m,+infinity}。{m} 等价于{m,m}，表示出现m次。 ? 等价于{0,1}，表示出现或者不出现。记忆方式：问号的意思表示，有吗？ +等价于{1,}，表示出现至少一次，即{1,+infinity}。 * 等价于{0,}，表示出现任意次，即{0,+infinity}。 贪婪匹配和惰性匹配先看一个栗子： 1234import repattern = re.compile('\d&#123;2,5&#125;')print(pattern.findall('123a1234b12345c123456d'))# ['123', '1234', '12345', '12345'] 上面的正则表达式会匹配连续出现的数值型字符串，长度范围为：2，3，4，5。这种模式是贪婪模式，即尽可能的匹配到最长范围。 而对应的由惰性匹配，即尽可能少的匹配： 1234import repattern = re.compile('\d&#123;2,5&#125;?')print(pattern.findall('123a1234b12345c123456d'))# ['12', '12', '34', '12', '34', '12', '34', '56'] 量词组通过问号实现惰性匹配，还有其他情况： {m,n}? {m,}? ?? +? ? 记忆技巧：问号的含义是反问：还不知足吗？意思就是要惰性一点。。。。 1.4 多选分支一个模式可以实现横向和纵向模糊匹配。而多选分支可以支持多个子模式任选其一。 具体形式如下：(p1|p2|p3)，其中p1、p2和p3是子模式，用|（管道符）分隔，表示其中任何之一。 例如要匹配”good”和”nice”可以使用/good|nice/。测试如下： 1234import repattern = re.compile('goodby|good')print(pattern.findall('googbygood'))# ['good'] 也就是说，分支结构也是惰性的，即当前面的匹配上了，后面的就不再尝试了。 1.5 案例分析 匹配时间字符串 例如需要匹配字符串中24小时制的时间字符串，比如：10:30，23:59 12345import re# 错误：pattern = re.compile('\d&#123;2&#125;:\d&#123;2&#125;')pattern = re.compile('[01]\d:[0-5]\d|2[0-3]:[0-5]\d')print(pattern.findall('ahjs10:21s7:2sd23:59wi01:01iis09:02w'))# ['10:21', '23:59', '01:01', '09:02'] 第二章 正则表达式位置匹配正则表达式属于模式匹配，主要两种匹配目标：字符和位置。 字符串的位置位置定义：相邻字符之间的位置。 锚字符Python中的锚字符 ^(脱字符)和$(美元符) ^(脱字符)，用来匹配字符串开头。多行模式下，匹配每行的开头位置。 $(美元符)，用来匹配字符串结尾。多行模式下，匹配每行的结尾位置。 举个栗子： 第三章 正则表达式括号的作用第四章 正则表达式回溯原理第五章 正则表达式的拆分第六章 正则表达式的构建第七章 正则表达式的性能调优第八章 Python的正则表达式处理函数写在最后参考文献1、]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-集成学习综述]]></title>
    <url>%2F2018%2F06%2F09%2F2018-06-09-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[文章分为两个部分： 第一部分，介绍集成学习的理论原理。 第二部分，实践部分。主要使用sklearn封装的包。 第一部分 理论原理什么是集成学习​ 首先集成学习（ensemble learning）不是具体的算法，应该算一种算法思想（Algorithm Framework，类比EM算法）。在机器学习中，通过一定组合策略，将多种学习器组合起来，以此提升泛化能力的框架统称为集成学习。 ​ 集成学习框架可以分成两个部分：基分类器集合、组合策略。 ​ 最常用的集成学习框架有：Bagging、Boosting、Stacking，在机器学习的比赛中被广泛使用。 BaggingBootstrap aggregating 简称Bagging，下面的流程图很清楚的说明了Bagging的框架流程。 算法流程 算法输入：$TrainData \ D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$ ​ 基学习器： $L$ ​ 训练迭代的次数： $T$ 算法过程： 1、For t =1,2,3，…,T DO: 2、对训练集$D$进行第t次随机采样，共采集m次，得到包含m个样本的采样集$Dt$ 3、$h_t=L(D,D_t)$ 4、end for 算法输出：$H(x)=argmax_{y \in Y}{\sum_{t=1}^{T}I(h_t(x)=y)}$ 关键步骤： 随机采样：每次采样，从训练集中采样固定数量的样本。并且是有放回的。每次未被采样的数据称为袋外数据（Out Of Bag, 简称OOB ），作为测试集，检测基学习器的泛化能力。 Bagging的基学习器可以是“异质”的。 Bagging的组合策略有：投票法（分类问题）、平均值（回归问题）。 随机森林（Random Forest）是最常用的Bagging算法架构，基学习器使用CART决策树。当然RF对原架构有部分改动： RF中每个基分类器（决策树），每个节点对特征进行部分采样，然后在采样集中选择最优特征。提高基分类器的泛化能力。 普通的CART并不对特征进行采样。 Boosting（Adaboost）在Bagging框架下，基学习器是相互独立的学习过程。而在boosing框架中，基学习器存在前后依赖关系。可以将弱学习器（强于随机猜想）提升为强学习器。 数据集有$n$个点，Boosting框架给数据集赋予了权重分布结构${w_i}_{i=1}^n$表示这个点重要性（这些权重值会在代价函数中体现）。对于前项学习器分类错误的点，提高权重（即提高下一项学习器代价函数对于“错”点关注）。最后得到一个学习器序列${y_i}_{i=1}^M$,最后通过加权得到最终的模型。 上面的例子只是简单介绍。Boosting最出名的算法框架为Adaboost（adaptive boosting）。 Adaboost 算法流程以二元分类为例： 算法输入：$TrainData \ D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$ ​ 基学习器： $L$ ​ 训练迭代的次数： $T$ 算法过程： 1、初始化数据集权值分布：$D_1(x)=1/m$ 2、For t=1,2,3,…,T DO 3、$h_t=L(D,D_t)$ 4、$\epsilon_t=P_{x\sim D_t}(h_t(x)\ne f(x))$ 5、if $\epsilon_t &gt;0.5$ then break 6、$\alpha_t=\dfrac{1}{2}ln(\dfrac{1-\epsilon_t}{\epsilon_t} )$ 7、$D_{t+1}(x)=\dfrac{D_t(x)}{Z_t}exp(-\alpha_tf(x)h_t(x))$ 8、end for 算法输出：$H(x)=sign{\sum_{t=1}^{T}}\alpha_t h_t(x)$ 注记：Boosting只能针对二分类的问题。对于多分类问题已经有相关的推广工作。 Multi-class AdaBoost ：https://web.stanford.edu/~hastie/Papers/samme.pdf 该论文的算法已经在sklearn中进行封装实现。第二部分我们将讲解。 Stacking第二部分 实践sklearn中有集成学习丰富的封装包： The sklearn.ensemble module includes ensemble-based methods for classification, regression and anomaly detection. User guide: See the Ensemble methods section for further details. ensemble.AdaBoostClassifier([…]) An AdaBoost classifier. ensemble.AdaBoostRegressor([base_estimator, …]) An AdaBoost regressor. ensemble.BaggingClassifier([base_estimator, …]) A Bagging classifier. ensemble.BaggingRegressor([base_estimator, …]) A Bagging regressor. ensemble.ExtraTreesClassifier([…]) An extra-trees classifier. ensemble.ExtraTreesRegressor([n_estimators, …]) An extra-trees regressor. ensemble.GradientBoostingClassifier([loss, …]) Gradient Boosting for classification. ensemble.GradientBoostingRegressor([loss, …]) Gradient Boosting for regression. ensemble.IsolationForest([n_estimators, …]) Isolation Forest Algorithm ensemble.RandomForestClassifier([…]) A random forest classifier. ensemble.RandomForestRegressor([…]) A random forest regressor. ensemble.RandomTreesEmbedding([…]) An ensemble of totally random trees. ensemble.VotingClassifier(estimators[, …]) Soft Voting/Majority Rule classifier for unfitted estimators. AdaBoostClassifier参数介绍源码路径：sklearn\ensemble\weight_boosting.py 主要参数： base_estimator 12345&gt; base_estimator : object, optional (default=DecisionTreeClassifier)&gt; The base estimator from which the boosted ensemble is built.&gt; Support for sample weighting is required, as well as proper `classes_`&gt; and `n_classes_` attributes.&gt; 基分类器。默认为决策树（DecisionTreeClassifier）。基分类器的选择有个trick在algorithm参数中一并介绍。 n_estimators n_estimators : integer, optional (default=50) The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. 最大训练迭代次数。默认值为50。另外从boosting原理看，也可以理解为最大生成基学习器的序列长度。 learning_rate 12345&gt; learning_rate : float, optional (default=1.)&gt; Learning rate shrinks the contribution of each classifier by&gt; ``learning_rate``. There is a trade-off between ``learning_rate`` and&gt; ``n_estimators``.&gt; 学习率（步长）。源码中的介绍也提到参数的选择和参数n_estimators是一个trade-off。 algorithm 1234567&gt; algorithm : &#123;'SAMME', 'SAMME.R'&#125;, optional (default='SAMME.R')&gt; If 'SAMME.R' then use the SAMME.R real boosting algorithm.&gt; ``base_estimator`` must support calculation of class probabilities.&gt; If 'SAMME' then use the SAMME discrete boosting algorithm.&gt; The SAMME.R algorithm typically converges faster than SAMME,&gt; achieving a lower test error with fewer boosting iterations.&gt; 算法，其实应该是权重更新算法。参数有两个选择：’SAMME’, ‘SAMME.R’，默认是“’SAMME.R”。理解两个算法的区别就需要我们回溯到原论文：《Multi-class AdaBoost》： SAMME 算法 SAMME.R 算法 简单的理解：如果使用SAMME.R算法，需要基分类器的输出是概率值。即在sklearn框架中需要class中有predict_proba函数。 可以通过下面的python脚本查看哪些分类器是满足的。 12345import inspectfrom sklearn.utils.testing import all_estimatorsfor name, clf in all_estimators(type_filter='classifier'): if 'sample_weight' in inspect.getargspec(clf().fit)[0]: print(name) random_state 123456&gt; random_state : int, RandomState instance or None, optional (default=None)&gt; If int, random_state is the seed used by the random number generator;&gt; If RandomState instance, random_state is the random number generator;&gt; If None, the random number generator is the RandomState instance used&gt; by `np.random`.&gt; 随机种子。参数随意，如果要结果能复现，需要相同的随机种子。 AdaBoostRegressor参数简介源码路径：sklearn\ensemble\weight_boosting.py 主要参数： base_estimator 1234&gt; base_estimator : object, optional (default=DecisionTreeRegressor)&gt; The base estimator from which the boosted ensemble is built.&gt; Support for sample weighting is required.&gt; 基回归器。默认为回归树（DecisionTreeRegressor）。 n_estimators 1234&gt; n_estimators : integer, optional (default=50)&gt; The maximum number of estimators at which boosting is terminated.&gt; In case of perfect fit, the learning procedure is stopped early.&gt; 最大训练迭代次数。默认值为50。同AdaBoostClassifier。 learning_rate 12345&gt; learning_rate : float, optional (default=1.)&gt; Learning rate shrinks the contribution of each regressor by&gt; ``learning_rate``. There is a trade-off between ``learning_rate`` and&gt; ``n_estimators``.&gt; 学习率（步长）。 loss 1234&gt; loss : &#123;'linear', 'square', 'exponential'&#125;, optional (default='linear')&gt; The loss function to use when updating the weights after each&gt; boosting iteration.&gt; 误差。 这个参数只有AdaBoostRegressor有，Adaboost.R2算法需要用到。有线性‘linear’, 平方‘square’和指数 ‘exponential’三种选择, 默认是线性，一般使用线性就足够了，除非你怀疑这个参数导致拟合程度不好。这个值的意义在原理篇我们也讲到了，它对应了我们对第k个弱分类器的中第i个样本的误差的处理，即：如果是线性误差，则eki=|yi−Gk(xi)|Ekeki=|yi−Gk(xi)|Ek；如果是平方误差，则eki=(yi−Gk(xi))2E2keki=(yi−Gk(xi))2Ek2，如果是指数误差，则eki=1−exp（−yi+Gk(xi))Ek）eki=1−exp（−yi+Gk(xi))Ek），EkEk为训练集上的最大误差Ek=max|yi−Gk(xi)|i=1,2…m random_state 123456&gt; random_state : int, RandomState instance or None, optional (default=None)&gt; If int, random_state is the seed used by the random number generator;&gt; If RandomState instance, random_state is the random number generator;&gt; If None, the random number generator is the RandomState instance used&gt; by `np.random`. &gt; 随机数种子。 调参心得传授集成学习的参数按照层次可以分为两层： 1、集成框架的参数。 2、基分类器的参数。 例子参考文献1、ROC和AUC介绍以及如何计算AUC（http://alexkong.net/2013/06/introduction-to-auc-and-roc/） 2、sklearn（http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics） 3、wiki百科 （https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF） 4、统计学习方法 5、深度学习]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[5分钟介绍深度学习（科普）]]></title>
    <url>%2F2018%2F04%2F16%2F2018-04-15-5%E5%88%86%E9%92%9F%E4%BB%8B%E7%BB%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%A7%91%E6%99%AE%EF%BC%89%2F</url>
    <content type="text"><![CDATA[历史背景最近几年Deep Learning、AI人工智能、机器学习等名词称为新闻热点，特别是Google Deep mind的Alpha Go战胜韩国棋手李世石，让深度学习妇孺皆知。 首先从概念范畴上讲，deep learning属于机器学习的一个分支，追根溯源其实是人工神经网络。顾名思义，人工神经网络是借鉴人类神经网路的结构原型，算生物仿生学（虽然人类到现在也没弄明白大脑原理）。 例如下面就是一个3层结构的人工神经网络（1个输入层、1个隐藏层、1个输出层）。 1989年Yann LeCun使用反向传播算法（Back Propagation）应用于多层神经网络训练。但一旦层数较大，网络的参数和训练计算量成倍增加，通常需要几周时间才能完成参数训练，另外反向传播算法容易梯度爆炸。研究人员获得好的结果，时间成本太大。 所以当时机器学习研究方向中，支持向量机（SVM）算法比多层神经网络更为热门，神经网络研究则相当冷门。 直到2012 年的ImageNet 图像分类竞赛中，Alex Krizhevsky使用CNN（卷积）多层网络（共8层、6千万个参数）赢得当年的比赛，领先第二名10.8个百分点。并且模型使用GPU芯片训练、引入正则技术（Dropout）。 从此多层神经网络成为机器学习中的研究热点。而为了“洗白”以前暗淡历史，被赋予了新的名称：Deep Learning。 深度学习背后的数学目前的人工智能均属于弱人工智能（不具备心智和意识）。事实上，深度学习目前主要在图像识别和声音识别场景中获得较好的效果。深度学习的成为热点，依赖于两方面条件的成熟: 算力的提升，训练中大量使用GPU。 大量数据的获得和沉淀。 弱人工智能背后的理论基础依赖于数学和统计理论，其实更应该算数据科学的范畴。 比如输入数据具有$m$维特征，而输出特征为$k$维（例如如果是个二分类问题，$k=2$）。我们使用3层神经网络（输入层为$m$维，即含有$m$个神经元；隐藏层为$n$维，输出层为$k$维）用来训练。 通常我们将输入数据看成$R^m$（$m$维欧几里得空间），输出数据看成$R^k$（$k$维欧式空间）,如下图： 从数学上看，神经网络的结构定义了一个函数空间：${R^{n}} \xrightarrow{\text{f}} {R^{h}} \xrightarrow{\text{g}} {R^{m}}$ 。这个函数空间中元素是非线性的（隐藏层和输出层有非线性的激活函数）。空间中每个函数由网络中的参数（w，b）唯一决定。 神经网络训练的过程可以形象的理解为寻找最佳函数的过程：输入层“吃进”大量训练数据，通过非线性函数的作用，观察输出层输出结果和实际值的差异。这是一个监督学习的过程。 如果差异（误差）在容忍范围内，停止训练，认为该函数是目标函数。 如果差异较大，反向传播算法根据梯度下降的反向更新网络中的参数（w，b），即挑选新的函数。 重新喂进数据，计算新挑选函数的误差。如此循环，直到找到目标函数（也可设置提前结束训练）。 为什么神经网络的发展最后偏向的是“深度”呢？即增加层数来提高网络的认知能力。为什么没有“宽度学习”？即增加网络隐藏层的维度（宽度）。 其实从数学上可以证明深度网络和“宽度网络”的等价性。证明提示：考虑网络定义的函数空间出发。 写在最后（畅想未来）深度学习的局限性思考目前深度学习被各行各业应用于各种场景，而且有些特定场景取得了良好结果。但是传统的深度学习仍属于监督学习，更像一个被动的执行者，按照人类既定的规则，吃进海量数据，然后训练。 那么深度网络是否真的理解和学到了模式？还是只学会对有限数据的模式识别？甚至就是一个庞大的记忆网络？这都是值得我们深度思考的。 GAN对抗网络那么怎么能说明模型真的学习并理解了。我们提出了一个原则：如果你理解了一个事物，那么你就可以创造它。这样就发明了GAN对抗网络。让网络自己去创造事物，然后用现实数据去监督，当网络的创造能力和现实接近时，我们认为网络学会了。 其实思想类似传统的遗传算法。 强化学习另外传统的深度学习，输入的环境（数据）是固定的。然而现实中我们学习过程其实是：环境（数据）与学习个体互相作用的交互过程。这个学习过程人类由于时间有限，是个漫长的过程。但是计算机有个天然优势，可以同时启用成千上万的学习个体完成与环境数据的交互学习过程。例如Alpha Go启用上万个体，两两互搏，配上强大算力，短时间完成学习，这是人类不可企及的。 迁移学习人类学习中还有个方法叫：触类旁通。其实就是不同场景训练模型的借鉴。例如A场景得到训练好的模型（网络参数），对于新的场景B，可以尝试直接用A场景的网络（或部分使用，拼接），以此来减少训练成本。 那么新的问题来了：是否具有统一的迁移标准，即什么模型是适合迁移的？如果这些问题没有理论基础支持，迁移学习也摆脱不了“炼丹术”的非议。 深度学习从过去的暗淡无色到现在的光耀夺目。 然而任何方法都是有边际效应的。 人工智能的终点还很遥远，谁是下一颗耀眼的明星，需要学界和工业界共同探索。 ​ ​ 2018年4月15日 夜]]></content>
      <categories>
        <category>network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[计算机语言中编译和解释的总结]]></title>
    <url>%2F2018%2F04%2F14%2F2018-04-14-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%BC%96%E8%AF%91%E5%92%8C%E8%A7%A3%E9%87%8A%E7%9A%84%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 几个概念 第二部分 编译器和解释器 第三部分 解释型语言和编译型语言 参考文献及资料 背景 非计算机科班，主要是总结给自己看的，如果有表达错误，请大家指正。 第一部分 几个概念高级语言与低级语言高级语言（High Level Programming Language）和低级语言（Low Level Programming Language）是一对相对的共生概念（没有一个严格的量化区分标准）。 低级语言更接近计算机底层资源（直接与硬件资源进行交互）。例如汇编语言。 高级语言进行了封装和抽象，语言设计更容易被人类思维逻辑所理解（和低级语言比较，学习曲线较缓）。例如C、C++、java、python等。 随着计算机语言的蓬勃发展（计算机语言的文艺复兴），过去一些高级语言，也有人重新定位成低级语言，例如C语言。 字节码与机器码字节码（Byte Code）不是一种计算机语言。属于高级语言预编译生成的中间码。高级语言源码在预编译的过程中，就完成这部分工作，生成字节码。 机器码（Machine Code）是一组可以直接被CPU执行的指令集。所有语言（低级和高级）最后都需要编译或解释成机器码（CPU指令集），才能执行。 第二部分 编译器和解释器编译器（Interpreter） A compiler is a computer program (or a set of programs) that transforms source code written in a programming language (the source language) into another computer language (the target language), with the latter often having a binary form known as object code. The most common reason for converting source code is to create an executable program. 编译器是一种计算机程序。 编译器是一个计算机语言的翻译工具，直接将源代码文件预编译（形象的说：翻译）成更低级的代码语言（字节码码、机器码）。 编译器不会去执行编译的结果，只生成编译的结果文件。 解释器（Compiler） In computer science, an interpreter is a computer program that directly executes, i.e. performs, instructions written in a programming or scripting language, without previously compiling them into a machine language program. An interpreter generally uses one of the following strategies for program execution: 1、parse the source code and perform its behavior directly. 2、translate source code into some efficient intermediate representation and immediately execute this. 3、explicitly execute stored precompiled code made by a compiler which is part of the interpreter system. 解释器是一种计算机程序。 解释器读取源代码或者中间码文件，转换成机器码并与计算机硬件交互。即逐行执行源码。 解释器会将源代码转换成一种中间代码不会输出更低级的编译结果文件。输出执行结果。 第三部分 解释型语言和编译型语言两者的区别主要是源码编译时间的差异。相同点都要翻译成机器码后由计算机执行。 编译型语言 编译语言的源码文件需要提前通过编译器编译成机器码文件（比如win中的exe可执行文件）。 执行时，只需执行编译结果文件。不需要重复翻译。 这类语言有：C、C++、Fortran、Pascal等。 解释型语言 解释型语言在运行时进行翻译。比如VB语言，在执行的时候，解释器将语言翻译成机器码，然后执行。 这类语言有：Ruby、Perl、JavaScript、PHP等。 混合型语言但是随着计算机语言的发展，有些语言兼具两者的特点。 JAVA语言 JAVA编译过程只是将.java文件翻译成字节码（Byte Code）（.class文件）。字节码文件交由java虚拟机（JVM）解释运行。也就是说Java源码文件既要编译也要JVM虚拟机进行解释后运行。所以有种说法认为Java是半解释型语言（semi-interpreted” language）。 Python语言 python其实类似Java。例如一个python文件test.py ，解释器首先尝试读取该文件历史编译结果（pyc文件）即test.pyc文件或者test.pyo 。如果没有历史文件或者编译文件的日期较旧（即py文件可能有更新），解释器会重新编译生成字节码文件（pyc文件），然后Python虚拟机对字节码解释执行。 参考文献及资料【1】 你知道「编译」与「解释」的区别吗？]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入理解Python语言中import机制]]></title>
    <url>%2F2018%2F04%2F14%2F2018-04-14-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Python%E8%AF%AD%E8%A8%80%E4%B8%ADimport%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 包和模块 第二部分 Import 方法 第三部分 命名空间（namespace） 第四部分 Import的过程 第五部分 将模块、包的路径加入检索路径 参考文献及资料 背景在Python语言使用过程中，经常会import第三方包，使用过程中或多或少遇到一些问题。本文将从原理层面简介Python中import机制，明白原理后，在遇到问题时候就会自己排错了。 第一部分 Python中的包和模块首先介绍Python中两个概念：包和模块。简单的理解（从文件系统角度），包（package）是一个文件夹，而模块（module）是一个Python源码文件（扩展名为.py）。 包（package）：文件夹（文件夹中含有文件__init__.py），包里面含有很多模块组成。 __init__.py文件，在里面自定义初始化操作，或为空。 模块（module）：即python文件，文件中定义了函数、变量、常量、类等。 第二部分 Import 方法2.1 Import 模块方法先看一个例子。我们经常使用的模块math ，背后对应其实是一个python文件：math.py 。该文件在C:\Anaconda3\Lib\site-packages\pymc3目录里面（具体环境会有差异）。 123import mathmath.sqrt(2)#1.4142135623730951 如果只要import math.py中具体的函数： 123from math import sqrt,sinsqrt(2)sin(1) 另外可以将模块中所有内容导入： 12from math import *sqrt(2) 2.2 Import 包方法包（package）可以简单理解为文件夹。该文件夹下须存在 __init__.py 文件, 内容可以为空。另外该主文件夹下面可以有子文件夹，如果也有 __init__.py 文件，这是子包。类似依次嵌套（套娃）。 例如Tensorflow的包（文件树）： 123456789101112root@vultr:~/anaconda3/lib/python3.6/site-packages/tensorflow# tree -L 1.├── aux-bin├── contrib├── core├── examples├── include├── __init__.py├── libtensorflow_framework.so├── __pycache__├── python└── tools __init__.py 文件在import包时，优先导入，作为import包的初始化。 我们以Tensorflow为例： 12345678#导入包import tensorflow as tf#导入子包：contribimport tensorflow.contrib as contribfrom tensorflow import contrib#导入具体的模块：mnistfrom tensorflow.examples.tutorials import mnistimport tensorflow.examples.tutorials.mnist 第三部分 命名空间（namespace）Namespace是字典数据，供编译器、解释器对源代码中函数名、变量名、模块名等信息进行关联检索（这是一个名称资源登记簿）。 3.1 定义Python语言使用namespace（命名空间）来存储变量，namespace是一个mapping（映射）。namespace可以理解是一个字典（dict）数据类型，其中键名（key）为变量名，而键值（value）为变量的值。 A namespace is a mapping from names to objects. Most namespaces are currently implemented as Python dictionaries。 每一个函数拥有自己的namespace。称为local namespace（局部命名空间），记录函数的变量。 每一个模块（module）拥有自己的namespace。称为global namespace（全局命名空间），记录模块的变量，包括包括模块中的函数、类，其他import（导入）的模块，还有模块级别的变量和常量。 每一个包（package）拥有自己的namespace。 也是global namespace ，记录包中所有子包、模块的变量信息。 Python的built-in names（内置函数、内置常量、内置类型）。 即内置命名空间。在Python解释器启动时创建，任何模块都可以访问。当退出解释器后删除。 3.2 命名空间的检索顺序当代码中需要访问或获取变量时（还有模块名、函数名），Python解释器会对命名空间进行顺序检索，直到根据键名（变量名）找到键值（变量值）。查找的顺序为（LEGB）： local namespace，即当前函数或者当前类。如找到，停止检索。 enclosing function namespace，嵌套函数中外部函数的namespace。 global namespace，即当前模块。如找到，停止检索。 build-in namespace，即内置命名空间。如果前面两次检索均为找到，解释器才会最后检索内置命名空间。如果仍然未找到就会报NameRrror（类似：NameError: name &#39;a&#39; is not defined）。 3.3 举栗子讲完了理论介绍，我们来举栗子，直观感受一下。 12345678910111213#进入python环境Python 3.5.3 |Anaconda custom (64-bit)| (default, May 11 2017, 13:52:01) [MSC v.1900 64 bit (AMD64)] on win32Type "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; print(globals())&#123;'__name__': '__main__', '__doc__': None, '__spec__': None, '__package__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__builtins__': &lt;module 'builtins' (built-in)&gt;&#125;&gt;&gt;&gt; x=1&gt;&gt;&gt; print(globals())&#123;'__name__': '__main__', '__doc__': None, '__spec__': None, '__package__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__builtins__': &lt;module 'builtins' (built-in)&gt;, 'x': 1&#125; 上面的例子我们查看了global namespace的字典（dict），其中&#39;__builtins__&#39;就是内置命名空间。新建变量x=1后，全局命名空间会新增这个K-V对（&#39;x&#39;: 1）。 还可以通过下面的方法查看import模块、包的namespace。 当我们import一个module（模块）或者package（包）时，伴随着新建一个global namespace（全局命名空间）。 1234567891011import mathmath.__dict__&#123;'__name__': 'math', 'tanh': &lt;built-in function tanh&gt;, 'nan': nan, 'atanh': &lt;built-in function atanh&gt;,'acosh': &lt;built-in function acosh&gt;, #中间略'trunc': &lt;built-in function trunc&gt;, 'acos': &lt;built-in function acos&gt;, 'sqrt': &lt;built-in function sqrt&gt;, 'floor': &lt;built-in function floor&gt;, 'gamma': &lt;built-in function gamma&gt;, 'cosh': &lt;built-in function cosh&gt;&#125;import tensorflowtensorflow.__dict__#包的所有模块、函数等命名空间信息。大家可以试一下。 大家可以动手试试其他的场景，比如函数内部查看locals() 。函数内部的变量global声明后，查看globals()字典会有怎样变化。这里就不再一一验证举栗了。 对于包，我们以tensorflow为例： 123456import tensorflowtensorflow.__dict__##中间略，只摘取部分信息。命名空间中包含module和function的信息。'angle': &lt;function tensorflow.python.ops.math_ops.angle&gt;, 'app': &lt;module 'tensorflow.python.platform.app' from '/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py'&gt;, 'arg_max': &lt;function tensorflow.python.ops.gen_math_ops.arg_max&gt;, 第四部分 Import的过程当我们执行import 模块、包时，主要有三个过程：检索、加载、名字绑定。 第一步：检索（Finder）Python解释器会对模块所属位置进行搜索： （1）检索：内置模块（已经加载到缓存中的模块）内置模块（已经加载到缓存中的模块），即在 sys.modules 中检索。Python已经加载到内存中的模块均会在这个字典中进行登记。如果已经登记，不再重复加载。直接将模块的名字加入正在import的模块的namespace。可以通过下面方法查看： 123456789&gt;&gt;&gt; import sys&gt;&gt;&gt; print(sys.modules)&#123;'_signal': &lt;module '_signal' (built-in)&gt;, 'os.path': &lt;module 'ntpath' from 'C:\Anaconda3\\lib\\ntpath.py'&gt;,pickle': &lt;module 'pickle' from 'C:\\Anaconda3\\lib\\pickle.py'&gt;, #中间略'subprocess':module 'subprocess' from 'C:\\Anaconda3\\lib\\subprocess.py'&gt;, 'sys': &lt;module 'ys' (built-in)&gt;, 'ctypes.util': &lt;module 'ctypes.util' from 'C:\\Anaconda3\\lib\ctypes\\util.py'&gt;, '_weakref': &lt;module '_weakref' (built-in)&gt;, '_imp': &lt;module_imp' (built-in)&gt;&#125; 如果不是built-in，value中会有模块的绝对路径信息。 通过key查找模块位置，如果value为None，就会抛出错误信息：ModuleNotFoundError。 如果key不存在，就会进入下一步检索。 如果我们导入过包，例如tensorflow。当我们要使用其中模块，需要该模块的全名（即全路径信息），例如：tensorflow.examples.tutorials.mnist.input_data ，因为sys.modules中只有全路径的key。 1234import tensorflowprint(sys.modules)##这个字典中会有tensorflow所有子包、模块的信息和具体的路径。#'tensorflow.examples.tutorials.mnist.input_data': &lt;module 'tensorflow.examples.tutorials.mnist.input_data' from '/root/anaconda3/lib/python3.6/site-packages/tensorflow/examples/tutorials/mnist/input_data.py'&gt; （2）检索 sys.meta_path逐个遍历其中的 finder 来查找模块。否则进入下一步检索。 （3）检索模块所属包目录如果模块Module在包（Package）中（如import Package.Module），则以Package.__path__为搜索路径进行查找。 （4）检索环境变量如果模块不在一个包中（如import Module），则以 sys.path 为搜索路径进行查找。 如果上面检索均为找到，抛出错误信息：ModuleNotFoundError。 第二步：加载（Loader）加载完成对模块的初始化处理： 设置属性。包括__name__、__file__、__package__和__loader__ 。 编译源码。编译生成字节码文件（.pyc文件），如果是包，则是其对应的__init__.py文件编译为字节码（*.pyc）。如果字节码文件已存在且仍然是最新的（时间戳和py文件一致），则不会重新编译。 加载到内存。模块在第一次被加载时被编译，载入内存，并将信息加入到sys.modules中。 也可以强制用reload()函数重新加载模块（包）。 第三步：名字绑定将模块和包的命名空间信息导入到当前执行Python文件的namespace（命名空间）。 第五部分 将模块、包的路径加入检索路径讲完了枯燥的理论背景，下面我们来介绍实际应用。当你写好一个模块文件，如何正确完成import模块？主要有下面两类方法： 5.1 动态方法（sys.path中添加）我们知道检索路径中sys.path，所以可以在import模块之前将模块的绝对路径添加到sys.path中。同样导入包需要加入包的文件夹绝对路径。具体方法如下： 12345import sys##sys.path.append(dir)sys.path.append('your\module（package）\file\path')##sys.path.insert(pos,dir)sys.path.insert(0,'your\module（package）\file\path') 注意：` 1、这里pos参数是插入sys.path这个list数据的位置，pos=0，即list第一位，优先级高。 2、需要注意有效范围，python程序向sys.path添加的目录只在此程序的生命周期之内有效。程序结束，失效。所以这是一种动态方法。 123456789#win7import sysprint(sys.path)#输出['', 'C:\\Python27\\lib\\site-packages\\pip-8.1.1-py2.7.egg', 'C:\\windows\\system32\\python27.zip', 'C:\\Python27\\DLLs', 'C:\\Python27\\lib', 'C:\\Python27\\lib\\plat-win', 'C:\\Python27\\lib\\lib-tk', 'C:\\Python27', 'C:\\Users\\rongxiang\\AppData\\Roaming\\Python\\Python27\\site-packages', 'C:\\Python27\\lib\\site-packages'] 5.2 静态方法（1）另外检索路径还有系统环境变量，所以可以将模块（包）路径添加在系统环境变量中。 （2）粗暴一点直接将模块（包）拷贝到sys.path的其中一个路径下面。但是这种管理比较乱。 （3）Python在遍历sys.path的目录过程中，会解析 .pth 文件，将文件中所记录的路径加入到sys.path ，这样.pth 文件中的路径也可以找到了。例如我们在C:\Python27\lib\site-packages 中新建一个.pth文件。例如： 12# .pth file for the your module or package'your\module（package）\file\path' 这样在模块（包）上线时，我们只需要将模块（包）的目录或者文件绝对路径放在新建的.path文件中即可。 参考文章1、http://www.cnblogs.com/russellluo/p/3328683.html# 2、https://github.com/Liuchang0812/slides/tree/master/pycon2015cn]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python系列文章-Argparse包使用介绍]]></title>
    <url>%2F2018%2F04%2F14%2F2022-03-14-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Argparse%E5%8C%85%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 argparse包介绍 第二部分 源码分析 参考文献及资料 背景对于完成部署的Python程序文件，启动的时候，通常使用下面的Shell命令行进行启动： 1$ python test.py arg1 arg2 其中arg1 arg2是程序的运行参数，运行前传给程序。对于参数解析的支持目前常用的方式有4种。 sys.argv方式 sys.argv是一个字符串的列表，包含了命令行参数的列表。例如下面的案例： 1234import sysprint '参数列表:', str(sys.argv)# 输入命令为：python test.py arg1 arg2# 回显：['test.py', 'arg1', 'arg2'] 所以可以通过sys.argv[1]获取参数值。优点是简单，缺点是需要注意参数顺序，缺参错位。 getopt 模块 处理参数更为灵活，支持短选项模式 - 和长选项模式 –。接口设计类似C语言中getopt函数。命令格式如下： 1getopt.getopt( [命令行参数列表], "短选项", [长选项列表] ) 官方案例（修改版）： 12345678910111213141516171819202122232425import getopt, sysdef main(): try: opts, args = getopt.getopt(sys.argv[1:], "ho:v", ["help", "output="]) except getopt.GetoptError as err: print(err) sys.exit(2) output = None verbose = False # 对参数进行解析 # 其中参数案例opts=[('-o', 'arg1'), ('-v', ''), ('--output', 'arg2')] for o, a in opts: if o == "-v": verbose = True elif o in ("-h", "--help"): print("help") sys.exit() elif o in ("-o", "--output"): output = a else: assert False, "unhandled option"if __name__ == "__main__": main() 上面的案例中，从 sys.argv[1:] 获取所有命令行参数。&quot;ho:v&quot;表示短参数定义，其中冒号表示改参数需要参数值。[&quot;help&quot;, &quot;output=&quot;]定义了长参数，其中等号表示参数需要有参数值。其他参数可以追加在命令后面（代码中args值）。例如下面案例： 1# python test.py -h -o arg1 -v --help --output=arg2 arg3 缺点：过于简单的功能支持。 optparse 内置包 optparse使用更具声明性的命令行分析样式：创建 OptionParser，用选项填充它，并解析命令行。例如： 12345678910from optparse import OptionParserparser = OptionParser()parser.add_option("-f", "--file", dest="filename",type='string', help="write report to FILE", metavar="FILE")parser.add_option("-q", "--quiet", action="store_false", dest="verbose", default=True, help="don't print status messages to stdout")(options, args) = parser.parse_args() 案例中，type表示参数值的类型，dest表示最终参数值的赋值对象，action表示存储方式，分为三种store、store_false、store_true，default表示缺失情况下的默认值，help表示帮助信息。 优点：提供部分封装功能，例如组装生成帮助信息。 argparse 内置包 最新包，用来替代老旧的optparse模块和getopt 模块。本文将详细介绍。 注：本文代码环境为Python 3.8.8，编译器为Cpython。 第一部分 argparse包介绍argparse是Python 3.2新引入的内置模块。argparse模块出现目的是对标准库的 optparse 模块进行了增强和替换。 处理位置参数。 支持子命令。 允许替代选项前缀例如 + 和 /。 处理零个或多个以及一个或多个风格的参数。 生成更具信息量的用法消息。 提供用于定制 type 和 action 的更为简单的接口。 1.1 简单使用使用流程如下： 12345678910# 导入模板import argparse# 创建parser对象parser = argparse.ArgumentParser()# 添加参数parser.add_argument()# 参数解析args = parser.parse_args()# 获取参数值print(args.变量名) 我们举一个最简单的案例（Python脚本需要输入文件名filename参数）： 12345678910111213#-*- coding: UTF-8 -*-import argparsedef parse_args(): parser = argparse.ArgumentParser(description="描述，在参数帮助文档之前显示的文本") parser.add_argument('--filename', '-f', type=str, required=True,dest='fileName', default='/temp/test.txt',help='The path of filname') args = parser.parse_args() return argsif __name__ == '__main__': args = parse_args() # 获取参数值 print(args.fileName) 命令行： 123$ python test1.py -f /temp/test.txt# 回显：/temp/test.txt 第二部分 源码分析argparse内置模块比较简单。源码位置：Lib\argparse.py 1234567Command-line parsing library# 命令行解析包This module is an optparse-inspired command-line parsing library that: - handles both optional and positional arguments - produces highly informative usage messages - supports parsers that dispatch to sub-parsers 举一个案例： 123456789101112131415#The following is a simple usage example that sums integers from the#command-line and writes the result to a file::# 求和并将结果写文件 parser = argparse.ArgumentParser( description='sum the integers at the command line') parser.add_argument( 'integers', metavar='int', nargs='+', type=int, help='an integer to be summed') parser.add_argument( '--log', default=sys.stdout, type=argparse.FileType('w'), help='the file where the sum should be written') args = parser.parse_args() args.log.write('%s' % sum(args.integers)) args.log.close() 2.1 ArgumentParser 对象123456789101112131415161718class ArgumentParser(_AttributeHolder, _ActionsContainer): """Object for parsing command line strings into Python objects. Keyword Arguments: - prog -- The name of the program (default: sys.argv[0]) - usage -- A usage message (default: auto-generated from arguments) - description -- A description of what the program does - epilog -- Text following the argument descriptions - parents -- Parsers whose arguments should be copied into this one - formatter_class -- HelpFormatter class for printing help messages - prefix_chars -- Characters that prefix optional arguments - fromfile_prefix_chars -- Characters that prefix files containing additional arguments - argument_default -- The default value for all arguments - conflict_handler -- String indicating how to handle conflicts - add_help -- Add a -h/-help option - allow_abbrev -- Allow long options to be abbreviated unambiguously """ 参数解释： prog ，Python程序名 (默认值: os.path.basename(sys.argv[0])) usage - 描述程序用途的字符串（默认值：从添加到解析器的参数生成） description - 在参数帮助文档之前显示的文本,程序功能描述（默认值：无） epilog - 在参数帮助文档之后显示的文本（默认值：无） parents - 一个 ArgumentParser 对象的列表，它们的参数也应包含在内 formatter_class - 用于自定义帮助文档输出格式的类 prefix_chars - 可选参数的前缀字符集合（默认值： ‘-‘） fromfile_prefix_chars - 当需要从文件中读取其他参数时，用于标识文件名的前缀字符集合（默认值： None） argument_default - 参数的全局默认值（默认值： None） conflict_handler - 解决冲突选项的策略（通常是不必要的） add_help - 为解析器添加一个 -h/--help 选项（默认值： True） allow_abbrev - 如果缩写是无歧义的，则允许缩写长选项 （默认值：True） 2.2 add_argument() 方法2.3 parse_args() 方法参考文章]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python系列文章-Argparse包使用介绍]]></title>
    <url>%2F2018%2F04%2F14%2F2022-03-31-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-networkx%E5%8C%85%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 argparse包介绍 第二部分 源码分析 参考文献及资料 背景NetworkX提供： 研究社会，生物和基础设施网络的结构和动态的工具; 适用于许多应用的标准编程接口和图形实现; 一个快速发展的多学科项目的环境; 现有数值算法的接口和用C，C ++和FORTRAN编写的代码;和 能够无痛地处理大型非标准数据集。 使用NetworkX，您可以以标准和非标准数据格式加载和存储网络，生成多种类型的随机和经典网络，分析网络结构，构建网络模型，设计新的网络算法，绘制网络等等。 环境部署12 本文运行环境： win7 python=3.8.8 networkx=2.5（依赖包：decorator=4.4.2） 第一部分 图简单实现1.1 创建图通过实例化创建图对象： 12import networkx as nxgraph = nx.Graph() 1.2 添加和删除节点 单个节点添加： 向图graph中添加一个名称为node1的节点。 1graph.add_node("node1") 多节点添加： 向图graph中添加名称为node2、node3的节点。 向图graph中添加名称为node4的节点，并且增加该节点的一个属性信息：&#39;id&#39;: 1 12graph.add_nodes_from(["node2", "node3"])graph.add_nodes_from([("node4", &#123;'id': 1, 'operation': 'add'&#125;)]) 删除节点 单节点和多节点删方法： 12graph.remove_node("node1")graph.remove_nodes_from(["node1","node2"]) 1.3 添加和删除边 添加节点和边 如果边的节点不存在会自动创建相应的节点。 12graph.add_edge('node5', "node6")graph.add_edge('node1', "node2") 批量添加边 1graph.add_edges_from([('node2', "node3"), ('node3', "node4")]) 1.4 图可视化networkx的可视化通过matplotlib包依赖实现。 12345import matplotlib.pyplot as pltnx.draw(graph, with_labels=True)plt.show()plt.legend() 1.5 图属性查询四个基本图形属性有：G.nodes，G.edges，G.adj和G.degree。这些是图中节点，边，邻居（邻接）和节点度的集合。 目前为止, 查看一下图中到底有多少个节点和边: 1g.number_of_nodes() 1g.number_of_edges() 第二部分 进阶使用2.1 有向图报错汇总1、在使用networkx的时候，在一台电脑上可以运行相关代码，而另一台则报错，出现如下错误：“networkx.exception.NetworkXError: random_state_index is incorrect” 原因：其实是decorator版本问题，报错电脑上的decorator的版本是5.0，而没有报错的版本为4.4.2，注意，我的networkx版本为2.5. 解决办法：将decorator版本降为4.4.2 输入下面代码安装即可 pip install decorator==4.4.2 最后问题得到解决。 第三部分 图算法实现图表也可以使用传统方法进行分析。这些方法通常是算法。它们包括： 最短路径算法，例如Dijkstra 算法 搜索算法，例如广度优先搜索算法 生成树算法，例如Prim 算法 图数据集数据集练习中使用了Stanford大型网络数据集集合（Leskovec和Krevl 2014）的3个数据集： 从2003年3月2日开始的Amazon产品共同购买网络，26.2万个节点，120万个边缘 Google的网络图，875k节点，边长5.1m Pokec在线社交网络，160万个节点，30.6m的边缘 尽管最容易根据算法的运行时间对软件包进行排名，但这只是构成一个好的软件包的众多考虑因素之一。我会根据我在这些软件包中的经验尝试提供更主观的观点。 参考文章1、networkx项目主页，链接：https://networkx.org/]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python系列文章-Argparse包使用介绍]]></title>
    <url>%2F2018%2F04%2F14%2F2022-03-31-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-networkx%E5%8C%85%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 argparse包介绍 第二部分 源码分析 参考文献及资料 背景NetworkX提供： 研究社会，生物和基础设施网络的结构和动态的工具; 适用于许多应用的标准编程接口和图形实现; 一个快速发展的多学科项目的环境; 现有数值算法的接口和用C，C ++和FORTRAN编写的代码;和 能够无痛地处理大型非标准数据集。 使用NetworkX，您可以以标准和非标准数据格式加载和存储网络，生成多种类型的随机和经典网络，分析网络结构，构建网络模型，设计新的网络算法，绘制网络等等。 环境部署12 本文运行环境： win7 python=3.8.8 networkx=2.5（依赖包：decorator=4.4.2） 第一部分 图简单实现1.1 创建图通过实例化创建图对象： 12import networkx as nxgraph = nx.Graph() 1.2 添加和删除节点 单个节点添加： 向图graph中添加一个名称为node1的节点。 1graph.add_node("node1") 多节点添加： 向图graph中添加名称为node2、node3的节点。 向图graph中添加名称为node4的节点，并且增加该节点的一个属性信息：&#39;id&#39;: 1 12graph.add_nodes_from(["node2", "node3"])graph.add_nodes_from([("node4", &#123;'id': 1, 'operation': 'add'&#125;)]) 删除节点 单节点和多节点删方法： 12graph.remove_node("node1")graph.remove_nodes_from(["node1","node2"]) 1.3 添加和删除边 添加节点和边 如果边的节点不存在会自动创建相应的节点。 12graph.add_edge('node5', "node6")graph.add_edge('node1', "node2") 批量添加边 1graph.add_edges_from([('node2', "node3"), ('node3', "node4")]) 1.4 图可视化networkx的可视化通过matplotlib包依赖实现。 12345import matplotlib.pyplot as pltnx.draw(graph, with_labels=True)plt.show()plt.legend() 1.5 图属性查询四个基本图形属性有：G.nodes，G.edges，G.adj和G.degree。这些是图中节点，边，邻居（邻接）和节点度的集合。 目前为止, 查看一下图中到底有多少个节点和边: 1g.number_of_nodes() 1g.number_of_edges() 第二部分 进阶使用2.1 有向图1DiGraph 报错汇总1、在使用networkx的时候，在一台电脑上可以运行相关代码，而另一台则报错，出现如下错误：“networkx.exception.NetworkXError: random_state_index is incorrect” 原因：其实是decorator版本问题，报错电脑上的decorator的版本是5.0，而没有报错的版本为4.4.2，注意，我的networkx版本为2.5. 解决办法：将decorator版本降为4.4.2 输入下面代码安装即可 pip install decorator==4.4.2 最后问题得到解决。 第三部分 图算法实现图表也可以使用传统方法进行分析。这些方法通常是算法。它们包括： 最短路径算法，例如Dijkstra 算法 搜索算法，例如广度优先搜索算法 生成树算法，例如Prim 算法 图数据集数据集练习中使用了Stanford大型网络数据集集合（Leskovec和Krevl 2014）的3个数据集： 从2003年3月2日开始的Amazon产品共同购买网络，26.2万个节点，120万个边缘 Google的网络图，875k节点，边长5.1m Pokec在线社交网络，160万个节点，30.6m的边缘 尽管最容易根据算法的运行时间对软件包进行排名，但这只是构成一个好的软件包的众多考虑因素之一。我会根据我在这些软件包中的经验尝试提供更主观的观点。 参考文章1、networkx项目主页，链接：https://networkx.org/]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据科学实践中常用开放数据集介绍]]></title>
    <url>%2F2018%2F04%2F05%2F2018-04-01-%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%9E%E8%B7%B5%E4%B8%AD%E5%B8%B8%E7%94%A8%E5%BC%80%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开放数据集 参考文献及资料 背景数据科学研究的对象是数据，学习过程中需要相关数据集辅助大家练习、做实验。从而体会数据科学中算法方法论。中国古语云：巧妇难有无米之炊，说的就是数据对于数据科学学习的重要性。 这篇文章收集介绍了各种常用的开放数据集，供大家学习参考。会持续更新。 第二部分 开放数据集这里主要将开放数据分为三类：图像类、自然语言（NLP）类、音频类。 1.1 图像类MNIST手写数据集 介绍： MNIST（全称：Modified National Institute of Standards and Technology database）数据集是常见的深度学习开放数据集（基本属于深度学习的hello world数据集）。这是一个手写阿拉伯数据集（0-9数字），数据主要采集于美国高中学生。数据集总量为7W个手写数字图像（训练集6w个、测试机1w个）。 文件 内容 train-images-idx3-ubyte.gz 训练集图片 - 60000张训练图片 train-labels-idx1-ubyte.gz 训练集图片对应的数字标签（0-9） t10k-images-idx3-ubyte.gz 测试集图片 - 10000 张 图片 t10k-labels-idx1-ubyte.gz 测试集图片对应的数字标签 数据存储大小：二进制文件，50M，压缩形式约10M。每张图像被归一化成28*28的像素矩阵。 图像数据格式：像素值为0到255. 0表示背景（白色），255表示前景（黑色）。例如下面手写数字1的数据矩阵表示： 官方网页连接：http://yann.lecun.com/exdb/mnist/ 读取数据案例（Python）： Tensorflow中已经有对MNIST数据集解析的脚本，我们可以直接调用： 文件 目的 input_data.py、mnist.py 用于读取MNIST数据集 12345678910111213141516import tensorflow as tf#tf为1.7版本import numpy as npfrom tensorflow.examples.tutorials.mnist import input_datadata_dir = '/root/tftest/mnistdata/'#data_dir为数据集文件存放目录mnist = input_data.read_data_sets(data_dir, one_hot=True，validation_size=5000)#mnist = input_data.read_data_sets(data_dir, one_hot=False)#one_hot参数True，表示标签进行one-hot编码处理。#validation_size参数可以从训练集中划出一部分数据作为验证集。默认是5w个，可以自己调节。x_train,y_train,x_test,y_test,x_vali,y_vali = \mnist.train.images,mnist.train.labels,mnist.test.images,mnist.test.labels,\mnist.validation.images,mnist.validation.labels#x_train的数据类型为：&lt;class 'numpy.ndarray'&gt; ​ 上面的例子划分好数据就可以喂给各种算法模型进行训练。 扩展：EMNIST数据集：https://arxiv.org/abs/1702.05373。 按照MNIST规范，数据集更大：包含240,000个训练图像和40,000个手写数字测试图像。 MS-COCO图像分割数据集 介绍： MS-COCO（全称是Common Objects in Context）是微软团队提供的一个可以用来进行图像识别的数据集。数据集中的图像分为训练、验证和测试集。COCO数据集现在有3种标注类型：object instances（目标实例）, object keypoints（目标上的关键点）, 和image captions（看图说话），使用JSON文件存储。 一共有33w张图像，80个对象类别，每幅图5个字母、25w个关键点。 数据存储大小：约25G（压缩形式） 数据格式：中文介绍可以参考知乎这篇文章：COCO数据集的标注格式 。 官方网站：http://mscoco.org/ ImageNet图像数据集 介绍： Imagenet是深度学习中大名鼎鼎的数据集。数据集有1400多万幅图片，涵盖2万多个类别；其中有超过百万的图片有明确的类别标注和图像中物体位置的标注。深度学习中关于图像分类、定位、检测等研究工作大多基于此数据集展开。Imagenet数据集文档详细，有专门的团队维护，使用非常方便，在计算机视觉领域研究论文中应用非常广，几乎成为了目前深度学习图像领域算法性能检验的“标准”数据集。 数据存储大小：约150G 官方网站：http://www.image-net.org/ Open Image图像数据集 介绍： Open Image为Google提供。数据集包含近900万个图像URL。这些图像已经用数千个类的图像级标签边框进行了注释。该数据集包含9,011,219张图像的训练集，41,260张图像的验证集以及125,436张图像的测试集。 数据大小：500G 官方网站：https://github.com/openimages/dataset VisualQA图像数据库 介绍： VQA是一个包含有关图像的开放式问题的数据集。这些问题需要理解视野和语言。数据集有265,016张图片。 数据大小：25G 官方网站：http://www.visualqa.org/ The Street View House Numbers (SVHN) Dataset街边号码牌数据集 介绍： SVHN图像数据集用于开发机器学习和对象识别算法，对数据预处理和格式化的要求最低。它可以被看作与MNIST相似，但是将更多标记数据（超过600,000个数字图像）并入一个数量级并且来自显着更难以解决的真实世界问题（识别自然场景图像中的数字和数字）。SVHN数据从谷歌街景图片中的房屋号码中获得的。书记含有用于训练的73257个数字，用于测试的26032个数字以及用作额外训练数据的531131个附加数字。 数据集大小： [train.tar.gz]， [test.tar.gz]， [extra.tar.gz ] 共三个文件。 官方网站：http://ufldl.stanford.edu/housenumbers/ CIFAR-10图像数据集 介绍： CIFAR-10数据集由10个类的60,000个图像组成（每个类在上图中表示为一行）。总共有50,000个训练图像和10,000个测试图像。数据集分为6个部分 - 5个培训批次和1个测试批次。每批有10,000个图像。 数据大小：170M 官方网站：http://www.cs.toronto.edu/~kriz/cifar.html Fashion-MNIST 介绍 Fashion-MNIST包含60,000个训练图像和10,000个测试图像。它是一个类似MNIST的时尚产品数据库。开发人员认为MNIST已被过度使用，因此他们将其作为该数据集的直接替代品。每张图片都以灰度显示，并与10个类别的标签相关联。 数据集大小：30M 官方网站：https://github.com/zalandoresearch/fashion-mnist 1.2 自然语言类数据库IMDB电影评论数据集 介绍： 这是电影爱好者的梦幻数据集。它具有比此领域以前的任何数据集更多的数据。除了训练和测试评估示例之外，还有更多未标记的数据供您使用。原始文本和预处理的单词格式包也包括在内。 数据集大小：80 M 官方网站：http://ai.stanford.edu/~amaas/data/sentiment/ 模型案例：https://arxiv.org/abs/1705.09207 Twenty Newsgroups Data Set 介绍： 该数据集包含有关新闻组的信息。为了管理这个数据集，从20个不同的新闻组中获取了1000篇Usenet文章。这些文章具有典型特征，如主题行，签名和引号。 数据集大小：20 M 官方网站：https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups 模型案例：https://arxiv.org/abs/1606.01781 Sentiment140情感分析数据集 介绍： Sentiment140是一个可用于情感分析的数据集。 数据集大小：80 M 官方网站：http://help.sentiment140.com/for-students/ 模型案例：http://www.aclweb.org/anthology/W17-5202 WordNet 介绍： WordNet是英语synsets的大型数据库。Synsets是同义词组，每个描述不同的概念。WordNet的结构使其成为NLP非常有用的工具。 数据集大小：10 M 官方网站：https://wordnet.princeton.edu/ 模型案例：https://aclanthology.info/pdf/R/R11/R11-1097.pdf Yelp评论 介绍： 这是Yelp为了学习目的而发布的一个开放数据集。它由数百万用户评论，商业属性和来自多个大都市地区的超过20万张照片组成。这是一个非常常用的全球NLP挑战数据集。 数据集大小：2.66 GB JSON，2.9 GB SQL和7.5 GB照片（全部压缩） 官方网站：https://www.yelp.com/dataset 模型案例：https://arxiv.org/pdf/1710.00519.pdf 维基百科语料库 介绍： 该数据集是维基百科全文的集合。它包含来自400多万篇文章的将近19亿字。什么使得这个强大的NLP数据集是你可以通过单词，短语或段落本身的一部分进行搜索。 数据集大小： 20 MB 官方网站：https://corpus.byu.edu/wiki/ 模型案例：https://arxiv.org/pdf/1711.03953.pdf 博客作者身份语料库 介绍： 此数据集包含从数千名博主收集的博客帖子，从blogger.com收集。每个博客都作为一个单独的文件提供。每个博客至少包含200次常用英语单词。 数据集大小： 300 MB 官方网站：http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm 模型案例：https://arxiv.org/pdf/1609.06686.pdf 欧洲语言的机器翻译 介绍： 数据集包含四种欧洲语言。 数据集大小： 约15 G 官方网站：http://statmt.org/wmt11/translation-task.html 模型案例：https://arxiv.org/abs/1706.03762 1.3 音频/语音数据集口语数字数据集 介绍： 为了解决识别音频样本中的口头数字的任务而创建。这是一个开放的数据集，所以希望随着人们继续贡献更多样本，它会不断增长。 数据集大小： 约10 G=M 记录数量：1500个音频样本 官方网站：https://github.com/Jakobovski/free-spoken-digit-dataset 模型案例：https://arxiv.org/pdf/1712.00866 免费音乐档案（FMA） 介绍： FMA是音乐分析的数据集。数据集由全长和HQ音频，预先计算的特征以及音轨和用户级元数据组成。它是一个开放数据集，用于评估MIR中的几个任务。以下是数据集连同其包含的csv文件列表： tracks.csv：所有106,574首曲目的每首曲目元数据，如ID，标题，艺术家，流派，标签和播放次数。 genres.csv：所有163种风格的ID与他们的名字和父母（用于推断流派层次和顶级流派）。 features.csv：用librosa提取的共同特征 。 echonest.csv：由Echonest （现在的 Spotify）为13,129首音轨的子集提供的音频功能 。 数据集大小： 约1T 记录数量：1500个音频样本 官方网站：https://github.com/mdeff/fma 模型案例：https://arxiv.org/pdf/1803.05337.pdf 舞厅 介绍： 该数据集包含舞厅跳舞音频文件。以真实音频格式提供了许多舞蹈风格的一些特征摘录。 以下是数据集的一些特征： 数据集大小： 约14 G 记录数量：约700个音频样本 官方网站：http://mtg.upf.edu/ismir2004/contest/tempoContest/node5.html 模型案例：https://pdfs.semanticscholar.org/0cc2/952bf70c84e0199fcf8e58a8680a7903521e.pdf 百万歌曲数据集 介绍： 百万歌曲数据集是音频功能和元数据的一百万当代流行音乐曲目可自由可用的集合。 其目的是： 鼓励对扩大到商业规模的算法进行研究 为评估研究提供参考数据集 作为使用API创建大型数据集的捷径（例如Echo Nest的） 帮助新研究人员在MIR领域开始工作 数据集的核心是一百万首歌曲的特征分析和元数据。该数据集不包含任何音频，只包含派生的功能。示例音频可以通过使用哥伦比亚大学提供的代码从7digital等服务中获取。 数据集大小： 约280 G 记录数量：它的一百万首歌曲！ 官方网站：https://labrosa.ee.columbia.edu/millionsong/ 模型案例：http://www.ke.tu-darmstadt.de/events/PL-12/papers/08-aiolli.pdf LibriSpeech 介绍： 该数据集是大约1000小时的英语语音的大型语料库。这些数据来自LibriVox项目的有声读物。它已被分割并正确对齐。如果您正在寻找一个起点，请查看已准备好的声学模型，这些模型在kaldi-asr.org和语言模型上进行了训练，适合评估，网址为http://www.openslr.org/11/。 数据集大小： 约60 G 记录数量：1000小时的演讲 官方网站：http://www.openslr.org/12/ 模型案例：https://arxiv.org/abs/1712.09444 VoxCeleb 介绍： VoxCeleb是一个大型的说话人识别数据集。它包含约1,200名来自YouTube视频的约10万个话语。数据大部分是性别平衡的（男性占55％）。名人跨越不同的口音，职业和年龄。开发和测试集之间没有重叠。对于隔离和识别哪个超级巨星来说，这是一个有趣的用例。 数据集大小： 约150 M 记录数量： 1,251位名人的100,000条话语 官方网站：http://www.robots.ox.ac.uk/~vgg/data/voxceleb/ 模型案例：https://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf 1.4 比赛数据Twitter情绪分析数据 介绍： 仇恨以种族主义和性别歧视为形式的言论已成为叽叽喳喳的麻烦，重要的是将这类推文与其他人分开。在这个实践问题中，我们提供既有正常又有仇恨推文的Twitter数据。您作为数据科学家的任务是确定推文是仇恨推文，哪些不是。 数据集大小： 约3 M 记录数量： 31,962条推文 官方网站：https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/ 印度演员的年龄检测 介绍： 对于任何深度学习爱好者来说，这是一个令人着迷的挑战。该数据集包含数千个印度演员的图像，你的任务是确定他们的年龄。所有图像都是手动选择的，并从视频帧中剪切，导致尺度，姿势，表情，照度，年龄，分辨率，遮挡和化妆的高度可变性。 数据集大小： 约48 M 记录数量： 训练集中的19,906幅图像和测试集中的6636幅图像 官方网站：https://datahack.analyticsvidhya.com/contest/practice-problem-age-detection/ 城市声音分类 介绍： 这个数据集包含超过8000个来自10个班级的城市声音摘录。这个实践问题旨在向您介绍常见分类方案中的音频处理。 数据集大小： 训练集 - 3 GB（压缩），测试集 - 2 GB（压缩） 记录数量： 来自10个班级的8732个城市声音标注的声音片段（&lt;= 4s） 官方网站：https://datahack.analyticsvidhya.com/contest/practice-problem-urban-sound-classification/ 参考文献及资料【1】 https://www.tensorflow.org/versions/r1.1/get_started/mnist/beginners 【2】 https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/ 【3】 https://deeplearning4j.org/cn/opendata]]></content>
      <categories>
        <category>Bigdata</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo:解决Typora编辑table无法被解析问题]]></title>
    <url>%2F2018%2F04%2F01%2F2018-04-05-Hexo%E8%A7%A3%E5%86%B3Typora%E7%BC%96%E8%BE%91table%E6%97%A0%E6%B3%95%E8%A2%AB%E8%A7%A3%E6%9E%90%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 掉坑背景 第二部分 爬坑过程和解决办法 第三部分 提示 参考文献及资料 第一部分 掉坑背景使用Typora编辑Makedown文件，添加表格，但是提交给Hexo渲染网页，无法正常解析显示，而是显示源码。例如：| Table Header 1 | Table Header 2 || ————– | ————– || Division 1 | Division 2 || Division 1 | Division 2 | 第二部分 爬坑过程和解决办法一开始认为是Hexo的bug，Google也没人遇到类似情况，都准备在github上建问题单了。最后本着严谨的态度，以文本的格式打开文档，发现表格源码和正文之间没有空行！！！！！ 这尼玛坑爹呀，所以Hexo无法解析，但是Typora能正常解析。空出一行后正常解析： 123456&lt;正文&gt;(空一行)| Table Header 1 | Table Header 2 || - | - | | Division 1 | Division 2 | | Division 1 | Division 2 | Table Header 1 Table Header 2 Division 1 Division 2 Division 1 Division 2 这一点Typora做的不够兼容（只怪他太过于强大的解析能力。。。。）。Tyopra不服了，我强大也有错？？哈哈哈 记录该坑供掉坑小伙伴参考。 第三部分 提示如果掉坑小伙伴，上面办法没解决。用文本方式打开文件，逐个排查原因。 参考文献及资料暂无]]></content>
      <categories>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu挂载新的硬盘（2T以上）]]></title>
    <url>%2F2018%2F04%2F01%2F2018-04-01-Ubuntu%E6%8C%82%E8%BD%BD%E6%96%B0%E7%9A%84%E7%A1%AC%E7%9B%98%EF%BC%882T%E4%BB%A5%E4%B8%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 查看硬盘信息 第二部分 新挂载硬盘分区 第三部分 使用parted分区 第四部分 格式化新建分区 第五部分 挂载分区 第六部分 配置开机自动挂载分区 第七部分 附录 参考文献及资料 背景系统环境： Linux version 4.13.0-37-generic (Ubuntu 5.4.0-6ubuntu1~16.04.9)。root用户登入操作。 第一部分 查看硬盘信息机器断电时，接入硬盘。开机后用下面的命令查看硬盘状况（非root用户需sudo）。 123456789101112131415161718root@deeplearning:~# fdisk -lDisk /dev/sda: 465.8 GiB, 500107862016 bytes, 976773168 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: gptDisk identifier: CC8004FC-D422-48FA-8ACF-54C3F48E860BDevice Start End Sectors Size Type/dev/sda1 2048 1050623 1048576 512M EFI System/dev/sda2 1050624 909946879 908896256 433.4G Linux filesystem/dev/sda3 909946880 976771071 66824192 31.9G Linux swapDisk /dev/sdb: 3.7 TiB, 4000787030016 bytes, 7814037168 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 4096 bytesI/O size (minimum/optimal): 4096 bytes / 4096 bytes 查看到系统由两块硬盘：/dev/sda和/dev/sdb，如果还有其他硬盘会继续sdc、sdd编号。 正在使用的系统盘sda已经有三个分区（sda1、sda2、sda3），新挂载的硬盘sdb位分区。 第二部分 新挂载硬盘分区新硬盘存储空间一共4T，我们对硬盘进行分区。划分为两个分区： 1234567891011121314root@deeplearning:~# fdisk /dev/sdbWelcome to fdisk (util-linux 2.27.1).Changes will remain in memory only, until you decide to write them.Be careful before using the write command./dev/sdb: device contains a valid 'ext4' signature; it is strongly recommended to wipe the device with wipefs(8) if this is sible collisionsDevice does not contain a recognized partition table.The size of this disk is 3.7 TiB (4000787030016 bytes). DOS partition table format can not be used on drives for volumes lar512-byte sectors. Use GUID partition table format (GPT).Created a new DOS disklabel with disk identifier 0x6b028a17.Command (m for help): 注意这里已经有警告：The size of this disk is 3.7 TiB (4000787030016 bytes). DOS partition table format can not be used on drives for volumes lar512-byte sectors. Use GUID partition table format (GPT) 这里情况特殊，新加入的磁盘为4T。fdisk命令对于大于2T的分区无法划分。如果继续使用fdisk工具，最多只能分出2T的分区，剩下的空间无法利用。这不坑爹嘛。提示我们使用parted命令。 第三部分 使用parted分区parted命令可以划分单个分区大于2T的GPT格式的分区。 更改分区表类型： 1root@deeplearning:~# parted -s /dev/sdb mklabel gpt 使用parted进行分区： 123456789101112131415161718192021222324252627282930313233root@deeplearning:~# parted /dev/sdbGNU Parted 3.2Using /dev/sdbWelcome to GNU Parted! Type 'help' to view a list of commands.(parted) print Model: ATA WDC WD40EFRX-68N (scsi)Disk /dev/sdb: 4001GBSector size (logical/physical): 512B/4096BPartition Table: gptDisk Flags: Number Start End Size File system Name Flags(parted) mklabel gpt Warning: The existing disk label on /dev/sdb will be destroyed and all data on this disk will be lost. Do you want to continue?Yes/No? yes (parted) mkpart Partition name? []? File system type? [ext2]? ext4 Start? 0% End? 100% (parted) print Model: ATA WDC WD40EFRX-68N (scsi)Disk /dev/sdb: 4001GBSector size (logical/physical): 512B/4096BPartition Table: gptDisk Flags: Number Start End Size File system Name Flags 1 1049kB 4001GB 4001GB ext4(parted) quit Information: You may need to update /etc/fstab. 最后我们验证一下，sdb1分区成功，提示我们要更新系统文件：/etc/fstab。 12345678910111213141516171819202122232425root@deeplearning:~# ls /dev/sd* /dev/sda /dev/sda1 /dev/sda2 /dev/sda3 /dev/sdb /dev/sdb1root@deeplearning:~# fdisk -lDisk /dev/sda: 465.8 GiB, 500107862016 bytes, 976773168 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: gptDisk identifier: CC8004FC-D422-48FA-8ACF-54C3F48E860BDevice Start End Sectors Size Type/dev/sda1 2048 1050623 1048576 512M EFI System/dev/sda2 1050624 909946879 908896256 433.4G Linux filesystem/dev/sda3 909946880 976771071 66824192 31.9G Linux swapDisk /dev/sdb: 3.7 TiB, 4000787030016 bytes, 7814037168 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 4096 bytesI/O size (minimum/optimal): 4096 bytes / 4096 bytesDisklabel type: gptDisk identifier: 0D8B0FBC-83F6-4D77-ABDB-98875EC511E4Device Start End Sectors Size Type/dev/sdb1 2048 7814035455 7814033408 3.7T Linux filesystem 第四部分 格式化新建分区将分区格式化为ext4格式的文件系统。 1234567891011121314root@deeplearning:~# mkfs.ext4 /dev/sdb1mke2fs 1.42.13 (17-May-2015)Creating filesystem with 976754176 4k blocks and 244195328 inodesFilesystem UUID: dfcd419f-38a5-4a5c-9b93-9f236d2c2444Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 102400000, 214990848, 512000000, 550731776, 644972544Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: done 如果有多个分区需要依次执行格式化。 第五部分 挂载分区新建硬盘即将挂载的目录，然后将硬盘挂载到该目录下。并验证挂载成功，检查硬盘空间。 12345678910111213root@deeplearning:/# mkdir /dataroot@deeplearning:/# mount /dev/sdb1 /dataroot@deeplearning:/# df -hFilesystem Size Used Avail Use% Mounted onudev 16G 0 16G 0% /devtmpfs 3.2G 9.3M 3.2G 1% /run/dev/sda2 427G 21G 385G 5% /tmpfs 16G 0 16G 0% /dev/shmtmpfs 5.0M 4.0K 5.0M 1% /run/locktmpfs 16G 0 16G 0% /sys/fs/cgroup/dev/sda1 511M 3.5M 508M 1% /boot/efitmpfs 3.2G 12K 3.2G 1% /run/user/1000/dev/sdb1 3.6T 68M 3.4T 1% /data 上面我们把新的硬盘挂载到了/data目录，硬盘空间大小正常。 第六部分 配置开机自动挂载分区6.1 查看分区的UUID123root@deeplearning:/# blkid#（略）.../dev/sdb1: UUID="dfcd419f-38a5-4a5c-9b93-9f236d2c2444" TYPE="ext4" PARTUUID="fe373bd5-5b19-4ed0-8713-716455a8ebb4" 6.2 配置/etc/fstab将分区信息写到/etc/fstab文件中让它永久挂载: 将下面的配置信息加入配置文件尾部： 1UUID=dfcd419f-38a5-4a5c-9b93-9f236d2c2444 /data ext4 defaults 0 1 第七部分 附录7.1 /etc/fstab配置说明12345678910111213# Use 'blkid' to print the universally unique identifier for a# device; this may be used with UUID= as a more robust way to name devices# that works even if disks are added and removed. See fstab(5).&lt;file system&gt; &lt;mount point&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt; 1 2 3 4 5 6对应参数说明：1、指代文件系统的设备名。最初，该字段只包含待挂载分区的设备名（如/dev/sda1）。现在，除设备名外，还可以包含LABEL或UUID2、文件系统挂载点。文件系统包含挂载点下整个目录树结构里的所有数据，除非其中某个目录又挂载了另一个文件系统3、文件系统类型。下面是多数常见文件系统类型（ext3,tmpfs,devpts,sysfs,proc,swap,vfat）4、mount命令选项。mount选项包括noauto（启动时不挂载该文件系统）和ro（只读方式挂载文件系统）等。在该字段里添加用户或属主选项，即可允许该用户挂载文件系统。多个选项之间必须用逗号隔开。其他选项的相关信息可参看mount命令手册页（-o选项处）5、转储文件系统？该字段只在用dump备份时才有意义。数字1表示该文件系统需要转储，0表示不需要转储6、文件系统检查？该字段里的数字表示文件系统是否需要用fsck检查。0表示不必检查该文件系统，数字1示意该文件系统需要先行检查（用于根文件系统）。数字2则表示完成根文件系统检查后，再检查该文件系统。 7.2 Parted命令说明（本文使用交互模式完成配置）Parted 命令分为两种模式：命令行模式和交互模式。 命令行模式： parted [option] device [command] ,该模式可以直接在命令行下对磁盘进行分区操作，比较适合编程应用。 交互模式：parted [option] device 类似于使用fdisk /dev/xxx MBR：MBR分区表(即主引导记录)大家都很熟悉。所支持的最大卷：2T，而且对分区有限制：最多4个主分区或3个主分区加一个扩展分区 GPT： GPT（即GUID分区表）。是源自EFI标准的一种较新的磁盘分区表结构的标准，是未来磁盘分区的主要形式。与MBR分区方式相比，具有如下优点。突破MBR 4个主分区限制，每个磁盘最多支持128个分区。支持大于2T的分区，最大卷可达18EB。 parted是一个可以分区并进行分区调整的工具，他可以创建，破坏，移动，复制，调整ext2 linux-swap fat fat32 reiserfs类型的分区，可以创建，调整，移动Macintosh的HFS分区，检测jfs，ntfs，ufs，xfs分区。 使用方法：parted [options] [device [command [options...]...]] 12345678910111213141516171819202122232425262728options-h 显示帮助信息-l 显示所有块设备上的分区device 对哪个块设备进行操作，如果没有指定则使用第一个块设备command [options...]check partition 对分区做一个简单的检测cp [source-device] source dest 复制source-device设备上的source分区到当前设备的dest分区mklabel label-type 创建新分区表类型，label-type可以是："bsd", "dvh", "gpt", "loop","mac", "msdos", "pc98", or "sun" 一般的pc机都是msdos格式，如果分区大于2T则需要选用gpt格式的分区表。mkfs partition fs-type 在partition分区上创建一个fs-type文件系统，fs-type可以是："fat16", "fat32", "ext2", "linux-swap","reiserfs" 注意不支持ext3格式的文件系统，只能先分区然后用专有命令进行格式化。mkpart part-type [fs-type] start end 创建一个part-type类型的分区，part-type可以是："primary", "logical", or "extended" 如果指定fs-type则在创建分区的同时进行格式化。start和end指的是分区的起始位置，单位默认是M。eg：mkpart primary 0 -1 0表示分区的开始 -1表示分区的结尾 意思是划分整个硬盘空间为主分区mkpartfs part-type fs-type start end 创建一个fs-type类型的part-type分区，不推荐使用，最好是使用mkpart分区完成后使用mke2fs进行格式化。name partition name 给分区设置一个名字，这种设置只能用在Mac, PC98, and GPT类型的分区表，设置时名字用引号括起来select device 在机器上有多个硬盘时，选择操作那个硬盘resize partition start end 调整分区大小rm partition 删除一个分区rescue start end 拯救一个位于stat和end之间的分区unit unit 在前面分区时，默认分区时数值的单位是M，这个参数卡伊改变默认单位，"kB", "MB", "GB", "TB"move partition start end 移动partition分区print 显示分区表信息 quit 退出parted 参考文献【1】 Setting up a large (2TB+) hard disk drive on Linux]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-深入理解PCA算法]]></title>
    <url>%2F2018%2F03%2F28%2F2018-03-06-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3PCA%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[背景数据科学中有个经典数据处理技术：Principal components analysis（简称PCA）。英文直译为：主成分分析。 数据的抽象数据科学研究中，一个数据样本通常抽象成特征向量（feature vector），例如：$$x=(x^{(1)},x^{(2)},…,x^{(i)},…,x^{(n)})^T$$这里$x^{(i)}$表示第$i$个特征（数值），这是一个$n$维特征的样本数据。例如人脸图像（灰色），每张图片的像素值可以转换成一个列向量，像素值即特征。 进一步抽象，如果给这些特征向量赋予欧式度量（拓扑结构），这些特征向量取自$R^n$空间（n维欧式空间）。这样我们就可以在欧式空间框架下研究数据集了。 当然你可以赋予别的度量，甚至赋予一个拓扑结构（这是一件有趣的事情，读者可以思考探索）。 数据的降维降维的目的当我们将数据抽象为欧式空间的向量，欧式空间 剔除数据集的特征存在冗余信息。 数据的采集过程中，采集特征之间本身存在相关性（线性或非线性）。去除冗余信息后，减少计算量。 减少数据集中噪声信息。 高维数据的降维可视化。 将高维空间的数据映射到3维或者2维欧式空间中，降维映射会保持高维数据的结构信息。比如高维空间相似的点，在低维可视化空间中体现为距离较近，我们说降维映射是保持距离的。 降维和特征选择在数据特征工程中，我们会对原始数据进行特征选择（feature selection），提取主要的特征因素，直接删除冗余或者认为次要的数据特征。广义上，这也属于数据降维技术，并且具有较强的可解释性。 另外数据降维还包含： 3.1 主成分分析PCA 3.2 多维缩放(MDS) 3.3 线性判别分析(LDA) 3.4 等度量映射(Isomap) 3.5 局部线性嵌入(LLE) 3.6 t-SNE 3.7 Deep Autoencoder Networks 主成分分析 最大方差形式假设${x_i}_{i=1}^{S}$ 是取自$R^n$欧式空间的数据点（列向量），即$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(j)},…,x_i^{(n)})^T$，我们将$R^n$中的数据点投影到$R^m$（$m&lt;n$）空间中，同时最大化投影（正交子空间投影）数据在$R^m$空间中的方差。 约定向量均为列向量 首先我们考虑$m=1$的情况，即$R^1$空间。这时我们可以挑选单位向量$u\in R^n$作为$R^1$的方向。因为$u^Tu=1$（欧式距离），所以数据点在$u$上的投影向量为 $(x_i^Tu)u$，坐标值为标量$x_i^Tu$。数据点在新的空间中的均值为：$$A=\frac{1}{S}\sum_{i=1}^S{x_i^Tu}=\frac{1}{S}\sum_{i=1}^S\sum_{j=1}^nx_{i}^{j}u_{i}^{j}=\sum_{j=1}^n(\frac{1}{S}\sum_{i=1}^Sx_{i}^{j})u_{i}^{j}=\overline{x}^Tu$$其中$\overline{x}=(\frac{1}{S}\sum_{i=1}^Sx_{i}^{j})_{j=1,…,n}^T$ 数据点在新的空间中方差为：$$V=\frac{1}{S}\sum_{i=1}^{S}(x_i^Tu-\overline{x}^Tu)^2=u^TCu$$其中C是数据的协方差矩阵（对称矩阵），定义为：$$C=\frac{1}{S}\sum_{i=1}^{S}(x_i-\overline{x})^T(x_{i}-\overline{x})$$关于$u$最大化方差$V=u^TCu$ ，引入拉格朗日乘子，即求解最大化函数：$$f(u,\lambda)=u^TCu+\lambda(1-u^Tu)$$关于$u$求偏导数：$$\frac{\partial f}{\partial u}=Cu-\lambda u=0$$显然$u$是矩阵$C$的特征值$\lambda$的特征向量。这时候方差 $V=u^TCu=u^T\lambda u=\lambda$ 所以我们选择协方差矩阵$C$的最大特征值对应的特征向量即可。 对于$m&gt;1$的情况，我们的最优目标是使得原始数据点在$R^m$的每个方向上$u_i$（$u_i \in U={u_1,u_2,…u_m}， \Vert u_i\Vert=1 $）的投影方差均最大化。即我们要挑选合适U集合，使得数据点在每个$u_i \in U$上的投影方差最大。 协方差矩阵是实对称矩阵，可以正交对角化，对角线上的元素为矩阵特征值集合。 有上面的推导容易知道U为特征值大小前$m$名所对应的特征向量的集合。 最小误差形式假设${u_j}_{j=1}^n$是$R^n$空间的单位正交基（$u_i u_j=\delta_{ij}$），那么对于数据点${x_i}_{i=1}^S$可以由基向量线性表出：$$x_i=\sum_{j=1}^na_{ij}u_j=\sum_{j=1}^n(x_iu_j)u_j$$我们的目标是对空间降维重建，使得数据点在新的坐标系（维度为$m&lt;n$）下得到近似表达，不失一般性，我们假设挑选基向量为${u_i}_{i=1}^m$,数据点在新的坐标系上的表示为：$$\hat{x_i}=\sum_{j=1}^ma_{ij}u_j$$我们的目标是最小化重建误差，定义为：$$J=\frac{1}{S}\sum_{i=1}^S\Vert x_i-\hat{x_i}\Vert^2=\frac{1}{S}\sum_{i=1}^S\Vert \sum_{j=1}^na_{ij}u_j-\sum_{j=1}^ma_{ij}u_j\Vert^2=\frac{1}{n}\sum_{i=1}^S\Vert\sum_{j=m+1}^na_{ij}u_j\Vert^2$$由单位正交性，得到：$$J=\frac{1}{S}\sum_{i=1}^S\sum_{j=m+1}^na_{ij}^2=\frac{1}{S}\sum_{i=1}^S\sum_{j=m+1}^nu_j^Tx_ix_i^Tu_j=\sum_{j=m+1}^nu_j^TCu_j$$ 降维和特征选择数学原理矩阵和线性变换算法实现过程特征分解奇异值分解（SVD）算法的深度理解知乎文章：https://www.zhihu.com/question/36348219 第一层理解（最大方差投影）第二层理解（最小重建误差）第三层理解（高斯先验误差）第四层理解（线性流行对齐）矩阵知识准备 KLT（Karhunen Loeve Transform）变换 KLT变换最早用于信号学中用于压缩信息量，用于除去输入数据的相关性。首先KLT是一个线性变换，所以除去的是线性相关性。]]></content>
      <categories>
        <category>PCA</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jupyter notebook:主题和字体的美化]]></title>
    <url>%2F2018%2F03%2F19%2F2018-03-26-JupyterNotebook%E4%B8%BB%E9%A2%98%E5%92%8C%E5%AD%97%E4%BD%93%E7%9A%84%E7%BE%8E%E5%8C%96%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 安装 第二部分 命令格式 第三部分 案例 参考文献及资料 背景Jupyter notebook是数据科学常用的代码交互式工具。通常在server端启jupyter进程（web服务），client端打开浏览器，jupyter提供代码编写和调试交互环境。非常方便。 但是jupyter提供的默认界面不够美观，特别是windows操作系统默认字体为浏览器默认字体–宋体（下图），另外默认主题太难看了，没有通常IDE提供的主题美观。 发现一个Jupyter的美化工具：jupyterthemes ，和大家分享一下。简单介绍一下安装和配置。细节介绍参考项目的介绍文档。 第一部分 安装使用pip安装： 1root@vultr:~# pip install jupyterthemes 或者使用Anaconda的conda安装： 1root@vultr:~# conda install -c conda-forge jupyterthemes 第二部分 命令格式使用jt -h显示命令帮助说明： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354root@vultr:~# jt -husage: jt [-h] [-l] [-t THEME] [-f MONOFONT] [-fs MONOSIZE] [-nf NBFONT] [-nfs NBFONTSIZE] [-tf TCFONT] [-tfs TCFONTSIZE] [-dfs DFFONTSIZE] [-ofs OUTFONTSIZE] [-mathfs MATHFONTSIZE] [-m MARGINS] [-cursw CURSORWIDTH] [-cursc CURSORCOLOR] [-cellw CELLWIDTH] [-lineh LINEHEIGHT] [-altp] [-altmd] [-altout] [-P] [-T] [-N] [-vim] [-r] [-dfonts]optional arguments: -h, --help show this help message and exit #-h，--help显示此帮助信息并退出 -l, --list list available themes #-l， 列出可用主题 -t THEME, --theme THEME theme name to install（配置需要安装的主题） -f MONOFONT, --monofont MONOFONT monospace code font（代码的字体） -fs MONOSIZE, --monosize MONOSIZE code font-size（代码字体大小） -nf NBFONT, --nbfont NBFONT notebook font（notebook 字体） -nfs NBFONTSIZE, --nbfontsize NBFONTSIZE notebook fontsize（notebook 字体大小） -tf TCFONT, --tcfont TCFONT txtcell font（文本的字体） -tfs TCFONTSIZE, --tcfontsize TCFONTSIZE txtcell fontsize（文本的字体大小） -dfs DFFONTSIZE, --dffontsize DFFONTSIZE pandas dataframe fontsize（pandas类型的字体大小） -ofs OUTFONTSIZE, --outfontsize OUTFONTSIZE output area fontsize（输出区域字体大小） -mathfs MATHFONTSIZE, --mathfontsize MATHFONTSIZE mathjax fontsize (in %)（数学公式字体大小） -m MARGINS, --margins MARGINS fix margins of main intro page -cursw CURSORWIDTH, --cursorwidth CURSORWIDTH set cursorwidth (px)（设置光标宽度） -cursc CURSORCOLOR, --cursorcolor CURSORCOLOR cursor color (r, b, g, p)（设置光标颜色） -cellw CELLWIDTH, --cellwidth CELLWIDTH set cell width (px or %)（单元的宽度） -lineh LINEHEIGHT, --lineheight LINEHEIGHT code/text line-height (%)（行高） -altp, --altprompt alt input prompt style -altmd, --altmarkdown alt markdown cell style -altout, --altoutput set output bg color to notebook bg -P, --hideprompt hide cell input prompt -T, --toolbar make toolbar visible（工具栏可见） -N, --nbname nb name/logo visible -vim, --vimext toggle styles for vim -r, --reset reset to default theme（设置成默认主题） -dfonts, --defaultfonts force fonts to browser default（设置成浏览器默认字体） 第三部分 案例例如下面的命令完成效果： 使用的主题是：monokai，工具栏可见，命名笔记本的选项，代码的字体为13，代码的字体为consolamono。 1root@vultr:~# jt -t monokai -T -N -fs 13 -f consolamono 如果jupyter进程已启，需要重新启进程后生效。 实现的效果截图： 其他主题效果大家可以自己尝试。 参考文献及资料1、jupyter官网，链接：https://jupyter.org/]]></content>
      <categories>
        <category>jupyter</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[树莓派使用USB摄像头自制家庭监控]]></title>
    <url>%2F2018%2F03%2F18%2F2018-03-18-%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BD%BF%E7%94%A8USB%E6%91%84%E5%83%8F%E5%A4%B4%E8%87%AA%E5%88%B6%E5%AE%B6%E5%BA%AD%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 部署步骤 参考文献及资料 背景树莓派摄像头和USB摄像头 树莓派有配套的摄像头模块（Raspberry Pi camera board），如下图。 另外树莓派也支持USB摄像头。关于树莓派支持的USB摄像头有个清单参考（需要梯子）。大家购买前最好确认一下是否在兼容清单中。 第一部分 部署步骤第一步：检查USB摄像头和树莓派的兼容性将USB摄像头和树莓派连接，查看USB接口连接情况。 12345root@raspberrypi:/# lsusbBus 001 Device 004: ID 046d:0825 Logitech, Inc. Webcam C270Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. SMSC9512/9514 Fast Ethernet AdapterBus 001 Device 002: ID 0424:9514 Standard Microsystems Corp.Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub 发现004口上面连接并识别了摄像头（我的是Logitech 270摄像头）。 如果没有识别出来，需要查看USB兼容清单是否有该型号。另外由于树莓派供电功率较小，也有可能是USB供电功率不足，需要有外置电源的USB摄像头。 另外还可以查看设备驱动情况： 12root@raspberrypi:/# ls /dev/vid*/dev/video0 发现video0设备，说明识别了USB摄像头（罗技的c270i） 第二步：MOTION软件实现对于USB摄像头，有多种软件包可以实现拍照和摄像等功能，这里使用motion。 安装motion 1root@raspberrypi:/# sudo apt-get install motion 编辑配置配置文件 1root@raspberrypi:~# vi /etc/motion/motion.conf 调整相关参数 123456789101112# The mini-http server listens to this port for requests (default: 0 = disabled)stream_port 8082# web界面访问端口# TCP/IP port for the http server to listen on (default: 0 = disabled)webcontrol_port 8080#控制端口# Restrict control connections to localhost only (default: on)webcontrol_localhost off# Target base directory for pictures and films# Recommended to use absolute path. (Default: current working directory)target_dir /var/lib/motion#照片及视频存放路径 其他参数调整如下： ​ ffmpeg_output_movies=off ​ stream_localhost=off ​ webcontrol_localhost=off ​ locate_motion_mode=peview ​ locate_motion_style=redbox ​ text_changes=on 如果需要对网页进行加密，可以调整下面的配置实现： 123456789# Set the authentication method (default: 0)# 0 = disabled# 1 = Basic authentication# 2 = MD5 digest (the safer authentication)stream_auth_method 1# Authentication for the stream. Syntax username:password# Default: not defined (Disabled)stream_authentication admin:admin1234 开启motion进程修改motion文件，设置为守护进程运行（即参数配置为：yes）： 123# vi /etc/default/motion# set to 'yes' to enable the motion daemonstart_motion_daemon=yes 启进程： 1234567root@raspberrypi:/etc/init.d# motion start[0] [NTC] [ALL] conf_load: Processing thread 0 - config file /etc/motion/motion.conf[0] [ALR] [ALL] conf_cmdparse: Unknown config option "sdl_threadnr"[0] [NTC] [ALL] motion_startup: Motion 3.2.12+git20140228 Started[0] [NTC] [ALL] motion_startup: Logging to syslog[0] [NTC] [ALL] motion_startup: Using log type (ALL) log level (NTC)[0] [NTC] [ALL] become_daemon: Motion going to daemon mode 查看监控画面地址栏中输入地址和端口号（IP：8082），上面配置的web界面访问端口为8082： 查看监控数据存放目录另外目录/var/lib/motion中存放历史数据。 第三步：内网穿透（外网访问web监控界面）实现上面的步骤，你只能在家里本地局域网访问监控界面，意义不大。 由于目前中国宽带服务公司都不会给家庭网络外网地址。所以需要内网穿透，实现外网访问家庭内网。 具体可以使用frp软件实现内网穿透。具体做法参考的博客中另一篇介绍frp的分享文章。 参考文献及资料1、Building a Motion Activated Security Camera with the Raspberry Pi Zero，链接： https://www.bouvet.no/bouvet-deler/utbrudd/building-a-motion-activated-security-camera-with-the-raspberry-pi-zero 2、How to make a DIY home alarm system with a raspberry pi and a webcam，链接：https://medium.com/@Cvrsor/how-to-make-a-diy-home-alarm-system-with-a-raspberry-pi-and-a-webcam-2d5a2d61da3d]]></content>
      <categories>
        <category>raspberry</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu系统常用命令汇总]]></title>
    <url>%2F2018%2F03%2F14%2F2018-03-15-Ubuntu%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 系统信息 第二部分 资源信息 第三部分 磁盘信息 第四部分 网络信息 第五部分 进程信息 第六部分 用户信息 参考文献及资料 背景本篇博客主要汇总查询Ubuntu系统的信息的相关命令及展示案例。会持续更新。 第一部分 系统信息1.1 查看：CPU信息12root@deeplearning:/# cat /proc/versionLinux version 4.13.0-37-generic (buildd@lcy01-amd64-012) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)) #42~16.04.1-Ubuntu SMP Wed Mar 7 16:03:28 UTC 2018 1.2 查看：内核、操作系统、CPU信息12root@deeplearning:/# uname -aLinux deeplearning 4.13.0-37-generic #42~16.04.1-Ubuntu SMP Wed Mar 7 16:03:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux 1.3 查看：操作系统版本信息123456root@deeplearning:/# lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 16.04.4 LTSRelease: 16.04Codename: xenial 1.4 查看：计算机名12root@vultr:~# hostnamevultr.guest 第二部分 资源信息2.1 查看：存储分区的使用信息123456789root@vultr:~# df -hFilesystem Size Used Avail Use% Mounted onudev 469M 0 469M 0% /devtmpfs 99M 11M 89M 11% /run/dev/vda1 25G 12G 12G 49% /tmpfs 495M 0 495M 0% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 495M 0 495M 0% /sys/fs/cgrouptmpfs 99M 0 99M 0% /run/user/0 2.2 查看：系统运行时间、用户数量12root@vultr:~# uptime 23:04:54 up 10 days, 17:21, 1 user, load average: 0.21, 0.06, 0.02 第三部分 磁盘信息3.1 查看:所有分区信息12345678910root@vultr:~# fdisk -lDisk /dev/vda: 25 GiB, 26843545600 bytes, 52428800 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0xcb855d49Device Boot Start End Sectors Size Id Type/dev/vda1 * 2048 52428257 52426210 25G 83 Linux 第四部分 网络信息4.1 查看：网络接口信息12345678910root@vultr:~# ifconfig#（略）lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 198660 bytes 27478459 (27.4 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 198660 bytes 27478459 (27.4 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 4.2 查看：防火墙信息1234root@vultr:~# iptables -LChain DOCKER-USER (1 references)target prot opt source destination RETURN all -- anywhere anywhere 4.3 查看：路由表12345root@vultr:~# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface#（略）172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 4.4 查看：监听端口、已经建立的连接1234root@vultr:~# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 749/sshd 1234root@vultr:~# netstat -antpActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 749/sshd 4.5 查看：网络统计信息123456root@vultr:~# netstat -sIp: Forwarding: 1 5729182 total packets received 9 with invalid addresses#略信息 第五部分 进程信息5.1 查看：所有进程信息123root@vultr:~# ps -ef UID PID PPID C STIME TTY TIME CMDroot 1 0 0 Mar25 ? 00:00:44 /sbin/init 5.2 查看：实时显示进程状态12345678910root@vultr:~# toptop - 23:24:19 up 10 days, 17:41, 1 user, load average: 0.02, 0.02, 0.00Tasks: 87 total, 1 running, 86 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.7 us, 0.3 sy, 0.0 ni, 99.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 1012392 total, 74384 free, 270236 used, 667772 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 556896 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 25691 root 20 0 554760 13348 3448 S 0.3 1.3 35:21.42 docker-containe #（略信息） 第六部分 用户信息6.1 查看：活动用户1234root@vultr:~# w 23:25:22 up 10 days, 17:42, 1 user, load average: 0.33, 0.08, 0.02USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 140.31.74.0 22:59 2.00s 0.06s 0.00s w 6.2 查看：用户登录日志信息12345root@vultr:~# lastroot pts/0 113.41.56.0 Wed Apr 4 22:59 still logged inroot pts/0 113.41.56.0 Wed Apr 4 08:37 - 13:38 (05:00)wtmp begins Sun Apr 1 19:59:32 2018 参考文献及资料1、The 50 Most Useful Linux Commands To Run in the Terminal，链接：https://www.ubuntupit.com/best-linux-commands-to-run-in-the-terminal/]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[动手实现Ubuntu系统WOL远程唤醒]]></title>
    <url>%2F2018%2F03%2F14%2F2018-03-14-%E5%8A%A8%E6%89%8B%E5%AE%9E%E7%8E%B0Ubuntu%E7%B3%BB%E7%BB%9FWOL%E8%BF%9C%E7%A8%8B%E5%94%A4%E9%86%92%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 检查主机板块和网卡是否支持Wol 第二部分 部署步骤 参考文献及资料 背景本篇博客主要介绍通过局域网唤醒服务器（远程启动计算机）。具体在Ubuntu操作系统上实现。具体数据流为：通过互联网远程登录长期开机的树莓派，然后通过树莓派唤醒同一个局域网的高性能服务器。 目前中国家庭宽带网络都是没有外网IP的，如果外网访问家庭网络，需要做端口映射，实现远程访问。 什么是WoL（Wake on LAN）电脑处在关机（或休眠）状态时，只要主机保持连接电源、网线连接网卡，其实网卡和主板仍然有微弱供电。这部分供电能让网卡监听和解读来自外部网络的广播信息。其中会对一种特殊的广播信息Magic Packet（魔法数据包）进行侦测。Magic Packet网络包以广播的形式发送，发送的范围可以是整个局域网或者指定的子网。另外Magic Packet中唤醒服务器IP可以是多个，侦测主机一旦发现包中的唤醒IP集中包含自己的IP，会通知主板、电源供电器，开始执行唤醒，打开机器。 第一部分 检查主机板块和网卡是否支持Wol 主板是否支持：进入BIOS，将“Power Management Setup”中的“Wake Up On LAN”或“Resume by LAN”项设置为“Enable”或“On” 网卡是否支持： 1# ethtool enp0s31f6 其中有下面的字段信息： 12Supports Wake-on: pumbgWake-on: g 第二部分 部署步骤2.1 方法1需要安装wakeonlan包： 1root@raspberrypi:~# sudo apt-get install wakeonlan 下面的命令通过树莓派发送魔术包： 12root@raspberrypi:~# wakeonlan -i 192.168.1.3 b0:6f:bf:b0:9f:2fSending magic packet to 192.168.1.3:9 with b0:6f:b0:bf:9f:2f 2.2 方法2在网关配置ARP信息（IP与物理地址进行绑定），发送网段的广播： 12root@raspberrypi:~# wakeonlan -i 192.168.1.0 b0:6f:bf:b0:9f:2fSending magic packet to 192.168.1.0:9 with b0:6f:b0:bf:9f:2f 参考文献及链接1、WakeOnLan 链接：https://help.ubuntu.com/community/WakeOnLan 2、wiki 链接：https://en.wikipedia.org/wiki/Wake-on-LAN 3、Ubuntu 與 Wake on LAN 链接：http://softsmith.blogspot.com/2014/05/ubuntu-wake-on-lan.html]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[动手实现深度学习相机]]></title>
    <url>%2F2018%2F03%2F14%2F2018-03-14-deeplearning_camera(%E6%9C%AA%E5%AE%8C%E7%A8%BF)%2F</url>
    <content type="text"><![CDATA[2017年年底，亚马逊（AWS）宣布将推出深度学习相机——DeepLens ，亚马逊官网已经开始预售，预计6月14日发货。但是售价为249刀（约1600人民币）。偏贵了。 具体介绍可以参考这篇文章：AWS深度学习摄像头，将对机器学习产业有何影响？ 看到有人利用树莓派和简易摄像头实现了一个深度学习相机，用来检测院子里面小鸟吃食。正好自己有一个树莓派，可以参考玩一下。进一步优化甚至可以放在家门口，对访客人脸识别。]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu添加国内apt更新源]]></title>
    <url>%2F2018%2F03%2F14%2F2018-03-15-Ubuntu%E6%B7%BB%E5%8A%A0%E5%9B%BD%E5%86%85apt%E6%9B%B4%E6%96%B0%E6%BA%90%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 更换步骤 参考文献及资料 背景本篇博客主要介绍如何更改Ubuntu系统的apt源。 关于源我们使用apt安装软件时，会到国外源下载软件包。但是由于各种原因（你懂的）国外站点到国内的下载速度非常缓慢，甚至1k/s。对于大的包，这是无法忍受的等待，经常会超时中断。所以我们换成国内的源站点。其中口碑比较好的源站点有：阿里源、清华源、中科大源等。 Ubuntu系统的源地址文件位置：/etc/apt/sources.list 第一部分 更换步骤第一步：备份 对于系统文件的修改建议实施备份。养成良好的变更习惯。关键时候能救命。 关于备份文件命名有两个建议：（1）含有backup字段提示为备份文件；（2）含有备份日期，便于区分多个备份。当然如果是多用户话应该含有用户名，便于区分。 1root@deeplearning:/# cp /etc/apt/sources.list /etc/apt/sources.list.backup.20180315 第二步：添加源地址 我们添加阿里源， 进入阿里云开源镜像站，找到ubuntu的帮助信息： 我们版本号Ubuntu 16.04.4 LTS，并且Codename: xenial。需要根据自己的版本对应相应的源。 123456789101112131415161718# deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricteddeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial universedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb http://mirrors.aliyun.com/ubuntu/ xenial multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-propertiesdeb http://archive.canonical.com/ubuntu xenial partnerdeb-src http://archive.canonical.com/ubuntu xenial partnerdeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial-security universedeb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse 直接将source.list中内容用上面的源地址内容替换，保存后退出。更新源： 1root@deeplearning:/# apt-get update 附录：apt的常用操作命令清单12345678910111213141516sudo apt-get update 更新源sudo apt-get install package 安装包sudo apt-get remove package 删除包sudo apt-cache search package 搜索软件包sudo apt-cache show package 获取包的相关信息，如说明、大小、版本等sudo apt-get install package --reinstall 重新安装包sudo apt-get -f install 修复安装sudo apt-get remove package --purge 删除包，包括配置文件等sudo apt-get build-dep package 安装相关的编译环境sudo apt-get upgrade 更新已安装的包sudo apt-get dist-upgrade 升级系统sudo apt-cache depends package 了解使用该包依赖那些包sudo apt-cache rdepends package 查看该包被哪些包依赖sudo apt-get source package 下载该包的源代码sudo apt-get clean &amp;&amp; sudo apt-get autoclean 清理无用的包sudo apt-get check 检查是否有损坏的依赖 参考文献及资料1、Ubuntu更换阿里云软件源，链接:https://yq.aliyun.com/articles/704603]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-不平衡数据集的采样技术]]></title>
    <url>%2F2018%2F03%2F01%2F2018-02-28-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E9%87%87%E6%A0%B7%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[在数据处理中，经常需要对给定随机变量（一维或多维）及其概率分布函数，需要生成随机变量的采样（sampling）数据集（随机变量的随机采样样本点）。 采样的方法有很多： 1、MCMC方法（Mento Carlo Markov Chain，即：蒙特卡洛-马尔科夫链）。主要的算法有： Metroplis-Hasting 算法 Gibbs Sampling 算法]]></content>
      <categories>
        <category>Sampling</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[树莓派更改APT为国内阿里云源]]></title>
    <url>%2F2018%2F03%2F01%2F2018-03-12-%E6%A0%91%E8%8E%93%E6%B4%BE%E6%9B%B4%E6%94%B9APT%E4%B8%BA%E5%9B%BD%E5%86%85%E9%98%BF%E9%87%8C%E4%BA%91%E6%BA%90%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 更改步骤 参考文献及资料 背景发现树莓派wget国外源异常慢。其实可以更改为国内的apt源，不用走海下光缆啦。 第一部分 更改步骤1.1 编辑sources.list文件还没安装我喜爱的vim，只能先用nano编辑文件： 1root@raspberrypi:~# nano /etc/apt/sources.list 1.2 修改源将原始的源注释掉，添加阿里云的源地址（在这里感谢阿里爸爸）。 12deb http://mirrors.aliyun.com/raspbian/raspbian/ jessie main non-free contrib rpideb-src http://mirrors.aliyun.com/raspbian/raspbian/ jessie main non-free contrib rpi 1.3 更软件索引清单最后更新一下，以后就可以快速apt-get啦。 1sudo apt-get update 参考文献及资料1、Raspberry Pi 使用阿里云OPSX镜像，链接：https://zihengcat.github.io/2018/05/14/using-alibaba-cloud-opsx-mirrors-in-raspberry-pi/]]></content>
      <categories>
        <category>raspberry</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[树莓派搭建家庭内网穿透服务器(frp实现)]]></title>
    <url>%2F2018%2F03%2F01%2F2018-03-12-%E6%A0%91%E8%8E%93%E6%B4%BE%E6%90%AD%E5%BB%BA%E5%AE%B6%E5%BA%AD%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F%E6%9C%8D%E5%8A%A1%E5%99%A8(frp%E5%AE%9E%E7%8E%B0)%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境准备 第二部分 FRP介绍 第三部分 实现步骤 参考文献及资料 背景对于目前家庭网络，有下面两个迫切需求： 需要远程（比如在单位、路途等）SSH访问家里PC服务器、后续实现远程唤醒服务器。 后续部署自制家庭监控，需要远程访问监控web界面。 鉴于上面的需求，技术上需要实现外网访问内网（即内网穿透）。下面详细介绍具体实现步骤。 第一部分 环境准备 一台树莓派主机。由于耗电较少，适合长期开机，作为中转服务器。 一个公网IP。由于电信宽带不提供公网IP，只能自己想办法。正好有一台外网VPS服务器（独有公网IP） 另外顺便打个不收钱的广告：VPS可以使用vultr的虚机，支持支付宝，非常方便。 第二部分 FRP介绍实现内网穿透有很多方法：frp软件、ngrok软件、还有花生壳。关于frp有详细的官方介绍文档 ，不再赘述。 第三部分 实现步骤第一步：配置VPS服务器VPS操作系统为Ubuntu，下载linux_amd64版本： 1root@vultr:~# wget https://github.com/fatedier/frp/releases/download/v0.16.1/frp_0.16.1_linux_amd64.tar.gz 解压缩： 123456789101112root@vultr:~# tar -zxvf frp_0.16.1_linux_amd64.tar.gzfrp_0.16.1_linux_amd64/frp_0.16.1_linux_amd64/frpc_full.inifrp_0.16.1_linux_amd64/LICENSEfrp_0.16.1_linux_amd64/frpc.inifrp_0.16.1_linux_amd64/frps.inifrp_0.16.1_linux_amd64/frpcfrp_0.16.1_linux_amd64/frps_full.inifrp_0.16.1_linux_amd64/frpsroot@vultr:~# cd frp_0.16.1_linux_amd64/root@vultr:~/frp_0.16.1_linux_amd64# lsfrpc frpc_full.ini frpc.ini frps frps_full.ini frps.ini LICENSE 对于VPS服务端只有两个文件是需要的：frps （服务）和frps.ini （配置文件）是需要的，我们拷贝到/bin目录下面（这一步主要是集中放在bin目录便于管理）。 frpc 和frpc.ini是客户端服务和配置文件，后面介绍。 12root@vultr:~/frp_0.16.1_linux_amd64# cp frps /bin/frpsroot@vultr:~/frp_0.16.1_linux_amd64# cp frps.ini /bin/frps.ini 然后对配置文件进行修改： 123456789101112131415161718192021222324252627282930313233root@vultr:/bin# vi frps.ini#修改配置文件[common]bind_addr = 10.66.2.137#VPS公网IP地址（为了保护隐私，上面地址是虚构的）bind_port = 7000#frp服务端口，用户自己定于一个空闲端口（不要和其他应用服务端口冲突）。需要注意的是必须与frpc.ini相同vhost_http_port = 80#http服务端口vhost_https_port = 443#https服务端口dashboard_port = 7500#web控制台端口，10.66.2.137：7500可以访问控制界面#下面两个参数是控制界面的用户名和密码dashboard_user = admindashboard_pwd = Password123！privilege_token = Password123！#特权模式密钥，需与frpc.ini相同log_file = /bin/frps_log/frps.log#日志文件存储路径log_level = info#日志记录级别log_max_days = 7#日志最大存储天数max_pool_count = 5#后端连接池最大连接数量#口令超时时间authentication_timeout = 900#subdomain_host = frp.com #服务端绑定域名tcp_mux = true 保存修改后的配置文件，后台启服务端进程，下面是命令格式： 12345root@vultr:/bin# nohup ./frps -c ./frps.ini &amp;#查看服务进程：root@vultr:/bin# ps -ef|grep frproot 1339 1 0 Mar25 ? 00:03:54 ./frps -c ./frps.iniroot 5320 4958 0 11:42 pts/1 00:00:00 grep --color=auto frp 以上完成服务端配置。 第二步：配置树莓派客户端 注意：树莓派的CPU处理器是ARM的，所以注意下载的版本包。 1root@raspberrypi:~# wget https://github.com/fatedier/frp/releases/download/v0.16.1/frp_0.16.1_linux_arm.tar.gz 解压缩下载的包： 123456789101112root@raspberrypi:~# tar -zxvf frp_0.16.1_linux_arm.tar.gz frp_0.16.1_linux_arm/frp_0.16.1_linux_arm/frpc_full.inifrp_0.16.1_linux_arm/LICENSEfrp_0.16.1_linux_arm/frpc.inifrp_0.16.1_linux_arm/frps.inifrp_0.16.1_linux_arm/frpcfrp_0.16.1_linux_arm/frps_full.inifrp_0.16.1_linux_arm/frpsroot@raspberrypi:~# cd frp_0.16.1_linux_armroot@raspberrypi:~/frp_0.16.1_linux_arm# lsfrpc frpc_full.ini frpc.ini frps frps_full.ini frps.ini LICENSE 类似服务端操作将frpc和frpc.ini拷贝到/bin目录下面。 12root@raspberrypi:~/frp_0.16.1_linux_arm# cp frpc /bin/frpcroot@raspberrypi:~/frp_0.16.1_linux_arm# cp frpc.ini /bin/frpc.ini 修改配置文件： 123456789101112131415161718192021222324252627282930313233343536373839root@raspberrypi:/bin# vi frpc.ini#修改客户端配置文件[common]server_addr = 10.66.2.137#VPS公网IP地址（为了保护隐私，上面地址是虚构的）server_port = 7000privilege_token = Password123！log_file = /bin/frpc.log#日志目录log_level = infolog_max_days = 7pool_count = 5tcp_mux = true#配置SSH端口映射[ssh]type = tcplocal_ip = 127.0.0.1#本地端口local_port = 22#映射端口remote_port = 6000[web]type = httplocal_ip = 127.0.0.1local_port = 80use_encryption = falseuse_compression = truesubdomain = web#所绑定的公网服务器域名，一级、二级域名都可以。这里没有就不用配置了custom_domains = web.frp.com#远程监控端口映射[motion]type = tcplocal_ip = 127.0.0.1local_port = 8082remote_port = 8000 保存修改，后台启客户端进程： 1234root@raspberrypi:/bin# nohup ./frpc -c ./frpc.ini &amp;root@raspberrypi:/bin# ps -ef|grep frproot 4627 1 0 4月02 ? 00:05:19 ./frpc -c ./frpc.iniroot 13731 13669 0 19:56 pts/0 00:00:00 grep frp 第三步：验证 FRP管理界面（http://公网IP:7500） 显示两个端口映射都是online可用的： 下面是整体视图： SSH服务 我们已经将本地访问ssh的服务端口（192.168.1.2：22）映射到外网端口（10.66.2.137：6000）。 例如使用putty工具，IP地址填写：10.66.2.137，端口：6000。 连接后使用树莓派本地ssh用户和密码即可登录。 参考文献及资料[1] FRP官方网站 https://github.com/fatedier/frp]]></content>
      <categories>
        <category>raspberry</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SAS 9.4 部署及SID文件调整]]></title>
    <url>%2F2018%2F02%2F06%2F2018-02-09-SAS%209.4%20%E9%83%A8%E7%BD%B2%E5%8F%8ASID%E6%96%87%E4%BB%B6%E8%B0%83%E6%95%B4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 下载SAS 9.4介质及安装 第二部分 SID文件的导入 第三部分 分享一个坑 第四部分 一点提示 参考文献及资料 背景本篇博客主要介绍个人部署SAS 9.4软件过程及安装过程中调整SID文件技巧。由于工作中经常使用SAS软件连接数据库查数，所以个人电脑准备也装备一下，部署过程分享给大家。 第一部分 下载SAS 9.4介质及安装相信大家的资源检索能力，肯定能找到可用的安装介质。 介质可以参考一个网盘地址：http://pan.baidu.com/s/1qYz7ZNA 密码：ulig 解压缩后windows 直接执行setup.exe文件，linux 执行setup.sh文件执行命令：sh setup.sh。 然后选择安装语言、需要安装的组件、选择（32bit或64bit）等，不再赘述。 第二部分 SID文件的导入 如果你有未过期的SID文件，直接导入后安装即可。下面主要介绍没有时咋办（本文介绍方法主要是个人学习使用哈，商业使用建议购买SID哈，毕竟不差钱） 1、闹心的方法首先找到旧的SID文件（授权时间已经过期），导入SID文件前，将操作系统修改为历史时间，然后顺利安装。但是每次使用SAS都要将系统时间后调，否则没法启动SAS正常使用。确实很闹心，而且修改系统时间后很多软件会提示异常，比如浏览器等。 2、奇技淫巧 其实认真查找SID文件，还是可以找到不过期的。 比如下面的链接：http://downloads.npust.edu.tw/otherFile/20170703022854.txt 过期时间为：30APR2018 [ ] 如果用这个SID提示无效或报错，这时候我们需要调整一下这个SID文件。 - 在安装源文件目录中找到order.xml文件，我的目录为：~\SAS 9.4\order_data\99YYS5\order.xm。在文件中找到两个参数：setnumid=&quot;51200421&quot;、number=&quot;99YYS5&quot; - 将SID文件中下面两个参数调整和order文件中相同： 123#调整后Order=99YYS5Setnumid=51200421 [ ] 最后重新加载SID文件，顺利安装。 &gt; 另外如果还是报错可能是遇到别的坑了。。。。。 第三部分 分享一个坑作者部署平台是win7。所以涉及“C:\ Program Files”（存放64bit软件）和“C:\ Program Files(x86)”（存放32bit软件）。 而下载的SAS 9.4是32bit的，路径选择了“C:\ Program Files”，所以导入SID一直报错。 最后缓过神，调整了安装路径为“C:\ Program Files（x86）”（存放32bit软件的地方），最后顺利安装。 第四部分 一点提示SID文件中会有各个组件的授权，如果使用SAS软件部分组件无效，可能是SID文件中未有该组件的授权信息。下面是截取一个SID的授权组件的信息。例如有基础SAS 、SAS EG、还有SAS和oracle、Teradata连接的组件等。 123456Base SAS 31DEC2017SAS Enterprise Guide 31DEC2017SAS Enterprise Miner Personal Client 31DEC2017SAS/ACCESS Interface to Oracle 31DEC2017SAS/ACCESS Interface to PC Files 31DEC2017SAS/ACCESS Interface to Teradata 31DEC2017 例如：Interface to Oracle组件用来和oracle数据链接。连接后SAS EG客户端可以读取数据库中表，新建映射逻辑库就可以用SAS EG来做表操作。 参考文献及资料1、SAS安装介质网盘，链接：介质可以参考一个网盘地址：http://pan.baidu.com/s/1qYz7ZNA 密码：ulig]]></content>
      <categories>
        <category>SAS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-生成式模型和判别式模型]]></title>
    <url>%2F2018%2F02%2F06%2F2018-02-09-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Spark内存管理详解 第二部分 Spark参数说明 第三部分 Spark内存优化 第四部分 常见线上问题解决 参考文献及资料 背景看机器学习的论文或材料，经常提到模型属于生成式模型（Generative Modeling）或者判别式模型（Discriminative Modeling）。这次对两个概念进行总结，如有理解偏差，希望大家指正。 第一部分 生成模型第二部分 判别模型方法（算法、学习方式、模型）有很多分类形式。比如常见的分为监督学习、无监督学习、强化学习。这篇博客介绍（学习总结）一种分类形式：生成式模型（Generative Modeling）和判别式模型（Discriminative Modeling）。 https://blog.csdn.net/u010358304/article/details/79748153]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-Autoencoder学习总结]]></title>
    <url>%2F2018%2F02%2F06%2F2018-02-09-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Autoencoder%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本篇博客。 AutoEncoder主要用于数据的降维和特征的提取。而现在也被扩展用于生成模型。 与其他神经网路（通常关注于输出层）不同的是：AutoEncoder主要关注于中间的隐藏层（Hidden Layer）。 1、AutoEncoder（自编码器） ​ 原始数据映射到原数据，通过压缩来提取数据的特征。如果将激活函数换成线性函数，这是一个PCA模型。 2、Sparse AutoEncoder（稀疏自编码器） 3、Denoising AutoEncoder（降噪自编码器） 4、Stacked AutoEncoder（堆叠自编码器，SAE） 4、Variational AutoEncoder（VAE）]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu操作系统设置静态IP地址]]></title>
    <url>%2F2018%2F02%2F06%2F2018-02-25-Ubuntu%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%AE%BE%E7%BD%AE%E9%9D%99%E6%80%81IP%E5%9C%B0%E5%9D%80%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 编辑interfaces文件 第二部分 添加静态IP信息及DNS信息 第三部分 重启服务生效 参考文献及资料 背景有一台Linux台式机（Ubuntu 16.04.3 LTS (GNU/Linux 4.13.0-32-generic x86_64)）。之前IP是DHCP服务分配的。准备给服务器分配静态IP，方便使用。 ubuntu的网络参数保存在文件/etc/network/interfaces中。 第一部分 编辑interfaces文件12345vi /etc/network/interfaces# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopback 第二部分 添加静态IP信息及DNS信息12345678910# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopbackauto enp0s31f6iface enp0s31f6 inet staticaddress 192.168.31.62gateway 192.168.31.254netmask 255.255.255.0dns-nameservers 192.168.31.1 8.8.8.8 第三部分 重启服务生效1234567891011root@vultr:~# service systemd-networkd status● systemd-networkd.service - Network Service Loaded: loaded (/lib/systemd/system/systemd-networkd.service; enabled-runtime; vendor preset: enabled) Active: active (running) since Wed 2020-03-18 09:18:58 UTC; 3 days ago Docs: man:systemd-networkd.service(8) Main PID: 843 (systemd-network) Status: "Processing requests..." Tasks: 1 (limit: 1108) CGroup: /system.slice/systemd-networkd.service └─843 /lib/systemd/systemd-networkd.... 如果如法生效，尝试reboot服务器。 第四部分 命令行方式（临时，os重启失效）1234root@vultr:~# ifconfig ens33 192.168.1.31 netmask 255.255.255.0# ens33 网卡名# 192.168.1.31 目标地址# qita 然后重启networking服务生效： 1root@vultr:~# service networking restart 参考文献及资料1、Configure static IP address on Ubuntu 16.04 LTS Server，链接：https://michael.mckinnon.id.au/2016/05/05/configuring-ubuntu-16-04-static-ip-address/]]></content>
      <categories>
        <category>Linux IP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python标准库timeit的使用简介]]></title>
    <url>%2F2018%2F01%2F25%2F2018-01-25-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Python%E6%A0%87%E5%87%86%E5%BA%93timeit%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 模块介绍 参考文献及资料 背景Python调试代码时，经常需要测算一些代码模块或函数的执行效率（即耗时）。常用手段会在代码前后分别用time.time()记下开始和结束的时间，然后相减获得执行耗时。 本篇博客是一篇学习笔记，介绍Python一个内置模块实现代码执行计时。 第一部分 模块介绍 timeit属于Python的标准库。文件路径在~Lib/timeit.py。 timeit同时具有命令行接口和可调用的函数接口。 一、命令行接口1、案例12&gt;&gt;&gt;python -m timeit '"-".join(str(n) for n in range(100))&gt;&gt;&gt;100000 loops, best of 3: 14.1 usec per loop 回显内容：语句&quot;-&quot;.join(str(n) for n in range(100)执行了10w次，平均耗时14.1 usec。 2、接口参数说明 123456789python -m timeit [-n N] [-r N] [-s S] [-t] [-c] [-h] [statement ...]#[-n N] 表示测试语句（statement）执行的次数。如果不指定，会连续执行10，100，1000，...即10的倍数次，直到总时间至少0.2秒，结束。#[-r N] 计数器重复次数。默认是3。返回一个list，记录每次耗时。#[-s S] statement之前前的初始化语句。默认为pass。#[-t] 使用time.time()。#[-c] 使用time.clock()。#[-v] 会输出更多的执行过程信息。10次的耗时，1000次耗时，等等#[-h] 单独使用，反馈接口的使用信息。 注意：statement和[-s S]的参数按照字符串的形式传入。 二、函数接口1、类timeit.Timer 案例 123456789import timeit#定义一个类t=timeit.Timer('char in text',setup='text="sample string";char="g"')#timeit()函数t.timeit()#回显：0.019882534000089436#repeat()函数t.repeat()#回显：[0.01990252700011297, 0.01574616299990339, 0.015739961000008407] 参数说明 123456#1、初始化一个Timer类的参数：timeit.Timer(stmt='pass', setup='pass', timer=&lt;timer function&gt;)#2、timeit(number=1000000)# 默认number执行100w次。#3、repeat(repeat=3，number=1000000)# 默认执行100w次，重复3次（返回list） 2、两个函数 类似Timer的类，timeit也有两个函数。 1234567#1、timeit函数timeit.timeit(stmt="pass", setup="pass", timer=default_timer,number=default_number)#参数说明：stmt即statement，重复执行的语句。setup即执行前的初始化语句（执行一次）。#2、repeat函数timeit.repeat(stmt='pass', setup='pass', timer=&lt;default timer&gt;, repeat=3, number=1000000)#类似Timer类中函数。 案例 1234567import timeitdef test_example(): for i in range(100): "-".join(str(i))if __name__ == '__main__': print(timeit.timeit("test_example()", setup="from __main__ import test_example"))#25.844697911000367 例子中statement是个函数，重复执行前需要在setup中提前import。 参考文献及资料]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-性能评估度量总结]]></title>
    <url>%2F2018%2F01%2F25%2F2018-01-27-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E5%BA%A6%E9%87%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景​ 机器学习方法（监督学习、无监督学习、强化学习）通常认为由模型、策略和算法三个部分构成。例如在监督学习，模型需要从数据中学习条件概率分布（概率模型）或者决策函数（非概率模型）。而模型的假设空间（hypothesis space）就是所有可能的条件概率分布或者决策函数的总体（集合）。 ​ 假设空间通常是庞大的，例如深度神经网络结构能够表示一大类输入层到输出层的函数集合。这样就需要我们设定一个可以量化评测假设空间中函数优劣的度量标准，即选出最优解。 我们从数学角度去理解：这里定义在假设空间上的度量，其实是一个泛函（函数的函数）。 第一部分 几个概念的理解​ 另外我们在优化模型中，经常遇到三个概念：损失函数（loss function）、代价函数（cost function）、目标函数（object function）。 ​ 很多书对三个概念的定义范畴有差异。例如《统计学习方法（李航）》中认为损失函数和代价函数等同，而《深度学习（Yoshua）》书中认为三者等同。个人比较赞同下面的理解： loss function ，为定义在单个样本（点）上的函数。 cost function 为全体样本误差的期望，也就是损失函数（loss function）的平均。 object function ，（目标函数 ）定义为：cost function + 正则化项。 ​ 这样，代价函数和目标函数就可以理解为：定义在假设空间上的性能度量（泛函）。 注意：大家在阅读其他材料时，在概念没有达成一致情况下，需根据前后文，寻找确切的定义，避免混淆。 第二部分 性能度量介绍常用性能度量按照学习类型分类主要如下： 学习类型 性能度量 分类 accuracy、precision、recall、F1 Score、ROC Curve、auc等 回归 MAE、MSE等 2.1 回归问题给定一个训练数据集：$Train={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，这里 $x_i \in R^n$ 和 $y_i \in R$ 分别是输入和输出。学习系统基于训练数据集构建拟合函数 $f st. Y=f(X)$，对新的输入$x_{new}$，函数$f$ 给出预测输出 $y_{new}$。 记测试集：$Test={(x_1,y_1),(x_2,y_2),…,(x_M,y_M)}$ ，记其中观测值：$Y={y_1,y_2,…,y_M}$ ，预测值为:$\hat{Y}={\hat{y_1},\hat{y_2},…,\hat{y_M}}$，则有下面两种泛函： MAE（Mean Absolute Error 平均绝对误差） $$MAE(f)=MAE(Y,\hat{Y})=\dfrac{1}{M}\sum_{i=1}^M{|y_i-\hat{y_i}|}$$ 注：数学形式是向量的$L_1$范数。 MSE（Mean Squared Error 均方误差） $$MSE(f)=MSE(Y,\hat{Y})=\dfrac{1}{M}\sum_{i=1}^M{|y_i-\hat{y_i}|^2}$$ 注：数学形式是向量的$L_2$范数。 在sklearn包中已经封装相关性能函数供调用： 性能容量函数 metrics.explained_variance_score(y_true, y_pred) Explained variance regression score function metrics.mean_absolute_error(y_true, y_pred) Mean absolute error regression loss metrics.mean_squared_error(y_true, y_pred[, …]) Mean squared error regression loss metrics.mean_squared_log_error(y_true, y_pred) Mean squared logarithmic error regression loss metrics.median_absolute_error(y_true, y_pred) Median absolute error regression loss metrics.r2_score(y_true, y_pred[, …]) R^2 (coefficient of determination) regression score function. 123456from sklearn.metrics import mean_absolute_error as maefrom sklearn.metrics import mean_squared_error as msey_true = [3, -0.5, 2, 7]y_pred = [2.5, 0.0, 2, 8]mae(y_true, y_pred)mse(y_true, y_pred) 注：其他性能函数可以查看sklearn的介绍。 2.2 分类问题对于分类问题我们先介绍一个重要概念：混淆矩阵（Confusion matrix）。为了方便讨论，我们以二分类问题为例。 注意：混淆矩阵的定义很多材料也有差异，主要是记号上的区别。这里使用wiki百科的记号标准。 True Positive(TP)：将正类预测为正类数（预测正确） True Negative(TN)：将负类预测为负类数（预测正确） False Positive(FP)：将负类预测为正类数 （预测错误）（Type I error） False Negative(FN)：将正类预测为负类数（预测错误）（Type II error） 预测\真实 Positive Negative True True Positive（TP） False Positive（FP） （Type I error） False False Negative（FN）（Type II error） True negative（TN） ACC（accuracy）准确率 分类正确的样本数占样本总数的比例 。 $$ACC=\dfrac{正确预测的正类+正确预测的反类}{预测样本总量}=\dfrac{TP+TN}{TP+TN+FP+FN}$$ 缺点：在正负类样本分布不均衡的情况下，例如异常检测中如果使用该性能指标，由于样本中异常点（负类）占比很小，这样即使样本全部预测成正类，准确率也有很好的表现。该性能度量无法指导算法寻找最优解。 Precision（精确度） ​ 预测为True中，预测正确的样本占比。$$Precision=\dfrac{正确预测的正类}{预测为正类样本总量}=\dfrac{TP}{TP+FP}$$ 缺点：和ACC相同。 Recall（召回率） ​ 正确预测的正例占实际正例的比例。$$Recall=\dfrac{正确预测的正类}{实际正类样本总量}=\dfrac{TP}{TP+FN}$$ F1（F-score） 精确率和召回率的调和均值。 $$\dfrac{1}{F_1}=\dfrac{1}{Precision}+\dfrac{1}{recall},整理：F_1=\dfrac{2TP}{2TP+FP+FN}$$ 精确率和召回率都高的情况下，F1 Score也会很高 。 ROC（receiver operating characteristic curve）曲线 对于分类问题，模型通常输出的结果为样本点属于类别的概率大小，例如样本属于正类（1类）的概率为0.76，等等。通常我们会设定一个阀值（例如0.5），概率值大于阀值则认为样本属于正类，否则属于负类。 上面的性能指标都是基于通过阀值判断后的结果（0或1）进行度量。 这里引入两个度量：$$TPR=\dfrac{TP}{TP+FN}；FPR=\dfrac{FP}{FP+TN}$$ TPR：实际为正样本中，被正确地判断为正例的比率 。 FPR：实际为负样本中，被错误地判断为正例的比率。 根据不同的概率阀值$threshold \in [0,1] $，计算FPR和TPR的值，得到坐标值$(FPR,TPR)$，即ROC曲线上的一点。采样足够多的坐标点就得到了ROC曲线。 AUC 有了ROC曲线，可以计算曲线下的面积，这就是AUC（Area under the Curve of ROC ）。 AUC值越大的分类器，正确率越高。对于AUC有下面的讨论： AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。下图红线。 0.5&lt;AUC&lt;1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。 AUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。 0&lt;0.5AUC&lt;0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在0&lt;0.5AUC&lt;0.5 的情况。 log_loss（对数损失 ） $$logloss(f)=\dfrac{1}{M}\sum_{i=1}^M \sum_{j=1}^Cy_{i,j}log(P_{i,j}(x_i))$$ ​ 其中$M$为样本数，$C$为类别数，$y_{i,j}$为第$x_i$属于$j$类的符号函数。$p_{i,j}$为$x_i$为$j$类的概率。 这个性能度量其实是在算数据真实分布和预测分布的交叉熵（cross entropy ）。交叉熵越小说明预测分布和真实分布越接近。表明模型具体有较强的泛化能力。 事实上，这里有个小的trick需要说明一下： 信息论中用相对熵用来度量同一个随机变量不同概率分布的“距离”，也称为KL散度。记随机变量$X$有两个分布$p和q$,数学形式为：$$KL(p||q)=\int_Xp(X)log(\dfrac{p(X)}{q(X)})dx=-\int_Xp(X)log(q(X))dx+\int_Xp(X)log(p(X))dx$$右边等式的第一项是交叉熵，第二项是p的信息熵（常量）。所以为了使得KL散度最小化，需要交叉熵越小。即极小化相对熵和交叉熵是一回事（等价）。 同样sklearn包中对上面的性能度量均有封装函数。 第三部分 性能度量使用和总结选择恰当的性能函数对于提升模型的泛化能力是一个重要因素。学习的目的是得到泛化误差小的模型，实际问题中即使得测试集上误差最小。通常会遇到下面两种情况： 当模型在训练集上表现良好、在测试集上误差较大，称为过拟合；而模型在训练集上误差就很大时，称为欠拟合。 欠拟合解决手段有：数据增强、增加训练次数等。 过拟合则有：减少模型参数，减少特征空间，交叉验证，正则化项，Dropout（神经网络中使用）等。 为了防止模型的过拟合通常性能度量会包含正则项。 另外也可以使用多个性能度量对模型进行度量，例如$F_1 score$为精确率和召回率两个性能度量的调和值。 学习的过程转化为最小化性能度量的最优化问题。接着我们得到性能度量函数的极大似然函数。问题转换log极大似然函数最大化问题。这时通常没有解析解（线性回归是有解析解的），只能通过数值方法来寻找最优参数。 这里不再展开说了。 当然对于其他学习任务同样是有性能度量的。比如无监督的聚类问题，K-Means算法中性能度量为：寻找最佳K个质心，使得所有点到所属质心的距离和最小。无监督的降维技术PCA算法的性能度量为：寻找坐标变化，使得新的坐标系下，使得主要坐标轴上的分量的方差最大化。强化学习中Q Learing的性能容量为：对于每个状态（state）选择最佳动作（action）使得奖赏（reword）最大化。 广义上理解，性能度量是假设空间上的泛函，用于量化（度量）假设空间，从而指导算法寻找最优解。 参考文献1、ROC和AUC介绍以及如何计算AUC，链接：http://alexkong.net/2013/06/introduction-to-auc-and-roc/ 2、sklearn，链接：http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics 3、wiki百科，链接：https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF 4、《统计学习方法》 5、《深度学习》]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记（数据类型及常用命令）]]></title>
    <url>%2F2018%2F01%2F25%2F2018-01-25-Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%8F%8A%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4)%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Redis的数据类型 参考文献及资料 背景Redis作为内存数据库，具有高效的读取性能。首次安装完Redis和Python连接redis的包。尝试做一些hello world的练习。 第一部分 Redis的数据类型 Redis是一个键值数据库（Key-Value） 其中Value值支持5中数据类型。分别是string（字符串）、list（列表）、set（集合）、hash（散列）、zset（有序集合） redis-cli控制台的使用及各种数据类型的操作 进入控制台 12&gt;&gt;&gt;redis-cli127.0.0.1:6379&gt; 表示进入控制台，可以和redis进行互动。首先尝试字符串类型的三个命令（set、get、del）。 1、string类型的3个常用命令set命令 12127.0.0.1:6379&gt; set hello worldOK set命令用于给redis中指定键赋值。这里key为hello，而键值value为world。如果key不存在会自动新建该键。 get命令 12127.0.0.1:6379&gt; get hello"world" get命令用于从redis中获取指定键的值。例子获取了hello键的值，返回键值字符串“world”。 del命令 1234127.0.0.1:6379&gt; del hello(integer) 1127.0.0.1:6379&gt; get hello(nil) del命令用于从redis中删除指定键的键值。且我们用get命令检验，确实被删除了。 2、list类型的4个命令rpush、lpush命令 12345678127.0.0.1:6379&gt; rpush list-test item1(integer) 1127.0.0.1:6379&gt; rpush list-test item2(integer) 2127.0.0.1:6379&gt; rpush list-test item1(integer) 3127.0.0.1:6379&gt; lpush list-test item3(integer) 4 上面的命令我们从右边先后推入了item1，item2，item1；然后从左边推入了item3。所以最后的形式应该是[“item3”,”item1”,”item2”,”item1”] lrange命令 12345127.0.0.1:6379&gt; lrange list-test 0 -11) "item3"2) "item1"3) "item2"4) "item1" lrange用于获取list中指定范围的值。这里0是起始索引，-1是最后一个索引（类似python中的list索引）。 lindex 命令 12127.0.0.1:6379&gt; lindex list-test 2"item2" lindex命令从list中获得指定索引位置的值。这里2实际是第三个值，所以返回“item2”。 lpop、rpop命令 123456127.0.0.1:6379&gt; lpop list-test"item3"127.0.0.1:6379&gt; lrange list-test 0 -11) "item1"2) "item2"3) "item1" lpop命令将list-test中最左边的值删除（弹掉），我们用lrange命令查看，确实已经删除。rpop类似使用。 3、set集合类型的四个命令sadd命令 12345678910127.0.0.1:6379&gt; sadd set-test item(integer) 1127.0.0.1:6379&gt; sadd set-test item1(integer) 1127.0.0.1:6379&gt; sadd set-test item2(integer) 1127.0.0.1:6379&gt; sadd set-test item3(integer) 1127.0.0.1:6379&gt; sadd set-test item4(integer) 1 使用sadd命令向集合set-test加入了5个字符串值。注意set类型类似python中的set类型。无序值不重复。 sismember命令 12127.0.0.1:6379&gt; sismember set-test item3(integer) 1 sismember用来查看值是否在集合中。上面检查item3是否在set-test，返回1，表示在集合中。 srem命令 1234127.0.0.1:6379&gt; srem set-test item2(integer) 1127.0.0.1:6379&gt; srem set-test item6(integer) 0 srem命令查看值是否在集合中，如果在返回1且删除该值。否则返回0。 smembers命令 12345127.0.0.1:6379&gt; smembers set-test1) "item"2) "item4"3) "item1"4) "item3" smembers命令查看集合中所有值。上面的结果也验证了srem确实将item2删除了。 set类型还有集合之间的运算（数学），例如sinter、sunion、sdiff分别是集合的交集、并集、差集运算。 4、hash散列类型的命令散列的数据类型是存储多个键值对之间的映射。 hset命令 12345678127.0.0.1:6379&gt; hset hash-test sub-key1 value1(integer) 1127.0.0.1:6379&gt; hset hash-test sub-key2 value2(integer) 1127.0.0.1:6379&gt; hset hash-test sub-key3 value3(integer) 1127.0.0.1:6379&gt; hset hash-test sub-key2 value2(integer) 0 hset 向hash-test中插入键及键值。返回1表示原hash中无该值，0表示重复插入。 hgetall命令 1234567127.0.0.1:6379&gt; hgetall hash-test1) "sub-key1"2) "value1"3) "sub-key2"4) "value2"5) "sub-key3"6) "value3" hgetall命令从hash-test中获取所有的键值。 hget命令 12127.0.0.1:6379&gt; hget hash-test sub-key2"value2" hget命令从hash-test中获得指定键的键值。 hdel命令 1234567127.0.0.1:6379&gt; hdel hash-test sub-key2(integer) 1127.0.0.1:6379&gt; hgetall hash-test1) "sub-key1"2) "value1"3) "sub-key3"4) "value3" hdel命令删除指定键及键值。 5、有序集合的命令zadd命令 123456127.0.0.1:6379&gt; zadd zset-test 123 number1(integer) 1127.0.0.1:6379&gt; zadd zset-test 456 number2(integer) 1127.0.0.1:6379&gt; zadd zset-test 123 number1(integer) 0 zadd命令zset-test中插入分值（score）及成员名（member）。 zrange命令 12345678127.0.0.1:6379&gt; zrange zset-test 0 -1 withscores1) "number1"2) "123"3) "number2"4) "456"127.0.0.1:6379&gt; zrange zset-test 0 -11) "number1"2) "number2" zrange命令获取指定范围的分值和成员名。其中withscores参数用来控制是否同时获得score值。 zrangebyscores命令 123127.0.0.1:6379&gt; zrangebyscore zset-test 0 200 withscores1) "number1"2) "123" zrangebyscores命令获取scores指定范围的分值和成员名。 zrem命令 12345127.0.0.1:6379&gt; zrem zset-test number1(integer) 1127.0.0.1:6379&gt; zrange zset-test 0 -1 withscores1) "number2"2) "456" zrem命令检查zset-test中知否有该分值和成员名。如果有返回1，并且删除。 参考文献及资料1、redisbook，链接：https://github.com/huangz1990/redisbook]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于Whitney嵌入定理]]></title>
    <url>%2F2012%2F06%2F15%2F2012-06-15-%E5%85%B3%E4%BA%8EWhitney%E5%B5%8C%E5%85%A5%E5%AE%9A%E7%90%86%20%2F</url>
    <content type="text"><![CDATA[前言文件从多年前网易博客迁移。 正文Whitney嵌入定理是微分拓扑中重要的定理，也许可以认为正是Whitney发现了这个定理，开创了微分拓扑。在这个定理之前，人们对于流形还是把握不定的。但是在这个定理后，由于流形可以嵌入到维数较大的欧氏空间中，所以有了一系列的关于流形的重要结果，形成了微分拓扑这个分支。 Whitney嵌入定理：设M是m维光滑流形，存在M到欧氏空间Rn的光滑嵌入映射f，其中n&gt;=2m+1。并且f的像是Rn中的闭集。 定理的证明要对流形的拓扑性质有好的认识。通常我们定义流形时都是首先假定它是一个Hausdorff和第二可数的拓扑空间，然后是空间中的任意一点都存在一个开领域同胚与欧氏空间中的某个开集。也就是流形局部和欧氏空间是一样的。当然这样一个特殊的拓扑空间一定具有某些良好的性质。首先它是一个局部紧空间，对于空间中的每一点，该点的每个开领域都含有一个闭包是紧的开领域。然后它还是一个仿紧空间。正是这个仿紧性，建立了流形局部性质和整体的关系，通过单位分解的技术，我们可以从局部到整体。 当然在证明中Sard定理起了重要的作用，Sard定理把测度和拓扑性质联系，在微分拓扑中起着基础性作用。 证明方法是纯分析的技术，首先考虑流形到欧氏空间的浸入，再是单浸入，而后引入常态映射，最后证明定理。 张筑生老师的微分拓扑新讲是不错的参考书。]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[重读本科毕业设计--那年我们一起做毕业论文]]></title>
    <url>%2F2012%2F03%2F01%2F2012-03-31-%E9%87%8D%E8%AF%BB%E6%9C%AC%E7%A7%91%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1--%E9%82%A3%E5%B9%B4%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%81%9A%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[前言文章从多年前网易博客迁移。 正文再过几个月就要硕士毕业了，现在正在忙于毕业论文的撰写。突然有种冲动，从电脑里翻出了本科的毕业论文，想重温一下当年的成果。打开文件，一看页数是27页，惊讶于当时自己很能写呀，和现在的硕士论文一样长。 记得当时是2008年的年末，院里通知要选毕业论文的题目和指导老师了。当时忙于考研，很晚才看到，已经有不少同学已经选好了。由于临近毕业，大家都很忙，所以记得当时大家的选择标准是：选最简单的题目和最容易通过的导师。等到我选的时候，显然可选性已经很低了。当时曹老师的一个论文题目一下子吸引了我：有限域上的立方幂等阵的个数。当时脑中就闪过了一个想法，这个问题应该用相似标准型来做。于是就选择了曹老师的这个题目。 选题的当天晚上，记得是从汇文楼下自习回来后，就开始试图解决这个问题。在计算了几个实例之后感觉没有这么简单。有个亟须解决的问题就是：有限域上的可逆矩阵的个数如何计算？晚上折腾了很长时间，还是没有突破这个问题。 第二天，准备就去曹老师的办公室去找他，记得当时是和老苗一起去的，但是老师不在，和老苗讨论了这个问题。后来约了一个时间才见到曹老师。这时才从曹老师那知道，华罗庚的典型群这本书上，已经得到有限域上可逆矩阵的计算公式了。于是我这个问题瞬间被秒杀了。同时也感叹一下，大师就是大师呀，我折腾一个晚上都没有解决。晚上回去，就开始演算。因为要利用相似标准型，所以显然要建立有限域上的或者一般域上的相似标准型的理论。很快这个工作就完成了（但是后来答辩时才知道这个工作已经是已知结果了），然后计算幂等阵的相似标准型（这地方要分特征是2和非2两种情况分析）。然后就是简单的组合数学的知识了，但是这地方有一个表示的唯一性问题，想到用等价关系瞬间秒杀。于是论文的初稿在凌晨2点多完成了。后来才发现，问题想简单了，其实这里的相似标准型有很多情况的，这是后话了。 之后，除了开题报告时讲了一下论文思路，就再也没有花时间润色论文了。 到了09年的4月份吧，复试结束后。就开始用TEX开始敲论文了。记得有天晚上回寝室，和阿珂聊起我的论文时，在陈述我的思路时，突然发现了论文中有些相似标准型的情况没有考虑。这还得感谢阿珂，没有他的讨论，估计要等到答辩才能发现这个重大错误，所以说做数学时讨论是很重要的学习方法。 第二天，重新撰写了论文，然后就算定稿了。交给曹老师审稿也通过了。这地方想说一个细节：曹老师的审稿很细致的，在英文摘要里指出了我的一个语法不规范。然后润色了一下文字，就等着答辩了。 答辩之前，学院有个安排是：每个同学可以申请毕业论文优秀组答辩，而且只有这个组的论文可以评优秀本科毕业论文。但是自己比较懒，没有特别在意这件事。但是结果出来时，自己分在了优秀组，有点意外，应该是曹老师的帮我申请的吧。真的很感谢老师的关心。但是后来还是没有评上优毕，估计是答辩时准备的不充分，没有讲得有条理。不过在这过程中还是收获颇多。特别是被曹老师的治学风范和关心学生的精神所感染吧。而且是第一次体会纯数学研究的方法。 现在重读这篇论文，对于有些自己写的定理都已经陌生了，现在如果让我重新证明，也许要花很长时间了…… 不管怎样这篇文章算是成长的经历吧 也许多少年后，我的数学知识估计会退化到连里面的定义都看不懂了……但是那年做论文的事却依然很清晰…… 记得当年那个喜欢数学的纯粹的自己……还有我那亲爱的老师和同学……]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于紧致的Hausdorff空间上的连续函数空间C(X)]]></title>
    <url>%2F2010%2F05%2F04%2F2010-05-04-%E5%85%B3%E4%BA%8E%E7%B4%A7%E8%87%B4%E7%9A%84Hausdorff%E7%A9%BA%E9%97%B4%E4%B8%8A%E7%9A%84%E8%BF%9E%E7%BB%AD%E5%87%BD%E6%95%B0%E7%A9%BA%E9%97%B4C(X)%2F</url>
    <content type="text"><![CDATA[前言文件从多年前网易博客迁移。 正文在泛函分析中，定义在紧致的Hausdorff空间（Compact Hausdorff Space）上的复值连续函数（Continuous Complex-valued Functions）全体C(X)是重要的空间。在通常的函数加法，数乘和函数乘法（pointwise multiplication）下是一个交换代数（communicative algebra）。如果赋予最大值范数，还构成交换Banach代数。而且有更深刻的定理表明：任何一个Banach 空间H都等距同构与某个紧致的Hausdorff空间X上的连续空间。某种意义下可以说这个定理对Banach空间给了一个很好的刻画。事实上这个紧致的Hausdorff空间X就是H的对偶空间中的单位球，其拓扑是弱拓扑（由Alaoglu定理容易知道它是弱紧的）。 关于紧致的Hausdorff空间上的连续函数空间有一个有趣的命题： X是紧致的Haosdorff空间，则C(X)是有限维空间当且仅当X是有限的。 充分性：要考虑到当X是有限时，由于X是Hausdorff空间，这时X的拓扑很简单就是离散拓扑，即X的任意子集都是开集。所以X上的任意映射都是连续的，这样不难验证所有的单点集的特征函数就是C(X)的一个基底，推出C(X)是有限维的。 必要性：反证法，如果X非有限，存在可列点集（xi）。注意到点集拓扑中有个结果：紧致的Hausdorff空间是T4空间。当然也是正规空间，而且X中的有限点集是闭集，由Urysohn Lemma知道，有连续函数fj（xi）=0；当i小于等于j时，fj（xj+1）=1。这样至少有可列个线性无关的连续函数fj。这与C(X)有限维矛盾。]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于凸集分离定理]]></title>
    <url>%2F2010%2F05%2F03%2F2010-05-03-%E5%85%B3%E4%BA%8E%E5%87%B8%E9%9B%86%E5%88%86%E7%A6%BB%E5%AE%9A%E7%90%86%20%2F</url>
    <content type="text"><![CDATA[前言文件从多年前网易博客迁移。 正文泛函分析中凸集分离定理是Hahn-Banach定理的几何形式。描述的是在一定条件下，赋范线性空间中的两个互不相交的凸集被连续线性泛函分离。通常的泛函书上有两种形式：1.赋范线性空间X上的两个互不相交的凸集E和F，其中有一个有内点，这时候存在连续线性泛函分离两凸集。（证明中需要这个内点来定义Minkowski泛函）2.赋范线性空间X上的两个互不相交的凸集E、F，当E和F的距离大于0时，也有连续线性泛函分离两个凸集。（证明中由于两个凸集距离大于零，所以可以常用技巧在其中一个凸集上“镶”一个适当的“开环”仍然是凸集而且有内点，利用情形1即可）。3.赋范线性空间X上的两个互不相交的凸闭集E和凸紧集F，这是同样有连续线性泛函隔离两个集合。（证明是由于紧集和闭集的距离大于0，利用2即可）。 以上是凸集分离定理通常的三种形式，但是对于有限维的赋范线性空间（即欧氏空间），这时对于任意的两个互不相交的凸集都有连续线性泛函隔离之。]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数学系的恩师与他们的课]]></title>
    <url>%2F2009%2F08%2F01%2F2009-08-01-%E6%95%B0%E5%AD%A6%E7%B3%BB%E7%9A%84%E6%81%A9%E5%B8%88%E4%B8%8E%E4%BB%96%E4%BB%AC%E7%9A%84%E8%AF%BE%20%2F</url>
    <content type="text"><![CDATA[前言文章从多年前网易博客迁移。 悼念在迁移文章的时候，曹重光老师、吕万金老师已经永远离开了我们。在此深深怀念两位恩师。 正文曹重光老师和Linear Algebra &amp; Advanced Algebra &amp; Modern Algebra 大学里的第一节课在记忆中还是那样清晰。实验楼的一间的大教室，05级的信息与计算科学六十多人几乎坐满了。曹先自我介绍，他手里拿着我们的教材，指着书面上的作者名，说我就叫这名，接着便开始讲课（学数学的很简洁，没太多废话）。 他主讲Linear Algebra。助教是杨魏（曹的研究生），主持习题课和答疑。听说为了做好助教，她把曹老师的书后习题都挨个做完。所以我大一时不会的题问杨魏都会得到解决。 大一的上学期专业课有三门，还有李春明老师的Mathematical Analysis.和吴彦平的Analytical Geometry。这两门课的助教都很无为所以没记住名字。个人感觉三者中我比较喜欢听曹老师的Linear Algebra，在我的印象中他是没有讲课笔记的，上课时也很少翻书（除讲习题时），但是课讲得却很流畅，即使用自己的书，也从不照本宣科。让人感觉他的知识是自然流淌出来。他喜欢用一个数学问题作为引子，然后分析它，创造出新的更有力的数学工具来解决它。经常是根据我们已经了解的数学提出一些有意义的问题，然后接连几节课都是解决这些问题。听他的课能让人体会到数学知识是如何产生的，是一种莫名的享受。四年学习后，重新品味曹的课，深深感觉到一个老师如果讲课没能做到自然流畅，那是因为他对自己所讲的课根本没有很深的体会，甚至自己也是一知半解的。即使有体会也是局部的，没有高度。没有从宏观上，从数学的整体上把握这门课的结构和内涵。 曹喜欢课上提问，讲习题时还喜欢叫人上黑板做题，有时还有随堂考试。所以他的课上必须得认真听。这对于数学系的学生是很好的训练。计算专业的学生都得到良好的代数基础训练。 后来大三上学期我还旁听了他给研究生开的Modern Algebra。教材是刘绍学的An Introduction to Modern Algebra，这本书事实上是刘绍学为本科生写的教材，内容要比张禾瑞的小册子要丰富得多。每周五节课（安排在一个上午），中间只休息半个小时，作业是全部的课后习题。事实上，由于自己大二暑假留校自学了张禾瑞的小册子，所以是带着很多问题听的，受益匪浅呀。 李春明老师 和Mathematical Analysis李的Mathematical Analysis讲得也是很不错的，虽然讲得没有太多深度，也没有联系和自己的体会，但是随着课本讲得却是相当明白。他的课思路清晰，一听就知道认真备课了。听了他两年（四学期）的Mathematical Analysis，感觉李是位负责的老师。课上他喜欢跟我们提起分析中的先哲。印象深刻的是当他讲到Cauchy和 Weierstrass时总是深深的说，这些都是大师，大师了……然后用手推一推鼻梁上的眼镜。 李除了开Mathematical Analysis，也给数学班开实变函数。由于与我的课时也不冲突，差不多全听了。教材是黑大自己编的，与周民强的体系略有差异。比如关于Lebesgue积分概念的引入：强的书利用简单函数积分的引进方式，而黑大这本书采用了一种类似Riemann积分定义的方法。这样对比得听，对于一些概念的理解还是不无好处的。 肖相武老师与高级语言设计（C 语言）大一的下学期开了C语言程序设计，教材是清华大学谭浩强的。记得当时学习计算机的热情十分高涨，每次课都很积极得坐在第一排。老肖（大家喜欢这样称呼他，感觉亲切）的课讲得很自信，课上他喜欢时不时的停下来，然后转身问我们：“是不是这样呀……”，略作停顿后，自己补充道：“恩……是这样的”。 当时每周三节课，课时很紧，但老肖讲得还是蛮细致、蛮清晰的，后来当然没有讲完全书。最后考试形式是开卷，不过好象只能带课本。还记得最后一道题是个大程序题，自己写了很长，但后来发现其中有个错误。不过觉得这道题出的很好的，能检验C语言的功底。 郝翠霞老师和Real Analysis郝是在Australia拿的PhD，她的Real Analysis是双语教学。这可能是大学专业课唯一的一门课。教材是北大周民强的《实变函数》，参考书是Rudin的Real Analysis。每周还有一次习题课，内容是作业本上的习题和郝留的补充题，还是有一定难度的。班上20个人每个人都要上黑板讲几题，形式有点象讨论班，很锻炼人的。把自己的解题方法讲给别人听，使别人了解自己的思想，我个人感觉这是一件很有趣的事情。所以记得当时自己还是很积极的，有时会多讲几道的。 我觉得这门课对我的影响很大。郝老师的方法是板书和讲解是英文的，而后再加一遍中文讲解。开始不太适应，但后来也就慢慢跟上了。但这门课却有个“后裔症”：使我养成了阅读外文书籍的习惯，以后每门专业课我都会找几本外文书作为参考书。大一时是没有去西文图书馆的意识，但以后成了西文的常客。 还有在大二开学后，当时由于考虑到黑大数学系的现实状况和现实因素的影响（毕业就业等），有一种想弃数学学计算机的想法。当时甚至很快弄来一本《计算机组成原理》开始自学。但是开了这门课后，没有那么多的时间学计算机了，后来也就不了了之了。可以说是这门课使我又回到了数学。 任洪善老师 和Ordinary Differential EquationsOrdinary Differential Equations是大二下学期开的课程。由数学系方程方向领袖人物任洪善老师主讲。助教名字我忘了，但是一个很负责的老师。习题课和答疑都有的，任老师有时也会在。任的研究方向是Functional Differential Equations，科研很好的。但讲课我喜欢用Boring来形容，大家喜欢开玩笑的形容为：读报课（大家都在下面看报纸了）。有一次我坐在前面，发现他的讲课笔记可能还是他在电视广播大学时的讲义，纸张都有些发黄了。他的讲课内容完全是讲义上的，甚至是定理标号都不带变的。可能讲授时间太长了，讲义早已烂熟于心，通常在讲某个定理的证明时，“由刚才擦掉的定理3.5.4知定理4.2.3成立”，而对于定理3.5.4是什么内容从不重复。所以如果上课没有笔记，经常跟不上他，导致不知所云。 有时也会和同学们讲讲其他的东西。 （一） 做题与扫雷 印象深刻的是期末考试前期，在宣布考试标准时： 某女问：老师，如果答案写错了，但过程正确怎么给分呀？ 任：嗯……嗯……这……做题……题……就像扫雷一样，你……你一步走错了，那不就全……全错了…… 原来任老师钟情于扫雷游戏。 （二）数学老师VS英语老师 考研时旁听过任老师给研究生开的Stability Theory，课间休息时，和徒弟们聊天：“嗯……嗯……现在老师……也就……也就英语老师挣得多，在外面开个班，教室就能坐满……嗯……你看……看我们……这么大教室……就……就三个人（包括我，他只有两个研究生）……这……这没法比……”。 吕万金老师和Functional Analysis &amp; Methods of Mathematical Physics吕老师讲课很潇洒，他的风格是一切问题课堂上解决,所以讲课时通常挂黑板的。但他却不慌不忙，在黑板上擦擦写写，时不时的还翻翻书，不一会又推出结果了（不知道对不对，反正我是没用明白过）。也有时候，干脆挥一挥手中的粉笔，很轻松的说：“这疙瘩就这样了……你们自己回去看吧”。然后就开始讲下一个内容了。所以对于大三的Functional Analysis 和 Methods of Mathematical Physics是一场“灾难”。 Methods of Mathematical Physics学完后除了会几类特殊方程的求解公式，几乎没有任何物理和几何背景知识，俨然是工科的Methods of Mathematical Physics。Functional Analysis 还算幸运，但一学期讲得内容实在可怜得很。吕老师的课讲得实在不敢恭维，但为人平易近人，对待学生热情，没有老师的架子，和学生打成一片。课前课后，课上课间和学生唠家常，给学生出谋划策。这却是数学系任何一位老师都不能比的。有目共睹呀。 王士模老师和Probability Theory 王士模老师，北大数学系硕士研究生毕业，给数学班讲授Probability Theory。教材是黑大数学系自己编的，但是他却很少用，除了用习题（此书习题全部选自北大教材，其实我个人认为没有必要出这种垃圾书，还不如直接用北大的教材）。我旁听了大半学期，后来快期末考试了，害怕被自己班的老师给挂了，所以后面关于大数定理的内容便没有听了（其实以王士模老师深厚的实变功底，这部分讲得应该相当精彩，无论如何这是一个遗憾）。王老师是江苏扬州人氏，带有浓厚的扬州口音。大部分北方同学都抱怨听不懂，虽然我是安徽的，但安徽和扬州同属楚地方言区，跟上他的课还是没有问题的。王讲课最重要的特点就是充满激情，讲到高兴处通常是“手舞足蹈”，在讲台上180度的旋转，说“好……大家一看就知道”，然后再在黑板上推导，定理证明结束他还有个口头禅“这没什么……”。适当的时候还会讲一些生活中的实例，他喜欢运动，尤其是篮球，所以经常举投篮的例子。印象中王老师为人朴实，平易近人，有时甚至给人“可爱”的感觉；做学问求真、务实、执着；做老师认真负责，有真才实学。有时和大家聊起王老师，总感觉王老师怎么来黑大数学系，太屈才了，于是都喜欢猜测其中的缘由，但后来观点统一了，王老师带有浓重口音的普通话是最大的原因。 唐孝敏老师和Modern Algebra大三下学期开了Modern Algebra，教材用的是张禾瑞的那本黄色的小册子（个人感觉教材太老了，不过听说全国很多数学系还在用的）。唐讲课很有书卷气，娓娓道来，思路清晰。当时自己已经自学了张禾瑞的小册子，也旁听了曹老师的课。所以听起来要轻松得多，但是再听一遍，对于一些地方的理解还是很有收获的。当时正在准备考研，所以把张的小册上的后一章的习题都做了，并且仔细自学了姚慕生老师的Modern Algebra和群、环及域的大部分课后习题。感觉大学中Modern Algebra的确花了不少时间的，这也使得后来做毕业论文时，我的选题也是Modern Algebra 这一块的，有关有限域上的矩阵理论。当然在做论文时，对于一些概念又有了新的理解，特别是有关域论的。 秦家虎老师和Point Set TopologyPoint Set Topology是大三下学期开的课程，用的教材是熊金成的，很经典的一部教材。虽然当时的全部精力都投入在考研复习中，但是对于这门课还是投入一定精力了。作业是秦所选的部分课后习题，并且会定期交作业的，秦亲自批改，并且有时课上会选讲一些题目。虽然当时时间紧张，但每次作业都能交上。临近考试时感觉这门课自己把握不是太好，就用两个星期的时间，把前七章的余下课后习题都做了。但是感觉自己对于一些地方还是理解得不透彻。然而自己对于分析中的问题却有了一个全新的观点。记得当时正在复习分析，在做Euclidean Space的课后习题时，有些命题都是推广到Topological Space，用拓扑的语言给出证明，还是一件很有趣的事情。秦讲课语速甚快，所以写字也快。后来秦还组织同学上去讲一部分内容，个人认为这种方式很锻炼人的，非常好。但是由于当时没有太多时间去准备，没有上去锻炼一下，也是一个遗憾吧。]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[离别哈尔滨]]></title>
    <url>%2F2009%2F08%2F01%2F2009-08-01-%E7%A6%BB%E5%88%AB%E5%93%88%E5%B0%94%E6%BB%A8%2F</url>
    <content type="text"><![CDATA[前言文件从多年前网易博客迁移。 正文公元2009年7月3日。坐在64路公共汽车，看着夹在学生证中的火车票，我知道：我即将离开美丽的哈尔滨，离开我生活学习四年的黑大，告别我四年的同窗们。望着窗外的城市，忽然间有一种莫名的伤感，而这种感觉却和四年前当我离开安庆时心情是那样的相似……不知道何时能再回到这片留下我太多记忆和挂念的土地。 但依稀记得4年前，那个北上的火车载着我和一个高中生对大学的懵懂憧憬来到了这片黑土，来到了黑大数学系，开始了我大学生活。]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在FD数院面试时的self-introduction]]></title>
    <url>%2F2009%2F08%2F01%2F2009-08-01-%E5%9C%A8FD%E6%95%B0%E9%99%A2%E9%9D%A2%E8%AF%95%E6%97%B6%E7%9A%84self-introduction%20%2F</url>
    <content type="text"><![CDATA[前言文件从多年前网易博客迁移。 正文Good afternoon, my dear teachers. I am very glad to be informed to have this interview. Now I will introduce myself briefly. My name is ???. I was born in September,??th ,??? in ??? province, and I am now a senior undergraduate student from the school of Mathematics, ????? University. And I will graduate in July this year. My hometown is Anqing, a beautiful ancient city near the Changjiang river. It is famous for its Huangmei opera. In Anhui’s history, Anqing played a very important role. So it has a profound culture background. I graduated from Anqing No.1 Middle School in 2005. And then I left Anqing for Haerbin to begin my college life. Although my university is not well known, I still appreciate the education it gave me. Because I’ve acquired basic knowledge of elementary mathematics during the four years. In college, my major was information and computing science when I was a freshman, but I changed it for mathematics and applied mathematics next year. Because I wanted to learn more professional knowledge about pure mathematics. In my junior year, I begin to learn the courses about abstract algebra and general topology. It was an exciting experience. I have a new view of the concepts which I had learned before. I spent almost my spare time learning by myself during the four years. And of all the courses I studied, I like real analysis most. But I think at present, I still have many things to learn. So further study is still urgent for me. And I hope I can lay a solid foundation after two years study. That’s all. Thank you.]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
</search>
