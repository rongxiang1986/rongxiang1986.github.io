<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RongXiang</title>
  
  <subtitle>我的烂笔头</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zjrongxiang.github.io/"/>
  <updated>2020-06-01T09:29:58.283Z</updated>
  <id>https://zjrongxiang.github.io/</id>
  
  <author>
    <name>rong xiang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kafka系列文章(第三篇 Kafka可视化管理界面)</title>
    <link href="https://zjrongxiang.github.io/2020/05/12/2020-05-11-Kafka%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88%E7%AC%AC%E4%B8%89%E7%AF%87Kafka%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AE%A1%E7%90%86%E7%95%8C%E9%9D%A2%EF%BC%89/"/>
    <id>https://zjrongxiang.github.io/2020/05/12/2020-05-11-Kafka系列文章（第三篇Kafka可视化管理界面）/</id>
    <published>2020-05-12T05:30:00.000Z</published>
    <updated>2020-06-01T09:29:58.283Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  kafka-manager安装</li><li>第二部分  kafka-manager配置</li><li>第三部分  kafka-manager管理</li><li>第四部分  总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在Kafka的监控系统中有很多优秀的开源监控系统。比如Kafka-manager，open-faclcon，zabbix等主流监控工具均可直接监控kafka。Kafka集群性能监控可以从消息网络传输，消息传输流量，请求次数等指标来衡量集群性能。这些指标数据可以通过访问kafka集群的JMX接口获取。Kafka-manager工具由Yahoo研发的Kafka管理和监控工具，并在github上开源。</p><p>对于非加密Kafka集群配置Kafka manager，目前互联网也有大量的资料。而对于加密集群（特别是云端集群还配置了域名方式），参考材料较为匮乏。本文针对云端加密Kafka集群配置Kafka Manager进行详细介绍，供大家参考。</p><h2 id="第一部分-kafka-manager安装"><a href="#第一部分-kafka-manager安装" class="headerlink" title="第一部分 kafka-manager安装"></a>第一部分 kafka-manager安装</h2><h3 id="1-1-版本选择"><a href="#1-1-版本选择" class="headerlink" title="1.1 版本选择"></a>1.1 版本选择</h3><p>版本使用<code>cmak-3.0.0.0</code>版本，依赖java11（使用<code>openjdk-11+28_linux-x64_bin.tar.gz</code>）。使用已经编译好的介质包<code>cmak-3.0.0.0.zip</code>。假设安装目录为<code>/dmqs</code>。</p><h3 id="1-2-介质部署"><a href="#1-2-介质部署" class="headerlink" title="1.2 介质部署"></a>1.2 介质部署</h3><h4 id="1-2-1-部署cmak"><a href="#1-2-1-部署cmak" class="headerlink" title="1.2.1 部署cmak"></a>1.2.1 部署cmak</h4><p>上传<code>cmak-3.0.0.0.zip</code>至安装目录<code>/dmqs</code>,使用命令解压：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f-itdw-4c8g-100g-11:/dmqs # unzip cmak-3.0.0.0.zip</span><br></pre></td></tr></table></figure><h4 id="1-2-2-部署java"><a href="#1-2-2-部署java" class="headerlink" title="1.2.2 部署java"></a>1.2.2 部署java</h4><p>上传<code>openjdk-11+28_linux-x64_bin.tar.gz</code>介质到<code>/dmqs/cmak-3.0.0.0</code>路径：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # tar -zxvf openjdk-11+28_linux-x64_bin.tar.gz</span><br></pre></td></tr></table></figure><p>重命名java路径名：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # mv jdk11 jdk</span><br></pre></td></tr></table></figure><h3 id="1-3-配置文件准备"><a href="#1-3-配置文件准备" class="headerlink" title="1.3 配置文件准备"></a>1.3 配置文件准备</h3><h4 id="1-3-1-配置application-conf文件"><a href="#1-3-1-配置application-conf文件" class="headerlink" title="1.3.1 配置application.conf文件"></a>1.3.1 配置<code>application.conf</code>文件</h4><p>备份文件并修改：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # cp application.conf application.conf.bak</span><br><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # vi application.conf</span><br></pre></td></tr></table></figure><p>调整<code>afka-manager.zkhosts</code>参数项的配置信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-manager.zkhosts="84.10.228.50:2181,84.10.228.55:2181,84.10.228.56:2181"</span><br></pre></td></tr></table></figure><h4 id="1-3-2-加密集群配置jaas文件"><a href="#1-3-2-加密集群配置jaas文件" class="headerlink" title="1.3.2 加密集群配置jaas文件"></a>1.3.2 加密集群配置<code>jaas</code>文件</h4><p>如果是加密集群需要准备<code>jaas</code>文件，文件名为：<code>kafka_server_jaas.conf</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # touch kafka_server_jaas.conf</span><br></pre></td></tr></table></figure><p>文件内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">KafkaClient &#123;</span><br><span class="line">    org.apache.kafka.common.security.scram.ScramLoginModule required</span><br><span class="line">    username="admin"</span><br><span class="line">    password="admin-secret";</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Client &#123;</span><br><span class="line">    org.apache.kafka.common.security.plain.PlainLoginModule required</span><br><span class="line">    username="admin"</span><br><span class="line">    password="admin-secret";</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>上面配置中<code>KafkaClient</code>为和kafka通信配置；<code>Client</code>为和zookeeper通信配置。</p><h4 id="1-3-3-配置consumer-properties"><a href="#1-3-3-配置consumer-properties" class="headerlink" title="1.3.3 配置consumer.properties"></a>1.3.3 配置<code>consumer.properties</code></h4><p> 首先备份：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # cp consumer.properties consumer.properties.bak</span><br><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # vi consumer.properties</span><br></pre></td></tr></table></figure><p>配置文件调整为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">security.protocol=PLAINTEXT</span></span><br><span class="line"><span class="meta">#</span><span class="bash">key.deserializer=org.apache.kafka.common.serialization.ByteArrayDeserializer</span></span><br><span class="line"><span class="meta">#</span><span class="bash">value.deserializer=org.apache.kafka.common.serialization.ByteArrayDeseriazer</span></span><br><span class="line"></span><br><span class="line">bootstrap.servers=kafka.itdw.node1:9093,kafka.itdw.node2:9093,kafka.itdw.node3:9093</span><br><span class="line">security.protocol=SASL_SSL</span><br><span class="line">sasl.mechanism=SCRAM-SHA-512</span><br><span class="line">ssl.truststore.location=/usr/ca/trust/client.truststore.jks</span><br><span class="line">ssl.truststore.password=itdw123 </span><br><span class="line">ssl.keystore.password=itdw123</span><br><span class="line">ssl.keystore.location=/usr/ca/client/client.keystore.jks</span><br><span class="line">ssl.key.password=itdw123</span><br><span class="line">ssl.endpoint.identification.algorithm=</span><br></pre></td></tr></table></figure><p>其中注释部分为源配置文件内容。</p><h3 id="1-4-准备ca信任证书"><a href="#1-4-准备ca信任证书" class="headerlink" title="1.4 准备ca信任证书"></a>1.4 准备ca信任证书</h3><p>对于已经配置为域名方式的Kafka集群需要配置域名信任证书。</p><p>创建<code>InstallCert.java</code>，java程序文件：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Copyright 2006 Sun Microsystems, Inc.  All Rights Reserved.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Redistribution and use in source and binary forms, with or without</span></span><br><span class="line"><span class="comment"> * modification, are permitted provided that the following conditions</span></span><br><span class="line"><span class="comment"> * are met:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *   - Redistributions of source code must retain the above copyright</span></span><br><span class="line"><span class="comment"> *     notice, this list of conditions and the following disclaimer.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *   - Redistributions in binary form must reproduce the above copyright</span></span><br><span class="line"><span class="comment"> *     notice, this list of conditions and the following disclaimer in the</span></span><br><span class="line"><span class="comment"> *     documentation and/or other materials provided with the distribution.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *   - Neither the name of Sun Microsystems nor the names of its</span></span><br><span class="line"><span class="comment"> *     contributors may be used to endorse or promote products derived</span></span><br><span class="line"><span class="comment"> *     from this software without specific prior written permission.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS</span></span><br><span class="line"><span class="comment"> * IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,</span></span><br><span class="line"><span class="comment"> * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR</span></span><br><span class="line"><span class="comment"> * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR</span></span><br><span class="line"><span class="comment"> * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,</span></span><br><span class="line"><span class="comment"> * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,</span></span><br><span class="line"><span class="comment"> * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR</span></span><br><span class="line"><span class="comment"> * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF</span></span><br><span class="line"><span class="comment"> * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING</span></span><br><span class="line"><span class="comment"> * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS</span></span><br><span class="line"><span class="comment"> * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.URL;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> java.security.*;</span><br><span class="line"><span class="keyword">import</span> java.security.cert.*;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> javax.net.ssl.*;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InstallCert</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    String host;</span><br><span class="line">    <span class="keyword">int</span> port;</span><br><span class="line">    <span class="keyword">char</span>[] passphrase;</span><br><span class="line">    <span class="keyword">if</span> ((args.length == <span class="number">1</span>) || (args.length == <span class="number">2</span>)) &#123;</span><br><span class="line">        String[] c = args[<span class="number">0</span>].split(<span class="string">":"</span>);</span><br><span class="line">        host = c[<span class="number">0</span>];</span><br><span class="line">        port = (c.length == <span class="number">1</span>) ? <span class="number">443</span> : Integer.parseInt(c[<span class="number">1</span>]);</span><br><span class="line">        String p = (args.length == <span class="number">1</span>) ? <span class="string">"changeit"</span> : args[<span class="number">1</span>];</span><br><span class="line">        passphrase = p.toCharArray();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">"Usage: java InstallCert &lt;host&gt;[:port] [passphrase]"</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    File file = <span class="keyword">new</span> File(<span class="string">"jssecacerts"</span>);</span><br><span class="line">    <span class="keyword">if</span> (file.isFile() == <span class="keyword">false</span>) &#123;</span><br><span class="line">        <span class="keyword">char</span> SEP = File.separatorChar;</span><br><span class="line">        File dir = <span class="keyword">new</span> File(System.getProperty(<span class="string">"java.home"</span>) + SEP</span><br><span class="line">            + <span class="string">"lib"</span> + SEP + <span class="string">"security"</span>);</span><br><span class="line">        file = <span class="keyword">new</span> File(dir, <span class="string">"jssecacerts"</span>);</span><br><span class="line">        <span class="keyword">if</span> (file.isFile() == <span class="keyword">false</span>) &#123;</span><br><span class="line">        file = <span class="keyword">new</span> File(dir, <span class="string">"cacerts"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(<span class="string">"Loading KeyStore "</span> + file + <span class="string">"..."</span>);</span><br><span class="line">    InputStream in = <span class="keyword">new</span> FileInputStream(file);</span><br><span class="line">    KeyStore ks = KeyStore.getInstance(KeyStore.getDefaultType());</span><br><span class="line">    ks.load(in, passphrase);</span><br><span class="line">    in.close();</span><br><span class="line"> </span><br><span class="line">    SSLContext context = SSLContext.getInstance(<span class="string">"TLS"</span>);</span><br><span class="line">    TrustManagerFactory tmf =</span><br><span class="line">        TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm());</span><br><span class="line">    tmf.init(ks);</span><br><span class="line">    X509TrustManager defaultTrustManager = (X509TrustManager)tmf.getTrustManagers()[<span class="number">0</span>];</span><br><span class="line">    SavingTrustManager tm = <span class="keyword">new</span> SavingTrustManager(defaultTrustManager);</span><br><span class="line">    context.init(<span class="keyword">null</span>, <span class="keyword">new</span> TrustManager[] &#123;tm&#125;, <span class="keyword">null</span>);</span><br><span class="line">    SSLSocketFactory factory = context.getSocketFactory();</span><br><span class="line"> </span><br><span class="line">    System.out.println(<span class="string">"Opening connection to "</span> + host + <span class="string">":"</span> + port + <span class="string">"..."</span>);</span><br><span class="line">    SSLSocket socket = (SSLSocket)factory.createSocket(host, port);</span><br><span class="line">    socket.setSoTimeout(<span class="number">10000</span>);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">"Starting SSL handshake..."</span>);</span><br><span class="line">        socket.startHandshake();</span><br><span class="line">        socket.close();</span><br><span class="line">        System.out.println();</span><br><span class="line">        System.out.println(<span class="string">"No errors, certificate is already trusted"</span>);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (SSLException e) &#123;</span><br><span class="line">        System.out.println();</span><br><span class="line">        e.printStackTrace(System.out);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    X509Certificate[] chain = tm.chain;</span><br><span class="line">    <span class="keyword">if</span> (chain == <span class="keyword">null</span>) &#123;</span><br><span class="line">        System.out.println(<span class="string">"Could not obtain server certificate chain"</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    BufferedReader reader =</span><br><span class="line">        <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in));</span><br><span class="line"> </span><br><span class="line">    System.out.println();</span><br><span class="line">    System.out.println(<span class="string">"Server sent "</span> + chain.length + <span class="string">" certificate(s):"</span>);</span><br><span class="line">    System.out.println();</span><br><span class="line">    MessageDigest sha1 = MessageDigest.getInstance(<span class="string">"SHA1"</span>);</span><br><span class="line">    MessageDigest md5 = MessageDigest.getInstance(<span class="string">"MD5"</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; chain.length; i++) &#123;</span><br><span class="line">        X509Certificate cert = chain[i];</span><br><span class="line">        System.out.println</span><br><span class="line">            (<span class="string">" "</span> + (i + <span class="number">1</span>) + <span class="string">" Subject "</span> + cert.getSubjectDN());</span><br><span class="line">        System.out.println(<span class="string">"   Issuer  "</span> + cert.getIssuerDN());</span><br><span class="line">        sha1.update(cert.getEncoded());</span><br><span class="line">        System.out.println(<span class="string">"   sha1    "</span> + toHexString(sha1.digest()));</span><br><span class="line">        md5.update(cert.getEncoded());</span><br><span class="line">        System.out.println(<span class="string">"   md5     "</span> + toHexString(md5.digest()));</span><br><span class="line">        System.out.println();</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    System.out.println(<span class="string">"Enter certificate to add to trusted keystore or 'q' to quit: [1]"</span>);</span><br><span class="line">    String line = reader.readLine().trim();</span><br><span class="line">    <span class="keyword">int</span> k;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        k = (line.length() == <span class="number">0</span>) ? <span class="number">0</span> : Integer.parseInt(line) - <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (NumberFormatException e) &#123;</span><br><span class="line">        System.out.println(<span class="string">"KeyStore not changed"</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    X509Certificate cert = chain[k];</span><br><span class="line">    String alias = host + <span class="string">"-"</span> + (k + <span class="number">1</span>);</span><br><span class="line">    ks.setCertificateEntry(alias, cert);</span><br><span class="line"> </span><br><span class="line">    OutputStream out = <span class="keyword">new</span> FileOutputStream(<span class="string">"jssecacerts"</span>);</span><br><span class="line">    ks.store(out, passphrase);</span><br><span class="line">    out.close();</span><br><span class="line"> </span><br><span class="line">    System.out.println();</span><br><span class="line">    System.out.println(cert);</span><br><span class="line">    System.out.println();</span><br><span class="line">    System.out.println</span><br><span class="line">        (<span class="string">"Added certificate to keystore 'jssecacerts' using alias '"</span></span><br><span class="line">        + alias + <span class="string">"'"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">char</span>[] HEXDIGITS = <span class="string">"0123456789abcdef"</span>.toCharArray();</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> String <span class="title">toHexString</span><span class="params">(<span class="keyword">byte</span>[] bytes)</span> </span>&#123;</span><br><span class="line">    StringBuilder sb = <span class="keyword">new</span> StringBuilder(bytes.length * <span class="number">3</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> b : bytes) &#123;</span><br><span class="line">        b &amp;= <span class="number">0xff</span>;</span><br><span class="line">        sb.append(HEXDIGITS[b &gt;&gt; <span class="number">4</span>]);</span><br><span class="line">        sb.append(HEXDIGITS[b &amp; <span class="number">15</span>]);</span><br><span class="line">        sb.append(<span class="string">' '</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sb.toString();</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SavingTrustManager</span> <span class="keyword">implements</span> <span class="title">X509TrustManager</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> X509TrustManager tm;</span><br><span class="line">    <span class="keyword">private</span> X509Certificate[] chain;</span><br><span class="line"> </span><br><span class="line">    SavingTrustManager(X509TrustManager tm) &#123;</span><br><span class="line">        <span class="keyword">this</span>.tm = tm;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">public</span> X509Certificate[] getAcceptedIssuers() &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">checkClientTrusted</span><span class="params">(X509Certificate[] chain, String authType)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> CertificateException </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">checkServerTrusted</span><span class="params">(X509Certificate[] chain, String authType)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> CertificateException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.chain = chain;</span><br><span class="line">        tm.checkServerTrusted(chain, authType);</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上传至目的目录,并编译：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # /dmqs/cmak-3.0.0.0/jdk/bin/javac InstallCert.java</span><br></pre></td></tr></table></figure><p>编译后生成下面的文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # ll</span><br><span class="line">-rw-r--r-- 1 dmqs dmqs    975 May 28 02:23 InstallCert$SavingTrustManager.class</span><br><span class="line">-rw-r--r-- 1 dmqs dmqs   6126 May 28 02:23 InstallCert.class</span><br><span class="line">-rw-r--r-- 1 dmqs dmqs   6884 May 28 02:21 InstallCert.java</span><br></pre></td></tr></table></figure><p>添加域名（kafka集群配置为域名方式）到<code>jssecacerts</code>文件中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # /dmqs/cmak-3.0.0.0/jdk/bin/java InstallCert kafka.itdw.node1:9093</span><br></pre></td></tr></table></figure><p>这时在当前目录就生成了<code>jssecacerts</code>文件。如果集群是多节点，需要将其他节点域名信息追加到这个文件中。执行命令即为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # /dmqs/cmak-3.0.0.0/jdk/bin/java InstallCert kafka.itdw.node2:9093</span><br><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # /dmqs/cmak-3.0.0.0/jdk/bin/java InstallCert kafka.itdw.node3:9093</span><br></pre></td></tr></table></figure><p>这样就生成了集群所有的节点域名的信任证书。</p><p>最后将<code>jssecacerts</code>文件拷贝至<code>jdk/lib/security</code>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # cp jssecacerts jdk/lib/security</span><br></pre></td></tr></table></figure><p>完成所有配置的准备。</p><h3 id="1-5-服务启动"><a href="#1-5-服务启动" class="headerlink" title="1.5 服务启动"></a>1.5 服务启动</h3><p>完成配置文件准备后，使用下面的命令启动<code>Kafka-manager</code>服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/bin # ./cmak -java-home ../jdk -Djava.security.auth.login.config=../conf/kafka_server_jaas.conf -Dapplication.home=/dmqs/cmak-3.0.0.0 &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>其中参数命令说明如下：</p><ul><li><p>参数<code>-java-home</code>指定服务启动的java依赖环境目录；</p></li><li><p>参数<code>-Djava.security.auth.login.config</code>指定和kafka和zookeeper交互的jaas文件路径；</p></li><li>参数<code>-Dapplication.home</code>指定了应用的主目录；</li><li>参数<code>-Dhttp.port=8888</code>指定了应用的监听端口，默认9000；</li><li>参数<code>-Dconfig.file=../conf/application.conf</code>指定了应用的应用配置文件；</li></ul><p>启动后应用目录下面生成<code>logs</code>目录，作为日志存放目录。启动命令不指定端口的情况下，默认监听<code>9000</code>端口。</p><h3 id="1-6-自动化脚本"><a href="#1-6-自动化脚本" class="headerlink" title="1.6 自动化脚本"></a>1.6 自动化脚本</h3><p>为了提高服务运维管理，对服务启停进行自动化管理。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash -e</span></span><br><span class="line"> </span><br><span class="line">RETVAL=0</span><br><span class="line">cmak="/dmqs/cmak-3.0.0.0/bin/cmak"</span><br><span class="line">start() &#123;</span><br><span class="line">        $cmak -java-home ../jdk -Djava.security.auth.login.config=../conf/kafka_server_jaas.conf -Dapplication.home=/dmqs/cmak-3.0.0.0 &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line">        RETVAL=$?</span><br><span class="line">        [ $RETVAL -eq 0 ] &amp;&amp; echo "Start Kafka Manager Success!" ||echo "Start Kafka Manager failed!"</span><br><span class="line">        return $RETVAL</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">stop() &#123;</span><br><span class="line">        CMAKPID=$(ps -ef|grep cmak|grep -v grep| awk '&#123;print $2&#125;')</span><br><span class="line">        if [[ -a /dmqs/cmak-3.0.0.0/RUNNING_PID ]]</span><br><span class="line">        then</span><br><span class="line">         rm /dmqs/cmak-3.0.0.0/RUNNING_PID &amp;&amp; echo -e "\n已删除文件:RUNNING_PID\n" &amp;&amp; kill -9 $CMAKPID &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line">         RETVAL=$?</span><br><span class="line">        else</span><br><span class="line">          kill -9 $CMAKPID &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line">          RETVAL=$?</span><br><span class="line">        fi;</span><br><span class="line">        [ $? -eq 0 ] &amp;&amp; echo "Stop Kafka Manager Success!" ||echo "Stop Kafka Manager failed!"</span><br><span class="line">        return $RETVAL</span><br><span class="line">&#125;</span><br><span class="line">case "$1" in</span><br><span class="line">  start)</span><br><span class="line">        start</span><br><span class="line">    ;;</span><br><span class="line">  stop)</span><br><span class="line">        stop</span><br><span class="line">        ;;</span><br><span class="line">  restart)</span><br><span class="line">       </span><br><span class="line">       sh $0 stop</span><br><span class="line">       sh $0 start</span><br><span class="line">        ;;</span><br><span class="line">   *)</span><br><span class="line">        echo "Format error!"</span><br><span class="line">        echo $"Usage: $0 &#123;start|stop|restart&#125;"</span><br><span class="line">        exit 1</span><br><span class="line">        ;;</span><br><span class="line">esac</span><br><span class="line">exit $RETVAL</span><br></pre></td></tr></table></figure><p>对于启动命令，可以自定义修改。</p><h2 id="第二部分-kafka-manager配置"><a href="#第二部分-kafka-manager配置" class="headerlink" title="第二部分 kafka-manager配置"></a>第二部分 kafka-manager配置</h2><h3 id="2-1-创建新集群管理"><a href="#2-1-创建新集群管理" class="headerlink" title="2.1 创建新集群管理"></a>2.1 创建新集群管理</h3><p>创建新的管理集群，需要填入下面的信息：</p><ul><li><p>Cluster Name</p><p>集群名称；</p></li><li><p>Cluster Zookeeper Hosts</p><p>配置kafka集群背后的zookeeper集群的信息。例如：192.168.1.1:2181；</p></li><li><p>Kafka Version</p><p>Kafka的版本信息；</p></li><li><p>Enable JMX Polling (Set JMX_PORT env variable before starting kafka server)</p><p>是否启用集群的监控组件。</p></li><li><p>Security Protocol</p><p>安全协议。目前支持：SSL、SASL_PLAINTEXT、SASL_SSL、PLAINTEXT</p></li><li><p>SASL Mechanism (only applies to SASL based security)</p><p>SASL的权限管理协议：DEFAULT、PLAIN、GSSAPI、SCRAM-SHA-256、SCRAM-SHA-512</p></li><li><p>SASL JAAS Config (only applies to SASL based security)</p><p>SASL的用户配置信息。例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret";</span><br></pre></td></tr></table></figure><p>需要注意的是配置以分号结束，否则会报错。</p></li></ul><h2 id="第三部分-kafka-manager管理"><a href="#第三部分-kafka-manager管理" class="headerlink" title="第三部分 kafka-manager管理"></a>第三部分 kafka-manager管理</h2><p>Kafka Manager服务启动后，默认监听9000端口，所以服务URL地址为：<code>http://102.168.1.1:9000</code>。目前组件支持的管理功能有：</p><ul><li>管理多个集群</li><li>轻松检查集群状态（主题，使用者，偏移量，代理，副本分发，分区分发）</li><li>运行首选副本选择</li><li>生成带有选项的分区分配，以选择要使用的代理</li><li>运行分区的重新分配（基于生成的分配）</li><li>使用可选的主题配置创建主题（0.8.1.1与0.8.2+具有不同的配置）</li><li>删除主题（仅在0.8.2+上受支持，并记住在代理配置中设置delete.topic.enable = true）</li><li>现在，主题列表指示标记为删除的主题（仅在0.8.2+上受支持）</li><li>批量生成多个主题的分区分配，并可以选择要使用的代理</li><li>批量运行分区的多个主题的重新分配</li><li>将分区添加到现有主题</li><li>更新现有主题的配置</li><li>（可选）为代理级别和主题级别的度量启用JMX轮询。</li><li>（可选）过滤出在Zookeeper中没有id / owner /＆offsets /目录的使用者。</li></ul><p>对于具体的组件使用，可以参文献中的[3]。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、kafka-manager项目地址，链接：<a href="https://github.com/yahoo/kafka-manager" target="_blank" rel="noopener">https://github.com/yahoo/kafka-manager</a></p><p>2、kafka-manager项目下载地址，链接：<a href="https://blog.wolfogre.com/posts/kafka-manager-download/" target="_blank" rel="noopener">https://blog.wolfogre.com/posts/kafka-manager-download/</a></p><p>3、Apache Kafka集群管理工具CMAK(Cluster Manager for Apache Kafka)从安装启动到配置使用，链接：<a href="http://www.luyixian.cn/news_show_324464.aspx" target="_blank" rel="noopener">http://www.luyixian.cn/news_show_324464.aspx</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  kafka-manager安装&lt;/li&gt;
&lt;li&gt;第二部分  kafka-manager配置
      
    
    </summary>
    
      <category term="Kafka" scheme="https://zjrongxiang.github.io/categories/Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux系统远程执行shell命令</title>
    <link href="https://zjrongxiang.github.io/2020/05/07/2020-05-07-Linux%E7%B3%BB%E7%BB%9F%E8%BF%9C%E7%A8%8B%E6%89%A7%E8%A1%8Cshell%E5%91%BD%E4%BB%A4/"/>
    <id>https://zjrongxiang.github.io/2020/05/07/2020-05-07-Linux系统远程执行shell命令/</id>
    <published>2020-05-07T04:42:00.000Z</published>
    <updated>2020-05-07T14:18:15.322Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分  远程执行shell命令 </p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="第一部分-远程执行shell命令"><a href="#第一部分-远程执行shell命令" class="headerlink" title="第一部分 远程执行shell命令"></a>第一部分 远程执行shell命令</h2><h2 id="第二部分-Python实现远程执行shell命令"><a href="#第二部分-Python实现远程执行shell命令" class="headerlink" title="第二部分 Python实现远程执行shell命令"></a>第二部分 Python实现远程执行shell命令</h2><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、iptables命令详解，链接</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分  远程执行shell命令 &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;参考文献及
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux系统中环境变量详解</title>
    <link href="https://zjrongxiang.github.io/2020/04/21/2020-04-21-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E8%AF%A6%E8%A7%A3/"/>
    <id>https://zjrongxiang.github.io/2020/04/21/2020-04-21-Linux系统中环境变量详解/</id>
    <published>2020-04-21T04:42:00.000Z</published>
    <updated>2020-04-21T13:41:41.936Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分   </p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="第一部分-环境变量分类"><a href="#第一部分-环境变量分类" class="headerlink" title="第一部分 环境变量分类"></a>第一部分 环境变量分类</h2><h3 id="1-1-生命周期"><a href="#1-1-生命周期" class="headerlink" title="1.1 生命周期"></a>1.1 生命周期</h3><p>永久环境变量</p><p>临时环境变量</p><h3 id="1-2-作用域"><a href="#1-2-作用域" class="headerlink" title="1.2 作用域"></a>1.2 作用域</h3><p>系统级环境变量</p><p>用户环境变量</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、iptables命令详解，链接：<a href="https://wangchujiang.com/linux-command/c/iptables.html" target="_blank" rel="noopener">https://wangchujiang.com/linux-command/c/iptables.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分   &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;参考文献及资料&lt;/p&gt;
&lt;/li
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux系统中的shell解释器介绍</title>
    <link href="https://zjrongxiang.github.io/2020/04/21/2020-04-21-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84shell%E4%BB%8B%E7%BB%8D/"/>
    <id>https://zjrongxiang.github.io/2020/04/21/2020-04-21-Linux系统中的shell介绍/</id>
    <published>2020-04-21T04:42:00.000Z</published>
    <updated>2020-05-02T15:09:55.212Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分  shell解释器种类</p></li><li><p>第二部分 shell执行命令模式</p></li><li><p>第三部分 shell命令的形式</p></li><li><p>第四部分 shell初始化</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>shell是用C语言编写的程序，用户用它来和Linux进行交互。我们通常说的 shell 都是指 shell 脚本，但 shell 和 shell script 是两个不同的概念。</p><h2 id="第一部分-shell解释器种类"><a href="#第一部分-shell解释器种类" class="headerlink" title="第一部分 shell解释器种类"></a>第一部分 shell解释器种类</h2><h3 id="1-1-shell常见解释器类型"><a href="#1-1-shell常见解释器类型" class="headerlink" title="1.1 shell常见解释器类型"></a>1.1 shell常见解释器类型</h3><p>Linux shell常用类型有：<code>bash</code>、<code>ksh</code>、<code>csh</code>、<code>zsh</code>、<code>ash</code>等。可以通过下面的命令查看操作系统支持的shell类型（<code>ubuntu</code>）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/# cat /etc/shells</span><br><span class="line"><span class="meta">#</span><span class="bash"> /etc/shells: valid login shells</span></span><br><span class="line">/bin/sh</span><br><span class="line">/bin/dash</span><br><span class="line">/bin/bash</span><br><span class="line">/bin/rbash</span><br></pre></td></tr></table></figure><p>使用下面的命令查看当前（<code>ubuntu</code>）正在使用的shell：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/# echo $SHELL</span><br><span class="line">/bin/bash</span><br></pre></td></tr></table></figure><p>这里<code>SHELL</code>是一个环境变量，用于记录当前用户使用的shell类型。</p><p>在当前shell环境中，可以打开其他的shell（子shell）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/# /bin/dash</span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br></pre></td></tr></table></figure><p>上面命令我们打开了<code>dash</code>，如果要退出，直接使用<code>exit</code>命令即可退出子shell。</p><h3 id="1-2-常用shell详细介绍"><a href="#1-2-常用shell详细介绍" class="headerlink" title="1.2 常用shell详细介绍"></a>1.2 常用shell详细介绍</h3><h4 id="1-2-1-ash"><a href="#1-2-1-ash" class="headerlink" title="1.2.1 ash"></a>1.2.1 ash</h4><p>ash Shell是由Kenneth Almquist编写的，是Linux 中占用系统资源最少的一个小Shell。但是它只包含24个内部命令，因而使用起来很不方便。</p><h4 id="1-2-2-bash"><a href="#1-2-2-bash" class="headerlink" title="1.2.2 bash"></a>1.2.2 bash</h4><p>bash是Linux系统默认使用的Shell，它由Brian Fox 和Chet Ramey共同完成，是BourneAgain Shell的缩写，内部命令一共有40 个。Linux 使用它作为默认的Shell是因为它具有以下特色：</p><ul><li>可以使用类似DOS下面的doskey的功能，用上下方向键查阅和快速输入并修改命令。</li></ul><ul><li>自动通过查找匹配的方式，给出以某字串开头的命令。</li></ul><ul><li>包含了自身的帮助功能，你只要在提示符下面键入help就可以得到相关的帮助信息。</li></ul><h4 id="1-2-3-ksh"><a href="#1-2-3-ksh" class="headerlink" title="1.2.3 ksh"></a>1.2.3 ksh</h4><p>ksh是Korn Shell的缩写，由Eric Gisin编写，共有42 条内部命令。该Shell最大的优点是几乎和商业发行版的ksh 完全相容，这样就可以在不用花钱购买商业版本的情况下尝试商业版本的性能了。</p><h4 id="1-2-4-csh"><a href="#1-2-4-csh" class="headerlink" title="1.2.4 csh"></a>1.2.4 csh</h4><p>csh 是Linux 比较大的内核，它由以William Joy 为代表的共计47 位作者编成，共有52个内部命令。该Shell其实是指向/bin/tcsh这样的一个Shell，也就是说，csh其实就是tcsh。</p><h4 id="1-2-5-zch"><a href="#1-2-5-zch" class="headerlink" title="1.2.5 zch"></a>1.2.5 zch</h4><p>zch是Linux 最大的Shell之一，由Paul Falstad完成，共有84 个内部命令。如果只是一般的用途，没有必要安装这样的Shell</p><h2 id="第二部分-Shell执行命令的模式"><a href="#第二部分-Shell执行命令的模式" class="headerlink" title="第二部分 Shell执行命令的模式"></a>第二部分 Shell执行命令的模式</h2><p>Shell有交互式和非交互两种模式。这就像Python这个解释型语言类似，有交互式和非交互式。</p><h3 id="2-1-交互式"><a href="#2-1-交互式" class="headerlink" title="2.1 交互式"></a>2.1 交互式</h3><p>交互式（Interactive）：解释执行用户的命令，用户输入一条命令，Shell就解释执行一条。</p><h3 id="2-2-非交互式"><a href="#2-2-非交互式" class="headerlink" title="2.2 非交互式"></a>2.2 非交互式</h3><p>非交互式即批处理（batch），用户提前编写好Shell脚本，文本文件中有很多条命令，让Shell一次把这些命令执行完。另外对于使用管道连接的多个命令也算批处理。当读到脚本文件的结尾，shell也就终止。</p><h2 id="第三部分-shell命令的形式"><a href="#第三部分-shell命令的形式" class="headerlink" title="第三部分 shell命令的形式"></a>第三部分 shell命令的形式</h2><h3 id="3-1-内部命令"><a href="#3-1-内部命令" class="headerlink" title="3.1 内部命令"></a>3.1 内部命令</h3><p>内部命令内置于Shell源码中，即存在于内存中，一般比较简短，代码量很少，执行起来速度快，经常会使用，比如<code>cd</code>、<code>echo</code>。它与shell本身处在同一进程内（它就写在Shell这个程序里面）,当打开Shell时，操作系统会将Shell程序放入内存 。</p><blockquote><p>类似Python的程序中的内置函数（Build-in Function），python解析器初始化化就会加载这些函数。</p></blockquote><h3 id="3-2-外部命令"><a href="#3-2-外部命令" class="headerlink" title="3.2 外部命令"></a>3.2 外部命令</h3><p>外部命令一般功能比较强大，包含的代码量也较大，所以在系统加载时并不随系统一起被加载到内存中，而是在需要时才调用，它们是存在于文件系统中某个目录下的单独的程序，当执行外部命令时，会到文件系统中文件的目录中寻找，例如 <code>cp</code>、<code>rm</code>。</p><h3 id="3-3-查看命令类型"><a href="#3-3-查看命令类型" class="headerlink" title="3.3 查看命令类型"></a>3.3 查看命令类型</h3><p>对于一个命令是否是内部或者外部命令，可以使用<code>type</code>命令来检测。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@vultr:~# type cd </span><br><span class="line">cd is a shell builtin</span><br><span class="line">root@vultr:~# type cp</span><br><span class="line">cp is /bin/cp</span><br></pre></td></tr></table></figure><p>其中<code>builtin</code>就是指是内部命令，类似Python中<code>builtin</code>包。另外type本身也是一个内部命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@vultr:~# type type</span><br><span class="line">type is a shell builtin</span><br></pre></td></tr></table></figure><h2 id="第四部-shell的初始化"><a href="#第四部-shell的初始化" class="headerlink" title="第四部 shell的初始化"></a>第四部 shell的初始化</h2><p>当shell被调用时，会读取一些初始化启动文件。主要作用是为shell本身或用户设定运行环境，包含一些函数、 变量、别名等等。</p><p>shell有两种类型的初始化文件：</p><ul><li><strong>系统级启动文件</strong>。这些包含适用于系统上所有用户的全局配置，通常位于<code>/etc</code>目录中。 它们包括：<code>/etc/profiles</code>和<code>/etc/bashrc</code>或<code>/etc/bash.bashrc</code> 。</li><li><strong>用户级启动文件</strong> 。这些存储配置适用于系统上的单个用户，通常位于用户主目录中作为点文件。 它们可以覆盖系统范围的配置。 它们包括： <code>.profiles</code> ， <code>.bash_profile</code> ， <code>.bashrc</code>和<code>.bash_login</code>。</li></ul><p>shell可以以三种可能的模式被调用：</p><h3 id="4-1-交互式登录shell"><a href="#4-1-交互式登录shell" class="headerlink" title="4.1 交互式登录shell"></a>4.1 交互式登录shell</h3><p>当用户成功登录系统后,使用<code>/bin/login</code>登录，随后读取<code>/etc/passwd</code>文件,获取凭证后调用shell。</p><p>当启动交互式shell后，将读取<code>/etc/profile</code>文件以及特定的用户文件<code>~/.bash_profile</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">login as: pi</span><br><span class="line">pi@pi.raspi.in's password:</span><br><span class="line"></span><br><span class="line">The programs included with the Debian GNU/Linux system are free software;</span><br><span class="line">the exact distribution terms for each program are described in the</span><br><span class="line">individual files in /usr/share/doc/*/copyright.</span><br><span class="line"></span><br><span class="line">Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent</span><br><span class="line">permitted by applicable law.</span><br><span class="line">Last login: Sat Feb  8 11:02:44 2020 from 192.168.2.14</span><br><span class="line"></span><br><span class="line">pi@raspi:~ $</span><br></pre></td></tr></table></figure><p>然后这个登录shell 将查找几个不同的启动文件来处理其中的命令（它的作用是初始化linux系统相关配置）, bash shell 处理文件的顺序如下：</p><ul><li><p>系统登录后，shell首先执行<code>/etc/profile</code>文件中的命令(超级用户)。设置这个文件后，可以为系统内所有的bash用户建立默认的特征（不同版本的Linux在此文件放置的命令不尽相同）；<code>/etc/profile</code> :系统级的初始化文件。如果是超级用户则提示符用#，如果是普通用户则提示符用$.</p></li><li><p>当某个用户登录后，shell依次查找<code>~/.bash_profile</code>、<code>~/.bash_login</code>、<code>～/.profile</code>这几个文件，并执行它找到的第一个文件中的命令，可以将命令放在这些文件中，以重写<code>/etc/profile</code>文件中默认的设置；</p></li></ul><ul><li>当用户注销时，bash执行文件<code>~/.bash_logou</code>中的命令，这个文件包含了退出会话时执行的清理命令，退出等，如：<code>exit</code>退出。</li></ul><h3 id="4-2-非登录交互式Shell"><a href="#4-2-非登录交互式Shell" class="headerlink" title="4.2 非登录交互式Shell"></a>4.2 非登录交互式Shell</h3><p>当使用如<code>/bin/bash</code> or <code>/bin/zsh</code>的命令行时，进入系统非登录交互式shell。也可以通过<code>/bin/su</code>命令来运行。另外，在打开如<code>konsole</code>、<code>terminator</code>和<code>xterm</code>等图形化终端程序时 ，非登录交互式shell也将被调用。</p><p>这种情况下调用时，它将拷贝父shell的环境，并读取相应用户级的<code>~/.bashrc</code>配置文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">pi@raspberrypi:/home/pi# cd</span><br><span class="line">pi@raspberrypi:~# ls -la</span><br><span class="line">pi@raspberrypi:~ $ ls -la</span><br><span class="line">总用量 152</span><br><span class="line">drwxr-xr-x 25 pi   pi   4096 7月   1  2019 .</span><br><span class="line">drwxr-xr-x  3 root root 4096 7月   5  2017 ..</span><br><span class="line">-rw-------  1 pi   pi   3772 2月   9 02:12 .bash_history</span><br><span class="line">-rw-r--r--  1 pi   pi    220 7月   5  2017 .bash_logout</span><br><span class="line">-rw-r--r--  1 pi   pi   3512 7月   5  2017 .bashrc</span><br><span class="line">drwxr-xr-x  7 pi   pi   4096 3月  11  2018 .cache</span><br><span class="line">drwx------ 10 pi   pi   4096 3月  11  2018 .config</span><br><span class="line">-rwxr-xr-x  1 pi   pi     44 5月   5  2019 deeppc.sh</span><br><span class="line">drwxr-xr-x  2 pi   pi   4096 7月   5  2017 Desktop</span><br><span class="line">drwxr-xr-x  6 pi   pi   4096 3月  11  2018 Documents</span><br><span class="line">drwxr-xr-x  2 pi   pi   4096 7月   5  2017 Downloads</span><br><span class="line">drwx------  2 pi   pi   4096 4月  18  2018 .gconf</span><br><span class="line">drwxr-xr-x  2 pi   pi   4096 7月   5  2017 .gstreamer-0.10</span><br><span class="line">drwxr-xr-x  2 pi   pi   4096 7月  10  2017 .idlerc</span><br><span class="line">drwxr-xr-x  3 pi   pi   4096 7月   5  2017 .local</span><br><span class="line">drwxr-xr-x  3 pi   pi   4096 7月  10  2017 .minecraft</span><br><span class="line">drwxr-xr-x  2 pi   pi   4096 7月   5  2017 Music</span><br><span class="line">drwxr-xr-x  2 pi   pi   4096 7月   5  2017 Pictures</span><br><span class="line">drwxr-xr-x  2 pi   pi   4096 8月  16  2018 .pip</span><br><span class="line">drwx------  3 pi   pi   4096 7月  10  2017 .pki</span><br><span class="line">-rw-r--r--  1 pi   pi    675 7月   5  2017 .profile</span><br><span class="line">drwxr-xr-x  2 pi   pi   4096 7月   5  2017 Public</span><br><span class="line">drwxr-xr-x  2 pi   pi   4096 7月   5  2017 python_games</span><br><span class="line">drwx------  3 pi   pi   4096 7月  10  2017 .scim</span><br><span class="line">drwx------  2 pi   pi   4096 4月  25  2018 .ssh</span><br><span class="line">drwxr-xr-x  2 pi   pi   4096 7月   5  2017 Templates</span><br><span class="line">drwxr-xr-x  3 pi   pi   4096 7月   5  2017 .themes</span><br><span class="line">drwxr-xr-x  2 pi   pi   4096 7月   5  2017 Videos</span><br><span class="line">-rw-------  1 pi   pi   3340 5月   5  2019 .viminfo</span><br><span class="line">drwxr-xr-x  3 pi   pi   4096 3月  11  2018 .Wolfram</span><br><span class="line">drwxr-xr-x 10 pi   pi   4096 3月  11  2018 .WolframEngine</span><br><span class="line">-rw-------  1 pi   pi     56 7月   1  2019 .Xauthority</span><br><span class="line">-rw-------  1 pi   pi   8541 7月   1  2019 .xsession-errors</span><br><span class="line">-rw-------  1 pi   pi   8541 12月 13  2018 .xsession-errors.old</span><br></pre></td></tr></table></figure><p>交互式非登录shell 就是指你在当前图形界面中打开“终端”出来的那种窗口程序，和登录shell相比，它是“非登录”的，你并不需要输入用户名和码；和非交互式shell相比，这是“交互式”的，就像你说的那它：你输入什么，它就解释出什么。</p><p>并不执行前面提到的初始化文件中的命令，继承了登录 shell 设置的 shell 变量。</p><p>在交互式非登录shell里，只读取/etc/bash.bashrc和~/.bashrc（该本件包含了专用于你的bash shell的shell信息，当登录时或者每次打开新shell时，该文件即被读取</p><h3 id="4-3-非交互式Shell"><a href="#4-3-非交互式Shell" class="headerlink" title="4.3 非交互式Shell"></a>4.3 非交互式Shell</h3><p>当执行脚本时，则调用非交互式shell。在这种模式下，它将处理所运行的脚本中的命令、函数等操作，不需要进行交互式输入（除非脚本需要交互式输入）。使用的环境继承自父shell。</p><h2 id="第五部分-启动文件介绍"><a href="#第五部分-启动文件介绍" class="headerlink" title="第五部分 启动文件介绍"></a>第五部分 启动文件介绍</h2><h3 id="5-1-系统级启动文件"><a href="#5-1-系统级启动文件" class="headerlink" title="5.1 系统级启动文件"></a>5.1 系统级启动文件</h3><p><code>/etc/profile</code>，文件保存了登录时系统级环境配置和启动程序。如果你想配置应用于所有用户的环境设置，可以加入此文件。</p><p><code>/etc/bash.bashrc</code>，包含应用于所有用户的系统级函数、变量、别名等配置信息。</p><h3 id="5-2-用户级启动文件"><a href="#5-2-用户级启动文件" class="headerlink" title="5.2 用户级启动文件"></a>5.2 用户级启动文件</h3><p>在<code>/home/用户名</code>该目录下面有文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.bash_history</span><br><span class="line">.bash_logout</span><br><span class="line">.bashrc</span><br><span class="line">.profile</span><br><span class="line">.bash_profile</span><br></pre></td></tr></table></figure><p>最后一个是 <code>~/.bash_logout</code>文件，它不用于<code>shell</code>启动，但存储一些特殊的指令当用户注销（登出）时执行。</p><p>去其家目录读取~/.bash_profile，如果这读取不了就读取~/.bash_login，这个也读取不了才会读取<br>~/.profile，这三个文档设定基本上是一样的，读取有优先关系</p><h2 id="第六部分-su命令注意事项"><a href="#第六部分-su命令注意事项" class="headerlink" title="第六部分 su命令注意事项"></a>第六部分 su命令注意事项</h2><p>在Linux系统使用中，很多用户无法区分<code>su 用户名</code>和<code>su - 用户名</code>两个命令的区别。甚至不懂差异，经常混用，非常危险，特别是生产运维中使用。</p><p>两个命令的区别我们举个栗子说明一下。假如当前是root用户，<code>su gest</code>执行后，只是切换了用户身份由root切换成gest，但是shell环境仍然继承了root用户的shell。但是<code>su - gest</code>命令不仅切换了用户身份，而且shell环境也切换成gest登录后的shell环境。</p><p>这个差异可以通过命令执行后的用户目录更为形象的感知：<code>su gest</code>执行后，工作目录并没有切换，而<code>su - gest</code>执行后工作目录切换为<code>/home/gest</code>。</p><p>对于生产环境，今年使用<code>su - 用户名</code>的方式，避免用户shell的继承，造成对切换用户后环境变量环境的污染。千万不可大意。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、iptables命令详解，链接：</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分  shell解释器种类&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第二部分 sh
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>如何禁止云服务器端口被嗅探</title>
    <link href="https://zjrongxiang.github.io/2020/04/11/2020-04-14-%E5%A6%82%E4%BD%95%E7%A6%81%E6%AD%A2%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E5%8F%A3%E8%A2%AB%E5%97%85%E6%8E%A2/"/>
    <id>https://zjrongxiang.github.io/2020/04/11/2020-04-14-如何禁止云服务器端口被嗅探/</id>
    <published>2020-04-11T04:42:00.000Z</published>
    <updated>2020-05-07T16:21:44.063Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分   实施</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>为了保证云服务器的安全，特别是一些匿名服务器嗅探你的服务端口，然后对你的端口进行封禁。为了保证云机器上服务安全，可以使用<code>linux</code>自带的<code>IPTABLES</code>工具进行防护，相当于操作系统级别的防火墙。</p><h2 id="第一部分-实施"><a href="#第一部分-实施" class="headerlink" title="第一部分 实施"></a>第一部分 实施</h2><p>使用下面的命令只允许<code>222.69.213.94</code>ip对<code>40000</code>端口进行访问，其他ip对该端口的访问全部拒绝。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -A INPUT -s 218.92.0.202 -p tcp --dport 11111 -j ACCEPT</span><br></pre></td></tr></table></figure><p>避免外界对该端口进行嗅探，甄别该端口功能后，进行封禁端口。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、iptables命令详解，链接：<a href="https://wangchujiang.com/linux-command/c/iptables.html" target="_blank" rel="noopener">https://wangchujiang.com/linux-command/c/iptables.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分   实施&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;参考文献及资料&lt;/p&gt;
&lt;/
      
    
    </summary>
    
      <category term="路由器" scheme="https://zjrongxiang.github.io/categories/%E8%B7%AF%E7%94%B1%E5%99%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>小米路由器MT工具箱配置</title>
    <link href="https://zjrongxiang.github.io/2020/03/22/2020-03-22-%E5%B0%8F%E7%B1%B3%E8%B7%AF%E7%94%B1%E5%99%A8MT%E5%B7%A5%E5%85%B7%E7%AE%B1%E9%85%8D%E7%BD%AE/"/>
    <id>https://zjrongxiang.github.io/2020/03/22/2020-03-22-小米路由器MT工具箱配置/</id>
    <published>2020-03-22T04:42:00.000Z</published>
    <updated>2020-03-22T04:58:37.568Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分   装备工作</p></li><li><p>第二部分   配置MT工具箱</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>家庭使用的小米路由器，需要折腾一下，开始全局小飞机。</p><h2 id="第一部分-准备工作"><a href="#第一部分-准备工作" class="headerlink" title="第一部分 准备工作"></a>第一部分 准备工作</h2><p>首先需要小米路由器准备好下面的配置前提：</p><ul><li>安装了开发版的<code>rom</code>，获得root权限；</li><li>路由器开启了ssh；</li><li>路由器默认的<code>IP</code>地址为“192.168.31.1”；</li></ul><p>具体可以参考文献中文章《小米路由器安装MT工具箱》。</p><h2 id="第二部分-配置MT工具箱"><a href="#第二部分-配置MT工具箱" class="headerlink" title="第二部分 配置MT工具箱"></a>第二部分 配置MT工具箱</h2><p>使用SSH登录小米路由器，执行下面的安装命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -s -k https://beta.misstar.com/download/$(uname -m)/mtinstall -o /tmp/mtinstall &amp;&amp; chmod +x /tmp/mtinstall &amp;&amp; /tmp/mtinstall</span><br></pre></td></tr></table></figure><p>选择网络安装（选项2），按照提示配置用户和密钥。提示安装<code>Misstar tools 3.0beta</code>版成功：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.31.1:1314/</span><br></pre></td></tr></table></figure><p>但是提示页面<code>url</code>并不能打开。事实上监听的端口是1024。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@XiaoQiang:~# netstat -ant|grep 1024</span><br><span class="line">tcp        0      0 :::1024                 :::*                    LISTEN       0 0</span><br></pre></td></tr></table></figure><p>使用下面的<code>url</code>进入MT工具箱：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.31.1:1024</span><br></pre></td></tr></table></figure><p>然后部署你的小飞机吧。新建节点等不再赘述。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、小米路由器安装MT工具箱，链接：<a href="https://whrr.blog/2019/01/06/install-mt-tools-on-a-xiaomi-router/" target="_blank" rel="noopener">https://whrr.blog/2019/01/06/install-mt-tools-on-a-xiaomi-router/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分   装备工作&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第二部分   配置MT工
      
    
    </summary>
    
      <category term="路由器" scheme="https://zjrongxiang.github.io/categories/%E8%B7%AF%E7%94%B1%E5%99%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>Elasticsearch集群升级指引</title>
    <link href="https://zjrongxiang.github.io/2020/03/07/2020-03-07-Elasticsearch%E9%9B%86%E7%BE%A4%E5%8D%87%E7%BA%A7%E6%8C%87%E5%BC%95/"/>
    <id>https://zjrongxiang.github.io/2020/03/07/2020-03-07-Elasticsearch集群升级指引/</id>
    <published>2020-03-07T14:30:00.000Z</published>
    <updated>2020-03-22T03:42:15.763Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   版本升级指引</li><li>第二部分   升级方法和具体步骤</li><li>总结</li><li>参考文献及资料 </li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Elasticsearch集群的版本升级是一项重要的集群维护工作。本篇文章参考官方文档，将详细介绍相关细节。</p><h2 id="第一部分-版本升级指引"><a href="#第一部分-版本升级指引" class="headerlink" title="第一部分 版本升级指引"></a>第一部分 版本升级指引</h2><h3 id="1-1-同步升级Elastic-Stack组件"><a href="#1-1-同步升级Elastic-Stack组件" class="headerlink" title="1.1 同步升级Elastic Stack组件"></a>1.1 同步升级Elastic Stack组件</h3><p>对于Elasticsearch的生态圈组件需要同步升级，具体配套版本可以参考官方提供的升级指南。</p><blockquote><p><a href="https://www.elastic.co/cn/products/upgrade_guide" target="_blank" rel="noopener">https://www.elastic.co/cn/products/upgrade_guide</a></p></blockquote><h3 id="1-2-索引兼容性"><a href="#1-2-索引兼容性" class="headerlink" title="1.2 索引兼容性"></a>1.2 索引兼容性</h3><p>Elasticsearch对于老版本的索引（index）兼容性如下：</p><ul><li>Elasticsearch 6.x兼容Elasticsearch 5.x中创建的索引，但不兼容Elasticsearch 2.x或更旧版本的索引。</li><li>Elasticsearch 5.x兼容Elasticsearch 2.x中创建的索引，但不不兼容Elasticsearch1.x或更旧版本的索引。</li></ul><p>如果升级过程中遇到索引不兼容场景，升级后集群将无法正常启动。</p><h3 id="1-3-版本升级路线"><a href="#1-3-版本升级路线" class="headerlink" title="1.3 版本升级路线"></a>1.3 版本升级路线</h3><p>Elasticsearch版本升级具体路线总结如下：</p><table><thead><tr><th style="text-align:center">序号</th><th style="text-align:center">原版本</th><th style="text-align:center">升级目标版本</th><th style="text-align:center">支持的升级类型</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center"><code>5.x</code></td><td style="text-align:center"><code>5.y</code></td><td style="text-align:center">滚动升级（其中 <code>y &gt; x</code>）</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center"><code>5.6</code></td><td style="text-align:center"><code>6.x</code></td><td style="text-align:center">滚动升级</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center"><code>5.0-5.5</code></td><td style="text-align:center"><code>6.x</code></td><td style="text-align:center">集群重启</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center"><code>&lt;5.x</code></td><td style="text-align:center"><code>6.x</code></td><td style="text-align:center"><code>reindex</code>升级</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center"><code>6.x</code></td><td style="text-align:center"><code>6.y</code></td><td style="text-align:center">滚动升级（其中 y &gt; x）</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center"><code>1.x</code></td><td style="text-align:center"><code>5.x</code></td><td style="text-align:center"><code>reindex</code>升级</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center"><code>2.x</code></td><td style="text-align:center"><code>2.y</code></td><td style="text-align:center">滚动升级（其中 <code>y &gt; x</code>）</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center"><code>2.x</code></td><td style="text-align:center"><code>5.x</code></td><td style="text-align:center">集群重启</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center"><code>5.0.0 pre GA</code></td><td style="text-align:center"><code>5.x</code></td><td style="text-align:center">集群重启</td></tr><tr><td style="text-align:center">10</td><td style="text-align:center"><code>5.x</code></td><td style="text-align:center"><code>5.y</code></td><td style="text-align:center">滚动升级（其中 <code>y &gt; x</code>）</td></tr></tbody></table><blockquote><p>关于Elasticsearch的版本序列需要特别说明一下。Elasticsearch版本序列不是连续递增的，从<code>2.4.x</code>版本后直接跳跃到<code>5.0.x</code>。所以对于<code>5.x</code>版本，如果按照严格顺序递增编号，应该是<code>3.x</code>。之所以没有连续编号，主要是为了保持ELK（<code>Elasticsearch 、 Logstash 、 Kibana</code>）整体版本的统一。</p></blockquote><p>其中第4种情况，小于<code>5.x</code>其实就是<code>2.x</code>和<code>1.x</code>。由于<code>6.x</code>对于更低版本的索引不兼容，所以需要对原集群的中索引实施<code>reindex</code>。方案分别为：</p><h4 id="1-3-1-2-x升级到6-x"><a href="#1-3-1-2-x升级到6-x" class="headerlink" title="1.3.1 2.x升级到6.x"></a>1.3.1 2.x升级到6.x</h4><p>按照上面的升级路线有两种升级方案：</p><ul><li>方案1：先由2.x升级到5.6版本（reindex升级索引版本），然后由5.6升级到6.x（滚动升级）；</li><li>方案2：创建全新的6.x集群，然后将旧集群中的索引数据远程reindex到新集群中；</li></ul><h4 id="1-3-2-1-x升级到6-x"><a href="#1-3-2-1-x升级到6-x" class="headerlink" title="1.3.2 1.x升级到6.x"></a>1.3.2 1.x升级到6.x</h4><p>同样有两个方案：</p><ul><li>方案1：先由1.x升级到2.4.x版本（reindex升级索引版本），最后按照上面2.x升级到6.x的方案实施；</li><li>方案2：创建全新的6.x集群，然后将旧集群中的索引reindex到新集群中；</li></ul><h2 id="第二部分-升级方法和具体步骤"><a href="#第二部分-升级方法和具体步骤" class="headerlink" title="第二部分 升级方法和具体步骤"></a>第二部分 升级方法和具体步骤</h2><p>集群升级路线中，针对不同的版本之间升级，一共有三种升级方案：滚动升级、集群重启、reindex。下面将分别介绍。</p><h3 id="2-1-滚动升级"><a href="#2-1-滚动升级" class="headerlink" title="2.1 滚动升级"></a>2.1 滚动升级</h3><p>所谓滚动升级指的是集群中节点逐个将版本升级至目标（高）版本，升级期间集群保持对外服务不中断。这种升级方案都是针对同一个大版本内的升级，即x.y升级到x.z（z&gt;y）。特别的，5.6升级到6.x也是支持使用滚动升级方式的。</p><blockquote><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/rolling-upgrades.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/rolling-upgrades.html</a></p></blockquote><p>通常滚动升级的步骤如下：</p><h4 id="第1步-禁用副本分片（shards）分配"><a href="#第1步-禁用副本分片（shards）分配" class="headerlink" title="第1步 禁用副本分片（shards）分配"></a>第1步 禁用副本分片（shards）分配</h4><p>在下宕升级节点前，需要提前禁止副本分片的分配。</p><blockquote><p>节点下宕后，副本分配进程会等待<code>index.unassigned.node_left.delayed_timeout</code>（默认情况下为1分钟），然后再开始将该节点上的分片复制到群集中的其他节点，这会导致大量I/O。由于节点很快将重新启动，所以并不需要重新分配。</p></blockquote><p>API命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PUT _cluster/settings</span><br><span class="line">&#123;</span><br><span class="line">  "persistent": &#123;</span><br><span class="line">    "cluster.routing.allocation.enable": "primaries"</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="第2步-执行同步刷新"><a href="#第2步-执行同步刷新" class="headerlink" title="第2步 执行同步刷新"></a>第2步 执行同步刷新</h4><blockquote><p>重启集群时如果translog过大，日志回放恢复数据耗时较长，建议手动同步刷新，减少translog。</p><p>注意：这个过程较为缓慢。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">POST _flush/synced</span><br></pre></td></tr></table></figure><h4 id="第3步-停止机器学习作业"><a href="#第3步-停止机器学习作业" class="headerlink" title="第3步 停止机器学习作业"></a>第3步 停止机器学习作业</h4><p>如果集群中运行了机器学习任务，需要停止任务运行。</p><blockquote><p>参考：<a href="https://www.elastic.co/guide/en/elastic-stack-overview/6.8/stopping-ml.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elastic-stack-overview/6.8/stopping-ml.html</a></p></blockquote><h4 id="第4部-下宕待升级节点并安装主版本和插件"><a href="#第4部-下宕待升级节点并安装主版本和插件" class="headerlink" title="第4部 下宕待升级节点并安装主版本和插件"></a>第4部 下宕待升级节点并安装主版本和插件</h4><p>对升级节点实施下宕，开始文件系统的升级。</p><h4 id="第5步-启动节点"><a href="#第5步-启动节点" class="headerlink" title="第5步 启动节点"></a>第5步 启动节点</h4><p>启动节点，并用下面的API检查节点是否加入集群。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">GET</span> _cat/nodes</span><br></pre></td></tr></table></figure><h4 id="第6步-重启分片分配"><a href="#第6步-重启分片分配" class="headerlink" title="第6步 重启分片分配"></a>第6步 重启分片分配</h4><p>节点加入集群后，设置启用分片分配开始使用该节点。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PUT _cluster/settings</span><br><span class="line">&#123;</span><br><span class="line">  "persistent": &#123;</span><br><span class="line">    "cluster.routing.allocation.enable": null</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在升级下一个节点前，等待集群分片完成。可以通过下面的API检查集群状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET _cat/health?v</span><br></pre></td></tr></table></figure><p>等待集群的状态由red变成yellow，再到green。说明集群完成所有主分片和副分片的分配。</p><h4 id="第7步-重复升级其他节点"><a href="#第7步-重复升级其他节点" class="headerlink" title="第7步 重复升级其他节点"></a>第7步 重复升级其他节点</h4><p>重复滚动升级集群其他节点。</p><h4 id="第8步-重启机器学习任务"><a href="#第8步-重启机器学习任务" class="headerlink" title="第8步 重启机器学习任务"></a>第8步 重启机器学习任务</h4><p>如果集群中有机器学习任务，需要从新启动。</p><h3 id="2-2-集群整体重启"><a href="#2-2-集群整体重启" class="headerlink" title="2.2 集群整体重启"></a>2.2 集群整体重启</h3><p>集群整体重启指的是升级前将集群所有节点均下宕，集群停止对外服务，待所有节点完成升级后，整体启动集群，恢复对外服务。例如：5.6之前的版本升级到6.x需要重启集群实施升级。</p><blockquote><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/restart-upgrade.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/restart-upgrade.html</a></p></blockquote><p>集群重启升级步骤和滚动方式相似，主要步骤如下：</p><h4 id="第1步-禁用副本分片（shards）分配-1"><a href="#第1步-禁用副本分片（shards）分配-1" class="headerlink" title="第1步 禁用副本分片（shards）分配"></a>第1步 禁用副本分片（shards）分配</h4><p>下宕升级节点前需要，提前禁止副本分片的分配。（参考滚动升级）</p><h4 id="第2步-停止不必要的索引并执行同步刷新"><a href="#第2步-停止不必要的索引并执行同步刷新" class="headerlink" title="第2步 停止不必要的索引并执行同步刷新"></a>第2步 停止不必要的索引并执行同步刷新</h4><p>参考滚动升级。</p><h4 id="第3步-停止机器学习作业-1"><a href="#第3步-停止机器学习作业-1" class="headerlink" title="第3步 停止机器学习作业"></a>第3步 停止机器学习作业</h4><p>参考滚动升级</p><h4 id="第4部-下宕所有节点并安装主版本和插件"><a href="#第4部-下宕所有节点并安装主版本和插件" class="headerlink" title="第4部 下宕所有节点并安装主版本和插件"></a>第4部 下宕所有节点并安装主版本和插件</h4><p>对集群所有节点实施下宕，开始文件系统版本升级。</p><h4 id="第5步-启动节点并等待集群状态为yellow"><a href="#第5步-启动节点并等待集群状态为yellow" class="headerlink" title="第5步 启动节点并等待集群状态为yellow"></a>第5步 启动节点并等待集群状态为yellow</h4><p>启动所有节点，并用下面的API检查所有节点是否加入集群。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">GET</span> _cat/nodes</span><br></pre></td></tr></table></figure><h4 id="第6步-重启分片分配-1"><a href="#第6步-重启分片分配-1" class="headerlink" title="第6步 重启分片分配"></a>第6步 重启分片分配</h4><p>节点加入集群后，设置启用分片分配开始使用该节点。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PUT _cluster/settings</span><br><span class="line">&#123;</span><br><span class="line">  "persistent": &#123;</span><br><span class="line">    "cluster.routing.allocation.enable": null</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在升级下一个节点前，等待集群分片完成。可以通过下面的API检查集群状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET _cat/health?v</span><br></pre></td></tr></table></figure><p>等待集群的状态由yellow变为green。说明集群完成所有主分片和副分片的分配。</p><h4 id="第7步-重启机器学习任务"><a href="#第7步-重启机器学习任务" class="headerlink" title="第7步 重启机器学习任务"></a>第7步 重启机器学习任务</h4><p>参考滚动升级</p><h3 id="2-3-reindex"><a href="#2-3-reindex" class="headerlink" title="2.3 reindex"></a>2.3 reindex</h3><p>Elasticsearch中相邻版本的index具有兼容性，但是跨度较大的版本不再向下兼容。在上文（1.2 索引兼容性）中已做介绍。而在ElasticSearch中，索引的field设置是不能被修改的，如果要修改一个field，那么应该重新按照新的mapping，建立一个index，然后将数据批量查询出来，重新用bulk api写入新index中。</p><blockquote><p>批量查询的时候，建议采用scroll api，并且采用多线程并发的方式来reindex数据，每次scroll就查询指定日期的一段数据，交给一个线程即可。</p></blockquote><h4 id="第1步-搭建新版本集群"><a href="#第1步-搭建新版本集群" class="headerlink" title="第1步 搭建新版本集群"></a>第1步 搭建新版本集群</h4><p>申请服务器资源，搭建全新版本的ElasticSearch集群。将对外服务全部指向新集群。</p><h4 id="第2步-将老集群中数据reindex到新集群"><a href="#第2步-将老集群中数据reindex到新集群" class="headerlink" title="第2步 将老集群中数据reindex到新集群"></a>第2步 将老集群中数据reindex到新集群</h4><p>在老集群上使用reindex API将老集群中index历史数据逐步迁移至新集群。</p><blockquote><p>如果集群数据量较大，迁移过程是一个很缓慢的过程。</p></blockquote><p>API案例（下面是简单的配置）：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"source"</span>: &#123;</span><br><span class="line">    <span class="attr">"remote"</span>: &#123;</span><br><span class="line">      <span class="attr">"host"</span>: <span class="string">"http://otherhost:9200"</span>,</span><br><span class="line">      <span class="attr">"username"</span>: <span class="string">"user"</span>,</span><br><span class="line">      <span class="attr">"password"</span>: <span class="string">"pass"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"index"</span>: <span class="string">"source"</span>,</span><br><span class="line">    <span class="attr">"query"</span>: &#123;</span><br><span class="line">      <span class="attr">"match"</span>: &#123;</span><br><span class="line">        <span class="attr">"test"</span>: <span class="string">"data"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"dest"</span>: &#123;</span><br><span class="line">    <span class="attr">"index"</span>: <span class="string">"dest"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//host为远程集群（新集群）的地址。</span><br><span class="line">//username和password针对安全集群的密钥验证。</span><br><span class="line">//"index": "source"为旧集群中index名，dest的所对应的是新集群目标index名。</span><br></pre></td></tr></table></figure><p>迁移完成后，可以对旧集群中数据实施清理。清理完成后根据情况需要，旧节点可以离线升级文件系统，最后作为全新的节点加入新集群。</p><blockquote><p>如果旧集群中历史数据不重要，可以删除数据后，搭建全新的集群。</p></blockquote><h3 id="2-4-分步升级"><a href="#2-4-分步升级" class="headerlink" title="2.4 分步升级"></a>2.4 分步升级</h3><p>对于跨度较大的版本升级，如果不采用新建集群再实施reindex方式，那么就需要分步升级。例如A、B、C依次为三个版本，版本级别A&lt;B&lt;C，其中index数据B兼容A，C兼容B，但是C不兼容A。这种情况需要分步升级：</p><ul><li>A升级到B，使用滚动升级或者集群整体重启方式。</li><li>对于B版本的集群，将A版本的所有数据reindex到B版本。这个过程较为耗时。</li><li>等到集群中所有历史index（新建的index自然是B版本）均为B版本后，升级集群版本到C版本。</li></ul><blockquote><p>如果index数据是时间序列类的数据，可以不实施reindex，等到历史数据生命周期结束后（集群中不在有A版本的index数据），再从B版本升级到C版本。</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>（1）一般Elasticsearch大版本之间跨度升级需要重启整体集群。</p><p>（2）部分ElasticSearch大版本间index并不兼容，需要对数据重索引（reindex）。</p><p>（3）大版本中的小版本升级，通常只需要滚动重启方式即可。</p><h2 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h2><p>1、Elasticsearch官网  链接：<a href="https://www.elastic.co/cn/" target="_blank" rel="noopener">https://www.elastic.co/cn/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   版本升级指引&lt;/li&gt;
&lt;li&gt;第二部分   升级方法和具体步骤&lt;/li&gt;
&lt;li&gt;总结&lt;
      
    
    </summary>
    
      <category term="Elasticsearch" scheme="https://zjrongxiang.github.io/categories/Elasticsearch/"/>
    
    
  </entry>
  
  <entry>
    <title>Kafka系列文章（第五篇 Kafka安全集群）</title>
    <link href="https://zjrongxiang.github.io/2020/03/02/2020-01-01-Kafka%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88%E7%AC%AC%E4%BA%94%E7%AF%87Kafka%E5%AE%89%E5%85%A8%EF%BC%89/"/>
    <id>https://zjrongxiang.github.io/2020/03/02/2020-01-01-Kafka系列文章（第五篇Kafka安全）/</id>
    <published>2020-03-02T05:30:00.000Z</published>
    <updated>2020-05-01T15:01:31.983Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   Kafka集群加密传输</li><li>第二部分   Kafka集群权限认证</li><li>第三部分   加密认证集群的客户端</li><li>第四部分   加密认证集群的性能压测</li><li>第五部分  总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Kafka在0.9.0.0版本前没有安全机制功能。Kafka Client程序可以直接获取到Kafka集群元数据信息和Kafka Broker地址后，连接到Kafka集群，然后完全操作集群上的所有topic数据资源。另外集群节点间通讯、broker和zookeeper通讯、客户端和集群的网络层通信都是无加密模式。集群的数据存在极大的安全风险。</p><p>自0.9.0.0版本开始，Kafka社区逐步添加了较多功能用于提高Kafka群集的安全性。目前Kafka安全集群安全机制主要有三个方面的设置：通信加密（encryption）、身份认证（authentication）和授权（authorization）。</p><p>本文重点介绍生产安全集群的一种配置方案。数据通讯传输配置SSL，认证配置SASL，授权通过ACL接口命令来完成的，即：<strong>SSL+SASL/SCRAM+ACL</strong>。</p><h2 id="第一部分-Kafka集群加密传输"><a href="#第一部分-Kafka集群加密传输" class="headerlink" title="第一部分   Kafka集群加密传输"></a>第一部分   Kafka集群加密传输</h2><h3 id="1-1-背景知识介绍"><a href="#1-1-背景知识介绍" class="headerlink" title="1.1 背景知识介绍"></a>1.1 背景知识介绍</h3><p>涉及的技术知识不做详细介绍。</p><h4 id="1-1-1-密码学基础"><a href="#1-1-1-密码学基础" class="headerlink" title="1.1.1 密码学基础"></a>1.1.1 密码学基础</h4><p>加密算法分为两类：</p><ul><li><p>对称密钥算法（Symmetric Cryptography）：数据加密和解密时使用相同的密钥。例如常用的DES就是对称加密算法。</p></li><li><p>非对称密钥算法（Asymmetric Cryptography）：数据加密和解密时使用不同的密钥，分为：公开的公钥（public key）和用户保存的私钥（private key），私钥和公钥在数学上是相关的。利用公钥（或私钥）加密的数据只能用相应的私钥（或公钥）才能解密。举一个例子：客户在银行网银上做一笔交易，首先向银行申请公钥，银行分发公钥给用户，用户使用公钥对请求数据进行加密。银行收到加密数据后通过银行侧保存的私钥进行解密处理，并处理后更新后台数据库。这个通讯过程中银行不需要通过互联网分发私钥。因此保证了私钥的安全。目前最常用的非对称加密算法是RSA算法。</p><blockquote><p>非对称密钥算法中，私钥来解密公钥加密的数据，公钥来解密私钥加密的数据。</p></blockquote></li></ul><p>两种加密算法的比较：</p><ul><li><p>对称密钥的强度和密钥长度成正比，但是解密效率和密钥长度成反比。另外私钥的分发存在安全风险。</p></li><li><p>非对称加密保证了私钥的安全性，但是加密和解密的效率比对称加密低。</p></li></ul><p>所以通常加密场景是两种密钥结合使用。使用数据主体使用对称秘钥算法，但是私钥的传输使用非对称算法在互联网环境分发非对称密钥。最常见的就是SSL/TLS。</p><h4 id="1-1-2-CA数字证书"><a href="#1-1-2-CA数字证书" class="headerlink" title="1.1.2 CA数字证书"></a>1.1.2 CA数字证书</h4><p>对于非对称密钥算法存在一个安全风险点，那就是公钥的分发存在中间人攻击。还是以客户和银行的通信为例（例子简单化处理）。客户和银行分别有自己的公钥和私钥，私钥各自保留本地。公钥通过互联网分发给对方。那么公钥就是有安全风险的。存在被黑客截取风险。客户向银行申请银行公钥，结果被黑客截取，黑客伪装成银行，返回给用户自己的黑客公钥，用户收到黑客公钥后，将信息加密发给黑客。黑客用黑客私钥进行解密，获取到真实信息。这时候黑客伪装成客户用相同的方法完成和银行的数据交互。这就是中间人攻击的案例。</p><p>所以非对称加密算法的公钥传输同样存在风险。当然如果使用原始的离线方式交换密钥是安全的，但是随着互联网通信的爆炸式增长，这是落后低效的。为了保证公钥的真实性和安全性，这时候我们引入第三个角色：公开密钥认证（Public key certificate，简称CA），又称数字证书（digital certificate）或身份证书（identity certificate）。</p><p>通常CA是一家第三方权威机构。负责管理和签发证书。整个实现原理也是非对称加密算法：</p><ul><li>机构将自己的公钥以及身份信息交给CA机构（安全的），CA使用自己的私钥对各机构的公钥进行加密。这个过程称为验签。输出的加密后的公钥及身份信息称为数字证书。</li><li>当其他机构请求A机构公钥的时候，返回的是A机构的数字证书。其他机构可以使用CA的公钥对该数字证书中加密公钥进行解密获取A机构的通信公钥。</li></ul><p>那么新得安全问题又来了，如何保证CA机构的公钥不被伪造？通常CA的公钥是集成在浏览器或者操作系统中，并且被很好的保护起来。</p><blockquote><p>当然CA证书还涉及更多的安全细节设计（Hash算法防篡改、信任链等大量细节），这里只是简单的介绍。详细介绍可以查看：维基（<a href="https://zh.wikipedia.org/zh-hans/%E8%AF%81%E4%B9%A6%E9%A2%81%E5%8F%91%E6%9C%BA%E6%9E%84" target="_blank" rel="noopener">证书颁发机构</a>）</p></blockquote><p>对于企业内部的应用系统就没必要花钱购买CA机构的证书服务了，可以自建 Root CA，自己给自己颁发证书，充当内网的CA机构。当然这时候客户端就需要导入CA的证书了（浏览器和操作系统没有自建的CA证书）。</p><h4 id="1-1-3-SSL-TLS加密协议"><a href="#1-1-3-SSL-TLS加密协议" class="headerlink" title="1.1.3 SSL/TLS加密协议"></a>1.1.3 SSL/TLS加密协议</h4><p>SSL（<strong>S</strong>ecure <strong>S</strong>ockets <strong>L</strong>ayer）是一种安全协议，目的是为保障互联网上数据传输安全，利用数据加密技术，确保数据在网络上之传输过程中不会被截取。</p><p>从网络协议层看，SSL协议位于TCP/IP协议与应用层协议之间，为数据通讯提供安全支持。SSL协议自身可分为两层： </p><ul><li>SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。 </li><li>SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。例如HTTPS就是在HTTP应用层上增加了SSL加密协议支持（HTTP over SSL）。</li></ul><p>TLS(Transport Layer Security，传输层安全协议)，同样用于两个应用程序之间提供保密性和数据完整性。 TLS 1.0建立在SSL 3.0协议规范之上，是SSL 3.0的后续版本，可以理解为SSL 3.1，即是SSL的升级版。TLS的主要目标是使SSL更安全，并使协议的规范更精确和完善。另外,TLS版本号也与SSL的不同(TLS的版本1.0使用的版本号为SSLv3.1)</p><p>SSL通过握手过程在client和server之间协商会话參数，并建立会话。一共有三种方式：</p><ul><li>仅仅验证server的SSL握手过程（单向SSL）</li><li>验证server和client的SSL握手过程（双向SSL）</li><li>恢复原有会话的SSL握手过程</li></ul><p><strong>第一种：</strong>单向SSL通信过程如下（SSL 客户端和SSL 服务端通信）：</p><p>(1)SSL客户端向SSL服务端发起请求，请求信息包括SSL版本号、加密算法、密钥交换算法、MAC算法等信息；</p><p>(2)SSL服务端确定本次通话的SSL版本和加密套件后，将携带公钥信息的证书回给客户端。如果通话可从重用，还会返回会话ID；</p><p>(3)SSL服务端发送Server Hello Done消息。通知SSL客户端版本号和加密套件协商结束，开始进行密钥交换；</p><p>(4)SSL客户端对CA证书进行验证，证书合法则继续、不成功弹出选择页面；</p><p>(5)SSL客户端生产随机私有对称密钥key，并使用服务端公开密钥进行加密后，发给服务端；</p><p>(6)SSL服务端使用自己的私钥解密，获取对称密钥key；</p><p>(7)最后SSL客户端与SSL服务端将使用该对称密钥key进行加密通信。</p><p><strong>第二种：</strong>单向认证，仅仅是客户端需要检验服务端证书是否是正确的。双向SSL和单向认证几乎一样，只是在客户端认证完服务器证书后，客户端会将自己的证书传给服务器。服务器验证通过后，才开始秘钥协商。</p><p><strong>第三种：</strong>协商会话参数、建立会话的过程中，需要使用非对称密钥算法来加密密钥、验证通信对端的身份，计算量较大，占用了大量的系统资源。为了简化SSL握手过程，SSL允许重用已经协商过的会话。即可以重用会话ID。这就是第三种建立会话方式。</p><h4 id="1-1-4-Openssl工具"><a href="#1-1-4-Openssl工具" class="headerlink" title="1.1.4 Openssl工具"></a>1.1.4 Openssl工具</h4><p>对于企业内部（内部局域网）的应用系统通讯，如果需要CA证书服务，可以使用Openssl自建CA，并完成证书签发。</p><p>先说一下常用密钥类文件的规范：</p><ul><li><p>后缀名规范</p><p>通常约定后缀含义：crt或者cert 表示证书, key表示私钥, req和csr表示请求文件。</p></li><li><p>文件格式</p><p>pem表示pem格式（经过加密的文本文件），der表示der格式（经过加密的二进制文件）。所有证书和私钥可以是pem,也可以是der格式，取决于需要。两个格式可以转换。</p></li></ul><p>Openssl的配置文件（<code>openssl.cnf</code>）定义CA的默认参数，例如<code>ubuntu</code>系统中配置文件位置在<code>/usr/lib/ssl/openssl.cnf</code>。如果不适用默认参数需要在命令中重新指定。</p><ul><li><p>CA证书的制作</p><p>首先生成CA的私钥，使用下面的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> openssl genrsa -out private/ca.key.pem 2048</span></span><br></pre></td></tr></table></figure><p><code>private/ca.key.pem</code>是CA私钥,格式为pem，长度（加密位数）为2048。</p><blockquote><p>前面密码学知识知道CA使用一对密钥的（私钥和公钥），并且两个密钥是数学相关的。公钥可以通过私钥算出来。</p></blockquote></li><li><p>CA证书自签发</p><p>参考命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> openssl req -new -x509 -key private/ca.key.pem -out certs/ca.cert.pem</span></span><br></pre></td></tr></table></figure><p> <code>certs/ca.cert.pem</code> 即CA的自签证书。部署导入到客户端（例如浏览器）。</p></li><li><p>用户证书签发</p><p>用户证书的签发和CA自签相同，用户证书由CA私钥签发。用户需要提供请求文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> openssl ca -<span class="keyword">in</span> app.csr -out app.crt -days 365</span></span><br></pre></td></tr></table></figure><p><code>app.crt</code>为签发的证书。部署在应用服务器上。</p></li></ul><h4 id="1-1-5-Keytool工具介绍"><a href="#1-1-5-Keytool工具介绍" class="headerlink" title="1.1.5 Keytool工具介绍"></a>1.1.5 Keytool工具介绍</h4><p>在密钥证书管理时，通常使用JAVA的Keytool工具程序。Keytool 是一个JAVA数据证书的管理工具 ,Keytool 将密钥（key）和证书（certificates）存在一个称为keystore的文件中，通常称为密钥库文件。文件的扩展名通常使用：jks，全名java key store file。</p><p>Keytool是一个Java数据证书的管理工具，所以节点需要配置JAVA_HOME环境变量。</p><p>这里列举了命令支持的参数含义及注意点（供后续使用查阅）：</p><ul><li>keystore 参数指定保存证书的文件（密钥库二进制文件）。密钥库文件包含证书的私钥，必须对其进行安全保存。</li><li>validity 参数指定密钥有效期，单位是天。默认为90天。</li><li>keyalg 参数指定密钥使用的加密算法（例如RSA，如果不指定默认采用DSA）。</li><li>keysize 参数指定密钥的长度。该参数是选项参数，默认长度是1024位。为了保证密钥安全强度，建议密码长度设置为2048位。</li><li>keypass 参数指定生成密钥的密码（私钥密码）。</li><li>storepass 指定密钥库的密码(获取keystore信息所需的密码)。另外密钥库创建后，要对其做任何修改都必须提供该密码，以便访问密钥库。</li><li>alias 参数指定密钥别名。每个密钥文件有一个唯一的别名，别名不区分大小写。</li><li>dname 参数指定证书拥有者信息。例如： “CN=名字与姓氏,OU=组织单位名称,O=组织名称,L=城市或区域名称,ST=州或省份名称,C=单位的两字母国家代码”。</li><li>list 参数显示密钥库中的证书信息。keytool -list -v -keystore 指定keystore -storepass 密码</li><li>v 参数显示密钥库中的证书详细信息。</li><li>export 将别名指定的证书导出到文件。keytool -export -alias 需要导出的别名 -keystore 指定keystore -file 指定导出的证书位置及证书名称 -storepass 密码。</li><li>file  参数指定导出到文件的文件名。</li><li>delete   删除密钥库中某条目。keytool -delete -alias 指定需删除的别名 -keystore 指定keystore -storepass 密码</li><li>printcert  查看导出的证书信息。keytool -printcert -file yushan.crt</li><li>keypasswd   修改密钥库中指定条目口令。keytool -keypasswd -alias 需修改的别名 -keypass 旧密码 -new 新密码 -storepass keystore密码 -keystore sage</li><li>storepasswd 修改keystore口令。keytool -storepasswd -keystore e:/yushan.keystore(需修改口令的keystore) -storepass 123456(原始密码) -new newpasswd(新密码)</li><li>import   将已签名数字证书导入密钥库。keytool -import -alias 指定导入条目的别名 -keystore 指定keystore -file 需导入的证书</li></ul><blockquote><p>关于Keytool工具的详细介绍，可以参考<a href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/keytool.html" target="_blank" rel="noopener">oracle的官网</a>。</p></blockquote><h3 id="1-2-Kafka集群配置SSL加密"><a href="#1-2-Kafka集群配置SSL加密" class="headerlink" title="1.2 Kafka集群配置SSL加密"></a>1.2 Kafka集群配置SSL加密</h3><p>Apache Kafka允许客户端通过SSL连接。默认情况下，SSL是禁用的，可以根据需要打开。</p><h4 id="1-2-1-集群环境准备"><a href="#1-2-1-集群环境准备" class="headerlink" title="1.2.1 集群环境准备"></a>1.2.1 集群环境准备</h4><p>为了后文讲解方便，我们部署了Kafka集群（3节点）和Zookeeper集群（3节点）测试环境。其中zookeeper和kafka混合部署。</p><table><thead><tr><th>节点编号</th><th>hostname</th><th>IP地址</th></tr></thead><tbody><tr><td>1</td><td>kafka.app.node1</td><td>192.168.1.5</td></tr><tr><td>2</td><td>kafka.app.node2</td><td>192.168.1.6</td></tr><tr><td>3</td><td>kafka.app.node3</td><td>192.168.1.7</td></tr></tbody></table><p>Kafka集群节点对外服务端口为：9092；Zookeeper集群节点对外服务端口为：2181。</p><h4 id="1-2-2-配置主机名验证"><a href="#1-2-2-配置主机名验证" class="headerlink" title="1.2.2 配置主机名验证"></a>1.2.2 配置主机名验证</h4><p>从Kafka 2.0.0版开始，默认会为客户端连接以及broker之间的连接启用服务器的主机名验证（SSL端点识别算法），以防止中间人攻击。可以通过设置参数<code>ssl.endpoint.identification.algorithm</code>为空字符串来禁用服务器主机名验证。例如:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssl.endpoint.identification.algorithm=</span><br></pre></td></tr></table></figure><p>另外高版本支持不停集群服务下，进行动态配置，使用脚本<code>kafka-configs.sh</code>，参考命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-configs.sh --bootstrap-server localhost:9093 --entity-type brokers --entity-name 0 --alter --add-config "listener.name.internal.ssl.endpoint.identification.algorithm="</span><br></pre></td></tr></table></figure><p>对于较旧的Kafka版本，<code>ssl.endpoint.identification.algorithm</code>默认情况下未定义，因此不会启用主机名验证。若该属性设置<code>HTTPS</code>，则启用主机名验证，例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssl.endpoint.identification.algorithm=HTTPS</span><br></pre></td></tr></table></figure><p>需要注意的是，一旦启用主机名验证，客户端将根据以下两个字段之一验证服务器的完全限定域名（FQDN）：</p><ul><li><p>通用名称（CN，Common Name）</p></li><li><p>主题备用名称（SAN，Subject Alternative Name）</p></li></ul><p>两个字段都有效，但RFC-2818建议使用SAN。 SAN也更灵活，允许声明多个DNS条目。 另一个优点是，CN可以设置为更有意义的值用于授权。如要添加SAN字段，需要将以下参数<code>-ext SAN = DNS：{FQDN}</code>添加到<code>keytool</code>命令中，例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -keystore server.keystore.jks -<span class="built_in">alias</span> localhost -validity &#123;validity&#125; -genkey -keyalg RSA -ext SAN=DNS:&#123;FQDN&#125;</span></span><br></pre></td></tr></table></figure><p>更通俗一点讲，SSL 握手期间验证主机名时，它会检查服务器证书是否具有 SAN 集。如果检测到 SAN 集，那么只使用 SAN 集中的名称或 IP 地址。如果未检测到 SAN 集，那么只使用主题专有名称 (DN) 最重要的属性，通常是通用名称(CN)。将该值与客户端尝试连接的服务器启的主机名进行比较。如果它们相同，主机名验证成功，允许建立连接。</p><h4 id="1-2-3-生成SSL密钥和证书"><a href="#1-2-3-生成SSL密钥和证书" class="headerlink" title="1.2.3 生成SSL密钥和证书"></a>1.2.3 生成SSL密钥和证书</h4><p>为了方便管理证书密钥，我们使用统一的路径保存。例如统一放在<code>/usr/ca</code>作为文件目录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p /usr/ca/&#123;root,server,client,trust&#125;</span></span><br></pre></td></tr></table></figure><blockquote><p>这里各文件夹的功能是：root：存储CA私钥和证书；server：存储服务端的私钥和证书；client：存储客户端私钥和证书；trust：存储信任库文件；</p></blockquote><ul><li><strong>节点1（kafka.app.node1）</strong></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -keystore /usr/ca/server/server.keystore.jks -<span class="built_in">alias</span> kafka.app -validity 3650 -genkey -keypass app123 -keyalg RSA -dname <span class="string">"CN=kafka.app.node1,OU=depart,O=org,L=shanghai,S=shanghai,C=cn"</span> -storepass app123 -ext SAN=DNS:kafka.app.node1</span></span><br></pre></td></tr></table></figure><p>  其中<code>dname</code>参数的含义参考<code>Keytool</code>工具介绍，文件名为：<code>server.keystore.jks</code>，这是密钥库。</p><ul><li><strong>节点2（kafka.app.node2）</strong></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -keystore /usr/ca/server/server.keystore.jks -<span class="built_in">alias</span> kafka.app -validity 3650 -genkey -keypass app123 -keyalg RSA -dname <span class="string">"CN=kafka.app.node1,OU=depart,O=org,L=shanghai,S=shanghai,C=cn"</span> -storepass app123 -ext SAN=DNS:kafka.app.node2</span></span><br></pre></td></tr></table></figure><ul><li><strong>节点3（kafka.app.node3）</strong></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -keystore /usr/ca/server/server.keystore.jks -<span class="built_in">alias</span> kafka.app -validity 3650 -genkey -keypass app123 -keyalg RSA -dname <span class="string">"CN=kafka.app.node1,OU=depart,O=org,L=shanghai,S=shanghai,C=cn"</span> -storepass app123 -ext SAN=DNS:kafka.app.node3</span></span><br></pre></td></tr></table></figure><p>证书生成后可以通过下面的命令进行查询（需要输入密钥库管理密码，即<code>keypass</code>的参数）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -list -v -keystore server.keystore.jks</span></span><br></pre></td></tr></table></figure><h4 id="1-2-4-创建Kafka集群CA证书"><a href="#1-2-4-创建Kafka集群CA证书" class="headerlink" title="1.2.4 创建Kafka集群CA证书"></a>1.2.4 创建Kafka集群CA证书</h4><p>集群中每个服务节点都有一对公钥和私钥，以及用于标识该节点的证书。但这个证书是未签名的，存在中间者攻击的风险。所以需要证书颁发机构（CA）负责签署颁发证书，使用<code>openssl</code>工具实现。</p><p>同一个集群的所有节点共用一个CA证书，所以只需要在集群的一个节点（集群外节点均可）生成CA证书，然后分发给集群其他节点。例如在<code>kafka.app.node1</code>节点上创建CA证书，命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> openssl req -new -x509 -keyout /usr/ca/root/ca.key.pem -out /usr/ca/root/ca.cert.pem -days 365 -passout pass:app123 -subj <span class="string">"/C=cn/ST=shanghai/L=shanghai/O=org/OU=depart/CN=kafka.app.node1"</span></span></span><br></pre></td></tr></table></figure><p>然后使用<code>scp</code>命令分发给其他节点：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> scp /usr/ca/root/* root@kafka.app.node2:/usr/ca/root/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> scp /usr/ca/root/* root@kafka.app.node3:/usr/ca/root/</span></span><br></pre></td></tr></table></figure><p>生成两个文件，分别是私钥（ca.key.pem）和证书（ca.cert.pem），它用来签署其他证书。</p><h4 id="1-2-5-集群服务节点签署证书"><a href="#1-2-5-集群服务节点签署证书" class="headerlink" title="1.2.5 集群服务节点签署证书"></a>1.2.5 集群服务节点签署证书</h4><p>首先给集群各服务节点签发证书（即签名）。步骤如下：</p><ul><li>第一步 从密钥容器中提取和导出服务端证书（输出文件：server.cert-file，未签名）</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -keystore /usr/ca/server/server.keystore.jks -<span class="built_in">alias</span> kafka.itdw -certreq -file /usr/ca/server/server.cert-file -storepass app123</span></span><br></pre></td></tr></table></figure><ul><li>第二步 给服务端证书签名（输出文件：server.cert-signed，已签名）</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> openssl x509 -req -CA /usr/ca/root/ca.cert.pem -CAkey /usr/ca/root/ca.key.pem -<span class="keyword">in</span> /usr/ca/server/server.cert-file -out /usr/ca/server/server.cert-signed -days 365 -CAcreateserial -passin pass:app123</span></span><br></pre></td></tr></table></figure><ul><li>第三步 将CA证书导入服务端密钥容器中</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -keystore /usr/ca/server/server.keystore.jks -<span class="built_in">alias</span> CARoot -import -file /usr/ca/root/ca.cert.pem -storepass app123</span></span><br></pre></td></tr></table></figure><ul><li>第四步 将已签名的证书导入密钥容器中</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -keystore /usr/ca/server/server.keystore.jks -<span class="built_in">alias</span> kafka.app -import -file /usr/ca/server/server.cert-signed -storepass app123</span></span><br></pre></td></tr></table></figure><p>需要注意集群上每个服务节点均需要签署。</p><h4 id="1-2-6-生成服务端信任库"><a href="#1-2-6-生成服务端信任库" class="headerlink" title="1.2.6 生成服务端信任库"></a>1.2.6 生成服务端信任库</h4><p>如果kafka集群中配置中的参数<code>ssl.client.auth</code>设置为： <code>requested</code>或<code>required</code>，需要为集群节点提供一个信任库，这个库中需要包含所有CA证书。</p><p>使用下面的命令将CA证书导入服务端信任库，输出为信任库文件：<code>server.truststore.jks</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -keystore /usr/ca/trust/server.truststore.jks -<span class="built_in">alias</span> CARoot -import -file /usr/ca/root/ca.cert.pem -storepass app123</span></span><br></pre></td></tr></table></figure><p>将CA证书导入服务端信任库，意味着信任该CA证书签名的所有证书。此属性称为信任链，在大型Kafka群集上部署SSL时特别有用。您可以使用单个CA对群集中的所有证书进行签名，并使所有计算机共享信任该CA的同一信任库。这样，所有计算机都可以对所有其他计算机进行身份验证。</p><h4 id="1-2-7-配置Kafka-Brokers"><a href="#1-2-7-配置Kafka-Brokers" class="headerlink" title="1.2.7 配置Kafka Brokers"></a>1.2.7 配置Kafka Brokers</h4><p>Kafka Broker节点支持侦听多个端口上的连接。在server.properties中配置，多个端口类型使用逗号分隔，我们以集群中<code>kafka.app.node1</code>为例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">listeners=SSL://kafka.app.node1:9092</span><br></pre></td></tr></table></figure><p>代理端需要以下SSL配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ssl.keystore.location=/usr/ca/server/server.keystore.jks</span><br><span class="line">ssl.keystore.password=app123</span><br><span class="line">ssl.key.password=app123</span><br><span class="line">ssl.truststore.location=/usr/ca/trust/server.truststore.jks</span><br><span class="line">ssl.truststore.password=app123</span><br></pre></td></tr></table></figure><p>其他可选配置设置：</p><ul><li><p><code>ssl.client.auth</code>（可选）</p><p>参数控制SSL认证模式。默认参数值为<code>requested</code>，默认使用单向认证，即客户端认证Kafka brokers。此时，没有证书的客户端仍然可以连接集群。参数值为<code>required</code>，指定开启双向验证(2-way authentication)。Kafka服务器同时会验证客户端证书。生成集群建议开始双向认证。</p></li><li><p><code>ssl.cipher.suites</code>（可选）</p><p>密码套件是认证，加密，MAC和密钥交换算法的命名组合，用于协商使用TLS或SSL网络协议的网络连接的安全设置。（默认为空列表）</p></li><li><p><code>ssl.enabled.protocols</code></p><p>建议参数值为<code>TLSv1.2,TLSv1.1,TLSv1</code>。列出支持的SSL协议。生成环境不建议使用SSL，建议使用TLS。</p></li><li><p><code>ssl.keystore.type</code>和<code></code>ssl.truststore.type`</p><p>文件格式：<code>JKS</code></p></li><li><p><code>security.inter.broker.protocol</code>参数</p><p>kafka集群节点（brokers）之间启用<code>SSL</code>通讯，需要配置该配置参数为：<code>SSL</code>。</p></li></ul><p>最后我们总结合并一下所有的配置参数：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">listeners=SSL://kafka.app.node1:9092</span><br><span class="line">ssl.keystore.location=/usr/ca/server/server.keystore.jks</span><br><span class="line">ssl.keystore.password=app123</span><br><span class="line">ssl.key.password=app123</span><br><span class="line">ssl.truststore.location=/usr/ca/trust/server.truststore.jks</span><br><span class="line">ssl.truststore.password=app123</span><br><span class="line">ssl.client.auth=required</span><br><span class="line">ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1</span><br><span class="line">ssl.keystore.type=JKS </span><br><span class="line">ssl.truststore.type=JKS </span><br><span class="line">ssl.endpoint.identification.algorithm=HTTPS</span><br><span class="line">security.inter.broker.protocol=SSL</span><br></pre></td></tr></table></figure><h4 id="1-2-8-初步验证"><a href="#1-2-8-初步验证" class="headerlink" title="1.2.8 初步验证"></a>1.2.8 初步验证</h4><p>正常启动集群的Zookeeper集群，然后依次启动集群的所有节点。使用下面的命令检查：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> openssl s_client -debug -connect kafka.app.node1:9092 -tls1</span></span><br></pre></td></tr></table></figure><p>该命令检查服务器的密钥库和信任库是否正确设置。命令中<code>tls1</code>必须是集群配置参数<code>ssl.enabled.protocols</code>所支持的协议。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">Certificate chain</span><br><span class="line">（省略）</span><br><span class="line">---</span><br><span class="line">Server certificate</span><br><span class="line">-----BEGIN CERTIFICATE-----</span><br><span class="line">（省略）</span><br><span class="line">-----END CERTIFICATE-----</span><br><span class="line">subject=（省略）</span><br><span class="line">issuer=（省略）</span><br><span class="line">---</span><br><span class="line">No client certificate CA names sent</span><br><span class="line">---</span><br><span class="line">SSL handshake has read 2029 bytes and written 264 bytes</span><br><span class="line">---</span><br><span class="line">New, TLSv1/SSLv3, Cipher is ECDHE-RSA-DES-CBC3-SHA</span><br><span class="line">Server public key is 2048 bit</span><br><span class="line">Secure Renegotiation IS supported</span><br><span class="line">Compression: NONE</span><br><span class="line">Expansion: NONE</span><br><span class="line">SSL-Session:</span><br><span class="line">    Protocol  : TLSv1</span><br><span class="line">    Cipher    : ECDHE-RSA-DES-CBC3-SHA</span><br><span class="line">    Session-ID: 5E580D610AEB5DDD8BCD0D31E88180F45391109792CA3CDD1E861EB87C704261</span><br><span class="line">    Session-ID-ctx: </span><br><span class="line">    Master-Key: E544FF34B993B2C3B7F7CB28D8166213F8D3A9864A82247F6948E33B319CD1A8943127DDF9B528EA73435EBC73B0DD55</span><br><span class="line">    Key-Arg   : None</span><br><span class="line">    Start Time: 1582828897</span><br><span class="line">    Timeout   : 7200 (sec)</span><br><span class="line">    Verify return code: 7 (certificate signature failure)</span><br><span class="line">---</span><br><span class="line">（省略）</span><br></pre></td></tr></table></figure><p>如果证书未显示或有其他错误消息，则说明设置不正确。</p><blockquote><p>另外对于’OpenSSL 0.9.8j-fips 07 Jan 2009’版本的openssl版本，由于这个版本不能自己检测出ssl的版本。会报下面的错误信息。</p><p>1816:error:1408E0F4:SSL routines:SSL3_GET_MESSAGE:unexpected message:s3_both.c:463:</p></blockquote><h3 id="1-3-配置kafka客户端"><a href="#1-3-配置kafka客户端" class="headerlink" title="1.3 配置kafka客户端"></a>1.3 配置kafka客户端</h3><p>kafka集群需要支持集群内外的客户端交互访问。安全集群的客户端同样需要进行相关安全配置。这里客户端指的是Console客户端。</p><h4 id="1-3-1-签发客户端证书"><a href="#1-3-1-签发客户端证书" class="headerlink" title="1.3.1 签发客户端证书"></a>1.3.1 签发客户端证书</h4><p>类似集群内部服务端的证书签发步骤，客户端证书签发过程入下：</p><ul><li><p>生成客户端SSL密钥和证书，输出密钥容器：<code>client.keystore.jks</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -keystore /usr/ca/client/client.keystore.jks -<span class="built_in">alias</span> kafka.app</span></span><br><span class="line">.node1 -validity 365 -genkey -keypass app123 -keyalg RSA -dname "CN=kafka.app.node1,OU=dccsh,O=icbc,L=shanghai,S=shanghai,C=cn" -ext SAN=DNS:kafka.app.node1 -storepass app123</span><br></pre></td></tr></table></figure></li><li><p>从密钥容器中提取和导出客户端证书（输出文件：client.cert-file，未签名）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -keystore /usr/ca/client/client.keystore.jks -<span class="built_in">alias</span> kafka.app.node1 -certreq -file /usr/ca/client/client.cert-file -storepass app123</span></span><br></pre></td></tr></table></figure></li><li><p>给客户端证书签名（输出文件：client.cert-signed，已签名）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> openssl x509 -req -CA /usr/ca/root/ca.cert.pem -CAkey /usr/ca/root/ca.key.pem -<span class="keyword">in</span> /usr/ca/client/client.cert-file -out /usr/ca/client/client.cert-signed -days 365 -CAcreateserial -passin pass:app123</span></span><br></pre></td></tr></table></figure></li><li><p>将CA证书导入客户端密钥容器中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -keystore /usr/ca/client/client.keystore.jks -<span class="built_in">alias</span> CARoot -import -file /usr/ca/root/client.cert-file -storepass app123</span></span><br></pre></td></tr></table></figure></li><li><p>将已签名的证书导入密钥容器中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -keystore /usr/ca/client/client.keystore.jks -<span class="built_in">alias</span> kafka.app.node1 -import -file /usr/ca/client/client.cert-signed -storepass app123</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="1-3-2-生成客户端信任库"><a href="#1-3-2-生成客户端信任库" class="headerlink" title="1.3.2 生成客户端信任库"></a>1.3.2 生成客户端信任库</h4><p>使用下面的命令将CA证书导入客户端信任库，输出为信任库文件：<code>client.truststore.jks</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -keystore /usr/ca/trust/client.truststore.jks -<span class="built_in">alias</span> CARoot -import -file /usr/ca/root/ca.cert.pem -storepass app123</span></span><br></pre></td></tr></table></figure><h4 id="1-3-3-配置客户端"><a href="#1-3-3-配置客户端" class="headerlink" title="1.3.3 配置客户端"></a>1.3.3 配置客户端</h4><p>客户端的console-producer和console-consumer命令需要添加相关安全配置。</p><p>如果kafka集群不需要客户端身份验证，只需配置下面的配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">security.protocol=SSL</span><br><span class="line">ssl.truststore.location=/usr/ca/trust/client.truststore.jks</span><br><span class="line">ssl.truststore.password=app123</span><br></pre></td></tr></table></figure><p>如果需要客户端身份验证，还需要补充下面的配置信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssl.keystore.location=/usr/ca/client/client.keystore.jks</span><br><span class="line">ssl.keystore.password=app123</span><br><span class="line">ssl.key.password=app123</span><br></pre></td></tr></table></figure><p>根据我们的要求和代理配置，可能还需要其他配置设置：</p><ol><li>ssl.provider（可选）。用于SSL连接的安全提供程序的名称。</li><li>ssl.cipher.suites（可选）。密码套件是认证，加密，MAC和密钥交换算法的命名组合，用于协商使用TLS或SSL网络协议的网络连接的安全设置。</li><li>ssl.enabled.protocols = TLSv1.2，TLSv1.1，TLSv1。它应列出在代理方配置的至少一种协议</li><li>ssl.truststore.type = JKS</li><li>ssl.keystore.type = JKS</li></ol><p>最后我们总结合并一下所有的配置参数（编辑文件名为：<code>client-ssl.properties</code>）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">security.protocol=SSL</span><br><span class="line">ssl.truststore.location=/usr/ca/trust/client.truststore.jks</span><br><span class="line">ssl.truststore.password=app123</span><br><span class="line">ssl.keystore.location=/usr/ca/client/client.keystore.jks</span><br><span class="line">ssl.keystore.password=app123</span><br><span class="line">ssl.key.password=app123</span><br></pre></td></tr></table></figure><h4 id="1-3-4-消费者生产者"><a href="#1-3-4-消费者生产者" class="headerlink" title="1.3.4 消费者生产者"></a>1.3.4 消费者生产者</h4><p>使用console-producer的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-producer.sh --broker-list kafka.app.node1:9092,kafka.app.node2:9092,kafka.app.node3:9092 --topic test --producer.config client-ssl.properties</span><br></pre></td></tr></table></figure><p>使用console-consumer的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.sh --bootstrap-server kafka.app.node1:9092,kafka.app.node2:9092,kafka.app.node3:9092 --topic test --new-consumer --consumer.config client-ssl.properties</span><br></pre></td></tr></table></figure><p>这里<code>test</code>为<code>topic</code>名称，在只有SSL通信加密集群中，topic的创建、删除、生产、消费并没有权限管理，依然存在安全问题。所以kafka集群需要进一步配置权限管理。</p><h2 id="第二部分-Kafka集群权限认证"><a href="#第二部分-Kafka集群权限认证" class="headerlink" title="第二部分 Kafka集群权限认证"></a>第二部分 Kafka集群权限认证</h2><p>Kafka集群的权限认证管理主要涉及：</p><ul><li>身份认证（Authentication）。对客户端与服务器的连接进行身份认证，brokers和zookeeper之间的连接进行Authentication（producer 和 consumer）、其他 brokers、tools与 brokers 之间连接的认证。</li><li>权限控制（Authorization）。实现对于消息级别的权限控制，客户端的读写操作进行Authorization（生产、消费）管理。</li></ul><p>通俗的讲，身份认证解决的是证明你是谁，而权限控制解决的是你能干什么。在Kafka中身份认证和权限控制是两套独立的安全配置。</p><h3 id="2-1-集群权限认证策略"><a href="#2-1-集群权限认证策略" class="headerlink" title="2.1 集群权限认证策略"></a>2.1 集群权限认证策略</h3><p>Kafka从0.9.0.0版本后开始支持下面的SASL安全策略管理。这些安全功能为Kafka通信安全、多租户管理、集群云化提供了安全保障。截止目前Kafka 2.3版本，一共支持5种SASL方式。</p><table><thead><tr><th>验证方式</th><th>版本</th><th>说明</th></tr></thead><tbody><tr><td>SASL/PLAIN</td><td>0.10.0.0</td><td>不能动态增加用户</td></tr><tr><td>SASL/SCRAM</td><td>0.10.2.0</td><td>可以动态增加用户。有两种方式：SASL/SCRAM-SHA-256 和SASL/SCRAM-SHA-512</td></tr><tr><td>SASL/GSSAPI</td><td>0.9.0.0</td><td>需要独立部署验证服务（即Kerberos服务）</td></tr><tr><td>SASL/OAUTHBEARER</td><td>2.0.0</td><td>需自己实现接口实现token的创建和验证，需要额外Oauth服务</td></tr><tr><td>SASL/Delegation Token</td><td>1.1.0</td><td>补充现有 SASL 机制的轻量级认证机制</td></tr></tbody></table><p>对于生产环境，SASL/PLAIN方式有个缺点：只能在JAAS文件KafkaServer参数中配置用户，集群运行期间无法动态新增用户（需要重启重新加载JAAS文件），这对维护管理带来不便。而SASL/SCRAM方式，将认证数据存储在Zookeeper中，可以动态新增用户并分配权限。</p><p>SASL/GSSAPI方式需要依赖Kerberos服务。对于一些已经部署了集中式的Kerberos服务的大厂，只需要申请一个principal即可。如果生产Kerberos认证中出现TGT分发性能瓶颈，可以使用SASL/Delegation Token模式。使用 Kafka 提供的 API 去获取对应的 Delegation Token。Broker 和客户端在做认证的时候，可以直接使用这个 token，不用每次都去 KDC 获取对应的 ticket（Kerberos 认证），减少性能压力。</p><p>同样SASL/OAUTHBEARER方式需要Oauth服务。</p><p>各种方式引入版本不同，使用依赖各有差异，需要结合自身业务特点选择合适的架构方式。</p><h3 id="2-2-SASL-SCRAM策略配置介绍"><a href="#2-2-SASL-SCRAM策略配置介绍" class="headerlink" title="2.2 SASL/SCRAM策略配置介绍"></a>2.2 SASL/SCRAM策略配置介绍</h3><p>SASL/SCRAM方式将身份认证和权限控制的凭证（credential）数据均存储在Zookeeper中，需要对Zookeeper进行安全配置。</p><h4 id="2-2-1-Zookeeper集群侧配置"><a href="#2-2-1-Zookeeper集群侧配置" class="headerlink" title="2.2.1 Zookeeper集群侧配置"></a>2.2.1 Zookeeper集群侧配置</h4><p>对Zookeeper集群中所有节点更新下面的策略后，重启集群生效。</p><ul><li><p>配置<code>zoo.cfg</code>文件</p><p>文件尾部追加下面的配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider</span><br><span class="line">requireClientAuthScheme=sasl</span><br><span class="line">jaasLoginRenew=3600000</span><br></pre></td></tr></table></figure></li><li><p>新增<code>zk_server_jaas.conf</code>文件</p><p>配置文件内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Server &#123;</span><br><span class="line">org.apache.kafka.common.security.plain.PlainLoginModule required</span><br><span class="line">username="admin"</span><br><span class="line">password="admin-secret"</span><br><span class="line">user_admin="admin-secret;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>其中<code>username</code>和<code>password</code>定义的用户和密钥，用于Zookeeper与Kafka集群进行认证。配置项<code>user_admin=&quot;admin-secret&quot;</code> 中 admin为用户名，admin-secret为密码，用于Zookeeper集群外客户端和集群内进行认证。</p></li><li><p>拷贝依赖包</p><p>将kafka文件系统中<code>kafka/libs</code>目录下的jar包拷贝到<code>zookeeper/lib</code>目录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kafka-clients-2.1.1.jar</span><br><span class="line">lz4-java-1.5.0.jar</span><br><span class="line">osgi-resource-locator-1.0.1.jar</span><br><span class="line">slf4j-api-1.7.25.jar</span><br><span class="line">snappy-java-1.1.7.2.jar</span><br></pre></td></tr></table></figure><blockquote><p>若没有引入依赖包，启动时会报找不到org.apache.kafka.common.security.plain.PlainLoginModule包的错误。</p></blockquote></li><li><p>修改zookeeper启动参数</p><p>修改<code>bin/zkEnv.sh</code>文件, 在文件尾追加下面的配置内容。该配置完成引入的包的加载。变量<code>CLASSPATH</code>和<code>SERVER_JVMFLAGS</code>都会在Zookeeper启动时传给<code>JVM</code>虚拟机。</p><blockquote><p>下面的配置中<code>$ZOOKEEPER_HOME</code>是zookeeper的环境变量，如果没有配置，使用绝对路径即可。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for i in $ZOOKEEPER_HOME/lib/*.jar; do</span><br><span class="line">   CLASSPATH="$i:$CLASSPATH"</span><br><span class="line">done</span><br><span class="line">SERVER_JVMFLAGS=" -Djava.security.auth.login.config=$ZOOKEEPER_HOME/conf/zk_server_jaas.conf"</span><br></pre></td></tr></table></figure></li></ul><h4 id="2-2-2-kafka集群侧配置"><a href="#2-2-2-kafka集群侧配置" class="headerlink" title="2.2.2 kafka集群侧配置"></a>2.2.2 kafka集群侧配置</h4><p>kafka集群中每一台节点均需要更新下面的配置。</p><ul><li><p>新增<code>kafka_server_scram_jaas.conf</code>文件（在<code>config</code>目录中）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">KafkaServer &#123;</span><br><span class="line">org.apache.kafka.common.security.scram.ScramLoginModule required</span><br><span class="line">username="admin"</span><br><span class="line">password="admin-secret";</span><br><span class="line"><span class="meta">#</span><span class="bash"> 自定义用户：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> user_admin=<span class="string">"admin-secret"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> user_alice=<span class="string">"alice-secret"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> user_reader=<span class="string">"reader-secret"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> user_writer=<span class="string">"writer-secret"</span>;</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li></ul><p>其中配置<code>username</code>和<code>password</code>为Kafka集群之间通讯的SCRAM凭证，用户名为<code>admin</code>，密码为<code>admin-secret</code>。</p><p>  配置中类似<code>user_XXX</code>格式的配置项为自定义用户。如果是SASL/PLAIN方式，用户只能在该文件中定义，不能动态新增。我们使用SASL/SCRAM方式，可以后续动态声明admin用户，不再此处进行配置。</p><ul><li><p>更新Kafka的配置文件<code>server.properties</code>（在<code>config</code>目录中）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">SASL CONFIG</span></span><br><span class="line">listeners=SASL_SSL://kafka.app.node1:9092</span><br><span class="line">sasl.enabled.mechanisms=SCRAM-SHA-512</span><br><span class="line">sasl.mechanism.inter.broker.protocol=SCRAM-SHA-512</span><br><span class="line">authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer</span><br><span class="line"><span class="meta">#</span><span class="bash">allow.everyone.if.no.acl.found=<span class="literal">true</span></span></span><br><span class="line">super.users=User:admin</span><br><span class="line"><span class="meta">#</span><span class="bash">SSL CINFIG</span></span><br><span class="line">ssl.keystore.location=/usr/ca/server/server.keystore.jks</span><br><span class="line">ssl.keystore.password=app123</span><br><span class="line">ssl.key.password=app123</span><br><span class="line">ssl.truststore.location=/usr/ca/trust/server.truststore.jks</span><br><span class="line">ssl.truststore.password=app123</span><br><span class="line">ssl.client.auth=required</span><br><span class="line">ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1</span><br><span class="line">ssl.keystore.type=JKS</span><br><span class="line">ssl.truststore.type=JKS</span><br><span class="line">ssl.endpoint.identification.algorithm=HTTPS</span><br><span class="line">security.inter.broker.protocol=SASL_SSL</span><br></pre></td></tr></table></figure><p>需要注意参数<code>allow.everyone.if.no.acl.found</code>，如果开启参数开关，当客户端和集群交互时候未找到ACL策略时，允许所有类型的访问操作。建议该参数关闭（false）。</p><p>参数<code>security.inter.broker.protocol</code>指定集群brokers之间的通讯协议。不加密协议有：SASL_SSL、SASL_PLAINTEXT、PLAINTEXT；加密协议有：SSL。为了提高节点之间的交互性能，内部网络环境建议使用非加密协议。这里使用加密的<code>SASL_SSL</code>协议。</p><p>参数<code>super.users</code>指定了集群的超级用户为：<code>admin</code>。注意如果指定多个超级用户，每个用户使用分号隔开，例如：<code>super.users=User:admin;User:alice</code></p><p>参数<code>sasl.enabled.mechanisms</code>列出支持的认证方式。即可以支持多种。</p><p>参数<code>sasl.mechanism.inter.broker.protocol</code>指定集群内部的认证方式。Kafka仅支持最小迭代次数为4096的强哈希函数SHA-256和SHA-512。所以有SCRAM-SHA-512和SCRAM-SHA-256两种方式。</p></li><li><p>配置kafka启动环境变量（<code>bin</code>目录下面的<code>kafka-run-class.sh</code>）</p><p>为 Kafka 添加 java.security.auth.login.config 环境变量（配置文件路径）。并且在启动模式中添加<code>KAFKA_SASL_OPTS</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 截取配置文件片段：</span></span><br><span class="line">KAFKA_SASL_OPTS='-Djava.security.auth.login.config=/opt/software/kafka/config/kafka_server_scram_jaas.conf'</span><br><span class="line"><span class="meta">#</span><span class="bash"> Launch mode</span></span><br><span class="line">if [ "x$DAEMON_MODE" = "xtrue" ]; then</span><br><span class="line">  nohup $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_SASL_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS "$@" &gt; "$CONSOLE_OUTPUT_FILE" 2&gt;&amp;1 &lt; /dev/null &amp;</span><br><span class="line">else</span><br><span class="line">  exec $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_SASL_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS "$@"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure></li></ul><h4 id="2-2-3-SCRAM认证管理"><a href="#2-2-3-SCRAM认证管理" class="headerlink" title="2.2.3 SCRAM认证管理"></a>2.2.3 SCRAM认证管理</h4><p>在集群的配置文件<code>kafka_server_scram_jaas.conf</code>中，定义了集群内部的认证用户。对于客户端和集群之间认证可以使用<code>kafka-configs.sh</code>来动态创建。</p><ul><li><p>创建用户SCRAM凭证</p><p>例如集群中的超级用户<code>admin</code>用户，使用下面的命令创建：</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-configs.sh --zookeeper kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --alter --add-config <span class="string">'SCRAM-SHA-256=[password=admin-secret],SCRAM-SHA-512=[password=admin-secret]'</span> --entity-type users --entity-name admin</span></span><br></pre></td></tr></table></figure><p>创建自定义普通用户<code>alice</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-configs.sh --zookeeper kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --alter --add-config <span class="string">'SCRAM-SHA-256=[iterations=8192,password=alice-secret],SCRAM-SHA-512=[password=alice-secret]'</span> --entity-type users --entity-name alice</span></span><br></pre></td></tr></table></figure><ul><li>查看SCARM凭证</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-configs.sh --zookeeper kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --describe --entity-type users --entity-name admin</span></span><br></pre></td></tr></table></figure><ul><li>删除SCRAM凭证</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-configs.sh --zookeeper kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --alter --delete-config <span class="string">'SCRAM-SHA-512'</span> --entity-type users --entity-name alice</span></span><br></pre></td></tr></table></figure><h3 id="2-3-Kafka客户端配置"><a href="#2-3-Kafka客户端配置" class="headerlink" title="2.3 Kafka客户端配置"></a>2.3 Kafka客户端配置</h3><p>Kafka集群配置了认证，那么对于Console客户端访问集群自然需要配置认证信息。可集群节点内部通讯凭证的认知，同样需要定义JAAS文件。加入我们自定义了用户<code>alice</code>，JAAS文件名为：<code>kafka_console_client_jaas.conf</code>，配置内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">KafkaClient &#123;</span><br><span class="line">org.apache.kafka.common.security.scram.ScramLoginModule required</span><br><span class="line">username="alice"</span><br><span class="line">password="alice-secret";</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>然后更新<code>kafka-console-producer.sh</code>脚本和<code>kafka-console-consumer.sh</code>脚本的启动参数。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 文件截取更新部分：</span></span><br><span class="line"></span><br><span class="line">if [ "x$KAFKA_OPTS" ]; then</span><br><span class="line">export KAFKA_OPTS="-Djava.security.auth.login.config=/opt/software/kafka/config/kafka_write_jaas.conf"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>在配置SSL时候，我们新建了<code>client-ssl.properties</code>配置文件，作为Console客户端启动配置。在集群启用<code>SASL_SSL</code>后，我们同步更新如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">security.protocol=SASL_SSL</span><br><span class="line">ssl.truststore.location=/usr/ca/trust/client.truststore.jks</span><br><span class="line">ssl.truststore.password=app123</span><br><span class="line">ssl.keystore.location=/usr/ca/client/client.keystore.jks</span><br><span class="line">ssl.keystore.password=app123</span><br><span class="line">ssl.key.password=app123</span><br></pre></td></tr></table></figure><p>至此Console客户端已经配置完毕，但目前Console客户端还不能通过命令方式和集群进行交互，因为我们指定的用户对于集群的资源还没有任何权限。需要对用户进行集群资源的ACL控制设置，赋予相关权限。</p><h3 id="2-4-ACL控制"><a href="#2-4-ACL控制" class="headerlink" title="2.4 ACL控制"></a>2.4 ACL控制</h3><p>Kafka权限资源包含Topic、Group、Cluster、TransactionalId（事务id），每个资源涉及的权限内容如下：</p><table><thead><tr><th>资源类型</th><th style="text-align:left">权限类型</th></tr></thead><tbody><tr><td>Topic</td><td style="text-align:left">Read,Write,Describe,Delete,DescribeConfigs,AlterConfigs,All</td></tr><tr><td>Group</td><td style="text-align:left">Read,Describe,All</td></tr><tr><td>Cluster</td><td style="text-align:left">Create,ClusterAction,DescribeConfigs,AlterConfigs,IdempotentWrite,Alter,Describe,All</td></tr><tr><td>TransactionalId</td><td style="text-align:left">Describe,Write,All</td></tr></tbody></table><p>对于常用类型进行说明：</p><table><thead><tr><th>权限</th><th>说明</th></tr></thead><tbody><tr><td>Read</td><td>读取topic、group信息</td></tr><tr><td>Write</td><td>写topic、TransactionalId（存储在内部topic）</td></tr><tr><td>Delete</td><td>删除topic</td></tr><tr><td>Create</td><td>创建topic</td></tr><tr><td>ALTER</td><td>修改topic</td></tr><tr><td>Describe</td><td>获取topic、group、TransactionalId信息</td></tr><tr><td>ALL</td><td>所有权限</td></tr></tbody></table><p>Kafka提供ACL管理脚本：<code>kafka-acls.sh</code>。</p><h4 id="2-4-1-更新脚本配置"><a href="#2-4-1-更新脚本配置" class="headerlink" title="2.4.1 更新脚本配置"></a>2.4.1 更新脚本配置</h4><p>认证数据均存储在Zookeeper集群中，需要和Zookeeper交互自然需要配置相关认证信息。</p><p>首先需要新建JAAS文件，文件名为：<code>zk_client_jaas.conf</code>。这里的用户已经在Zookeeper集群中进行定义。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Client &#123;</span><br><span class="line">org.apache.kafka.common.security.plain.PlainLoginModule required</span><br><span class="line">username="admin"</span><br><span class="line">password="admin-secret";</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>最后更新<code>kafka-acls.sh</code>脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 截取更新部分</span></span><br><span class="line"></span><br><span class="line">if [ "x$KAFKA_OPTS" ]; then</span><br><span class="line">export KAFKA_OPTS="-Djava.security.auth.login.config=/opt/software/kafka/config/zk_client_jaas.conf"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>当然Kafka集群的配置文件中已经开启了ACL：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer</span><br></pre></td></tr></table></figure><p>至此完成配置。</p><h4 id="2-4-2-ACL配置"><a href="#2-4-2-ACL配置" class="headerlink" title="2.4.2 ACL配置"></a>2.4.2 ACL配置</h4><p>根据官网的介绍，ACL的格式如下：</p><p><em>“Principal P is [Allowed/Denied] Operation O From Host H On Resource R”</em></p><p>参数含义描述如下：</p><ul><li>principal：指定一个Kafka user；</li><li>operation：指定一个具体的操作类型，例如：Read, Write, Delete等；</li><li>Host：表示与集群交互的客户端IP地址，如果是通配符‘*’表示所有IP。目前不支持主机名（hostname）形式，只能是IP地址；</li><li><p>Resource：指定一种Kafka资源类型（共有4种类型）；</p><p>例如下面的ACL命令：</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sh kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --add --allow-principal User:alice --allow-host <span class="string">'*'</span> --operation ALL --topic <span class="built_in">test</span></span></span><br></pre></td></tr></table></figure><p>赋权之后，用户alice对test具有全部权限，并且访问请求可以是来自任何IP的客户端。</p><p>常用参数的补充说明：</p><ul><li>对主机IP的限制参数，<code>allow-host</code>指定允许的IP，<code>deny-host</code>指定禁用IP；</li><li>新增和删除一个赋权策略，分别使用：<code>add</code>和<code>remove</code></li></ul><h4 id="2-4-3-ACL策略查看"><a href="#2-4-3-ACL策略查看" class="headerlink" title="2.4.3 ACL策略查看"></a>2.4.3 ACL策略查看</h4><p>使用参数<code>list</code>参看ACL策略。例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sh kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --list --topic <span class="built_in">test</span>-topic</span></span><br></pre></td></tr></table></figure><p>该查看命令显示<code>test-topic</code>资源相关的所有ACL策略。</p><h4 id="2-4-4-超级用户"><a href="#2-4-4-超级用户" class="headerlink" title="2.4.4 超级用户"></a>2.4.4 超级用户</h4><p>kafka集群的配置文件<code>server.properties</code>中定义了超级用户（Super Users），超级用户不在ACL控制范围内，默认可以访问集群中所有资源，并具备所有权限。</p><h3 id="2-5-权限认证数据访问"><a href="#2-5-权限认证数据访问" class="headerlink" title="2.5 权限认证数据访问"></a>2.5 权限认证数据访问</h3><p>集群的认证数据存储在Zookeeper，可以通过Zookeeper的console客户端访问认证数据。</p><p>使用zookeeper自带的命令行客户端：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/dmqs/zookeeper/bin&gt; ./zkCli.sh</span><br><span class="line">Connecting to localhost:2181</span><br><span class="line">Welcome to ZooKeeper!</span><br><span class="line">JLine support is enabled</span><br><span class="line">[zk: localhost:2181(CONNECTING) 0]</span><br></pre></td></tr></table></figure><p>查看zookeeper中的数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /</span><br><span class="line">[cluster, controller_epoch, controller, brokers, zookeeper, kafka-acl, kafka-acl-changes, admin, isr_change_notification, consumers, config]</span><br></pre></td></tr></table></figure><p>其中<code>kafka-acl</code>中存储相关权限认证数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /kafka-acl</span><br><span class="line">[Cluster, Topic]</span><br></pre></td></tr></table></figure><p>可以查看其中的权限信息。</p><h3 id="2-6-SSL和SASL的说明"><a href="#2-6-SSL和SASL的说明" class="headerlink" title="2.6 SSL和SASL的说明"></a>2.6 SSL和SASL的说明</h3><p>SSL是传输层安全协议，是位于传输层（TCP/IP）和应用层（HTTP）的协议，SSL是对整个传输过程的加密，SSL是对客户端和服务器之间传输的所有数据进行加密。假如在配置的时候使用了SASL，但是没有使用SSL，那么除了账号密码外，所有的传输内容都是裸奔的。</p><p>所以生产集群采用SSL和SASL结合方式，即SSL_SASL方式。</p><h2 id="第三部分-安全集群的客户端"><a href="#第三部分-安全集群的客户端" class="headerlink" title="第三部分 安全集群的客户端"></a>第三部分 安全集群的客户端</h2><h3 id="3-1-开发语言类"><a href="#3-1-开发语言类" class="headerlink" title="3.1 开发语言类"></a>3.1 开发语言类</h3><h4 id="3-1-1-Python客户端"><a href="#3-1-1-Python客户端" class="headerlink" title="3.1.1 Python客户端"></a>3.1.1 Python客户端</h4><p>目前市面上kafka的python API常用的有三种：</p><ul><li><strong>第一种 kafka</strong></li></ul><p>该项目是<code>kafka-python</code>的老项目，2017年后调整为<code>kafka-python</code>项目。</p><ul><li><strong>第二种 kafka-python</strong></li></ul><p>最新版本为2.0，首先从客户端的密钥库中导出CA证书。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -exportcert -<span class="built_in">alias</span> CARoot -keystore client.keystore.jks -rfc -file ca.cert.pem</span></span><br></pre></td></tr></table></figure><p>生产者和消费者的案例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> kafka <span class="keyword">import</span> KafkaConsumer, KafkaProducer</span><br><span class="line"><span class="keyword">import</span> kafka</span><br><span class="line"><span class="keyword">import</span> ssl</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment">#logging.basicConfig(level=logging.DEBUG)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    bootstrap_servers = <span class="string">'kafka.itdw.node1:9092,kafka.itdw.node2:9092,kafka.itdw.node3:9092'</span></span><br><span class="line">    topic = <span class="string">"test"</span></span><br><span class="line">    sasl_mechanism = <span class="string">"SCRAM-SHA-512"</span></span><br><span class="line">    username = <span class="string">"alice"</span></span><br><span class="line">    password = <span class="string">"alice-secret"</span></span><br><span class="line">    security_protocol = <span class="string">"SASL_SSL"</span></span><br><span class="line">    <span class="comment"># CA 证书路径</span></span><br><span class="line">    ssl_cafile = <span class="string">'ca.cert.pem'</span></span><br><span class="line">    <span class="comment"># SSL</span></span><br><span class="line">    context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)</span><br><span class="line">    context.verify_mode = ssl.CERT_NONE</span><br><span class="line">    context.check_hostname = <span class="keyword">False</span></span><br><span class="line">    context.load_verify_locations(ssl_cafile)</span><br><span class="line">    <span class="comment"># 消费者</span></span><br><span class="line">    consumer = KafkaConsumer(topic, bootstrap_servers=bootstrap_servers,</span><br><span class="line">                               api_version=(<span class="number">0</span>, <span class="number">10</span>),</span><br><span class="line">                               security_protocol=security_protocol,</span><br><span class="line">                               ssl_context=context,</span><br><span class="line">                               sasl_mechanism = sasl_mechanism,</span><br><span class="line">                               sasl_plain_username = username,</span><br><span class="line">                               sasl_plain_password = password</span><br><span class="line">                              )</span><br><span class="line">    <span class="comment"># 生产者</span></span><br><span class="line">    producer = KafkaProducer(bootstrap_servers=bootstrap_servers,</span><br><span class="line">                             api_version=(<span class="number">0</span>, <span class="number">10</span>),</span><br><span class="line">                             acks=<span class="string">'all'</span>,</span><br><span class="line">                             retries=<span class="number">1</span>,</span><br><span class="line">                             security_protocol=security_protocol,</span><br><span class="line">                             ssl_context=context,</span><br><span class="line">                             sasl_mechanism=sasl_mechanism,</span><br><span class="line">                             sasl_plain_username=username,</span><br><span class="line">                             sasl_plain_password=password</span><br><span class="line">                             )</span><br><span class="line">    <span class="comment"># 生产数据</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        producer.send(topic, bytes(<span class="string">"测试"</span>,encoding=<span class="string">'utf8'</span>))</span><br><span class="line">    producer.flush()</span><br><span class="line">    <span class="comment"># 消费数据</span></span><br><span class="line">    <span class="keyword">for</span> msg <span class="keyword">in</span> consumer:</span><br><span class="line">        print(msg)</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure><p>需要的注意的事项有：</p><ul><li><p>Kafka集群启用主机名模式，所以应用程序运行节点的hosts文件需要配置Kafka集群节点的域名映射。</p></li><li><p>ssl_context参数为包装套接字连接的预配置SSLContext。如果非None，将忽略所有其他ssl_ *配置。</p></li><li><p>主机名验证问题。如果证书中域名和主机名不匹配，客户端侧需要配置需要调整如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssl_ctx.check_hostname=<span class="keyword">False</span></span><br><span class="line">ssl_ctx.verify_mode = CERT_NONE</span><br></pre></td></tr></table></figure></li><li><p>如果不提前预配置SSLContext，还需要客户端的证书。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> keytool -exportcert -<span class="built_in">alias</span> localhost -keystore client.keystore.jks -rfc -file client.cert.pem</span></span><br></pre></td></tr></table></figure><p>生产者的参数需要添加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ssl_certfile = <span class="string">"client.cert.pem"</span></span><br><span class="line">ssl_cafile = <span class="string">"ca.cert.pem"</span></span><br><span class="line">producer = KafkaProducer(bootstrap_servers=bootstrap_servers,</span><br><span class="line">                         api_version=(<span class="number">0</span>, <span class="number">10</span>),</span><br><span class="line">                         acks=<span class="string">'all'</span>,</span><br><span class="line">                         retries=<span class="number">1</span>,</span><br><span class="line">                         security_protocol=security_protocol,</span><br><span class="line">                         ssl_context=context,</span><br><span class="line">                         sasl_mechanism=sasl_mechanism,</span><br><span class="line">                         sasl_plain_username=username,</span><br><span class="line">                         sasl_plain_password=password,</span><br><span class="line">                         ssl_check_hostname=<span class="keyword">False</span>, </span><br><span class="line">                         ssl_certfile=ssl_certfile,</span><br><span class="line">                         ssl_cafile=ssl_cafile)</span><br></pre></td></tr></table></figure></li><li><p><strong>第三种 confluent-kafka</strong></p></li></ul><p>confluent-kafka包由confluent公司开源，主要是对C/C++客户端包（<a href="https://github.com/edenhill/librdkafka" target="_blank" rel="noopener">librdkafka</a>）的封装。案例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> confluent_kafka <span class="keyword">import</span> Producer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 回调函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">delivery_report</span><span class="params">(err, msg)</span>:</span></span><br><span class="line">     <span class="string">""" Called once for each message produced to indicate delivery result.</span></span><br><span class="line"><span class="string">         Triggered by poll() or flush(). """</span></span><br><span class="line">     <span class="keyword">if</span> err <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        print(’Message delivery failed: &#123;&#125;‘.format(err))</span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">        print(‘Message delivered to &#123;&#125; [&#123;&#125;]‘.format(msg.topic(),           msg.partition()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == ‘__main__‘:</span><br><span class="line">     producerConfing = &#123;<span class="string">"bootstrap.servers"</span>: <span class="string">'kafka.itdw.node1:9092,kafka.itdw.node2:9092,kafka.itdw.node3:9092'</span>,</span><br><span class="line">         <span class="string">"security.protocol"</span>: <span class="string">'SASL_SSL'</span>,</span><br><span class="line">         <span class="string">"sasl.mechanisms"</span>: <span class="string">'SCRAM-SHA-256'</span>,</span><br><span class="line">         <span class="string">"sasl.username"</span>: <span class="string">'alice'</span>,</span><br><span class="line">         <span class="string">"sasl.password"</span>: <span class="string">'alice-secret'</span>,</span><br><span class="line">         <span class="string">"ssl.ca.location"</span>: <span class="string">'ca.cert.pem'</span></span><br><span class="line">     &#125;</span><br><span class="line">     ProducerTest = Producer(producerConfing)</span><br><span class="line"> </span><br><span class="line">     ProducerTest.poll(<span class="number">0</span>)</span><br><span class="line">     ProducerTest.produce(‘testTopic‘, ‘confluent kafka test‘.encode(‘utf<span class="number">-8</span>‘),callback=delivery_report)</span><br><span class="line">     ProducerTest.flush()</span><br></pre></td></tr></table></figure><h4 id="3-1-2-Go客户端"><a href="#3-1-2-Go客户端" class="headerlink" title="3.1.2 Go客户端"></a>3.1.2 Go客户端</h4><p>我们的Go语言中常用的Kafka的客户端包有：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">"github.com/Shopify/sarama"</span><br><span class="line">"github.com/bsm/sarama-cluster"</span><br><span class="line">"github.com/confluentinc/confluent-kafka-go/kafka"</span><br><span class="line">"github.com/segmentio/ksuid"</span><br></pre></td></tr></table></figure><p>其中最常用的是<code>sarama</code>，案例参考<a href="https://github.com/Shopify/sarama/tree/master/examples/sasl_scram_client" target="_blank" rel="noopener">github项目</a>。</p><h4 id="3-1-3-Java客户端"><a href="#3-1-3-Java客户端" class="headerlink" title="3.1.3 Java客户端"></a>3.1.3 Java客户端</h4><p>生产者：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kafka.security;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.CommonClientConfigs;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.config.SaslConfigs;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.config.SslConfigs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducerWithSASL_SSL</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String KAFKA_TOPIC = <span class="string">"topsec"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String BOOTSTRAP_SERVER = <span class="string">"docker31:9092"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String[] strs = <span class="keyword">new</span> String[]&#123;<span class="string">"zhao"</span>, <span class="string">"qian"</span>, <span class="string">"sun"</span>, <span class="string">"li"</span>, <span class="string">"zhou"</span>, <span class="string">"wu"</span>, <span class="string">"zheng"</span>, <span class="string">"wang"</span>, <span class="string">"feng"</span>, <span class="string">"chen"</span>&#125;;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Random r = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            producer();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">producer</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVER);</span><br><span class="line">        <span class="comment">//SASL_SSL加密</span></span><br><span class="line">        props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, <span class="string">"SASL_SSL"</span>);</span><br><span class="line">        props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, <span class="string">"D:\\Download\\ca\\trust\\client.truststore.jks"</span>);</span><br><span class="line">        props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, <span class="string">"hadoop"</span>);</span><br><span class="line">        <span class="comment">// SSL用户认证</span></span><br><span class="line">        props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, <span class="string">"D:\\Download\\ca\\client\\client.keystore.jks"</span>);</span><br><span class="line">        props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, <span class="string">"hadoop"</span>);</span><br><span class="line">        props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, <span class="string">"hadoop"</span>);</span><br><span class="line">        <span class="comment">//SASL用户认证</span></span><br><span class="line">                                props.put(SaslConfigs.SASL_JAAS_CONFIG,<span class="string">"org.apache.kafka.common.security.scram.ScramLoginModule required username=\"admin\" password=\"admin-secret\";"</span>);</span><br><span class="line">        props.put(SaslConfigs.SASL_MECHANISM, <span class="string">"SCRAM-SHA-512"</span>);</span><br><span class="line"></span><br><span class="line">        props.put(ProducerConfig.ACKS_CONFIG, <span class="string">"all"</span>);</span><br><span class="line">        props.put(ProducerConfig.RETRIES_CONFIG, <span class="number">0</span>);</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">        props.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line">        props.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line">        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(KAFKA_TOPIC, strs[r.nextInt(<span class="number">10</span>)],strs[r.nextInt(<span class="number">10</span>)]));</span><br><span class="line">            Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>消费者：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.topsec.kafka.security;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.CommonClientConfigs;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.config.SaslConfigs;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.config.SslConfigs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumerWithSASLAndSSL</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String KAFKA_TOPIC = <span class="string">"topsec"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String BOOTSTRAP_SERVER = <span class="string">"docker31:9092"</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        consumer();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">consumer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">//SASL_SSL加密配置</span></span><br><span class="line">        props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, <span class="string">"SASL_SSL"</span>);</span><br><span class="line">        props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, <span class="string">"D:\\Download\\ca\\trust\\client.truststore.jks"</span>);</span><br><span class="line">        props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, <span class="string">"hadoop"</span>);</span><br><span class="line">        <span class="comment">//SSL身份验证配置</span></span><br><span class="line">        props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, <span class="string">"D:\\Download\\ca\\client\\client.keystore.jks"</span>);</span><br><span class="line">        props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, <span class="string">"hadoop"</span>);</span><br><span class="line">        props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, <span class="string">"hadoop"</span>);</span><br><span class="line">        <span class="comment">//SASL身份验证</span></span><br><span class="line">        props.put(SaslConfigs.SASL_JAAS_CONFIG,<span class="string">"org.apache.kafka.common.security.scram.ScramLoginModule required username=\"admin\" password=\"admin-secret\";"</span>);</span><br><span class="line">        props.put(SaslConfigs.SASL_MECHANISM, <span class="string">"SCRAM-SHA-512"</span>);</span><br><span class="line"></span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVER);</span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">"true"</span>);</span><br><span class="line">        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="string">"1000"</span>);</span><br><span class="line">        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, <span class="string">"6000"</span>);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Collections.singletonList(KAFKA_TOPIC));</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">2000</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(<span class="string">"offset = %d, key = %s, value = %s, partition = %d %n"</span>,</span><br><span class="line">                        record.offset(),</span><br><span class="line">                        record.key(),</span><br><span class="line">                        record.value(),</span><br><span class="line">                        record.partition());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-组件类"><a href="#3-2-组件类" class="headerlink" title="3.2 组件类"></a>3.2 组件类</h3><h4 id="3-2-1-Console客户端"><a href="#3-2-1-Console客户端" class="headerlink" title="3.2.1 Console客户端"></a>3.2.1 Console客户端</h4><p>客户端节点部署kafka项目，在bin目录下面我们已经更新了<code>kafka-console-consumer.sh</code>和<code>kafka-console-producer.sh</code>两个脚本。并分别新增了加密访问的配置文件<code>consumer.config</code>和<code>producer.config</code>。</p><p>命令案例参考：1.3.4 章节内容。</p><h4 id="3-2-2-Flume客户端"><a href="#3-2-2-Flume客户端" class="headerlink" title="3.2.2 Flume客户端"></a>3.2.2 Flume客户端</h4><p>目前Flume项目官网项目文档介绍支持下面三种方式：</p><ul><li>SASL_PLAINTEXT - 无数据加密的 Kerberos 或明文认证；</li><li>SASL_SSL - 有数据加密的 Kerberos 或明文认证；</li><li>SSL - 基于TLS的加密，可选的身份验证；</li></ul><p>事实上对于SASL/SCRAM方式Flume也是支持的。具体配置如下（以Flume1.9版本为例）：</p><h5 id="3-2-2-1-第一步-新增jaas配置文件"><a href="#3-2-2-1-第一步-新增jaas配置文件" class="headerlink" title="3.2.2.1 第一步 新增jaas配置文件"></a>3.2.2.1 第一步 新增jaas配置文件</h5><p>在Flume的conf配置目录下面新增flume_jaas.conf文件，文件内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Server &#123;</span><br><span class="line">    org.apache.kafka.common.security.plain.PlainLoginModule required</span><br><span class="line">    username="admin"</span><br><span class="line">    password="admin-secret"</span><br><span class="line">    user_admin="admin-secret";</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">KafkaClient &#123;</span><br><span class="line">    org.apache.kafka.common.security.scram.ScramLoginModule required</span><br><span class="line">    username="admin"</span><br><span class="line">    password="admin-secret";</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h5 id="3-2-2-2-第二步-更新flume-env-sh文件"><a href="#3-2-2-2-第二步-更新flume-env-sh文件" class="headerlink" title="3.2.2.2 第二步 更新flume-env.sh文件"></a>3.2.2.2 第二步 更新flume-env.sh文件</h5><p>Flume的conf配置目录下面<code>flume-env.sh</code>文件添加<code>JAVA_OPTS</code>配置更新：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_OPTS="$JAVA_OPTS -Djava.security.auth.login.config=/dmqs/apache-flume-1.9.0-bin/conf/flume_jaas.conf"</span><br></pre></td></tr></table></figure><p>其中路径为第一步中新增的<code>flume_jaas.conf</code>文件路径。</p><h5 id="3-2-2-3-测试案例（sinks）"><a href="#3-2-2-3-测试案例（sinks）" class="headerlink" title="3.2.2.3  测试案例（sinks）"></a>3.2.2.3  测试案例（sinks）</h5><p>我们使用一个简单的案例来测试，Flume的source为监控文件尾写入，Flume的sinks为加密kafka集群。具体配置如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">define</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">bind</span></span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -f /dmqs/apache-flume-1.9.0-bin/data/flume/flume.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 15000</span><br><span class="line">a1.channels.c1.transactionCapacity = 15000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> sink</span></span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers = kafka.itdw.node1:9093</span><br><span class="line">a1.sinks.k1.kafka.topic = flume</span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize = 15000</span><br><span class="line">a1.sinks.k1.kafka.producer.acks = 1</span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms = 1000</span><br><span class="line">a1.sinks.k1.kafka.producer.security.protocol=SASL_SSL</span><br><span class="line">a1.sinks.c1.kafka.producer.sasl.mechanism =SCRAM-SHA-512</span><br><span class="line">a1.sinks.c1.kafka.producer.sasl.jaas.config =org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret"</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">###</span></span></span><br><span class="line">a1.sinks.k1.kafka.producer.ssl.truststore.location =/usr/ca/trust/client.truststore.jks</span><br><span class="line">a1.sinks.k1.kafka.producer.sasl.mechanism =SCRAM-SHA-512</span><br><span class="line">a1.sinks.k1.kafka.producer.ssl.truststore.password=app123</span><br><span class="line">a1.sinks.k1.kafka.producer.ssl.keystore.location=/usr/ca/client/client.keystore.jks</span><br><span class="line">a1.sinks.k1.kafka.producer.ssl.keystore.password=app123</span><br><span class="line">a1.sinks.k1.kafka.producer.ssl.key.password=app123</span><br><span class="line">a1.sinks.k1.kafka.producer.timeout.ms = 100</span><br><span class="line">a1.sinks.k1.batchSize=15000</span><br><span class="line">a1.sinks.k1.batchDurationMillis=2000</span><br></pre></td></tr></table></figure><p>配置保存为<code>flume-sink-auth-kafka.conf</code>,为了检查输出结果使用下面命令启动（在bin目录中）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./flume-ng agent --conf ../conf --conf-file ../conf/flume-sink-auth-kafka.conf --name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>向文件尾部追加信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo "test" &gt;&gt; /dmqs/apache-flume-1.9.0-bin/data/flume/flume.log</span><br></pre></td></tr></table></figure><p>然后使用消费者客户端查看数据是否写入kafka的flume主题中。</p><h5 id="3-2-2-3-测试案例（source）"><a href="#3-2-2-3-测试案例（source）" class="headerlink" title="3.2.2.3  测试案例（source）"></a>3.2.2.3  测试案例（source）</h5><p>同样可以也可以将加密Kafka作为Flume的source，配置案例如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">define</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">bind</span></span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">a1.sources.r1.kafka.bootstrap.servers = kafka.app .node1:9093,kafka.app.node2:9093,kafka.app.node3:9093</span><br><span class="line">a1.sources.r1.kafka.topics = flume</span><br><span class="line">a1.sources.r1.kafka.consumer.group.id = flume</span><br><span class="line">a1.sources.r1.kafka.consumer.timeout.ms = 2000</span><br><span class="line">a1.sources.r1.batchSize=150</span><br><span class="line">a1.sources.r1.batchDurationMillis=1000</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">####</span></span></span><br><span class="line">a1.sources.r1.kafka.consumer.ssl.truststore.location =/usr/ca/trust/client.truststore.jks</span><br><span class="line">a1.sources.r1.kafka.consumer.sasl.mechanism =SCRAM-SHA-512</span><br><span class="line">a1.sources.r1.kafka.consumer.ssl.truststore.password=itdw123</span><br><span class="line">a1.sources.r1.kafka.consumer.ssl.keystore.location=/usr/ca/client/client.keystore.jks</span><br><span class="line">a1.sources.r1.kafka.consumer.ssl.keystore.password=itdw123</span><br><span class="line">a1.sources.r1.kafka.consumer.ssl.key.password=itdw123</span><br><span class="line">a1.sources.r1.kafka.consumer.security.protocol=SASL_SSL</span><br><span class="line">a1.sources.r1.kafka.consumer.sasl.mechanism =SCRAM-SHA-512</span><br><span class="line">a1.sources.r1.kafka.consumer.sasl.jaas.config =org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret";</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 15000</span><br><span class="line">a1.channels.c1.transactionCapacity = 15000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> sink</span></span><br><span class="line">a1.sinks.k1.type = file_roll</span><br><span class="line">a1.sinks.k1.sink.directory = /dmqs/apache-flume-1.9.0-bin/data/flume</span><br><span class="line">a1.sinks.k1.sink.serializer = TEXT</span><br></pre></td></tr></table></figure><p>案例中将加密Kafka中flume主题中的数据汇入到指定目录的文件中。</p><h4 id="3-2-3-Logstash客户端"><a href="#3-2-3-Logstash客户端" class="headerlink" title="3.2.3 Logstash客户端"></a>3.2.3 Logstash客户端</h4><p>Logstash和Kafka交互使用<code>Kafka output plugin</code>插件实现。其中配置文件中output部分如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">output &#123;</span><br><span class="line">  kafka &#123;</span><br><span class="line">    id =&gt; <span class="string">"kafkaSLL_SASL"</span></span><br><span class="line">    codec =&gt; <span class="string">"json"</span></span><br><span class="line">    ssl_endpoint_identification_algorithm =&gt; <span class="string">""</span></span><br><span class="line">    bootstrap_servers =&gt; <span class="string">"kafka.app.node1:9092,kafka.app.node2:9092,kafka.app.node3:9092"</span></span><br><span class="line">    ssl_keystore_location =&gt; <span class="string">"/etc/logstash/certificates/client.keystore.jks"</span></span><br><span class="line">    ssl_keystore_password =&gt; <span class="string">"app123"</span></span><br><span class="line">    ssl_keystore_type =&gt; <span class="string">"JKS"</span></span><br><span class="line">    ssl_truststore_location =&gt; <span class="string">"/etc/logstash/certificates/client.truststore.jks"</span></span><br><span class="line">    ssl_truststore_password =&gt; <span class="string">"app123"</span></span><br><span class="line">    ssl_truststore_type =&gt; <span class="string">"JKS"</span></span><br><span class="line">    sasl_mechanism =&gt; <span class="string">"SCRAM-SHA-512"</span></span><br><span class="line">    security_protocol =&gt; <span class="string">"SASL_SSL"</span></span><br><span class="line">    sasl_jaas_config =&gt; <span class="string">"org.apache.kafka.common.security.plain.PlainLoginModule required username='alice'  password='alice-secret';"</span></span><br><span class="line">    topic_id =&gt; <span class="string">"test"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="第四部分-加密认证集群的性能压测"><a href="#第四部分-加密认证集群的性能压测" class="headerlink" title="第四部分 加密认证集群的性能压测"></a>第四部分 加密认证集群的性能压测</h2><p>集群启用SSL后，数据交互中需要加密、解密。kafka集群的I/O性能会降低。我们使用Kafka自带的压侧工具对集群加密前和加密后性能进行评测。</p><h3 id="4-1生产者压力测试"><a href="#4-1生产者压力测试" class="headerlink" title="4.1生产者压力测试"></a>4.1生产者压力测试</h3><p>客户端写入参数配置为<code>acks=all</code>（即主题中Leader和fellow副本均写入成功）。每条消息大小为1M（消息体小吞吐量会大一些）。另外测试客户端为集群内部节点，忽略了数据网络传输的性能消耗。</p><h4 id="4-1-1不加密集群"><a href="#4-1-1不加密集群" class="headerlink" title="4.1.1不加密集群"></a>4.1.1不加密集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-consumer-perf-test.sh --topic topsec --throughput 50000 --num-records 1500000 --record-size 10000 --producer-props bootstrap.servers=kafka.itdw.node1:9093,kafka.itdw.node2:9093,kafka.itdw.node3:9093 acks=all</span><br></pre></td></tr></table></figure><p>测试结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1500000 records sent, 38538.615693 records/sec (37.64 MB/sec), 748.44 ms avg latency, 5485.00 ms max latency, 227 ms 50th, 3194 ms 95th, 3789 ms 99th, 3992 ms 99.9th.</span><br></pre></td></tr></table></figure><h4 id="4-1-2加密集群"><a href="#4-1-2加密集群" class="headerlink" title="4.1.2加密集群"></a>4.1.2加密集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-producer-perf-test.sh --topic topsec --throughput 50000 --num-records 1500000 --record-size 10000 --producer-props bootstrap.servers=kafka.itdw.node1:9093,kafka.itdw.node2:9093,kafka.itdw.node3:9093 acks=all --producer.config producer.config</span><br></pre></td></tr></table></figure><p>测试结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1500000 records sent, 16901.027582 records/sec (16.50 MB/sec), 1713.43 ms avg latency, 9345.00 ms max latency, 72 ms 50th, 1283 ms 95th, 2067 ms 99th, 2217 ms 99.9th.</span><br></pre></td></tr></table></figure><h3 id="4-2-压侧结论"><a href="#4-2-压侧结论" class="headerlink" title="4.2 压侧结论"></a>4.2 压侧结论</h3><p>加密改造前，生产者的吞吐量为3.8w 条/秒，改造后1.7W 条/秒。整体吞吐性能降低50%左右，数据的加密、解密导致吞吐量性能降低。平均时延也增加了一倍多（改造前700ms，改造后1700ms）。在实际生产中可参考这个性能折扣基线配置集群资源。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Kafka官网对安全类功能介绍，链接：<a href="http://kafka.apache.org/documentation/#security" target="_blank" rel="noopener">http://kafka.apache.org/documentation/#security</a></p><p>2、Kafka ACLs in Practice – User Authentication and Authorization，链接：<a href="https://developer.ibm.com/opentech/2017/05/31/kafka-acls-in-practice/" target="_blank" rel="noopener">https://developer.ibm.com/opentech/2017/05/31/kafka-acls-in-practice/</a></p><p>3、维基百科（数字证书），链接：<a href="https://zh.wikipedia.org/wiki/公開金鑰認證" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/公開金鑰認證</a></p><p>4、SSL技术白皮书，链接：<a href="https://blog.51cto.com/xuding/1732723" target="_blank" rel="noopener">https://blog.51cto.com/xuding/1732723</a></p><p>5、Kafka权限管理，链接：<a href="https://www.jianshu.com/p/09129c9f4c80" target="_blank" rel="noopener">https://www.jianshu.com/p/09129c9f4c80</a></p><p>5、Flume文档，链接：<a href="https://flume.apache.org/FlumeUserGuide.html#kafka-sinkorg/FlumeUserGuide.html#kafka-sink" target="_blank" rel="noopener">https://flume.apache.org/FlumeUserGuide.html#kafka-sinkorg/FlumeUserGuide.html#kafka-sink</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   Kafka集群加密传输&lt;/li&gt;
&lt;li&gt;第二部分   Kafka集群权限认证&lt;/li&gt;

      
    
    </summary>
    
      <category term="Kafka" scheme="https://zjrongxiang.github.io/categories/Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>Go语言学习系列(一)Go语言Win开发环境部署</title>
    <link href="https://zjrongxiang.github.io/2020/01/31/2020-01-31-Go%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97(%E4%B8%80)Go%E8%AF%AD%E8%A8%80Win%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%20-%20%E5%89%AF%E6%9C%AC/"/>
    <id>https://zjrongxiang.github.io/2020/01/31/2020-01-31-Go语言学习系列(一)Go语言Win开发环境部署 - 副本/</id>
    <published>2020-01-31T14:42:00.000Z</published>
    <updated>2020-03-22T03:42:07.618Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分   windows下安装</p></li><li><p>第二部分   配置环境变量</p></li><li><p>第三部分   IDE配置</p></li><li><p>第四部分   HelloWorld案例</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Go语言的官方网站：<a href="https://golang.org/" target="_blank" rel="noopener">https://golang.org/</a></p><blockquote><p>由于防火墙原因，请大家在这个网站下载：<a href="https://studygolang.com/dl" target="_blank" rel="noopener">https://studygolang.com/dl</a></p></blockquote><h2 id="第一部分-windows下安装"><a href="#第一部分-windows下安装" class="headerlink" title="第一部分 windows下安装"></a>第一部分 windows下安装</h2><p>下载的是msi包，直接执行安装即可。</p><blockquote><p>go1.12.5.windows-amd64.msi</p></blockquote><h2 id="第二部分-配置环境变量"><a href="#第二部分-配置环境变量" class="headerlink" title="第二部分 配置环境变量"></a>第二部分 配置环境变量</h2><p>Go 语言需要配置 GOROOT 和 Path 两个环境变量：GOROOT 和 GOPATH。如果使用msi包安装，那么会自动配置好两个环境变量。</p><p>可以使用下面的命令检查变量：</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">C:\<span class="title">Users</span>\<span class="title">rongxiang</span>&gt;<span class="title">go</span> <span class="title">env</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOARCH</span>=<span class="title">amd64</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOBIN</span>=</span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOCACHE</span>=<span class="title">C</span>:\<span class="title">Users</span>\<span class="title">rongxiang</span>\<span class="title">AppData</span>\<span class="title">Local</span>\<span class="title">go</span>-<span class="title">build</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOEXE</span>=.<span class="title">exe</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOFLAGS</span>=</span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOHOSTARCH</span>=<span class="title">amd64</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOHOSTOS</span>=<span class="title">windows</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOOS</span>=<span class="title">windows</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOPATH</span>=<span class="title">C</span>:\<span class="title">Users</span>\<span class="title">rongxiang</span>\<span class="title">go</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOPROXY</span>=</span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GORACE</span>=</span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOROOT</span>=<span class="title">C</span>:\<span class="title">Go</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOTMPDIR</span>=</span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOTOOLDIR</span>=<span class="title">C</span>:\<span class="title">Go</span>\<span class="title">pkg</span>\<span class="title">tool</span>\<span class="title">windows_amd64</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GCCGO</span>=<span class="title">gccgo</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">CC</span>=<span class="title">gcc</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">CXX</span>=<span class="title">g</span>++</span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">CGO_ENABLED</span>=1</span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOMOD</span>=</span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">CGO_CFLAGS</span>=-<span class="title">g</span> -<span class="title">O2</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">CGO_CPPFLAGS</span>=</span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">CGO_CXXFLAGS</span>=-<span class="title">g</span> -<span class="title">O2</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">CGO_FFLAGS</span>=-<span class="title">g</span> -<span class="title">O2</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">CGO_LDFLAGS</span>=-<span class="title">g</span> -<span class="title">O2</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">PKG_CONFIG</span>=<span class="title">pkg</span>-<span class="title">config</span></span></span><br><span class="line"><span class="function"><span class="title">set</span> <span class="title">GOGCCFLAGS</span>=-<span class="title">m64</span> -<span class="title">mthreads</span> -<span class="title">fno</span>-<span class="title">caret</span>-<span class="title">diagnostics</span> -<span class="title">Qunused</span>-<span class="title">arguments</span> -<span class="title">fmessag</span></span></span><br><span class="line"><span class="function"><span class="title">e</span>-<span class="title">length</span>=0 -<span class="title">fdebug</span>-<span class="title">prefix</span>-<span class="title">map</span>=<span class="title">C</span>:\<span class="title">Users</span>\<span class="title">RONGXI</span>~1\<span class="title">AppData</span>\<span class="title">Local</span>\<span class="title">Temp</span>\<span class="title">go</span>-<span class="title">build03096</span></span></span><br><span class="line"><span class="function">1398=/<span class="title">tmp</span>/<span class="title">go</span>-<span class="title">build</span> -<span class="title">gno</span>-<span class="title">record</span>-<span class="title">gcc</span>-<span class="title">switches</span></span></span><br></pre></td></tr></table></figure><p>查看版本：</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">C:\<span class="title">Users</span>\<span class="title">rongxiang</span>&gt;<span class="title">go</span> <span class="title">version</span></span></span><br><span class="line"><span class="function"><span class="title">go</span> <span class="title">version</span> <span class="title">go1</span>.12.5 <span class="title">windows</span>/<span class="title">amd64</span></span></span><br></pre></td></tr></table></figure><p>默认情况下，GOROOT = C:\Go；GOPATH = C:\Users\用户名\go。如果需要调整，修改环境变量参数即可。</p><h2 id="第三部分-IDE配置"><a href="#第三部分-IDE配置" class="headerlink" title="第三部分 IDE配置"></a>第三部分 IDE配置</h2><p>使用Jetbrain公司的GoLand IDE（<a href="https://www.jetbrains.com/go/）。" target="_blank" rel="noopener">https://www.jetbrains.com/go/）。</a></p><p>下载安装成功后，打来GoLand。菜单File–&gt;Settings–&gt;GO中有两个配置项：GOROOT、GOPATH。</p><p><img src="\images\picture\goland\GoLand ide.PNG" alt=""></p><h2 id="第四部分-HelloWorld案例"><a href="#第四部分-HelloWorld案例" class="headerlink" title="第四部分   HelloWorld案例"></a>第四部分   HelloWorld案例</h2><p>配置好IDE环境，我们新建第一个项目（project）。</p><h3 id="4-1-创建HelloWorld项目"><a href="#4-1-创建HelloWorld项目" class="headerlink" title="4.1 创建HelloWorld项目"></a>4.1 创建HelloWorld项目</h3><p>菜单栏File–&gt;New–&gt;Project,打开新建项目对话框。配置项目的文件位置（Location），例如我们配置为：</p><p>D:\golang\workspace\HelloWorld，然后确定就新建Go项目。</p><p>在项目中新建main.go文件：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mian</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">"fmt"</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">fmt.Println(<span class="string">"Hello World!"</span>)</span><br></pre></td></tr></table></figure><h3 id="4-2-编译并运行"><a href="#4-2-编译并运行" class="headerlink" title="4.2 编译并运行"></a>4.2 编译并运行</h3><p>选中main.go文件，邮件选择运行。IDE将编译，并运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Hello World!</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure><p>控制台上打印上面的信息，说明执行成功。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Go入门指南，<a href="https://github.com/Unknwon/the-way-to-go_ZH_CN" target="_blank" rel="noopener">https://github.com/Unknwon/the-way-to-go_ZH_CN</a></p><p>2、a tour of go，<a href="https://tour.golang.org/welcome/1" target="_blank" rel="noopener">https://tour.golang.org/welcome/1</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分   windows下安装&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第二部分  
      
    
    </summary>
    
      <category term="Go" scheme="https://zjrongxiang.github.io/categories/Go/"/>
    
    
  </entry>
  
  <entry>
    <title>第一个Spring Boot练习项目</title>
    <link href="https://zjrongxiang.github.io/2020/01/11/2020-01-01-SpringBoot%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88%E7%AC%AC%E4%B8%80%E4%B8%AASpringBoot%E5%AE%9E%E8%B7%B5%E7%BB%83%E4%B9%A0%E9%A1%B9%E7%9B%AE%EF%BC%89/"/>
    <id>https://zjrongxiang.github.io/2020/01/11/2020-01-01-SpringBoot系列文章（第一个SpringBoot实践练习项目）/</id>
    <published>2020-01-11T05:30:00.000Z</published>
    <updated>2020-03-22T03:41:59.540Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分 开发环境准备</li><li>第二部分 使用Maven构建项目</li><li>第三部分 项目目录结构</li><li>第四部分  编写HelloWorld项目</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>第一次使用Spring Boot构建测试项目，实现一个简单的Http请求处理，通过这个例子对Spring Boot有一个初步的了解。</p><h2 id="第一部分-开发环境准备"><a href="#第一部分-开发环境准备" class="headerlink" title="第一部分 开发环境准备"></a>第一部分 开发环境准备</h2><ul><li><p>Maven版本：Maven 3，version 3.3.9</p></li><li><p>Java版本：Java 1.8（Java8）</p></li></ul><h2 id="第二部分-使用Maven构建项目"><a href="#第二部分-使用Maven构建项目" class="headerlink" title="第二部分 使用Maven构建项目"></a>第二部分 使用Maven构建项目</h2><p>Spring官网提供Spring Initializr工具生成项目</p><ul><li>第一步：登录Spring Initializr网站： <a href="https://start.spring.io/" target="_blank" rel="noopener">https://start.spring.io/</a></li><li>第二步：配置项目的参数：</li></ul><p><img src="\images\picture\Spring\initializr.jpg" alt=""></p><blockquote><p>Group ID是项目组织唯一的标识符，实际对应项目中的package包。</p><p>Artifact ID是项目的唯一的标识符，实际对应项目的project name名称，Artifact不可包含大写字母。</p></blockquote><p>然后点击生成项目压缩文件，并下载到本地。</p><ul><li>第三步：使用IDE加载项目（使用IntelliJ IDEA）<ol><li>菜单中选择<code>File</code>–&gt;<code>New</code>–&gt;<code>Project from Existing Sources...</code></li><li>选择解压后的项目文件夹，点击<code>OK</code></li><li>点击<code>Import project from external model</code>并选择<code>Maven</code>，点击<code>Next</code>到底为止。</li></ol></li></ul><h2 id="第三部分-项目目录结构"><a href="#第三部分-项目目录结构" class="headerlink" title="第三部分 项目目录结构"></a>第三部分 项目目录结构</h2><p>开发环境是Win7环境，导入后，使用<code>tree /f</code>命令查看项目结构目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> tree /f</span></span><br><span class="line">│  .gitignore</span><br><span class="line">│  HELP.md</span><br><span class="line">│  mvnw</span><br><span class="line">│  mvnw.cmd</span><br><span class="line">│  pom.xml</span><br><span class="line">├─.idea</span><br><span class="line">│  │  azureSettings.xml</span><br><span class="line">│  │  compiler.xml</span><br><span class="line">│  │  misc.xml</span><br><span class="line">│  │  workspace.xml</span><br><span class="line">│  └─inspectionProfiles</span><br><span class="line">│          Project_Default.xml</span><br><span class="line">├─.mvn</span><br><span class="line">│  └─wrapper</span><br><span class="line">│          maven-wrapper.jar</span><br><span class="line">│          maven-wrapper.properties</span><br><span class="line">│          MavenWrapperDownloader.java</span><br><span class="line">└─src</span><br><span class="line">    ├─main</span><br><span class="line">    │  ├─java</span><br><span class="line">    │  │  └─com</span><br><span class="line">    │  │      └─example</span><br><span class="line">    │  │          └─demo</span><br><span class="line">    │  │                  DemoApplication.java</span><br><span class="line">    │  └─resources</span><br><span class="line">    │          application.properties</span><br><span class="line">    └─test</span><br><span class="line">        └─java</span><br><span class="line">            └─com</span><br><span class="line">                └─example</span><br><span class="line">                    └─demo</span><br><span class="line">                            DemoApplicationTests.java</span><br></pre></td></tr></table></figure><ul><li><code>.gitignore</code>文件是git控制文件。</li><li><code>mvnw</code>(linux shell)和<code>mvnw.cmd</code>(windows),还有<code>.mvn</code>文件夹(包含Maven Wrapper Java库及其属性文件)。mvnw全名是Maven Wrapper,它的原理是在maven-wrapper.properties文件中记录你要使用的Maven版本，当用户执行mvnw clean 命令时，发现当前用户的Maven版本和期望的版本不一致，那么就下载期望的版本，然后用期望的版本来执行mvn命令，比如刚才的mvn clean。带有mvnw文件项目，只要有java环境，仅仅通过使用本项目的mvnw脚本就可以完成编译，打包，发布等一系列操作。</li><li><code>HELP.md</code>Maven的帮助文件。</li><li><code>pom.xml</code>Project Object Model 的缩写，即项目对象模型。maven 的配置文件，用以描述项目的各种信息。</li><li><p><code>.idea/</code>文件夹来存放项目的配置信息。其中包括版本控制信息、历史记录等等。</p></li><li><p><code>src/main/java</code>下的程序入口：<code>DemoApplication.java</code>。</p></li><li><code>src/main/resources</code>下的配置文件：<code>application.properties</code>。</li><li><code>src/test/</code>下的测试入口：<code>DemoApplicationTests.java</code>。</li></ul><h2 id="第四部分-编写HelloWorld项目"><a href="#第四部分-编写HelloWorld项目" class="headerlink" title="第四部分 编写HelloWorld项目"></a>第四部分 编写HelloWorld项目</h2><p>引入Web模块，在pom.xml文件添加下面的依赖包：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>新建controller程序(HelloSpringBootController)：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.demo.controller;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloSpringBootController</span> </span>&#123;</span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">hello</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Hello, SpringBoot!"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>完成后项目的结构如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">├─src</span><br><span class="line">│  ├─main</span><br><span class="line">│  │  ├─java</span><br><span class="line">│  │  │  └─com</span><br><span class="line">│  │  │      └─example</span><br><span class="line">│  │  │          └─demo</span><br><span class="line">│  │  │              │  DemoApplication.java</span><br><span class="line">│  │  │              │</span><br><span class="line">│  │  │              └─controller</span><br><span class="line">│  │  │                      HelloSpringBootController.java</span><br><span class="line">│  │  │</span><br><span class="line">│  │  └─resources</span><br><span class="line">│  │          application.properties</span><br><span class="line">│  │</span><br><span class="line">│  └─test</span><br><span class="line">│      └─java</span><br><span class="line">│          └─com</span><br><span class="line">             └─example</span><br><span class="line">                 └─demo</span><br><span class="line">                          DemoApplicationTests.java</span><br></pre></td></tr></table></figure><p>最后使用maven编译：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mvn clean  </span><br><span class="line">mvn package  #编译项目</span><br></pre></td></tr></table></figure><p>会生成编译结果，在项目根目录生成target文件目录。其中<code>demo-0.0.1-SNAPSHOT.jar</code>为编译后的入口程序。在IDEA中，或者使用<code>java -jar demo-0.0.1-SNAPSHOT.jar</code>命令在win CMD命令窗口中运行。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">"C:\Program Files\Java\jdk-10.0.2\bin\java.exe" -Dfile.encoding=GBK -jar C:\Users\rongxiang\Desktop\SpringBoot\demo\target\demo-0.0.1-SNAPSHOT.jar</span><br><span class="line"></span><br><span class="line">  .   ____          _            __ _ _</span><br><span class="line"> /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \</span><br><span class="line">( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \</span><br><span class="line"> \\/  ___)| |_)| | | | | || (_| |  ) ) ) )</span><br><span class="line">  '  |____| .__|_| |_|_| |_\__, | / / / /</span><br><span class="line"> =========|_|==============|___/=/_/_/_/</span><br><span class="line"> :: Spring Boot ::        (v2.2.2.RELEASE)</span><br><span class="line"></span><br><span class="line">2020-01-12 13:56:37.333  INFO 5060 --- [           main] com.example.demo.DemoApplication         : Starting DemoApplication v0.0.1-SNAPSHOT on rongxiang-PC with PID 5060 (C:\Users\rongxiang\Desktop\SpringBoot\demo\target\demo-0.0.1-SNAPSHOT.jar started by rongxiang in C:\Users\rongxiang\Desktop\SpringBoot\demo)</span><br><span class="line">2020-01-12 13:56:37.338  INFO 5060 --- [           main] com.example.demo.DemoApplication         : No active profile set, falling back to default profiles: default</span><br><span class="line">2020-01-12 13:56:40.479  INFO 5060 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)</span><br><span class="line">2020-01-12 13:56:40.497  INFO 5060 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]</span><br><span class="line">2020-01-12 13:56:40.497  INFO 5060 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.29]</span><br><span class="line">2020-01-12 13:56:40.598  INFO 5060 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext</span><br><span class="line">2020-01-12 13:56:40.598  INFO 5060 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 3142 ms</span><br><span class="line">2020-01-12 13:56:40.945  INFO 5060 --- [           main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'</span><br><span class="line">2020-01-12 13:56:41.227  INFO 5060 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''</span><br><span class="line">2020-01-12 13:56:41.233  INFO 5060 --- [           main] com.example.demo.DemoApplication         : Started DemoApplication in 4.773 seconds (JVM running for 5.475)</span><br></pre></td></tr></table></figure><p>在本地启了一个web服务（Tomcat started on port(s): 8080 (http)），对外服务端口为8080。</p><p>浏览器中输入url地址（<a href="http://localhost:8080/），网页显示“Hello" target="_blank" rel="noopener">http://localhost:8080/），网页显示“Hello</a>, SpringBoot!”，说明服务服务正常。</p><p>注意：</p><blockquote><p>如果运行工程，出现这个报错信息：Failed to clean project: Failed to delete</p><p>由于之前编译的工程还在运行，无法clean，导致maven生命周期无法继续进行。即由于已启动了另一个tomcat 进程，导致报错,关闭tomcat进程即可。可以在程序控制台中终止该进程即可。</p></blockquote><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Spring官网，链接：<a href="https://spring.io/" target="_blank" rel="noopener">https://spring.io/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分 开发环境准备&lt;/li&gt;
&lt;li&gt;第二部分 使用Maven构建项目&lt;/li&gt;
&lt;li&gt;第三部分 
      
    
    </summary>
    
      <category term="Spring" scheme="https://zjrongxiang.github.io/categories/Spring/"/>
    
    
  </entry>
  
  <entry>
    <title>外网环境访问内网（NAT）Kafka集群介绍</title>
    <link href="https://zjrongxiang.github.io/2019/09/07/2019-09-07-%E5%A4%96%E7%BD%91%E7%8E%AF%E5%A2%83%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%EF%BC%88NAT%EF%BC%89Kafka%E9%9B%86%E7%BE%A4%E4%BB%8B%E7%BB%8D/"/>
    <id>https://zjrongxiang.github.io/2019/09/07/2019-09-07-外网环境访问内网（NAT）Kafka集群介绍/</id>
    <published>2019-09-07T05:30:00.000Z</published>
    <updated>2020-06-06T01:24:14.475Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   Kafka几个配置参数介绍</li><li>第二部分   外网环境访问内网（NAT）Kafka集群配置</li><li>第三部分  总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>其实这是一个踩坑笔记。首先介绍踩坑背景。生产环境我们有两个网络区域，记为网络区域A（内网）、网络区域B（外网），其中为了外网环境能访问内网环境，内网对内部IP实施了IP映射（NAT），将内网IP映射为外部IP。Kafka版本为：kafka_2.11-0.10.2.0。IP清单及网络数据流图如下：</p><table><thead><tr><th style="text-align:center">hostname</th><th style="text-align:center">内网ip</th><th style="text-align:center">外网Ip</th></tr></thead><tbody><tr><td style="text-align:center">kafka.node1</td><td style="text-align:center">192.168.1.1</td><td style="text-align:center">10.0.0.1</td></tr><tr><td style="text-align:center">kafka.node2</td><td style="text-align:center">192.168.1.2</td><td style="text-align:center">10.0.0.2</td></tr><tr><td style="text-align:center">kafka.node3</td><td style="text-align:center">192.168.1.3</td><td style="text-align:center">10.0.0.3</td></tr></tbody></table><p>整个架构图为：</p><p><img src="\images\picture\kafka\kafkaNat.png" alt=""></p><p>外网网络区域的客户端开始使用NAT地址（10.0.0.1-3）地址访问内部kafka，发现无法生产和消费kafka数据（telnet netip 9092是通的），会报解析服务器hostname失败的错误。而内部网络的客户端使用内网地址（192.168.1.1-3）是可以正常生产和消费kafka数据。</p><p>原因：advertised.listeners配置的是内网实地址，这个地址注册到Zookeeper中，当消费者和生产者访问时，Zookeeper将该地址提供给消费者和生产者。由于是内网地址，外网根本看不到这个地址（路由寻址）。所以无法获取元数据信息，通信异常。</p><h2 id="第一部分-Kafka几个配置参数介绍"><a href="#第一部分-Kafka几个配置参数介绍" class="headerlink" title="第一部分 Kafka几个配置参数介绍"></a>第一部分 Kafka几个配置参数介绍</h2><p>首先要了解一下几个配置：</p><ul><li><p>host.name<br><strong>已弃用。</strong> 仅当listeners属性未配置时被使用，已用listeners属性代替。表示broker的hostname。</p></li><li><p>advertised.host.name<br><strong>已弃用</strong>。仅当advertised.listeners或者listeners属性未配置时被使用。官网建议使用advertised.listeners。该配置的意思是注册到zookeeper上的broker的hostname或ip。是提供给客户端与kafka通信使用的。如果没有设置则使用host.name。</p></li><li><p>listeners<br>监听列表，broker对外提供服务时绑定的IP和端口。多个以逗号隔开，如果监听器名称是一个安全的协议， listener.security.protocol.map也必须设置。主机名称设置0.0.0.0绑定所有的接口，主机名称为空则绑定默认的接口。如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">listeners = PLAINTEXT://myhost:9092,SSL://:9091 CLIENT://0.0.0.0:9092,REPLICATION://localhost:9093</span><br></pre></td></tr></table></figure><p>如果未指定该配置，则使用java.net.InetAddress.getCanonicalHostName()函数的的返回值。</p></li><li><p>advertised.listeners<br>客户端使用。发布至zookeeper的监听，broker会上送此地址到zookeeper，zookeeper会将此地址提供给消费者和生产者，消费者和生产者根据此地址获取消息。如果和上面的listeners不同则以此为准，在IaaS环境，此配置项可能和 broker绑定的接口主机名称不同，如果此配置项没有配置则以上面的listeners为准。</p></li></ul><h2 id="第二部分-外网环境访问内网（NAT）Kafka集群配置"><a href="#第二部分-外网环境访问内网（NAT）Kafka集群配置" class="headerlink" title="第二部分 外网环境访问内网（NAT）Kafka集群配置"></a>第二部分 外网环境访问内网（NAT）Kafka集群配置</h2><h3 id="2-1-配置hosts方式"><a href="#2-1-配置hosts方式" class="headerlink" title="2.1 配置hosts方式"></a>2.1 配置hosts方式</h3><ul><li><p>Kafka集群节点配置</p><p>每一台Kafka节点的hosts节点配置内部地址映射：</p><p>192.168.1.1 kafka.node1<br>192.168.1.2 kafka.node2<br>192.168.1.3 kafka.node3</p><p>Kafka中的配置文件（config/server.properties配置文件）</p><p>advertised.listeners=PLAINTEXT://kafka.node1:9092</p><p>advertised.listeners=PLAINTEXT://kafka.node2:9092</p><p>advertised.listeners=PLAINTEXT://kafka.node3:9092</p></li><li><p>客户端节点配置</p><p>客户端的hosts文件也需要配置外部地址映射：</p><p>10.0.0.1 kafka.node1<br>10.0.0.2 kafka.node2<br>10.0.0.3 kafka.node3</p><p>应用程序使用</p><p>bootstrap.servers:  [‘kafka.node1:9092’,’kafka.node2:9092’,’kafka.node3:9092’]</p></li></ul><p>配置完成后，重启Kafka集群，重新使用客户端链接，测试客户端可以正常向Topic生产和消费数据。</p><h3 id="2-2-内外部流量分离"><a href="#2-2-内外部流量分离" class="headerlink" title="2.2 内外部流量分离"></a>2.2 内外部流量分离</h3><p>通常对于外部网络访问内网安全区域，架构使用安全套接字层 (SSL) 来保护外部客户端与 Kafka 之间的流量。而使用明文进行内部网络的broker间的通信。当 Kafka 侦听器绑定到用于内部和外部通信的网络接口时，配置侦听器就非常简单了。但在许多情况下，例如在云原生环境上部署时，集群中 Kafka broker 的外部通告地址将与 Kafka 使用的内部网络接口不同。在此情况下，可以 <code>server.properties</code> 中的参数 <code>advertised.listeners</code> 进行如下配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Configure protocol map</span></span><br><span class="line">listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:SSL</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use plaintext <span class="keyword">for</span> inter-broker communication</span></span><br><span class="line">inter.broker.listener.name=INTERNAL</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Specify that Kafka listeners should <span class="built_in">bind</span> to all <span class="built_in">local</span> interfaces</span></span><br><span class="line">listeners=INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Separately, specify externally visible address</span></span><br><span class="line">advertised.listeners=INTERNAL://kafkabroker-n.mydomain.com:9092,EXTERNAL://kafkabroker-n.mydomain.com:9093</span><br></pre></td></tr></table></figure><p>内部使用9092端口，而外部网络使用9093端口。</p><h2 id="第三部分-总结"><a href="#第三部分-总结" class="headerlink" title="第三部分 总结"></a>第三部分 总结</h2><p>Kafka集群在内外网网络环境下，需要关注地址映射。使用hosts 本地DNS进行主机名和内外地址的映射。至此爬出该坑。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、kafka - advertised.listeners和listeners，链接：<a href="https://www.cnblogs.com/fxjwind/p/6225909.html" target="_blank" rel="noopener">https://www.cnblogs.com/fxjwind/p/6225909.html</a></p><p>2、Kafka从上手到实践-Kafka集群：Kafka Listeners，链接：<a href="http://www.devtalking.com/articles/kafka-practice-16/" target="_blank" rel="noopener">http://www.devtalking.com/articles/kafka-practice-16/</a></p><p>3、使用 Cloud Dataflow 处理来自 Kafka 的外部托管消息 链接：<a href="https://cloud.google.com/solutions/processing-messages-from-kafka-hosted-outside-gcp?hl=zh-cn" target="_blank" rel="noopener">https://cloud.google.com/solutions/processing-messages-from-kafka-hosted-outside-gcp?hl=zh-cn</a></p><p>4、Kafka Listeners - Explained 链接：<a href="https://rmoff.net/2018/08/02/kafka-listeners-explained/" target="_blank" rel="noopener">https://rmoff.net/2018/08/02/kafka-listeners-explained/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   Kafka几个配置参数介绍&lt;/li&gt;
&lt;li&gt;第二部分   外网环境访问内网（NAT）Ka
      
    
    </summary>
    
      <category term="Kafka" scheme="https://zjrongxiang.github.io/categories/Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>Helm简介和安装部署</title>
    <link href="https://zjrongxiang.github.io/2019/08/08/2019-08-08-Helm%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    <id>https://zjrongxiang.github.io/2019/08/08/2019-08-08-Helm简介和安装部署/</id>
    <published>2019-08-08T13:30:00.000Z</published>
    <updated>2020-03-22T03:41:34.172Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   Minikube集群启动</li><li>第一部分   Kubernetes中StatefulSet介绍</li><li>第三部分   部署Zookeeper集群</li><li>第四部分   部署Kafka集群</li><li>第五部分   总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Application deployment management for Kubernetes。</p><h2 id="第一部分-Helm的部署"><a href="#第一部分-Helm的部署" class="headerlink" title="第一部分 Helm的部署"></a>第一部分 Helm的部署</h2><p>Helm是由helm CLI和Tiller组成，是典型的C/S应用。helm运行与客户端，提供命令行界面，而Tiller应用运行在Kubernetes内部。</p><h3 id="1-1-部署Helm-CLI客户端"><a href="#1-1-部署Helm-CLI客户端" class="headerlink" title="1.1 部署Helm CLI客户端"></a>1.1 部署Helm CLI客户端</h3><p>helm客户端是一个单纯的可执行文件，我们从github上直接下载压缩包（由于墙的原因可能很慢）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm# wget https://get.helm.sh/helm-v2.14.3-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>解压缩介质文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm# tar -zxvf helm-v2.14.3-linux-amd64.tar.gz </span><br><span class="line">linux-amd64/</span><br><span class="line">linux-amd64/helm</span><br><span class="line">linux-amd64/README.md</span><br><span class="line">linux-amd64/LICENSE</span><br><span class="line">linux-amd64/tiller</span><br></pre></td></tr></table></figure><p>部署：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm# mv linux-amd64/helm /usr/local/bin</span><br></pre></td></tr></table></figure><p>检查：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm# helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:"v2.14.3", GitCommit:"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085", GitTreeState:"clean"&#125;</span><br><span class="line">Error: could not find tiller</span><br></pre></td></tr></table></figure><p>提示未连接到服务端tiller。下面我们部署服务端。</p><h3 id="1-2-部署服务端"><a href="#1-2-部署服务端" class="headerlink" title="1.2 部署服务端"></a>1.2 部署服务端</h3><p>Helm 的服务器端部分 Tiller 通常运行在 Kubernetes 集群内部。但是对于开发，它也可以在本地运行，并配置为与远程 Kubernetes 群集通信。</p><p>安装 <code>tiller</code> 到群集中最简单的方法就是运行 <code>helm init</code>。这将验证 <code>helm</code> 本地环境设置是否正确（并在必要时进行设置）。然后它会连接到 <code>kubectl</code> 默认连接的任何集群（<code>kubectl config view</code>）。一旦连接，它将安装 <code>tiller</code> 到 <code>kube-system</code> 命名空间中。</p><p><code>helm init</code> 以后，可以运行 <code>kubectl get pods --namespace kube-system</code> 并看到 Tiller 正在运行。</p><p>你可以通过参数运行 <code>helm init</code>:</p><ul><li><code>--canary-image</code> 参数安装金丝雀版本</li><li><code>--tiller-image</code> 安装特定的镜像（版本）</li><li><code>--kube-context</code> 使用安装到特定群集</li><li><code>--tiller-namespace</code> 用一个特定的命名空间 (namespace) 安装</li><li><code>--service-account</code> 使用 Service Account 安装 <a href="https://whmzsu.github.io/helm-doc-zh-cn/quickstart/securing_installation-zh_cn.html#rbac" target="_blank" rel="noopener">RBAC enabled clusters</a></li><li><code>--automount-service-account false</code> 不适用 service account 安装</li></ul><p>一旦安装了 Tiller，运行 <code>helm version</code> 会显示客户端和服务器版本。（如果它仅显示客户端版本， helm 则无法连接到服务器, 使用 <code>kubectl</code> 查看是否有任何 tiller Pod 正在运行。）</p><p>除非设置 <code>--tiller-namespace</code> 或 <code>TILLER_NAMESPACE</code> 参数，否则 Helm 将在命名空间 <code>kube-system</code> 中查找 Tiller 。</p><blockquote><p>在缺省配置下， Helm 会利用 “<a href="http://gcr.io/kubernetes-helm/tiller" target="_blank" rel="noopener">gcr.io/kubernetes-helm/tiller</a>“ 镜像在Kubernetes集群上安装配置 Tiller；并且利用 “<a href="https://kubernetes-charts.storage.googleapis.com" target="_blank" rel="noopener">https://kubernetes-charts.storage.googleapis.com</a>“ 作为缺省的 stable repository 的地址。由于在国内可能无法访问 “<a href="http://gcr.io" target="_blank" rel="noopener">gcr.io</a>“, “<a href="http://storage.googleapis.com" target="_blank" rel="noopener">storage.googleapis.com</a>“ 等域名，阿里云容器服务为此提供了镜像站点。</p></blockquote><p>首先创建服务。创建rbac-config.yaml文件，文件内容为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">cluster-admin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">  - kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">    namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure><p>通过yaml文件创建服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm# kubectl create -f rbac-config.yaml</span><br><span class="line">serviceaccount/tiller created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/tiller created</span><br></pre></td></tr></table></figure><p>启Helm pod，即安装tiller：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm#helm init -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.10.0 \</span><br><span class="line">    --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts \</span><br><span class="line">    --service-account tiller </span><br><span class="line"><span class="meta">#</span><span class="bash"> 回显</span></span><br><span class="line">Creating /root/.helm </span><br><span class="line">Creating /root/.helm/repository </span><br><span class="line">Creating /root/.helm/repository/cache </span><br><span class="line">Creating /root/.helm/repository/local </span><br><span class="line">Creating /root/.helm/plugins </span><br><span class="line">Creating /root/.helm/starters </span><br><span class="line">Creating /root/.helm/cache/archive </span><br><span class="line">Creating /root/.helm/repository/repositories.yaml </span><br><span class="line">Adding stable repo with URL: https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts </span><br><span class="line">Adding local repo with URL: http://127.0.0.1:8879/charts </span><br><span class="line"><span class="meta">$</span><span class="bash">HELM_HOME has been configured at /root/.helm.</span></span><br><span class="line">Warning: Tiller is already installed in the cluster.</span><br><span class="line">(Use --client-only to suppress this message, or --upgrade to upgrade Tiller to the current version.)</span><br></pre></td></tr></table></figure><p>这时候我们再次执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm# helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:"v2.14.3", GitCommit:"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085", GitTreeState:"clean"&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:"v2.10.0", GitCommit:"9ad53aac42165a5fadc6c87be0dea6b115f93090", GitTreeState:"clean"&#125;</span><br></pre></td></tr></table></figure><p>这样Helm的服务端tiller就部署完毕。</p><h3 id="1-3-Helm-CLI-命令简要汇总"><a href="#1-3-Helm-CLI-命令简要汇总" class="headerlink" title="1.3 Helm CLI 命令简要汇总"></a>1.3 Helm CLI 命令简要汇总</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// 安装一个 Chart</span><br><span class="line">helm install stable/mysql</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 列出 Kubernetes 中已部署的 Chart</span><br><span class="line">helm list --all</span><br><span class="line"></span><br><span class="line">// helm repo 的操作</span><br><span class="line">helm repo update</span><br><span class="line">helm repo list</span><br><span class="line">helm repo add dev https://example.com/dev-charts</span><br><span class="line"></span><br><span class="line">// 创建一个 Chart，会产生一个 Chart 所需的目录结构</span><br><span class="line">helm create deis-workflow</span><br><span class="line"></span><br><span class="line">// 安装自定义 chart</span><br><span class="line">helm inspect values stable/mysql  # 列出一个 chart 的可配置项</span><br><span class="line"></span><br><span class="line">helm install -f config.yaml stable/mysql # 可以将修改的配置项写到文件中通过 -f 指定并替换</span><br><span class="line">helm install --set name: value stable/mysql # 也可以通过 --set 方式替换</span><br><span class="line"></span><br><span class="line">// 当新版本 chart 发布时，或者当你需要更改 release 配置时，helm 必须根据现在已有的 release 进行升级</span><br><span class="line">helm upgrade -f panda.yaml happy-panda stable/mariadb</span><br><span class="line"></span><br><span class="line">// 删除 release</span><br><span class="line">helm delete happy-panda</span><br></pre></td></tr></table></figure><h2 id="参考文献及材料"><a href="#参考文献及材料" class="headerlink" title="参考文献及材料"></a>参考文献及材料</h2><p>1、Helm User Guide - Helm 用户指南 <a href="https://whmzsu.github.io/helm-doc-zh-cn/" target="_blank" rel="noopener">https://whmzsu.github.io/helm-doc-zh-cn/</a></p><p>2、Kubernetes 包管理工具 Helm 简介 <a href="https://www.jianshu.com/p/d55e91e28f94" target="_blank" rel="noopener">https://www.jianshu.com/p/d55e91e28f94</a></p><p>3、Helm介绍 <a href="https://zhaohuabing.com/2018/04/16/using-helm-to-deploy-to-kubernetes/" target="_blank" rel="noopener">https://zhaohuabing.com/2018/04/16/using-helm-to-deploy-to-kubernetes/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   Minikube集群启动&lt;/li&gt;
&lt;li&gt;第一部分   Kubernetes中State
      
    
    </summary>
    
      <category term="Helm" scheme="https://zjrongxiang.github.io/categories/Helm/"/>
    
    
  </entry>
  
  <entry>
    <title>从Spark on Yarn到Python on Yarn</title>
    <link href="https://zjrongxiang.github.io/2019/08/01/2019-01-28-%E4%BB%8ESpark%20on%20Yarn%E5%88%B0Python%20on%20Yarn/"/>
    <id>https://zjrongxiang.github.io/2019/08/01/2019-01-28-从Spark on Yarn到Python on Yarn/</id>
    <published>2019-08-01T11:30:00.000Z</published>
    <updated>2020-03-22T03:40:17.610Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>术语说明</li><li>背景</li><li>第一部分   Apache Spark运行模式介绍</li><li>第二部分   Spark on Yarn</li><li>第三部分   Pyspark Application原理</li><li>第四部分   Python on Yarn配置及运行</li><li>第五部分   总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Apache Spark属于重要的大数据计算框架，另外spark还提供了Python的原生API和机器学习组件Spark Ml，使的可以通过Python编写机器学习任务由Spark运行。本篇文件从Spark运行模式开始讲起，重点介绍Spark on Yarn运行模式，最后重点介绍Python on Yarn（即Pyspark on Yarn）上运行原理和案例。</p><h2 id="第一部分-Apache-Spark运行模式"><a href="#第一部分-Apache-Spark运行模式" class="headerlink" title="第一部分 Apache Spark运行模式"></a>第一部分 Apache Spark运行模式</h2><p>目前 Apache Spark已知支持5种运行模式。按照节点资源数量可以分为单节点模式（2种）和集群模式（3种）。</p><ul><li>单节点模式：本地模式、本地伪集群模式</li><li>集群模式：Standalone模式、Spark on Yarn模式、Spark on Mesos模式</li><li>原生云模式：在Kubernetes上运行。随着Docker容器和原生云技术的兴起，Spark开始支持在Kubernetes上运行。</li></ul><blockquote><p>对于Spark on Kubernetes可以参考官方文档：<a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html。另外可以参考我的另外一篇技术总结：《在Minikube上运行Spark集群》。" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/running-on-kubernetes.html。另外可以参考我的另外一篇技术总结：《在Minikube上运行Spark集群》。</a></p></blockquote><h3 id="1-1-本地模式（单节点模式）"><a href="#1-1-本地模式（单节点模式）" class="headerlink" title="1.1 本地模式（单节点模式）"></a>1.1 本地模式（单节点模式）</h3><p>本地模式又称为Loacl[N]模式。该模式只需要在单节点上解压spark包即可运行，使用多个线程模拟Spark分布式计算，Master和Worker运行在同一个JVM虚拟机中。这里参数N代表可以使用（预申请）N个线程资源，每个线程拥有一个Core（默认值N=1）。</p><blockquote><p>如果参数为：Loacl[*]，表明：Run Spark locally with as many worker threads as logical cores on your machine。即线程数和物理核数相同。</p></blockquote><p>例如下面的启动命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ./spark-submit –class org.apache.spark.examples.JavaWordCount –master <span class="built_in">local</span>[*] spark-examples_2.11-2.3.1.jar file:///opt/README.md</span></span><br></pre></td></tr></table></figure><blockquote><p>该模式不依懒于HDFS分布式文件系统。例如上面的命令使用的本地文件系统。</p></blockquote><h3 id="1-2-本地伪集群模式（单节点模式）"><a href="#1-2-本地伪集群模式（单节点模式）" class="headerlink" title="1.2 本地伪集群模式（单节点模式）"></a>1.2 本地伪集群模式（单节点模式）</h3><p>该模式和Local[N]类似，不同的是，它会在单机启动多个进程来模拟集群下的分布式场景，而不像Local[N]这种多个线程在一个进程下共享资源。通常用来测试和验证应用程序逻辑上有没有问题，或者想使用Spark的计算框架而而受限于没有太多资源。</p><p>作业提交命令中使用local-cluster[x,y,z]参数模式：x代表要生成的executor数，y和z分别代表每个executor所拥有的core和memory数值。例如下面的命令作业申请了2个executor 进程，每个进程分配3个core和1G的内存，来运行应用程序。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ./spark-submit –master <span class="built_in">local</span>-cluster[2, 3, 1024]</span></span><br></pre></td></tr></table></figure><h3 id="1-3-Standalone模式（集群模式）"><a href="#1-3-Standalone模式（集群模式）" class="headerlink" title="1.3 Standalone模式（集群模式）"></a>1.3 Standalone模式（集群模式）</h3><h4 id="1-3-1-构架部署"><a href="#1-3-1-构架部署" class="headerlink" title="1.3.1 构架部署"></a>1.3.1 构架部署</h4><p>Standalone为spark自带资源管理系统（即经典的Master/Slaves架构模式）。该模式下集群由Master和Worker节点组成，程序通过与Master节点交互申请资源，Worker节点启动Executor运行。具体数据流图如下：</p><p><img src="\images\picture\python-on-yarn\spark.bmp" alt=""></p><p>另外考虑到Master节点存在单点故障。Spark支持使用Zookeeper实现HA高可用（high avalible）。Zookeeper提供一种领导选举的机制，通过该机制可以保证集群中只有一个Master节点处于RecoveryState.Active状态，其他Master节点处于RecoveryState.Standby状态。</p><p><img src="\images\picture\python-on-yarn\zk-spark.bmp" alt=""></p><h4 id="1-3-2-作业运行模式"><a href="#1-3-2-作业运行模式" class="headerlink" title="1.3.2 作业运行模式"></a>1.3.2 作业运行模式</h4><p>在该模式下，用户提交任务有两种方式：Standalone-client和Standalone-cluster。</p><h4 id="1-5-1-Client模式"><a href="#1-5-1-Client模式" class="headerlink" title="1.5.1 Client模式"></a>1.5.1 Client模式</h4><p>执行流程：</p><p>(1)客户端启动Driver进程。</p><p>(2)Driver向Master申请启动Application启动需要的资源。</p><p>(3)资源申请成功后，Driver将task发送到相应的Worker节点执行，并负责监控task运行情况。</p><p>(4)Worker将task执行结果返回到客户端的Driver进程。</p><blockquote><p>Client模式适用于调试程序。Driver进程在客户端侧启动，如果生产采用这种模式，当业务量较大时，客户端需要启动大量Driver进程，会消耗大量系统资源，导致资源枯竭。</p></blockquote><h4 id="1-5-2-Cluster模式"><a href="#1-5-2-Cluster模式" class="headerlink" title="1.5.2 Cluster模式"></a>1.5.2 Cluster模式</h4><p>执行流程：</p><p>(1)客户端会想Master节点申请启动Driver。</p><p>(2)Master受理客户端的请求，分配一个Work节点，启动Driver进程。</p><p>(3)Driver启动后，重新想Master节点申请运行资源，Master分配资源，并在相应的Worker节点上启动Executor进程。</p><p>(4)Driver发送task到相应的Worker节点运行，并负责监控task。</p><p>(5)Worker将task执行结果返回到Driver进程。</p><blockquote><p>Driver运行有Master在集群Worker节点上随机分配，相当于在集群上负载资源。</p></blockquote><p>两种方式最大的区别就是Driver进程运行的位置。Cluster模式相对于Client模式更适合于生成环境的部署。</p><h3 id="1-4-Spark-on-Yarn（集群模式）"><a href="#1-4-Spark-on-Yarn（集群模式）" class="headerlink" title="1.4 Spark on Yarn（集群模式）"></a>1.4 Spark on Yarn（集群模式）</h3><p>目前大部分企业级Spark都是跑在已有的Hadoop集群（hadoop 2.0系统）中，均使用Yarn来作为Spark的Cluster Manager，为Spark提供资源管理服务，Spark自身完成任务调度和计算。这部分内容会在后文中细致介绍。</p><h3 id="1-5-Spark-on-Mesos（集群模式）"><a href="#1-5-Spark-on-Mesos（集群模式）" class="headerlink" title="1.5 Spark on Mesos（集群模式）"></a>1.5 Spark on Mesos（集群模式）</h3><p>参考官方文档介绍：<a href="https://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/running-on-mesos.html</a></p><h2 id="第二部分-Spark-on-Yarn"><a href="#第二部分-Spark-on-Yarn" class="headerlink" title="第二部分 Spark on Yarn"></a>第二部分 Spark on Yarn</h2><p>我们知道MapReduce任务是运行在Yarn上的，同样Spark Application也可以运行在Yarn上。这种模式下，资源的管理、协调、执行和监控交给Yarn集群完成。</p><blockquote><p>Yarn集群上可以运行：MapReduce任务、Spark Application、Hbase集群、Storm集群、Flink集群等等，还有我们后续重点介绍的Python on Yarn。</p></blockquote><p>从节点功能上看，Yarn也采用类似Standalone模式的Master/Slave结构。资源框架中RM（ResourceManager）对应Master，NM（NodeManager）对应Slave。RM负责各个NM资源的统一管理和调度，NM节点负责启动和执行任务以及各任务间的资源隔离。</p><p>当集群中存在多种计算框架时，架构上选用Yarn统一管理资源要比Standalone更合适。类似Standalone模式，Spark on Yarn也有两种运行方式：Yarn-Client模式和Yarn-Cluster模式。从适用场景上看，Yarn-Cluster模式适用于生产环境，而Yarn-Client模式更适用于开发（交互式调试）。</p><h3 id="2-1-Client模式"><a href="#2-1-Client模式" class="headerlink" title="2.1 Client模式"></a>2.1 Client模式</h3><p>在Yarn-client模式下，Driver运行在本地Client上，通过AM（ApplicationMaster）向RM申请资源。本地Driver负责与所有的executor container进行交互，并将最后的结果汇总。结束掉Client，相当于kill掉这个spark应用。</p><p><img src="\images\picture\python-on-yarn\yarn-client-mode.png" alt=""></p><ul><li>Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend。</li><li>ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派。</li><li>Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）。</li><li>一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task。</li><li>client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。</li><li>应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。</li></ul><h3 id="2-2-Cluster模式"><a href="#2-2-Cluster模式" class="headerlink" title="2.2 Cluster模式"></a>2.2 Cluster模式</h3><p>在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：</p><ol><li>第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动。</li><li>第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成。</li></ol><p>应用的运行结果不能在客户端显示（可以在history server中查看），所以最好将结果保存在HDFS而非stdout输出，客户端的终端显示的是作为YARN的job的简单运行状况，下图是yarn-cluster模式：</p><p><img src="\images\picture\python-on-yarn\yarn-cluster-mode.png" alt=""></p><p>执行过程： </p><ul><li>Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等。</li><li>ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化。</li><li>ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束。</li><li>一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，而Executor对象的创建及维护是由。CoarseGrainedExecutorBackend负责的，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等。</li><li>ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。</li><li>应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。</li></ul><h3 id="2-3-两种模式的比较"><a href="#2-3-两种模式的比较" class="headerlink" title="2.3 两种模式的比较"></a>2.3 两种模式的比较</h3><p>在client模式下，Spark Application运行的Driver会在提交程序的节点上，而该节点可以是YARN集群内部节点，也可以不是。一般来说提交Spark Application的客户端节点不是YARN集群内部的节点，那么在客户端节点上可以根据自己的需要安装各种需要的软件和环境，以支撑Spark Application正常运行。在cluster模式下，Spark Application运行时的所有进程都在YARN集群的NodeManager节点上，而且具体在哪些NodeManager上运行是由YARN的调度策略所决定的。</p><p>对比这两种模式，最关键的是Spark Application运行时Driver所在的节点不同，而且，如果想要对Driver所在节点的运行环境进行配置，区别很大，但这对于PySpark Application运行来说是非常关键的。</p><h2 id="第三部分-Pyspark-Application原理"><a href="#第三部分-Pyspark-Application原理" class="headerlink" title="第三部分 Pyspark Application原理"></a>第三部分 Pyspark Application原理</h2><p>PySpark是Spark为使用Python程序编写Spark Application而实现的客户端库，通过PySpark也可以编写Spark Application并在Spark集群上运行。Python具有非常丰富的科学计算、机器学习处理库，如numpy、pandas、scipy等等。为了能够充分利用这些高效的Python模块，很多机器学习程序都会使用Python实现，同时也希望能够在Spark集群上运行。</p><p>理解PySpark Application的运行原理，有助于我们使用Python编写Spark Application，并能够对PySpark Application进行各种调优。PySpark构建于Spark的Java API之上，数据在Python脚本里面进行处理，而在JVM中缓存和Shuffle数据，数据处理流程如下图所示:</p><p><img src="http://www.uml.org.cn/bigdata/images/2017111321.png" alt="img"></p><p>Spark Application会在Driver中创建pyspark.SparkContext对象，后续通过pyspark.SparkContext对象来构建Job DAG并提交DAG运行。使用Python编写PySpark Application，在Python编写的Driver中也有一个pyspark.SparkContext对象，该pyspark.SparkContext对象会通过Py4J模块启动一个JVM实例，创建一个JavaSparkContext对象。PY4J只用在Driver上，后续在Python程序与JavaSparkContext对象之间的通信，都会通过PY4J模块来实现，而且都是本地通信。</p><p>PySpark Application中也有RDD，对Python RDD的Transformation操作，都会被映射到Java中的PythonRDD对象上。对于远程节点上的Python RDD操作，Java PythonRDD对象会创建一个Python子进程，并基于Pipe的方式与该Python子进程通信，将用户编写Python处理代码和数据发送到Python子进程中进行处理。</p><h2 id="第四部分-Python-on-Yarn配置及运行"><a href="#第四部分-Python-on-Yarn配置及运行" class="headerlink" title="第四部分 Python on Yarn配置及运行"></a>第四部分 Python on Yarn配置及运行</h2><h3 id="4-1-Yarn节点配置Python环境"><a href="#4-1-Yarn节点配置Python环境" class="headerlink" title="4.1 Yarn节点配置Python环境"></a>4.1 Yarn节点配置Python环境</h3><p>该模式需要在Yarn集群上每个NM节点（Node Manager）上部署Python编译环境，即安装Python安装包、依赖模块。用户编写的Pyspark Application由集群中Yarn调度执行。</p><blockquote><p>通常使用Anaconda安装包进行统一部署，简化环境的部署。</p></blockquote><p>该模式存在下面缺点：</p><ul><li>新增依赖包部署安装代价大。如果后续用户编写的Spark Application需要依赖新的Python模块或包，那么就需要依次在集群Node Manager上部署更新依赖包。</li><li>用户对于Python环境的依赖差异化无法满足。通常不同用户编写Spark Application会依赖不同的Python环境，比如Python2、Python3环境等等。该模式下只能支持一种环境，无法满足Python多环境的需求。</li><li>各节点的Python环境需要统一。由于用户提交的Spark Application具体在哪些Node Manager上执行，由YARN调度决定，所以必须保证每个节点的Python环境（基础环境+依赖环境）都是相同的，环境维护成本高。</li></ul><h3 id="4-2-Yarn节点不配置Python环境"><a href="#4-2-Yarn节点不配置Python环境" class="headerlink" title="4.2 Yarn节点不配置Python环境"></a>4.2 Yarn节点不配置Python环境</h3><p>该模式不需要提前在集群Node Manager上预安装Python环境。</p><blockquote><p>参考文章：<a href="http://quasiben.github.io/blog/2016/4/15/conda-spark/" target="_blank" rel="noopener">http://quasiben.github.io/blog/2016/4/15/conda-spark/</a></p></blockquote><p>我们基于华为C60集群（开源集群相同）以及Anaconda环境对该模式进行了测试验证。具体实现思路如下所示：</p><ol><li>在一台SUSE节点上部署Anaconda，并创建虚拟Python环境（如果需要可以部署安装部分依赖包）。</li><li>创建conda虚拟环境，并整体打包为zip文件。</li><li>用户提交PySpark Application时，使用<code>--archives</code>参数指定该zip文件路径。</li></ol><p>详细操作步骤如下：</p><ul><li><strong>第一步</strong></li></ul><p>下载Anaconda3-4.2.0-Linux-x86_64.sh安装软件（基于python3.5），在SUSE服务器上部署安装。Anaconda的安装路径为/usr/anaconda3。查看客户端服务器的python环境清单：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/usr/anaconda3 # conda env list</span><br><span class="line"><span class="meta">#</span><span class="bash"> conda environments:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line">root                  *  /usr/anaconda3</span><br></pre></td></tr></table></figure><p>其中root环境为目前的主环境。为了便于环境版本管理我们新建一个专用环境（mlpy_env）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/usr/anaconda3/envs # conda create -n mlpy --clone root</span><br></pre></td></tr></table></figure><p>上述命令创建了一个名称为mlpy_env的Python环境，clone选项将对应的软件包都安装到该环境中，包括一些C的动态链接库文件。</p><p>接着，将该Python环境打包，执行如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/usr/anaconda3/envs # cd /root/anaconda2/envs</span><br><span class="line">dkfzxwma07app08:/usr/anaconda3/envs # zip -r mlpy_env.zip mlpy_env</span><br></pre></td></tr></table></figure><p>将该zip压缩包拷贝到指定目录中（或者后续引用使用绝对路径），方便后续提交PySpark Application：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/usr/anaconda3/envs # cp mlpy_env.zip /tmp/</span><br></pre></td></tr></table></figure><p>最后，我们可以提交我们的PySpark Application，执行如下命令（或打包成shell脚本）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python </span><br><span class="line">spark-submit \</span><br><span class="line">--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python \</span><br><span class="line">--master yarn-cluster \</span><br><span class="line">--archives /tmp/mlpy_env.zip#ANACONDA \</span><br><span class="line">/var/lib/hadoop-hdfs/pyspark/test_pyspark_dependencies.py</span><br></pre></td></tr></table></figure><blockquote><p>注意：下面命令指的是zip包将在ANACONDA的目录中展开，需要注意路径。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> --archives /tmp/mlpy_env.zip<span class="comment">#ANACONDA</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>环境打包需要注意压缩路径。</p></blockquote><p>上面的依赖zip压缩包将整个Python的运行环境都包含在里面，在提交PySpark Application时会将该环境zip包上传到运行Application的所在的每个节点上。解压缩后为Python代码提供运行时环境。如果不想每次都从客户端将该环境文件上传到集群中运行节点上，也可以提前将zip包上传到HDFS文件系统中，并修改–archives参数的值为hdfs:///tmp/mlpy_env.zip #ANACONDA（注意环境差异），也是可以的。</p><p>另外，需要说明的是，如果我们开发的/var/lib/hadoop-hdfs/pyspark /test_pyspark_dependencies.py文件中，依赖多个其他Python文件，想要通过上面的方式运行，必须将这些依赖的Python文件拷贝到我们创建的环境中，对应的目录为mlpy_env/lib/python2.7/site-packages/下面。</p><blockquote><p>注意：pyspark不支持python3.6版本，所以python环境使用python3.5</p><p>否则程序执行回显会有这样的报错信息：</p><p>TypeError: namedtuple() missing 3 required keyword-only arguments: ‘verbose’, ‘rename’, and ‘module’</p><p><a href="https://issues.apache.org/jira/browse/SPARK-19019?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-19019?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel</a></p></blockquote><h3 id="4-3-一个机器学习任务栗子"><a href="#4-3-一个机器学习任务栗子" class="headerlink" title="4.3 一个机器学习任务栗子"></a>4.3 一个机器学习任务栗子</h3><p>举一个Kmeans无监督算法的Python案例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.clustering <span class="keyword">import</span> KMeans, KMeansModel</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> array</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建spark context</span></span><br><span class="line">sc = SparkContext(appName=<span class="string">"kmeans"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载和解析数据文件</span></span><br><span class="line">stg_path = <span class="string">"hdfs://hacluster"</span> + <span class="string">"/user/"</span> + str(os.environ[<span class="string">'USER'</span>]) + <span class="string">"/.sparkStaging/"</span> + str(sc.applicationId) + <span class="string">"/"</span> </span><br><span class="line">data = sc.textFile(os.path.join(stg_path,<span class="string">'kmeans_data.txt'</span>)) </span><br><span class="line">parsedData = data.map(<span class="keyword">lambda</span> line: array([float(x) <span class="keyword">for</span> x <span class="keyword">in</span> line.split(<span class="string">' '</span>)]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">clusters = KMeans.train(parsedData, <span class="number">2</span>, maxIterations=<span class="number">10</span>,runs=<span class="number">10</span>,initializationMode=<span class="string">"random"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">error</span><span class="params">(point)</span>:</span></span><br><span class="line">    i = clusters.predict(point)</span><br><span class="line">    center = clusters.centers[i]</span><br><span class="line">    print(<span class="string">"("</span> + str(point[<span class="number">0</span>]) + <span class="string">","</span> + str(point[<span class="number">1</span>]) + <span class="string">","</span> + str(point[<span class="number">2</span>]) + <span class="string">")"</span> + <span class="string">"blongs to cluster "</span> + str(i+<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># print("Cluster Number:" + str(len(clusters.centers)))</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum([x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> (point - center)]))</span><br><span class="line"></span><br><span class="line">WSSSE = parsedData.map(<span class="keyword">lambda</span> point: error(point)).reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">print(<span class="string">"Within Set Sum of Squared Error = "</span> + str(WSSSE))</span><br><span class="line"><span class="comment"># 打印类心</span></span><br><span class="line"><span class="keyword">for</span> mCenter <span class="keyword">in</span> clusters.centers:</span><br><span class="line">    print(mCenter)</span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">myModelPath = <span class="string">"hdfs://hacluster"</span>+<span class="string">"/user/model/"</span>+<span class="string">"KMeansModel.ml"</span></span><br><span class="line">clusters.save(sc, myModelPath)</span><br><span class="line"><span class="comment"># 加载模型并测试</span></span><br><span class="line">loadModel = KMeansModel.load(sc, myModelPath)</span><br><span class="line">print(loadModel.predict(array([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])))</span><br></pre></td></tr></table></figure><p>整理成下面的提交命令，将作业提交到Yarn集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/tmp/pyspark # cat run.sh</span><br><span class="line">PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python \</span><br><span class="line">/approot1/utility/hadoopclient/Spark/spark/bin/spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--archives /tmp/pyspark/mlpy_env.zip#ANACONDA \</span><br><span class="line">--files /tmp/pyspark/kmeans_data.txt \</span><br><span class="line">--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python \</span><br><span class="line">/tmp/pyspark/kmeanTest.py</span><br></pre></td></tr></table></figure><p>模型训练结果会写到路径下面：/user/model：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/tmp/pyspark/pythonpkg # hdfs dfs -ls /user/model</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x+  - itdw hadoop          0 2019-07-30 11:30 /user/model/KMeansModel.ml</span><br></pre></td></tr></table></figure><p>模型加载的预测结果可以在Yarn日志中查询：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/tmp/pyspark # yarn logs -applicationId application_1562162322775_150207</span><br><span class="line"><span class="meta">#</span><span class="bash"> 提取部分回显</span></span><br><span class="line">LogType:stdout</span><br><span class="line">Log Upload Time:星期二 七月 30 13:47:23 +0800 2019</span><br><span class="line">LogLength:120414</span><br><span class="line">Log Contents:</span><br><span class="line">Within Set Sum of Squared Error = 0.6928203230275529</span><br><span class="line">[ 9.1  9.1  9.1]</span><br><span class="line">[ 0.1  0.1  0.1]</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>当然对于输出可以选择其他输出源(表或者文件)。</p><h2 id="第五部分-总结"><a href="#第五部分-总结" class="headerlink" title="第五部分 总结"></a>第五部分 总结</h2><h3 id="5-1-混合多语言数据流"><a href="#5-1-混合多语言数据流" class="headerlink" title="5.1 混合多语言数据流"></a>5.1 混合多语言数据流</h3><p>通常一个完整的机器学习应用的数据流设计中，可以将数据ETL准备阶段和算法计算分离出来。使用Java/scala/sql进行数据的预处理，输出算法计算要求的数据格式。这会极大降低算法计算的数据输入规模，降低算法计算的节点的IO。</p><p>机器学习的算法计算部分具有高迭代计算特性，对于非分布式的机器学习算法，我们通常部署在高性能的节点上，基于丰富、高性能的Python科学计算模块，使用Python语言实现。而对于数据准备阶段，更适合使用原生的Scala/java编程语言实现Spark Application来处理数据，包括转换、统计、压缩等等，将满足算法输入格式的数据输出到HDFS文件系统中。特别对于数据规模较大的情况，在Spark集群上处理数据，Scala/Java实现的Spark Application运行（多机并行分布式处理）性能要好一些。然后输出数据交给Python进行迭代计算训练。</p><p>当然对于分布式机器学习框架，将数据迭代部分分解到多个节点并行处理，由参数服务器管理迭代参数的汇总和更新。在这种计算框架下可以利用数据集群天然的计算资源，实现分布式部署。这就形成了一个高效的混合的多语言的数据处理流。</p><h3 id="5-2-架构建议和总结"><a href="#5-2-架构建议和总结" class="headerlink" title="5.2 架构建议和总结"></a>5.2 架构建议和总结</h3><p>1、对于Python on Yarn架构下，采用“Yarn节点不配置Python环境”模式，便于Python环境的管理。这时候可以将Python环境zip文件上传至集群HDFS文件系统，避免每次提交任务都需要上传zip文件，但是不可避免集群内部HDFS文件系统分发到运行节点产生的网络IO。但比集群外部的上传效率高。</p><p>2、对于机器学习任务数据流建议采用混合多语言数据流方式，发挥各计算组件的优势。</p><p>3、对于分布式机器学习框架，建议结合集群的计算资源，直接在集群上展开分布式计算（例如Tensorflow计算框架）。而不是单独新建新的机器学习分布式集群。减少两个集群的数据搬运，并且使得数据和计算更加贴近，最重要的提高机器学习任务端到端的效率。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Running Spark Python Applications，链接：<a href="https://www.cloudera.com/documentation/enterprise/5-9-x/topics/spark_python.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-9-x/topics/spark_python.html</a></p><p>2、基于YARN集群构建运行PySpark Application，链接： <a href="http://shiyanjun.cn/archives/1738.html" target="_blank" rel="noopener">http://shiyanjun.cn/archives/1738.html</a></p><p>3、Running Spark on YARN，链接： <a href="https://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/running-on-yarn.html</a></p><p>4、Running Spark on Kubernetes，链接：<a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p><p>5、Apache Spark Resource Management and YARN App Models，链接：<a href="https://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/" target="_blank" rel="noopener">https://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/</a></p><p>6、Spark On Yarn的两种模式yarn-cluster和yarn-client深度剖析，链接：<a href="https://www.cnblogs.com/ITtangtang/p/7967386.html" target="_blank" rel="noopener">https://www.cnblogs.com/ITtangtang/p/7967386.html</a></p><p>7、Introducing Skein: Deploy Python on Apache YARN the Easy Way，链接：<a href="https://jcrist.github.io/introducing-skein.html" target="_blank" rel="noopener">https://jcrist.github.io/introducing-skein.html</a></p><p>8、当Spark遇上TensorFlow分布式深度学习框架原理和实践，链接：<a href="https://juejin.im/post/5ad4b620f265da23a04a0ad0" target="_blank" rel="noopener">https://juejin.im/post/5ad4b620f265da23a04a0ad0</a></p><p>9、Spark On Yarn的优势，链接：<a href="https://www.cnblogs.com/ITtangtang/p/7967386.html" target="_blank" rel="noopener">https://www.cnblogs.com/ITtangtang/p/7967386.html</a></p><p>10、基于YARN集群构建运行PySpark Application，链接：<a href="http://www.uml.org.cn/bigdata/201711132.asp" target="_blank" rel="noopener">http://www.uml.org.cn/bigdata/201711132.asp</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;术语说明&lt;/li&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   Apache Spark运行模式介绍&lt;/li&gt;
&lt;li&gt;第二部
      
    
    </summary>
    
      <category term="pyspark" scheme="https://zjrongxiang.github.io/categories/pyspark/"/>
    
    
  </entry>
  
  <entry>
    <title>TDengine时间序列数据库压力测试</title>
    <link href="https://zjrongxiang.github.io/2019/07/01/2019-07-01-TDengine%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95/"/>
    <id>https://zjrongxiang.github.io/2019/07/01/2019-07-01-TDengine时间序列数据库压力测试/</id>
    <published>2019-07-01T15:30:00.000Z</published>
    <updated>2020-03-22T03:41:16.651Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分   部署压测工具vegeta</p></li><li><p>第二部分   部署TDengine数据库</p></li><li><p>第三部分   TDengine数据库RESTful接口介绍</p></li><li><p>第四部分   压力测试</p></li><li><p>第五部分   压力测试结果</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近涛思数据团队开源了自己的时序数据库，关注度较高。按照官方介绍，性能较为强悍，所以使用压测工具对该数据库进行了性能压测。压测使用的工具Vegeta 是一个用 Go 语言编写的多功能的 HTTP 负载测试工具。Vegeta 提供了命令行工具和一个开发库。</p><h2 id="第一部分-部署压测工具vegeta"><a href="#第一部分-部署压测工具vegeta" class="headerlink" title="第一部分 部署压测工具vegeta"></a>第一部分 部署压测工具vegeta</h2><h3 id="1-1-部署"><a href="#1-1-部署" class="headerlink" title="1.1 部署"></a>1.1 部署</h3><p>从github上下载vegeta安装介质：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wget https://github.com/tsenart/vegeta/releases/download/cli%2Fv12.5.1/vegeta-12.5.1-linux-amd64.tar.gz</span></span><br></pre></td></tr></table></figure><p>该工具开箱即用，解压tar包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tar -zxvf vegeta-12.5.1-linux-amd64.tar.gz</span></span><br></pre></td></tr></table></figure><h3 id="1-2-简单测试使用"><a href="#1-2-简单测试使用" class="headerlink" title="1.2 简单测试使用"></a>1.2 简单测试使用</h3><p>我们使用vegeta测试一下下面的压力场景（对百度主页发起每秒100次（-rate=100）的请求，持续10秒（-duration=10s）），测试结果重定向到文件（result/results.bin）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"GET https://www.baidu.com"</span> |./vegeta attack -duration=10s -rate=100 &gt;result/results.bin</span></span><br></pre></td></tr></table></figure><p>查看一下测试结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/vegeta# ./vegeta report result/results.bin</span><br><span class="line">Requests      [total, rate]            1000, 100.10</span><br><span class="line">Duration      [total, attack, wait]    10.002241136s, 9.990128173s, 12.112963ms</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  2.315958984s, 1.889487082s, 6.064678156s, 6.583899297s, 6.961230585s</span><br><span class="line">Bytes In      [total, mean]            227000, 227.00</span><br><span class="line">Bytes Out     [total, mean]            0, 0.00</span><br><span class="line">Success       [ratio]                  100.00%</span><br><span class="line">Status Codes  [code:count]             200:1000  </span><br><span class="line">Error Set:</span><br></pre></td></tr></table></figure><p>另外可以生成html文件报告（可视化）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/vegeta# ./vegeta plot result/results.bin &gt; result/plot.html</span><br></pre></td></tr></table></figure><h2 id="第二部分-部署TDengine数据库"><a href="#第二部分-部署TDengine数据库" class="headerlink" title="第二部分 部署TDengine数据库"></a>第二部分 部署TDengine数据库</h2><h3 id="2-1-制作docker镜像"><a href="#2-1-制作docker镜像" class="headerlink" title="2.1 制作docker镜像"></a>2.1 制作docker镜像</h3><blockquote><p>由于墙的原因我们在VPS上打包镜像，然后本机拉取部署。</p></blockquote><p>为了保证测试环境的隔离性，我们制作docker镜像，使用docker环境进行测试。首先拉取ubuntu的基础镜像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run -t -i ubuntu:16.04 /bin/bash</span></span><br></pre></td></tr></table></figure><p>通过TDengine源码安装。在这过程有大量操作系统工具未安装，需要使用atp-get安装部署。</p><h4 id="2-1-1-第一步-clone项目"><a href="#2-1-1-第一步-clone项目" class="headerlink" title="2.1.1 第一步 clone项目"></a>2.1.1 第一步 clone项目</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/taosdata/TDengine.git</span></span><br></pre></td></tr></table></figure><h4 id="2-1-2-第二步-编译"><a href="#2-1-2-第二步-编译" class="headerlink" title="2.1.2 第二步 编译"></a>2.1.2 第二步 编译</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir build &amp;&amp; <span class="built_in">cd</span> build</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cmake .. &amp;&amp; cmake --build .</span></span><br></pre></td></tr></table></figure><h4 id="2-1-3-第三步-安装"><a href="#2-1-3-第三步-安装" class="headerlink" title="2.1.3 第三步 安装"></a>2.1.3 第三步 安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> make install</span></span><br></pre></td></tr></table></figure><h3 id="2-2-生成镜像"><a href="#2-2-生成镜像" class="headerlink" title="2.2 生成镜像"></a>2.2 生成镜像</h3><p>打包成镜像，推送到Docker Hub：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@vultr:~# docker commit a35c43242a8e rongxiang1986/tdengine</span><br><span class="line">root@vultr:~# docker tag rongxiang1986/tdengine rongxiang1986/tdengine:1.0</span><br><span class="line">root@vultr:~# docker push rongxiang1986/tdengine:1.0</span><br></pre></td></tr></table></figure><h3 id="2-3-拉取镜像部署"><a href="#2-3-拉取镜像部署" class="headerlink" title="2.3 拉取镜像部署"></a>2.3 拉取镜像部署</h3><p>拉取镜像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/TDengine/TDengine# docker pull rongxiang1986/tdengine:1.0</span><br></pre></td></tr></table></figure><p>启动一个docker容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/TDengine/TDengine# docker run -t -i --name tdengine -d -p 6020:6020 rongxiang1986/tdengine:1.0 /bin/bash</span><br></pre></td></tr></table></figure><p>查看正在运行的容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/TDengine/TDengine# docker ps</span><br><span class="line"><span class="meta">#</span><span class="bash"> bec2e166c29f</span></span><br></pre></td></tr></table></figure><p>进入容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/TDengine/TDengine# docker attach bec2e166c29f</span><br></pre></td></tr></table></figure><p>最后启动数据库服务，下面是启动回显信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/TDengine/build# ./build/bin/taosd -c test/cfg &amp;</span><br><span class="line">[1] 23</span><br><span class="line">root@bec2e166c29f:~/TDengine/TDengine/build# TDengine:[23]: Starting TDengine service...</span><br><span class="line">07/17 14:48:21.615248 7ff90254e700 UTL timezone not configured, set to system default:Etc/UTC (Etc, +0000)</span><br><span class="line">07/17 14:48:21.615261 7ff90254e700 UTL locale not configured, set to system default:C</span><br><span class="line">07/17 14:48:21.616266 7ff90254e700 UTL    taos config &amp; system info:</span><br><span class="line">07/17 14:48:21.616274 7ff90254e700 UTL ==================================</span><br><span class="line">07/17 14:48:21.616278 7ff90254e700 UTL  internalIp:             172.17.0.3 </span><br><span class="line">07/17 14:48:21.616283 7ff90254e700 UTL  localIp:                172.17.0.3 </span><br><span class="line">07/17 14:48:21.616288 7ff90254e700 UTL  httpIp:                 0.0.0.0 </span><br><span class="line">07/17 14:48:21.616294 7ff90254e700 UTL  httpPort:               6020 </span><br><span class="line">07/17 14:48:21.616299 7ff90254e700 UTL  mgmtShellPort:          6030 </span><br><span class="line">07/17 14:48:21.616304 7ff90254e700 UTL  vnodeShellPort:         6035 </span><br><span class="line">07/17 14:48:21.616308 7ff90254e700 UTL  configDir:              test/cfg </span><br><span class="line">07/17 14:48:21.616312 7ff90254e700 UTL  dataDir:                /root/TDengine/TDengine/build/test/data </span><br><span class="line">07/17 14:48:21.616318 7ff90254e700 UTL  logDir:                 /root/TDengine/TDengine/build/test/log </span><br><span class="line">07/17 14:48:21.616325 7ff90254e700 UTL  scriptDir:              /etc/taos </span><br><span class="line">07/17 14:48:21.616330 7ff90254e700 UTL  numOfThreadsPerCore:    1.000000 </span><br><span class="line">07/17 14:48:21.616338 7ff90254e700 UTL  ratioOfQueryThreads:    0.500000 </span><br><span class="line">07/17 14:48:21.616344 7ff90254e700 UTL  numOfVnodesPerCore:     8 </span><br><span class="line">07/17 14:48:21.616349 7ff90254e700 UTL  numOfTotalVnodes:       0 </span><br><span class="line">07/17 14:48:21.616355 7ff90254e700 UTL  tables:                 1000 </span><br><span class="line">07/17 14:48:21.616360 7ff90254e700 UTL  cache:                  16384(byte)</span><br><span class="line">07/17 14:48:21.616366 7ff90254e700 UTL  rows:                   4096 </span><br><span class="line">07/17 14:48:21.616371 7ff90254e700 UTL  fileBlockMinPercent:    0.250000 </span><br><span class="line">07/17 14:48:21.616376 7ff90254e700 UTL  ablocks:                4 </span><br><span class="line">07/17 14:48:21.616382 7ff90254e700 UTL  tblocks:                100 </span><br><span class="line">07/17 14:48:21.616386 7ff90254e700 UTL  monitorInterval:        30(s)</span><br><span class="line">07/17 14:48:21.616391 7ff90254e700 UTL  rpcTimer:               300(ms)</span><br><span class="line">07/17 14:48:21.616396 7ff90254e700 UTL  rpcMaxTime:             600(s)</span><br><span class="line">07/17 14:48:21.616402 7ff90254e700 UTL  ctime:                  3600(s)</span><br><span class="line">07/17 14:48:21.616407 7ff90254e700 UTL  statusInterval:         1(s)</span><br><span class="line">07/17 14:48:21.616414 7ff90254e700 UTL  shellActivityTimer:     3(s)</span><br><span class="line">07/17 14:48:21.616420 7ff90254e700 UTL  meterMetaKeepTimer:     7200(s)</span><br><span class="line">07/17 14:48:21.616423 7ff90254e700 UTL  metricMetaKeepTimer:    600(s)</span><br><span class="line">07/17 14:48:21.616428 7ff90254e700 UTL  maxUsers:               1000 </span><br><span class="line">07/17 14:48:21.616432 7ff90254e700 UTL  maxDbs:                 1000 </span><br><span class="line">07/17 14:48:21.616439 7ff90254e700 UTL  maxTables:              650000 </span><br><span class="line">07/17 14:48:21.616442 7ff90254e700 UTL  maxVGroups:             1000 </span><br><span class="line">07/17 14:48:21.616445 7ff90254e700 UTL  minSlidingTime:         10(ms)</span><br><span class="line">07/17 14:48:21.616454 7ff90254e700 UTL  minIntervalTime:        10(ms)</span><br><span class="line">07/17 14:48:21.616460 7ff90254e700 UTL  maxStreamCompDelay:     20000(ms)</span><br><span class="line">07/17 14:48:21.616463 7ff90254e700 UTL  maxFirstStreamCompDelay:10000(ms)</span><br><span class="line">07/17 14:48:21.616467 7ff90254e700 UTL  retryStreamCompDelay:   10(ms)</span><br><span class="line">07/17 14:48:21.616470 7ff90254e700 UTL  clog:                   1 </span><br><span class="line">07/17 14:48:21.616474 7ff90254e700 UTL  comp:                   2 </span><br><span class="line">07/17 14:48:21.616480 7ff90254e700 UTL  days:                   10 </span><br><span class="line">07/17 14:48:21.616483 7ff90254e700 UTL  keep:                   3650 </span><br><span class="line">07/17 14:48:21.616488 7ff90254e700 UTL  defaultDB:               </span><br><span class="line">07/17 14:48:21.616494 7ff90254e700 UTL  defaultUser:            root </span><br><span class="line">07/17 14:48:21.616503 7ff90254e700 UTL  defaultPass:            taosdata </span><br><span class="line">07/17 14:48:21.616508 7ff90254e700 UTL  timezone:               Etc/UTC (Etc, +0000) </span><br><span class="line">07/17 14:48:21.616515 7ff90254e700 UTL  locale:                 C </span><br><span class="line">07/17 14:48:21.616520 7ff90254e700 UTL  charset:                UTF-8 </span><br><span class="line">07/17 14:48:21.616525 7ff90254e700 UTL  maxShellConns:          2000 </span><br><span class="line">07/17 14:48:21.616531 7ff90254e700 UTL  maxMeterConnections:    10000 </span><br><span class="line">07/17 14:48:21.616535 7ff90254e700 UTL  maxMgmtConnections:     2000 </span><br><span class="line">07/17 14:48:21.616540 7ff90254e700 UTL  maxVnodeConnections:    10000 </span><br><span class="line">07/17 14:48:21.616543 7ff90254e700 UTL  enableHttp:             1 </span><br><span class="line">07/17 14:48:21.616549 7ff90254e700 UTL  enableMonitor:          1 </span><br><span class="line">07/17 14:48:21.616553 7ff90254e700 UTL  httpCacheSessions:      2000 </span><br><span class="line">07/17 14:48:21.616556 7ff90254e700 UTL  httpMaxThreads:         2 </span><br><span class="line">07/17 14:48:21.616561 7ff90254e700 UTL  numOfLogLines:          10000000 </span><br><span class="line">07/17 14:48:21.616565 7ff90254e700 UTL  asyncLog:               1 </span><br><span class="line">07/17 14:48:21.616570 7ff90254e700 UTL  debugFlag:              131 </span><br><span class="line">07/17 14:48:21.616574 7ff90254e700 UTL  mDebugFlag:             135 </span><br><span class="line">07/17 14:48:21.616579 7ff90254e700 UTL  dDebugFlag:             131 </span><br><span class="line">07/17 14:48:21.616584 7ff90254e700 UTL  sdbDebugFlag:           135 </span><br><span class="line">07/17 14:48:21.616590 7ff90254e700 UTL  taosDebugFlag:          131 </span><br><span class="line">07/17 14:48:21.616595 7ff90254e700 UTL  tmrDebugFlag:           131 </span><br><span class="line">07/17 14:48:21.616601 7ff90254e700 UTL  cDebugFlag:             131 </span><br><span class="line">07/17 14:48:21.616605 7ff90254e700 UTL  jniDebugFlag:           131 </span><br><span class="line">07/17 14:48:21.616612 7ff90254e700 UTL  odbcDebugFlag:          131 </span><br><span class="line">07/17 14:48:21.616617 7ff90254e700 UTL  uDebugFlag:             131 </span><br><span class="line">07/17 14:48:21.616621 7ff90254e700 UTL  httpDebugFlag:          131 </span><br><span class="line">07/17 14:48:21.616629 7ff90254e700 UTL  monitorDebugFlag:       131 </span><br><span class="line">07/17 14:48:21.616634 7ff90254e700 UTL  qDebugFlag:             131 </span><br><span class="line">07/17 14:48:21.616637 7ff90254e700 UTL  gitinfo:                82cbce3261d06ab37c3bd4786c7b2e3d2316c42a </span><br><span class="line">07/17 14:48:21.616643 7ff90254e700 UTL  buildinfo:              Built by ubuntu at 2019-07-05 18:42 </span><br><span class="line">07/17 14:48:21.616648 7ff90254e700 UTL  version:                1.6.0.0 </span><br><span class="line">07/17 14:48:21.616653 7ff90254e700 UTL  os pageSize:            4096(KB)</span><br><span class="line">07/17 14:48:21.616658 7ff90254e700 UTL  os openMax:             1048576</span><br><span class="line">07/17 14:48:21.616662 7ff90254e700 UTL  os streamMax:           16</span><br><span class="line">07/17 14:48:21.616666 7ff90254e700 UTL  os numOfCores:          8</span><br><span class="line">07/17 14:48:21.616670 7ff90254e700 UTL  os totalDisk:           426(GB)</span><br><span class="line">07/17 14:48:21.616676 7ff90254e700 UTL  os totalMemory:         32028(MB)</span><br><span class="line">07/17 14:48:21.616682 7ff90254e700 UTL  os sysname:             Linux</span><br><span class="line">07/17 14:48:21.616686 7ff90254e700 UTL  os nodename:            bec2e166c29f</span><br><span class="line">07/17 14:48:21.616690 7ff90254e700 UTL  os release:             4.15.0-51-generic</span><br><span class="line">07/17 14:48:21.616694 7ff90254e700 UTL  os version:             #55~16.04.1-Ubuntu SMP Thu May 16 09:24:37 UTC 2019</span><br><span class="line">07/17 14:48:21.616700 7ff90254e700 UTL  os machine:             x86_64</span><br><span class="line">07/17 14:48:21.616707 7ff90254e700 UTL ==================================</span><br><span class="line">07/17 14:48:21.616712 7ff90254e700 DND Server IP address is:172.17.0.3</span><br><span class="line">07/17 14:48:21.616717 7ff90254e700 DND starting to initialize TDengine engine ...</span><br><span class="line">07/17 14:48:21.619440 7ff90254e700 HTP failed to open telegraf schema config file:test/cfg/taos.telegraf.cfg, use default schema</span><br><span class="line">07/17 14:48:21.869736 7ff90254e700 DND vnode is initialized successfully</span><br><span class="line">07/17 14:48:21.869780 7ff90254e700 MND starting to initialize TDengine mgmt ...</span><br><span class="line">07/17 14:48:21.872494 7ff90254e700 MND first access, set total vnodes:64</span><br><span class="line">07/17 14:48:21.917758 7ff90254e700 MND TDengine mgmt is initialized successfully</span><br><span class="line">07/17 14:48:21.917781 7ff90254e700 HTP starting to initialize http service ...</span><br><span class="line">07/17 14:48:21.918417 7ff90254e700 DND TDengine is initialized successfully</span><br><span class="line">07/17 14:48:21.918533 7ff8e4f41700 HTP http service init success at ip:0.0.0.0:6020</span><br><span class="line">TDengine:[23]: Started TDengine service successfully.</span><br><span class="line">07/17 14:48:22.022278 7ff8ff835700 MON starting to initialize monitor service ..</span><br><span class="line">07/17 14:48:22.022747 7ff8eb109700 MND user:monitor login from 172.17.0.3, code:0</span><br><span class="line">07/17 14:48:22.024412 7ff901038700 MON dnode:172.17.0.3 is started</span><br><span class="line">07/17 14:48:22.026780 7ff901038700 MON monitor service init success</span><br></pre></td></tr></table></figure><p>上面回显service init success说明服务service启动成功。</p><h3 id="2-4-镜像使用说明"><a href="#2-4-镜像使用说明" class="headerlink" title="2.4 镜像使用说明"></a>2.4 镜像使用说明</h3><p>TDengine项目地址：<a href="https://github.com/taosdata/TDengine。使用ubuntu16.04作为基础镜像，部署安装TDengine。拉取镜像后，启动容器后：" target="_blank" rel="noopener">https://github.com/taosdata/TDengine。使用ubuntu16.04作为基础镜像，部署安装TDengine。拉取镜像后，启动容器后：</a></p><ul><li><p>启动服务：To start the TDengine server, run the command below in terminal:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /root/TDengine/TDengine/build<span class="comment"># ./build/bin/taosd -c test/cfg</span></span></span><br></pre></td></tr></table></figure></li><li><p>启动客户端：In another terminal, use the TDengine shell to connect the server:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /root/TDengine/TDengine/build<span class="comment">#./build/bin/taos -c test/cfg</span></span></span><br></pre></td></tr></table></figure></li></ul><h2 id="第三部分-TDengine数据库RESTful接口介绍"><a href="#第三部分-TDengine数据库RESTful接口介绍" class="headerlink" title="第三部分 TDengine数据库RESTful接口介绍"></a>第三部分 TDengine数据库RESTful接口介绍</h2><p>按照官方给的例子我们新建案例数据库和表，并且新增数据记录。首先进入shell交互：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@a35c43242a8e:~/TDengine/TDengine/build# ./build/bin/taos -c test/cfg</span><br><span class="line">07/17 04:30:19.818000 7fa154b0e700 MND user:root login from 172.17.0.2, code:0</span><br><span class="line"></span><br><span class="line">Welcome to the TDengine shell, server version:1.6.0.0  client version:1.6.0.0</span><br><span class="line">Copyright (c) 2017 by TAOS Data, Inc. All rights reserved.</span><br><span class="line"></span><br><span class="line"><span class="meta">taos&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure><p>创建案例数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">taos&gt;</span><span class="bash"> create database db;</span></span><br><span class="line">07/17 04:31:46.970580 7fa14ffff700 MND DB:0.db is created by root</span><br><span class="line">Query OK, 1 row(s) affected (0.001848s)</span><br><span class="line"></span><br><span class="line"><span class="meta">taos&gt;</span><span class="bash"> use db;</span></span><br><span class="line">Database changed.</span><br><span class="line"></span><br><span class="line"><span class="meta">taos&gt;</span><span class="bash"> create table t (ts timestamp, cdata int);</span></span><br><span class="line">Query OK, 1 row(s) affected (0.334998s)</span><br><span class="line"></span><br><span class="line"><span class="meta">taos&gt;</span><span class="bash"> insert into t values (<span class="string">'2019-07-15 10:00:00'</span>, 10);</span></span><br><span class="line">Query OK, 1 row(s) affected (0.001639s)</span><br><span class="line"></span><br><span class="line"><span class="meta">taos&gt;</span><span class="bash"> insert into t values (<span class="string">'2019-07-15 10:01:05'</span>, 20);</span></span><br><span class="line">Query OK, 1 row(s) affected (0.000245s)</span><br><span class="line"></span><br><span class="line"><span class="meta">taos&gt;</span><span class="bash"> select * from t;</span></span><br><span class="line">          ts          |   cdata   |</span><br><span class="line">===================================</span><br><span class="line"> 19-07-15 10:00:00.000|         10|</span><br><span class="line"> 19-07-15 10:01:05.000|         20|</span><br><span class="line">Query OK, 2 row(s) in set (0.001408s)</span><br></pre></td></tr></table></figure><p>退出shell交互后，我们使用Restfull接口与数据库交互。</p><blockquote><p>注意：目前RESTfull接口认证方式使用Http Basic Authorization请求格式，token使用base64(username:password)，即base64(root:taosdata)=cm9vdDp0YW9zZGF0YQ==</p><p>可以在在线网站上：<a href="https://www.base64encode.org/" target="_blank" rel="noopener">https://www.base64encode.org/</a> encode一下。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@a35c43242a8e:~/TDengine/TDengine/build# curl -H 'Authorization: Basic cm9vdDp0YW9zZGF0YQ==' -d 'select * from db.t' localhost:6020/rest/sql</span><br><span class="line"><span class="meta">#</span><span class="bash">下面是回显：</span></span><br><span class="line">07/17 04:33:34.834283 7fa154b0e700 MND user:root login from 172.17.0.2, code:0</span><br><span class="line">&#123;"status":"succ","head":["ts","cdata"],"data":[["2019-07-15 10:00:00.000",10],["2019-07-15 10:01:05.000",20]],"rows":2&#125;</span><br></pre></td></tr></table></figure><p>返回结果是一个JSON格式串，规范化一下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;  </span><br><span class="line">   <span class="attr">"status"</span>:<span class="string">"succ"</span>,</span><br><span class="line">   <span class="attr">"head"</span>:[  </span><br><span class="line">      <span class="string">"ts"</span>,</span><br><span class="line">      <span class="string">"cdata"</span></span><br><span class="line">   ],</span><br><span class="line">   <span class="attr">"data"</span>:[  </span><br><span class="line">      [  </span><br><span class="line">         <span class="string">"2019-07-15 10:00:00.000"</span>,</span><br><span class="line">         <span class="number">10</span></span><br><span class="line">      ],</span><br><span class="line">      [  </span><br><span class="line">         <span class="string">"2019-07-15 10:01:05.000"</span>,</span><br><span class="line">         <span class="number">20</span></span><br><span class="line">      ]</span><br><span class="line">   ],</span><br><span class="line">   <span class="attr">"rows"</span>:<span class="number">2</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="第四部分-对RESTful接口压力测试"><a href="#第四部分-对RESTful接口压力测试" class="headerlink" title="第四部分 对RESTful接口压力测试"></a>第四部分 对RESTful接口压力测试</h2><p>最后我们使用vegeta对restful接口进行压力测试：</p><h3 id="4-1-查询压测"><a href="#4-1-查询压测" class="headerlink" title="4.1 查询压测"></a>4.1 查询压测</h3><p>使用目标文件的内容进行压力测试。</p><p>首先创建target.txt文件以及数据文件data.json，内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat target.txt</span><br><span class="line">POST http://localhost:6020/rest/sql</span><br><span class="line">Authorization: Basic cm9vdDp0YW9zZGF0YQ==</span><br><span class="line">@data.json</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat data.json</span><br><span class="line">select * from db.t</span><br></pre></td></tr></table></figure><p>最后使用下面的命令开始压测（每秒15000次请求，持续5分钟）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest#../vegeta attack -rate 15000 -targets target.txt -duration 5m &gt; out.dat</span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成报告</span></span><br><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report out.dat</span><br><span class="line"><span class="meta">#</span><span class="bash"> 回显：</span></span><br><span class="line">Requests      [total, rate]            4500044, 15000.15</span><br><span class="line">Duration      [total, attack, wait]    5m0.000202809s, 4m59.999911126s, 291.683µs</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  314.712µs, 230.113µs, 869.502µs, 1.501018ms, 205.870168ms</span><br><span class="line">Bytes In      [total, mean]            954009328, 212.00</span><br><span class="line">Bytes Out     [total, mean]            90000880, 20.00</span><br><span class="line">Success       [ratio]                  100.00%</span><br><span class="line">Status Codes  [code:count]             200:4500044  </span><br><span class="line">Error Set:</span><br></pre></td></tr></table></figure><h3 id="4-2-写入压测"><a href="#4-2-写入压测" class="headerlink" title="4.2 写入压测"></a>4.2 写入压测</h3><p>首先创建writetarget.txt和数据文件：writedata.json，内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat writetarget.txt</span><br><span class="line">POST http://localhost:6020/rest/sql</span><br><span class="line">Authorization: Basic cm9vdDp0YW9zZGF0YQ==</span><br><span class="line">@writedata.json</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat writedata.json</span><br><span class="line">insert into db.cpu values (NOW,20,12);</span><br></pre></td></tr></table></figure><blockquote><p>这里写入语句使用时间函数NOW，保证写入时间无重复。</p></blockquote><p>最后使用命令开始压测（每秒30000次请求，持续1分钟）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta attack -rate 30000 -targets writetarget.txt -duration 1m &gt; writeout.dat</span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成报告</span></span><br><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report writeout30000.dat</span><br><span class="line">Requests      [total, rate]            1800018, 30000.31</span><br><span class="line">Duration      [total, attack, wait]    1m0.000154842s, 59.99998565s, 169.192µs</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  254.824µs, 137.651µs, 904.696µs, 1.687601ms, 12.528831ms</span><br><span class="line">Bytes In      [total, mean]            115201152, 64.00</span><br><span class="line">Bytes Out     [total, mean]            70200702, 39.00</span><br><span class="line">Success       [ratio]                  100.00%</span><br><span class="line">Status Codes  [code:count]             200:1800018  </span><br><span class="line">Error Set:</span><br></pre></td></tr></table></figure><h2 id="第五部分-压力测试结果"><a href="#第五部分-压力测试结果" class="headerlink" title="第五部分 压力测试结果"></a>第五部分 压力测试结果</h2><p>由于机器环境的差异，只是做了尝试性测试，不代表产品的实际性能。</p><blockquote><p>官方测试报告参考：<a href="https://www.taosdata.com/downloads/TDengine_Testing_Report_cn.pdf" target="_blank" rel="noopener">https://www.taosdata.com/downloads/TDengine_Testing_Report_cn.pdf</a></p></blockquote><h3 id="5-1-查询"><a href="#5-1-查询" class="headerlink" title="5.1 查询"></a>5.1 查询</h3><p>对于查询性能我们每秒15000的请求结果如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report out15000.dat</span><br><span class="line">Requests      [total, rate]            4500044, 15000.15</span><br><span class="line">Duration      [total, attack, wait]    5m0.000202809s, 4m59.999911126s, 291.683µs</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  314.712µs, 230.113µs, 869.502µs, 1.501018ms, 205.870168ms</span><br><span class="line">Bytes In      [total, mean]            954009328, 212.00</span><br><span class="line">Bytes Out     [total, mean]            90000880, 20.00</span><br><span class="line">Success       [ratio]                  100.00%</span><br><span class="line">Status Codes  [code:count]             200:4500044  </span><br><span class="line">Error Set:</span><br></pre></td></tr></table></figure><p>当提高到每秒20000次时，数据库出现响应失败，成功率只有38.78%：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report out20000.dat     </span><br><span class="line">Requests      [total, rate]            904794, 2824.86</span><br><span class="line">Duration      [total, attack, wait]    6m19.120695351s, 5m20.296894938s, 58.823800413s</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  38.726984614s, 43.82040722s, 1m13.377381823s, 1m25.843611324s, 1m52.393219406s</span><br><span class="line">Bytes In      [total, mean]            58181450, 64.30</span><br><span class="line">Bytes Out     [total, mean]            8016960, 8.86</span><br><span class="line">Success       [ratio]                  38.78%</span><br><span class="line">Status Codes  [code:count]             0:503946  200:350896  400:49952  </span><br><span class="line">Error Set:</span><br><span class="line">400 Bad Request</span><br><span class="line">Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in use</span><br><span class="line">Post http://localhost:6020/rest/sql: net/http: request canceled (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">Post http://localhost:6020/rest/sql: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in use (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:34037-&gt;127.0.0.1:6020: read: connection reset by peer</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:53020-&gt;127.0.0.1:6020: read: connection reset by peer</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:47081-&gt;127.0.0.1:6020: read: connection reset by peer</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:35442-&gt;127.0.0.1:6020: read: connection reset by peer</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:58175-&gt;127.0.0.1:6020: read: connection reset by peer</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:41857-&gt;127.0.0.1:6020: read: connection reset by peer</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:58943-&gt;127.0.0.1:6020: read: connection reset by peer</span><br></pre></td></tr></table></figure><h3 id="5-2-写入"><a href="#5-2-写入" class="headerlink" title="5.2 写入"></a>5.2 写入</h3><p>对于写入测试。对于查询性能我们每秒30000的请求结果如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report writeout30000.dat</span><br><span class="line">Requests      [total, rate]            1800018, 30000.31</span><br><span class="line">Duration      [total, attack, wait]    1m0.000154842s, 59.99998565s, 169.192µs</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  254.824µs, 137.651µs, 904.696µs, 1.687601ms, 12.528831ms</span><br><span class="line">Bytes In      [total, mean]            115201152, 64.00</span><br><span class="line">Bytes Out     [total, mean]            70200702, 39.00</span><br><span class="line">Success       [ratio]                  100.00%</span><br><span class="line">Status Codes  [code:count]             200:1800018  </span><br><span class="line">Error Set:</span><br></pre></td></tr></table></figure><p>当提高到每秒50000次时，数据库性能开始恶化，成功率只有55.73%：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report writeout50000.dat</span><br><span class="line">Requests      [total, rate]            160712, 2676.70</span><br><span class="line">Duration      [total, attack, wait]    1m47.789030336s, 1m0.041100866s, 47.74792947s</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  36.064568863s, 39.103737526s, 50.372069878s, 54.866214748s, 1m12.470342472s</span><br><span class="line">Bytes In      [total, mean]            5731840, 35.67</span><br><span class="line">Bytes Out     [total, mean]            3492840, 21.73</span><br><span class="line">Success       [ratio]                  55.73%</span><br><span class="line">Status Codes  [code:count]             0:71152  200:89560  </span><br><span class="line">Error Set:</span><br><span class="line">Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in use</span><br><span class="line">Post http://localhost:6020/rest/sql: net/http: request canceled (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">Post http://localhost:6020/rest/sql: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in use (Client.Timeout exceeded while awaiting headers)</span><br></pre></td></tr></table></figure><p>结论：单机环境下同等场景（都是压测RESTfull接口）下，TDengine可以抗住每秒15000次的读请求和每秒30000次写请求。influxdb只能抗住每秒6000次持续读、写。按照官网介绍如果写入是批量形式会更快。</p><h2 id="参考文献和材料"><a href="#参考文献和材料" class="headerlink" title="参考文献和材料"></a>参考文献和材料</h2><p>1、推荐一款高性能 HTTP 负载测试工具 Vegeta 链接：<a href="https://www.hi-linux.com/posts/4650.html" target="_blank" rel="noopener">https://www.hi-linux.com/posts/4650.html</a></p><p>2、TDengine官网 链接：<a href="https://www.taosdata.com/cn/" target="_blank" rel="noopener">https://www.taosdata.com/cn/</a></p><p>3、比Hadoop快至少10倍的物联网大数据平台，我把它开源了 链接：<a href="https://weibo.com/ttarticle/p/show?id=2309404394278649462890" target="_blank" rel="noopener">https://weibo.com/ttarticle/p/show?id=2309404394278649462890</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分   部署压测工具vegeta&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第二部分
      
    
    </summary>
    
      <category term="TDengine Vegeta" scheme="https://zjrongxiang.github.io/categories/TDengine-Vegeta/"/>
    
    
  </entry>
  
  <entry>
    <title>sqlflow初体验</title>
    <link href="https://zjrongxiang.github.io/2019/05/06/2019-05-06-Sqlflow%E5%88%9D%E4%BD%93%E9%AA%8C/"/>
    <id>https://zjrongxiang.github.io/2019/05/06/2019-05-06-Sqlflow初体验/</id>
    <published>2019-05-06T14:42:00.000Z</published>
    <updated>2020-03-22T03:41:05.378Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分   Sqlflow安装部署</p></li><li><p>第二部分   机器学习例子</p></li><li><p>第三部分   系统架构</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>2019年5 月 6 日，在QCon 全球软件开发大会（北京站）上，蚂蚁金服副 CTO 胡喜正式宣布开源机器学习工具 SQLFlow。实际上3个月前sqkflow项目已经在github上开源了。</p><p>本篇文件主要参考sqlflow官网的案例和说明对sqlflow进行了体验，并记录下来。</p><p>sqlflow按照官网的定义，将SQL引擎（例如MySQL，Hive，SparkSQL或SQL Server）和tensorflow和其他机器学习的桥梁。扩展了SQL语言，支持对机器学习模型训练、预测和推理。</p><blockquote><p>目前开源版本仅支持MySQL和TensorFlow</p></blockquote><p>介绍文档中也提到，在sqlflow之前也有SQL引擎提供了支持机器学习功能的扩展。</p><ul><li><a href="https://docs.microsoft.com/en-us/sql/advanced-analytics/tutorials/rtsql-create-a-predictive-model-r?view=sql-server-2017" target="_blank" rel="noopener">Microsoft SQL Server</a>：Microsoft SQL Server具有机器学习服务，可以将R或Python中的机器学习程序作为外部脚本运行。</li><li><a href="https://www.linkedin.com/pulse/sql-deep-learning-sql-dl-omri-shiv" target="_blank" rel="noopener">Teradata SQL for DL</a>：Teradata还提供RESTful服务，可以从扩展的SQL SELECT语法中调用。</li><li><a href="https://cloud.google.com/bigquery/docs/bigqueryml-intro" target="_blank" rel="noopener">Google BigQuery</a>：Google BigQuery通过引入<code>CREATE MODEL</code>语句在SQL中实现机器学习。</li></ul><h2 id="第一部分-Sqlflow安装部署"><a href="#第一部分-Sqlflow安装部署" class="headerlink" title="第一部分 Sqlflow安装部署"></a>第一部分 Sqlflow安装部署</h2><h3 id="1-1-部署mysql做为数据源"><a href="#1-1-部署mysql做为数据源" class="headerlink" title="1.1 部署mysql做为数据源"></a>1.1 部署mysql做为数据源</h3><h4 id="（1）构建镜像"><a href="#（1）构建镜像" class="headerlink" title="（1）构建镜像"></a>（1）构建镜像</h4><p>官网提供了一个dockerfile，可以git clone整个项目。</p><p><a href="https://github.com/sql-machine-learning/sqlflow/tree/7c873780bd8a3a9ea4d39ed7d0fcf154b2f8821f/example/datasets" target="_blank" rel="noopener">https://github.com/sql-machine-learning/sqlflow/tree/7c873780bd8a3a9ea4d39ed7d0fcf154b2f8821f/example/datasets</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入Dockerfile文件所在目录</span></span><br><span class="line">cd example/datasets</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用Dockerfile构建镜像</span></span><br><span class="line">docker build -t sqlflow:data .</span><br></pre></td></tr></table></figure><p>可以查看创建了一个docker images：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker images</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建了REPOSITORY：sqlflow镜像，TAG为：data</span></span><br></pre></td></tr></table></figure><h4 id="（2）启动mysql容器"><a href="#（2）启动mysql容器" class="headerlink" title="（2）启动mysql容器"></a>（2）启动mysql容器</h4><p>用镜像启mysql容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -d --name sqlflowdata \</span><br><span class="line">   -p 3306:3306 \</span><br><span class="line">   -e MYSQL_ROOT_PASSWORD=root \</span><br><span class="line">   -e MYSQL_ROOT_HOST=% \</span><br><span class="line">   sqlflow:data</span><br></pre></td></tr></table></figure><p>使用镜像：sqlflow:data，启动一个名为：sqlflowdata的容器，并且把3306端口映射到宿主机。mysql的root用户的密码为root。</p><h4 id="（3）生成测试数据"><a href="#（3）生成测试数据" class="headerlink" title="（3）生成测试数据"></a>（3）生成测试数据</h4><p>进入容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it sqlflowdata bash</span><br></pre></td></tr></table></figure><p>执行SQL语句：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 建库建表，注意宿主机目录：datasets</span></span><br><span class="line">cat /popularize_churn.sql | mysql -uroot -proot</span><br><span class="line">cat /popularize_iris.sql | mysql -uroot -proot</span><br><span class="line"><span class="meta">#</span><span class="bash"> 建库</span></span><br><span class="line">echo "CREATE DATABASE IF NOT EXISTS sqlflow_models;" | mysql -uroot -proot</span><br></pre></td></tr></table></figure><p>至此完成mysql容器的启动和测试数据的生成。按Ctrl+P+Q，正常退出不关闭容器。</p><h3 id="1-2-使用docker部署slqflow"><a href="#1-2-使用docker部署slqflow" class="headerlink" title="1.2 使用docker部署slqflow"></a>1.2 使用docker部署slqflow</h3><h4 id="（1）拉取镜像并启动容器"><a href="#（1）拉取镜像并启动容器" class="headerlink" title="（1）拉取镜像并启动容器"></a>（1）拉取镜像并启动容器</h4><p>首先从docker Hub上拉取镜像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker pull sqlflow/sqlflow:latest</span></span><br></pre></td></tr></table></figure><p>启动容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker run --rm -it --name sqlflowServer -p 8888:8888 sqlflow/sqlflow:latest \</span></span><br><span class="line">bash -c "sqlflowserver --datasource='mysql://root:root@tcp(192.168.31.3:3306)/?maxAllowedPacket=0' &amp;</span><br><span class="line">SQLFLOW_SERVER=localhost:50051 jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root"</span><br></pre></td></tr></table></figure><p>命令使用镜像：sqlflow/sqlflow:lates，启动了名为：sqlflowServer的容器。将8888端口映射到宿主机上。这里需要配置datasource，指向mysql使用套接字：192.168.31.3:3306。这里使用之前构建的mysql容器的连接信息，可以根据实际情况配置。</p><blockquote><p>如果mysql套接字配置错误，报错信息：connect: connection refused</p></blockquote><p>如果没有报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2019/05/06 14:47:30 Server Started at :50051</span><br><span class="line">[I 14:47:30.261 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret</span><br><span class="line">[I 14:47:30.874 NotebookApp] Serving notebooks from local directory: /</span><br><span class="line">[I 14:47:30.874 NotebookApp] The Jupyter Notebook is running at:</span><br><span class="line">[I 14:47:30.874 NotebookApp] http://(fd2b9b3f994b or 127.0.0.1):8888/?token=265b3fc832b5b48689fa9f88483125dc9335188dd7c1d863</span><br><span class="line">[I 14:47:30.874 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</span><br><span class="line">[W 14:47:30.877 NotebookApp] No web browser found: could not locate runnable browser.</span><br></pre></td></tr></table></figure><p>这里启动了Jupyter Notebook服务，对外服务端口为8888，并且映射到宿主机。例如这里可以使用下面的url范围web界面：<a href="http://192.168.31.3:8888/?token=265b3fc832b5b48689fa9f88483125dc9335188dd7c1d863" target="_blank" rel="noopener">http://192.168.31.3:8888/?token=265b3fc832b5b48689fa9f88483125dc9335188dd7c1d863</a></p><h4 id="（2）简单测试"><a href="#（2）简单测试" class="headerlink" title="（2）简单测试"></a>（2）简单测试</h4><p>Jupyter Notebook 新建一个python3交互环境。测试一下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">%</span><span class="bash">%sqlflow</span></span><br><span class="line">select * from iris.train limit 5;</span><br></pre></td></tr></table></figure><p><img src="\images\picture\jupyter book.png" alt=""></p><h3 id="第二部分-机器学习例子"><a href="#第二部分-机器学习例子" class="headerlink" title="第二部分 机器学习例子"></a>第二部分 机器学习例子</h3><p>使用iris数据集体验机器学习的例子，使用Jupyter Notebook 完成：</p><h4 id="（1）训练模型："><a href="#（1）训练模型：" class="headerlink" title="（1）训练模型："></a>（1）训练模型：</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%sqlflow</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> iris.train TRAIN DNNClassifier <span class="keyword">WITH</span> n_classes = <span class="number">3</span>, hidden_units = [<span class="number">10</span>, <span class="number">20</span>] <span class="keyword">COLUMN</span> sepal_length, sepal_width, petal_length, petal_width LABEL <span class="keyword">class</span> <span class="keyword">INTO</span> sqlflow_models.my_dnn_model;</span><br></pre></td></tr></table></figure><p>使用iris.train表中的数据训练神经网络。</p><p>模型训练结果输入到sqlflow_models.my_dnn_model，回显训练正确率为：0.97273</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Training <span class="keyword">set</span> accuracy: 0.97273</span><br><span class="line">Done <span class="comment">training</span></span><br></pre></td></tr></table></figure><h4 id="（2）模型应用"><a href="#（2）模型应用" class="headerlink" title="（2）模型应用"></a>（2）模型应用</h4><p>使用训练结果对数据进行预测应用：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%sqlflow</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> iris.test PREDICT iris.predict.class <span class="keyword">USING</span> sqlflow_models.my_dnn_model;</span><br></pre></td></tr></table></figure><p>使用iris.test中的数据喂给训练好的模型，预测结果输出到表：iris.predict。</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Done predicting. <span class="keyword">Predict</span> <span class="keyword">table</span> : iris.<span class="keyword">predict</span></span><br></pre></td></tr></table></figure><p>查看结果表中的数据案例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%sqlflow</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> iris.predict <span class="keyword">limit</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">+--------------+</span>-------------<span class="code">+--------------+</span>-------------<span class="code">+-------+</span></span><br><span class="line">| sepal<span class="emphasis">_length | sepal_</span>width | petal<span class="emphasis">_length | petal_</span>width | class |</span><br><span class="line"><span class="code">+--------------+</span>-------------<span class="code">+--------------+</span>-------------<span class="code">+-------+</span></span><br><span class="line">|     6.3      |     2.7     |     4.9      |     1.8     |   2   |</span><br><span class="line">|     5.7      |     2.8     |     4.1      |     1.3     |   1   |</span><br><span class="line"><span class="code">+--------------+</span>-------------<span class="code">+--------------+</span>-------------<span class="code">+-------+</span></span><br></pre></td></tr></table></figure><h3 id="第三部分-系统架构"><a href="#第三部分-系统架构" class="headerlink" title="第三部分 系统架构"></a>第三部分 系统架构</h3><p>系统原型使用下面的架构：</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SQL <span class="function"><span class="title">statement</span> -&gt;</span> <span class="function"><span class="title">our</span> SQL parser --standard SQL-&gt;</span> MySQL</span><br><span class="line">                                \-<span class="function"><span class="title">extended</span> SQL-&gt;</span> <span class="function"><span class="title">code</span> generator -&gt;</span> execution engine</span><br></pre></td></tr></table></figure><p>原型运行的数据流为：</p><ol><li>它通过<a href="https://dev.mysql.com/downloads/connector/python/" target="_blank" rel="noopener">MySQL Connector Python API</a>从MySQL检索数据</li><li>从MySQL检索模型</li><li>通过调用用户指定的TensorFlow估算器训练模型或使用训练模型进行预测</li><li>并将训练过的模型或预测结果写入表格</li></ol><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、sqlflow项目官网 链接：<a href="https://github.com/sql-machine-learning/sqlflow" target="_blank" rel="noopener">https://github.com/sql-machine-learning/sqlflow</a></p><p>2、会 SQL 就能搞定 AI！蚂蚁金服重磅开源机器学习工具 SQLFlow 链接：<a href="https://www.infoq.cn/article/vlVqC68h2MT-028lh68C" target="_blank" rel="noopener">https://www.infoq.cn/article/vlVqC68h2MT-028lh68C</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分   Sqlflow安装部署&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第二部分 
      
    
    </summary>
    
      <category term="sqlflow" scheme="https://zjrongxiang.github.io/categories/sqlflow/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive介绍及部署</title>
    <link href="https://zjrongxiang.github.io/2018/08/14/2018-08-13-Hive-install/"/>
    <id>https://zjrongxiang.github.io/2018/08/14/2018-08-13-Hive-install/</id>
    <published>2018-08-14T11:30:00.000Z</published>
    <updated>2020-03-22T03:39:57.055Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Hive（蜂巢）是Hadoop的组件，官方介绍为：</p><blockquote><p><a href="http://hive.apache.org/" target="_blank" rel="noopener"><strong>Hive™</strong></a>: A data warehouse infrastructure that provides data summarization and ad hoc querying.</p></blockquote><p>Hive有三种部署方式（本质是Hive Metastore的三种部署方式）：</p><ol><li><p>Embedded Metastore Database (Derby) 内嵌模式</p><p>内嵌模式使用的是内嵌的Derby数据库来存储元数据，也不需要额外起Metastore服务。这个是默认的，配置简单，但是一次只能一个客户端连接（Derby只提供单进程存储），适用于用来实验，不适用于生产环境。 </p></li><li><p>Local Metastore Server 本地元存储</p><p>采用外部数据库来存储元数据 。本地元存储不需要单独起metastore服务，用的是跟hive在同一个进程里的metastore服务 。</p><p>目前支持：Derby，Mysql，微软SQLServer，Oracle和Postgres </p></li><li><p>Remote Metastore Server 远程元存储</p><p>采用外部数据库来存储元数据 。远程元存储需要单独起metastore服务，然后每个客户端都在配置文件里配置连接到该metastore服务。远程元存储的metastore服务和hive运行在不同的进程里。</p><p>远程元存储是生产环境部署方式。</p></li></ol><h2 id="本地部署过程"><a href="#本地部署过程" class="headerlink" title="本地部署过程"></a>本地部署过程</h2><blockquote><p>由于设备资源限制，没有太多机器配置类似生产环境的集群环境。所以通过docker搭建大集群环境。</p></blockquote><h4 id="搭建目标："><a href="#搭建目标：" class="headerlink" title="搭建目标："></a>搭建目标：</h4><ul><li>集群中hadoop集群由3台构成（1台master，2台slaves）</li><li>Hive的元数据库使用Mysql，并且单独包裹在一个docker环境中。</li></ul><h4 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h4><p>准备hadoop集群环境。启docker集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CONTAINER ID   IMAGE    COMMAND      CREATED    STATUS   PORTS      NAMES</span><br><span class="line">c27312e13270  kiwenlau/hadoop:1.0  "sh -c 'service ssh …"  2 hours ago  Up 2 hours hadoop-slave2</span><br><span class="line">f8b69885f3ef  kiwenlau/hadoop:1.0  "sh -c 'service ssh …" 2 hours ago  Up 2 hours hadoop-slave1</span><br><span class="line">439b359d230e  kiwenlau/hadoop:1.0  "sh -c 'service ssh …"   2 hours ago  Up 2 hours  0.0.0.0:8088-&gt;8088/tcp, 0.0.0.0:50070-&gt;50070/tcp   hadoop-master</span><br></pre></td></tr></table></figure><h4 id="Hive部署"><a href="#Hive部署" class="headerlink" title="Hive部署"></a>Hive部署</h4><h5 id="下载安装包："><a href="#下载安装包：" class="headerlink" title="下载安装包："></a>下载安装包：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入hadoop-master主机，进入hadoop目录：/use/<span class="built_in">local</span>/hadoop</span></span><br><span class="line">wget http://apache.claz.org/hive/hive-2.3.3/apache-hive-2.3.3-bin.tar.gz</span><br><span class="line">tar -zxvf apache-hive-2.3.3-bin.tar.gz</span><br><span class="line">mv apache-hive-2.3.3-bin hive</span><br></pre></td></tr></table></figure><h5 id="配置Hive环境变量："><a href="#配置Hive环境变量：" class="headerlink" title="配置Hive环境变量："></a>配置Hive环境变量：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">hive</span></span><br><span class="line">export HIVE_HOME=/usr/local/hadoop/hive</span><br><span class="line">PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$PATH</span><br><span class="line"><span class="meta">#</span><span class="bash"> 生效环境变量</span></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h5 id="调整Hive的配置文件："><a href="#调整Hive的配置文件：" class="headerlink" title="调整Hive的配置文件："></a>调整Hive的配置文件：</h5><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 进入hive 配置文件目录：</span></span><br><span class="line">cd conf</span><br><span class="line">cp hive-<span class="keyword">default</span>.xml.template hive-site.xml</span><br><span class="line"><span class="meta"># 修改配置文件</span></span><br><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure><h5 id="新建HDFS分布式文件目录："><a href="#新建HDFS分布式文件目录：" class="headerlink" title="新建HDFS分布式文件目录："></a>新建HDFS分布式文件目录：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hadoop已经设置好环境变量，新建下面目录</span></span><br><span class="line">hadoop fs -mkdir -p /user/hive/warehouse  </span><br><span class="line">hadoop fs -mkdir -p /user/hive/tmp  </span><br><span class="line">hadoop fs -mkdir -p /user/hive/log </span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置目录权限</span></span><br><span class="line">hadoop fs -chmod -R 777 /user/hive/warehouse  </span><br><span class="line">hadoop fs -chmod -R 777 /user/hive/tmp  </span><br><span class="line">hadoop fs -chmod -R 777 /user/hive/log</span><br></pre></td></tr></table></figure><p>可以用下面命令进行检查：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@hadoop-master:/usr/local/hadoop/hive/conf# hadoop fs -ls /user/hive</span><br><span class="line">Found 3 items</span><br><span class="line">drwxrwxrwx   - root supergroup          0 2018-08-14 07:34 /user/hive/log</span><br><span class="line">drwxrwxrwx   - root supergroup          0 2018-08-14 07:34 /user/hive/tmp</span><br><span class="line">drwxrwxrwx   - root supergroup          0 2018-08-14 07:34 /user/hive/warehouse</span><br></pre></td></tr></table></figure><h5 id="修改配置文件（hive-site-xml）："><a href="#修改配置文件（hive-site-xml）：" class="headerlink" title="修改配置文件（hive-site.xml）："></a>修改配置文件（hive-site.xml）：</h5><p>hive数据仓库数据路径：/user/hive/warehouse</p><p>需要使用hdfs新建文件目录。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置查询日志存放目录：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.querylog.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/log/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Location of Hive run time structured log file<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>数据库JDBC连接配置（172.18.0.5为mysql的ip地址，暴露3306端口）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://172.18.0.5:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>数据库驱动：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>数据库用户名：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>数据库密码：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置Hive临时目录：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>hive<span class="regexp">/tmp</span></span><br></pre></td></tr></table></figure><p>并在 <code>hive-site.xml</code> 中修改:</p><p>把<code>${system:java.io.tmpdir}</code> 改成真实物理绝对路径  /usr/local/hadoop/hive/tmp</p><p>把 <code>${system:user.name}</code> 改成 <code>${user.name}</code></p><blockquote><p>可以在外面编辑好配置文件，拷贝进docke：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; docker cp hive-site.<span class="keyword">xml</span> <span class="title">439b359d230e</span>:/usr/local/hadoop/hive/conf/hive-site.xml</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h4 id="配置hive-env-sh文件："><a href="#配置hive-env-sh文件：" class="headerlink" title="配置hive-env.sh文件："></a>配置hive-env.sh文件：</h4><p>尾部加上下面的配置（或者修改注释部分的配置亦可）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/usr/local/hadoop</span><br><span class="line">export HIVE_CONF_DIR=/usr/local/hadoop/hive/conf</span><br><span class="line">export HIVE_AUX_JARS_PATH=/usr/local/hadoop/hive/lib</span><br></pre></td></tr></table></figure><h4 id="配置Mysql"><a href="#配置Mysql" class="headerlink" title="配置Mysql"></a>配置Mysql</h4><p>启mysql容器，容器名：first-mysql，使用和hadoop一个桥接网络hadoop，密码为123456</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name first-mysql --net=hadoop -p 3306:3306 -e MYSQL\_ROOT\_PASSWORD=123456 -d mysql:5.7</span><br></pre></td></tr></table></figure><p>回显：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                                              NAMES</span><br><span class="line">84ae224cee53        mysql:5.7             "docker-entrypoint.s…"   32 minutes ago      Up 32 minutes       0.0.0.0:3306-&gt;3306/tcp                             first-mysql</span><br></pre></td></tr></table></figure><p>在Hadoop-master中配置mysql客户端（用来访问mysql服务器）：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-<span class="builtin-name">get</span> install mysql-client-core-5.6</span><br></pre></td></tr></table></figure><p>测试远程连接：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">mysql</span> <span class="selector-tag">-h172</span><span class="selector-class">.18</span><span class="selector-class">.0</span><span class="selector-class">.5</span> <span class="selector-tag">-P3306</span> <span class="selector-tag">-uroot</span> <span class="selector-tag">-p123456</span></span><br></pre></td></tr></table></figure><p>新建数据库，数据库名为hive：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; <span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> hive;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br></pre></td></tr></table></figure><p>初始化（Hive主机上）：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> <span class="string">/usr/local/hadoop/hive/bin</span></span><br><span class="line"><span class="string">./schematool</span> -initSchema -dbType mysql</span><br></pre></td></tr></table></figure><p>回显：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@hadoop-master:/usr/local/hadoop/hive/bin# ./schematool -initSchema -dbType mysql</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">。。。（略）</span><br><span class="line">schemaTool completed</span><br><span class="line"><span class="meta">#</span><span class="bash"> 初始化成功</span></span><br></pre></td></tr></table></figure><p>下载配置mysql驱动包，放在Hive的lib路径下面：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/hive/lib</span><br><span class="line">wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.38/mysql-connector-java-5.1.38.jar</span><br></pre></td></tr></table></figure><h3 id="启动Hive"><a href="#启动Hive" class="headerlink" title="启动Hive"></a>启动Hive</h3><p>做完上面准备工作后，开始启动hive：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root@hadoop-master:/usr/local/hadoop/hive/bin# ./hive</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/usr/local/hadoop/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/usr/local/hadoop/hive/lib/hive-common-2.3.3.jar!/hive-log4j2.properties Async: true</span><br><span class="line">Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.</span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure><p>最后进入hive的命令界面。</p><h3 id="踩坑备注"><a href="#踩坑备注" class="headerlink" title="踩坑备注"></a>踩坑备注</h3><p>1、Hive提示SSL连接警告</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tue Aug 14 10:53:12 UTC 2018 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br></pre></td></tr></table></figure><p>虽然Hive SQL执行成功，但是报上面的错误。产生的原因是使用JDBC连接MySQL服务器时为设置<code>useSSL</code>参数 。</p><p>解决办法：javax.jdo.option.ConnectionURL 配置的value值进行调整，设置<code>useSSL=false</code> ，注意xml中的语法。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://172.18.0.5:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    JDBC connect string for a JDBC metastore.</span><br><span class="line">    To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.</span><br><span class="line">    For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br></pre></td></tr></table></figure><p>重启Hive，不再有警告。</p><h2 id="远程部署"><a href="#远程部署" class="headerlink" title="远程部署"></a>远程部署</h2><p>对于远程部署需要单独启metastore服务，具体需要调整下面的配置文件（hive-site.xml）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hadoop-master:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>启动metastore服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service metastore &amp;</span><br></pre></td></tr></table></figure><p>当然这属于简单方式将Hive都扎堆部署在一个容器中。可以在集群其他几点启metastore服务，提升架构的高可用性，避免单点问题。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>1、Apache Hive-2.3.0 快速搭建与使用，<a href="https://segmentfault.com/a/1190000011303459" target="_blank" rel="noopener">https://segmentfault.com/a/1190000011303459</a></p><p>2、Hive提示警告SSL，<a href="https://blog.csdn.net/u012922838/article/details/73291524" target="_blank" rel="noopener">https://blog.csdn.net/u012922838/article/details/73291524</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;Hive（蜂巢）是Hadoop的组件，官方介绍为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://hive.apa
      
    
    </summary>
    
      <category term="hadoop" scheme="https://zjrongxiang.github.io/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>在Minikube上运行Flink集群</title>
    <link href="https://zjrongxiang.github.io/2018/06/25/2019-04-23-%E5%9C%A8Minikube%E4%B8%8A%E8%BF%90%E8%A1%8CFlink%E9%9B%86%E7%BE%A4%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://zjrongxiang.github.io/2018/06/25/2019-04-23-在Minikube上运行Flink集群（一）/</id>
    <published>2018-06-25T11:30:00.000Z</published>
    <updated>2020-03-22T03:40:33.952Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   部署准备</li><li>第二部分  验证</li><li>总结</li><li>参考文献及资料 </li></ul><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><h3 id="第一部分-部署准备"><a href="#第一部分-部署准备" class="headerlink" title="第一部分 部署准备"></a>第一部分 部署准备</h3><p>首先当然需要部署minikube集群。启动minikube集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube start</span></span><br><span class="line">Starting local Kubernetes v1.10.0 cluster...</span><br><span class="line">Starting VM...</span><br><span class="line">Getting VM IP address...</span><br><span class="line">Moving files into cluster...</span><br><span class="line">Setting up certs...</span><br><span class="line">Connecting to cluster...</span><br><span class="line">Setting up kubeconfig...</span><br><span class="line">Starting cluster components...</span><br><span class="line">Kubectl is now configured to use the cluster.</span><br><span class="line">Loading cached images from config file.</span><br></pre></td></tr></table></figure><p> 上面的回显表明minikube已经启动成功。执行下面网络配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube ssh <span class="string">'sudo ip link set docker0 promisc on'</span></span></span><br></pre></td></tr></table></figure><h3 id="第二部分-部署Flink集群"><a href="#第二部分-部署Flink集群" class="headerlink" title="第二部分 部署Flink集群"></a>第二部分 部署Flink集群</h3><p>一个基本的Flink集群运行在minikube需要三个组件：</p><ul><li>Deployment/Job：运行 JobManager</li><li>Deployment for a pool of TaskManagers</li><li>Service exposing the JobManager’s REST and UI ports</li></ul><h4 id="1-1-创建命名空间"><a href="#1-1-创建命名空间" class="headerlink" title="1.1 创建命名空间"></a>1.1 创建命名空间</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl create -f namespace.yaml</span></span><br><span class="line">namespace/flink created</span><br></pre></td></tr></table></figure><p>其中namespace.yaml文件为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">flink</span></span><br></pre></td></tr></table></figure><p>查询minikube集群的的命名空间：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get namespaces</span></span><br><span class="line">NAME          STATUS    AGE</span><br><span class="line">flink         Active    1m</span><br><span class="line">kube-public   Active    254d</span><br><span class="line">kube-system   Active    254d</span><br></pre></td></tr></table></figure><h4 id="1-2-集群组件资源定义"><a href="#1-2-集群组件资源定义" class="headerlink" title="1.2 集群组件资源定义"></a>1.2 集群组件资源定义</h4><h5 id="1-2-1-启动flink-jobmanager组件"><a href="#1-2-1-启动flink-jobmanager组件" class="headerlink" title="1.2.1 启动flink-jobmanager组件"></a>1.2.1 启动flink-jobmanager组件</h5><p>Job Manager 服务是Flink集群的主服务，使用jobmanager-deployment.yaml创建。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">flink-jobmanager</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">        component:</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">flink:latest</span></span><br><span class="line"><span class="attr">        args:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6124</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">blob</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">8081</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">ui</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">JOB_MANAGER_RPC_ADDRESS</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">flink-jobmanager</span></span><br></pre></td></tr></table></figure><h5 id="1-2-2-启动flink-taskmanager组件"><a href="#1-2-2-启动flink-taskmanager组件" class="headerlink" title="1.2.2 启动flink-taskmanager组件"></a>1.2.2 启动flink-taskmanager组件</h5><p>使用taskmanager-deployment.yaml创建。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">flink-taskmanager</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">        component:</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">flink:latest</span></span><br><span class="line"><span class="attr">        args:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6121</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6122</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">JOB_MANAGER_RPC_ADDRESS</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">flink-jobmanager</span></span><br></pre></td></tr></table></figure><h5 id="1-2-3-启用flink服务"><a href="#1-2-3-启用flink服务" class="headerlink" title="1.2.3 启用flink服务"></a>1.2.3 启用flink服务</h5><p>使用jobmanager-service.yaml创建服务，并且将端口映射到minikube主机响应端口。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">flink-jobmanager</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">30123</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">blob</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6124</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">30124</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">30125</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ui</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">8081</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">30081</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">    component:</span> <span class="string">jobmanager</span></span><br></pre></td></tr></table></figure><h4 id="1-3-端口映射到虚拟机主机"><a href="#1-3-端口映射到虚拟机主机" class="headerlink" title="1.3 端口映射到虚拟机主机"></a>1.3 端口映射到虚拟机主机</h4><p>minikube虚拟机停止的情况下的端口转发命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage modifyvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30123_6123,tcp,,6123,,30123"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage modifyvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30124_6123,tcp,,6124,,30124"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage modifyvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30125_6125,tcp,,6125,,30125"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage modifyvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30081_8081,tcp,,8081,,30081"</span></span></span><br></pre></td></tr></table></figure><blockquote><p>格式说明：vboxmanage modifyvm 宿主机名称 natpf<1-n> “映射别名,tcp,,本机端口,,虚拟机端口” </1-n></p></blockquote><p>minikube虚拟机运行的情况下的端口转发命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage controlvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30123_6123,tcp,,6123,,30123"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage controlvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30124_6123,tcp,,6124,,30124"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage controlvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30125_6125,tcp,,6125,,30125"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage controlvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30081_8081,tcp,,8081,,30081"</span></span></span><br></pre></td></tr></table></figure><blockquote><p>格式说明：vboxmanage controlvm 宿主机名称 natpf<1-n> “映射别名,tcp,,本机端口,,宿主机端口” </1-n></p></blockquote><p>另外如果要删除上面转发规则：</p><blockquote><p>vboxmanage controlvm 宿主机名称 natpf<1-n> delete 映射别名</1-n></p><p>vboxmanage modifyvm 宿主机名称 natpf<1-n> delete 映射别名</1-n></p></blockquote><h3 id="第三部分-验证"><a href="#第三部分-验证" class="headerlink" title="第三部分 验证"></a>第三部分 验证</h3><h4 id="3-1-minikube控制台界面"><a href="#3-1-minikube控制台界面" class="headerlink" title="3.1 minikube控制台界面"></a>3.1 minikube控制台界面</h4><p>为了是主机局域网类服务器都能访问minikube控制台，需要将端口映射出去。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage modifyvm <span class="string">"minikube"</span> --natpf1 <span class="string">"kubedashboard,tcp,,30000,,30000"</span></span></span><br></pre></td></tr></table></figure><p><img src="\images\picture\flink\minikube_dashboard.png" alt=""></p><h4 id="3-2-Flink控制台"><a href="#3-2-Flink控制台" class="headerlink" title="3.2 Flink控制台"></a>3.2 Flink控制台</h4><p><img src="\images\picture\flink\minikube_flink.png" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>（1）部署前提前拉取镜像到本地镜像库。</p><p>（2）需要将服务端口映射到本地机器端口，供局域网服务访问，为后续访问Flink提供方便。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>1、Kubernetes Setup ：<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/kubernetes.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/kubernetes.html</a></p><p>2、How to Deploy Flink Cluster &amp; Flink-exporter in Kubernetes Cluster：<a href="https://medium.com/pharos-production/how-to-deploy-flink-cluster-flink-exporter-in-kubernetes-cluster-48e24b440446" target="_blank" rel="noopener">https://medium.com/pharos-production/how-to-deploy-flink-cluster-flink-exporter-in-kubernetes-cluster-48e24b440446</a></p><p>3、<a href="https://github.com/melentye" target="_blank" rel="noopener">melentye</a>/<strong>flink-kubernetes</strong> <a href="https://github.com/melentye/flink-kubernetes" target="_blank" rel="noopener">https://github.com/melentye/flink-kubernetes</a></p><p>4、Set up Ingress on Minikube with the NGINX Ingress Controller <a href="https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   部署准备&lt;/li&gt;
&lt;li&gt;第二部分  验证&lt;/li&gt;
&lt;li&gt;总结&lt;/li&gt;
&lt;li&gt;参
      
    
    </summary>
    
      <category term="Flink" scheme="https://zjrongxiang.github.io/categories/Flink/"/>
    
    
  </entry>
  
  <entry>
    <title>在Minikube上运行Spark集群</title>
    <link href="https://zjrongxiang.github.io/2018/06/25/2018-08-06-%E5%9C%A8Minikube%E4%B8%8A%E8%BF%90%E8%A1%8CSpark%E9%9B%86%E7%BE%A4/"/>
    <id>https://zjrongxiang.github.io/2018/06/25/2018-08-06-在Minikube上运行Spark集群/</id>
    <published>2018-06-25T11:30:00.000Z</published>
    <updated>2020-03-22T03:39:48.386Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>​     Spark2.3版本开始支持使用spark-submit直接提交任务给Kubernetes集群。执行机制原理：</p><ul><li>Spark创建一个在<a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/" target="_blank" rel="noopener">Kubernetes pod中</a>运行的Spark驱动程序。</li><li>驱动程序创建执行程序，这些执行程序也在Kubernetes pod中运行并连接到它们，并执行应用程序代码。</li><li>当应用程序完成时，执行程序窗格会终止并清理，但驱动程序窗格会保留日志并在Kubernetes API中保持“已完成”状态，直到它最终被垃圾收集或手动清理。</li></ul><p><img src="https://spark.apache.org/docs/latest/img/k8s-cluster-mode.png" alt="Spark集群组件"> </p><h3 id="第一部分-环境准备"><a href="#第一部分-环境准备" class="headerlink" title="第一部分 环境准备"></a>第一部分 环境准备</h3><h4 id="1-1-minikube虚拟机准备"><a href="#1-1-minikube虚拟机准备" class="headerlink" title="1.1 minikube虚拟机准备"></a>1.1 minikube虚拟机准备</h4><p>由于spark集群对内存和cpu资源要求较高，在minikube启动前，提前配置较多的资源给虚拟机。</p><blockquote><p>当minikube启动时，它以单节点配置开始，默认情况下占用<code>1Gb</code>内存和<code>2</code>CPU内核，但是，为了运行spark集群，这个资源配置是不够的，而且作业会失败。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube config <span class="built_in">set</span> memory 8192</span></span><br><span class="line">These changes will take effect upon a minikube delete and then a minikube start</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> minikube config <span class="built_in">set</span> cpus 2</span></span><br><span class="line">These changes will take effect upon a minikube delete and then a minikube start</span><br></pre></td></tr></table></figure><p>或者用下面的命令启集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube start --cpus 2 --memory 8192</span></span><br></pre></td></tr></table></figure><h4 id="1-2-Spark环境准备"><a href="#1-2-Spark环境准备" class="headerlink" title="1.2 Spark环境准备"></a>1.2 Spark环境准备</h4><p>第一步   下载saprk2.3</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> wget http://apache.mirrors.hoobly.com/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz</span></span><br></pre></td></tr></table></figure><p>解压缩：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> tar xvf spark-2.3.0-bin-hadoop2.7.tgz</span></span><br></pre></td></tr></table></figure><p>制作docker镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> spark-2.3.0-bin-hadoop2.7</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker build -t rongxiang/spark:2.3.0 -f kubernetes/dockerfiles/spark/Dockerfile .</span></span><br></pre></td></tr></table></figure><p>查看镜像情况：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker images</span></span><br><span class="line">REPOSITORY                                                              TAG                            IMAGE ID            CREATED             SIZE</span><br><span class="line">rongxiang1986/spark                                                     2.3.0                          c5c806314f25        5 days ago          346MB</span><br></pre></td></tr></table></figure><p>登录docker 账户：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker login</span></span><br><span class="line">Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.</span><br><span class="line">Username: </span><br><span class="line">Password: </span><br><span class="line">Login Succeeded</span><br></pre></td></tr></table></figure><p>将之前build好的镜像pull到docker hub上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker push rongxiang1986/spark:2.3.0</span></span><br></pre></td></tr></table></figure><blockquote><p>注意这里的格式要求（我踩坑了）：docker push 注册用户名/镜像名 </p></blockquote><p>在<a href="https://hub.docker.com/上查看，镜像确实push上去了。" target="_blank" rel="noopener">https://hub.docker.com/上查看，镜像确实push上去了。</a></p><h3 id="第二部分-提交Spark作业"><a href="#第二部分-提交Spark作业" class="headerlink" title="第二部分 提交Spark作业"></a>第二部分 提交Spark作业</h3><h4 id="2-1-作业提交"><a href="#2-1-作业提交" class="headerlink" title="2.1 作业提交"></a>2.1 作业提交</h4><p>提前配置serviceaccount信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl create serviceaccount spark</span></span><br><span class="line">serviceaccount/spark created</span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=default:spark --namespace=default</span></span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/spark-role created</span><br></pre></td></tr></table></figure><p> 提交作业：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ./spark-submit \</span></span><br><span class="line">--master k8s://https://192.168.99.100:8443 \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--name spark-pi \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \</span><br><span class="line">--conf spark.kubernetes.authenticate.executor.serviceAccountName=spark \</span><br><span class="line">--conf spark.executor.instances=2 \</span><br><span class="line">--conf spark.kubernetes.container.image=rongxiang1986/spark:2.3.0 \</span><br><span class="line">local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar</span><br></pre></td></tr></table></figure><p>提交命令的参数含义分别是：</p><ul><li><code>--class</code>：应用程序的入口点（命令中使用：org.apache.spark.examples.SparkPi）；</li><li><code>--master</code>：Kubernetes集群的URL（k8s://<a href="https://192.168.99.100:8443）；" target="_blank" rel="noopener">https://192.168.99.100:8443）；</a></li><li><code>--deploy-mode</code>：驱动程序部署位置（默认值：客户端），这里部署在集群中；</li><li><code>--conf spark.executor.instances=2</code>：运行作业启动的executor个数；</li><li><code>--conf spark.kubernetes.container.image=rongxiang1986/spark:2.3.0</code>：使用的docker镜像名称；</li><li><code>local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar</code>：应用程序依赖jar包路径；</li></ul><blockquote><p>注意：目前deploy-mode只支持cluster模式，不支持client模式。</p><p>Error: Client mode is currently not supported for Kubernetes.</p></blockquote><p>作业运行回显如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">2018-08-12 15:51:17 WARN  Utils:66 - Your hostname, deeplearning resolves to a loopback address: 127.0.1.1; using 192.168.31.3 instead (on interface enp0s31f6)</span><br><span class="line">2018-08-12 15:51:17 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address</span><br><span class="line">2018-08-12 15:51:18 INFO  LoggingPodStatusWatcherImpl:54 - State changed, new state: </span><br><span class="line"> pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver</span><br><span class="line"> namespace: default</span><br><span class="line"> labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver</span><br><span class="line"> pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0</span><br><span class="line"> creation time: 2018-08-12T07:51:18Z</span><br><span class="line"> service account name: spark</span><br><span class="line"> volumes: spark-token-rzrgk</span><br><span class="line"> node name: N/A</span><br><span class="line"> start time: N/A</span><br><span class="line"> container images: N/A</span><br><span class="line"> phase: Pending</span><br><span class="line"> status: []</span><br><span class="line">2018-08-12 15:51:18 INFO  LoggingPodStatusWatcherImpl:54 - State changed, new state: </span><br><span class="line"> pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver</span><br><span class="line"> namespace: default</span><br><span class="line"> labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver</span><br><span class="line"> pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0</span><br><span class="line"> creation time: 2018-08-12T07:51:18Z</span><br><span class="line"> service account name: spark</span><br><span class="line"> volumes: spark-token-rzrgk</span><br><span class="line"> node name: minikube</span><br><span class="line"> start time: N/A</span><br><span class="line"> container images: N/A</span><br><span class="line"> phase: Pending</span><br><span class="line"> status: []</span><br><span class="line">2018-08-12 15:51:18 INFO  LoggingPodStatusWatcherImpl:54 - State changed, new state: </span><br><span class="line"> pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver</span><br><span class="line"> namespace: default</span><br><span class="line"> labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver</span><br><span class="line"> pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0</span><br><span class="line"> creation time: 2018-08-12T07:51:18Z</span><br><span class="line"> service account name: spark</span><br><span class="line"> volumes: spark-token-rzrgk</span><br><span class="line"> node name: minikube</span><br><span class="line"> start time: 2018-08-12T07:51:18Z</span><br><span class="line"> container images: rongxiang1986/spark:2.3.0</span><br><span class="line"> phase: Pending</span><br><span class="line"> status: [ContainerStatus(containerID=null, image=rongxiang1986/spark:2.3.0, imageID=, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties=&#123;&#125;), name=spark-kubernetes-driver, ready=false, restartCount=0, state=ContainerState(running=null, terminated=null, waiting=ContainerStateWaiting(message=null, reason=ContainerCreating, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;)]</span><br><span class="line">2018-08-12 15:51:18 INFO  Client:54 - Waiting for application spark-pi to finish...</span><br><span class="line">2018-08-12 15:51:51 INFO  LoggingPodStatusWatcherImpl:54 - State changed, new state: </span><br><span class="line"> pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver</span><br><span class="line"> namespace: default</span><br><span class="line"> labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver</span><br><span class="line"> pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0</span><br><span class="line"> creation time: 2018-08-12T07:51:18Z</span><br><span class="line"> service account name: spark</span><br><span class="line"> volumes: spark-token-rzrgk</span><br><span class="line"> node name: minikube</span><br><span class="line"> start time: 2018-08-12T07:51:18Z</span><br><span class="line"> container images: rongxiang1986/spark:2.3.0</span><br><span class="line"> phase: Running</span><br><span class="line"> status: [ContainerStatus(containerID=docker://d43089c8340affc4534f796b94a90ae080670c36c095176575fbeebacaab648e, image=rongxiang1986/spark:2.3.0, imageID=docker-pullable://rongxiang1986/spark@sha256:3e93a2d462679015a9fb7d723f53ab1d62c5e3619e3f1564d182c3d297ddf75d, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties=&#123;&#125;), name=spark-kubernetes-driver, ready=true, restartCount=0, state=ContainerState(running=ContainerStateRunning(startedAt=Time(time=2018-08-12T07:51:51Z, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;), terminated=null, waiting=null, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;)]</span><br><span class="line">2018-08-12 15:51:57 INFO  LoggingPodStatusWatcherImpl:54 - State changed, new state: </span><br><span class="line"> pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver</span><br><span class="line"> namespace: default</span><br><span class="line"> labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver</span><br><span class="line"> pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0</span><br><span class="line"> creation time: 2018-08-12T07:51:18Z</span><br><span class="line"> service account name: spark</span><br><span class="line"> volumes: spark-token-rzrgk</span><br><span class="line"> node name: minikube</span><br><span class="line"> start time: 2018-08-12T07:51:18Z</span><br><span class="line"> container images: rongxiang1986/spark:2.3.0</span><br><span class="line"> phase: Succeeded</span><br><span class="line"> status: [ContainerStatus(containerID=docker://d43089c8340affc4534f796b94a90ae080670c36c095176575fbeebacaab648e, image=rongxiang1986/spark:2.3.0, imageID=docker-pullable://rongxiang1986/spark@sha256:3e93a2d462679015a9fb7d723f53ab1d62c5e3619e3f1564d182c3d297ddf75d, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties=&#123;&#125;), name=spark-kubernetes-driver, ready=false, restartCount=0, state=ContainerState(running=null, terminated=ContainerStateTerminated(containerID=docker://d43089c8340affc4534f796b94a90ae080670c36c095176575fbeebacaab648e, exitCode=0, finishedAt=Time(time=2018-08-12T07:51:57Z, additionalProperties=&#123;&#125;), message=null, reason=Completed, signal=null, startedAt=Time(time=2018-08-12T07:51:51Z, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;), waiting=null, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;)]</span><br><span class="line">2018-08-12 15:51:57 INFO  LoggingPodStatusWatcherImpl:54 - Container final statuses:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> Container name: spark-kubernetes-driver</span><br><span class="line"> Container image: rongxiang1986/spark:2.3.0</span><br><span class="line"> Container state: Terminated</span><br><span class="line"> Exit code: 0</span><br><span class="line">2018-08-12 15:51:57 INFO  Client:54 - Application spark-pi finished.</span><br><span class="line">2018-08-12 15:51:57 INFO  ShutdownHookManager:54 - Shutdown hook called</span><br><span class="line">2018-08-12 15:51:57 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-6dd1c204-4ad7-40c4-b47f-a34f18e1995d</span><br></pre></td></tr></table></figure><h4 id="2-2-日志查询"><a href="#2-2-日志查询" class="headerlink" title="2.2 日志查询"></a>2.2 日志查询</h4><p>可以通过命令查看容器执行日志，或者通过kubernetes-dashboard提供web界面查看。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl logs spark-pi-709e1c1b19813e7cbc1aeff45200c64e-driver</span></span><br></pre></td></tr></table></figure><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  DAGScheduler:54 - Job 0 finished: reduce at SparkPi.scala:38, took 0.576528 s</span><br><span class="line">Pi is roughly 3.1336756683783418</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  AbstractConnector:318 - Stopped Spark@9635fa&#123;HTTP/1.1,[http/1.1]&#125;&#123;0.0.0.0:4040&#125;</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  SparkUI:54 - Stopped Spark web UI at http://spark-pi<span class="string">-7314</span>d819cd3730b4bf7d02bfedd21373-driver-svc.default.svc:4040</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  KubernetesClusterSchedulerBackend:54 - Shutting down all executors</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint:54 - Asking each executor to shut down</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  KubernetesClusterSchedulerBackend:54 - Closing kubernetes client</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  MemoryStore:54 - MemoryStore cleared</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  BlockManager:54 - BlockManager stopped</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  SparkContext:54 - Successfully stopped SparkContext</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  ShutdownHookManager:54 - Shutdown hook called</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark<span class="string">-435</span>d5ab2-f7b4<span class="string">-45</span>d0-a00f<span class="string">-0</span>bd9f162f9db</span><br></pre></td></tr></table></figure><p>执行结束后executor pod被自动清除。计算得到pi的值为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pi is roughly 3.1336756683783418</span><br></pre></td></tr></table></figure><p>如果作业通过cluster提交，driver容器会被保留，可以查看：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube service list</span></span><br><span class="line">|-------------|------------------------------------------------------|-----------------------------|</span><br><span class="line">|  NAMESPACE  |                         NAME                         |             URL             |</span><br><span class="line">|-------------|------------------------------------------------------|-----------------------------|</span><br><span class="line">| default     | kubernetes                                           | No node port                |</span><br><span class="line">| default     | spark-pi-27fcc168740e372292b27185d124ad7b-driver-svc | No node port                |</span><br><span class="line">| kube-system | kube-dns                                             | No node port                |</span><br><span class="line">| kube-system | kubernetes-dashboard                                 | http://192.168.99.100:30000 |</span><br><span class="line">|-------------|------------------------------------------------------|-----------------------------|</span><br></pre></td></tr></table></figure><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>1、Running Spark on Kubernetes ：<a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p><p>2、在Minikube Kubernetes集群上运行Spark工作：<a href="https://iamninad.com/running-spark-job-on-kubernetes-minikube/" target="_blank" rel="noopener">https://iamninad.com/running-spark-job-on-kubernetes-minikube/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;​     Spark2.3版本开始支持使用spark-submit直接提交任务给Kubernetes集群。执行机
      
    
    </summary>
    
      <category term="Minikube spark Kubernetes" scheme="https://zjrongxiang.github.io/categories/Minikube-spark-Kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>在Minikube上运行Kafka集群</title>
    <link href="https://zjrongxiang.github.io/2018/06/25/2019-07-27-%E5%9C%A8Minikube%E4%B8%8A%E8%BF%90%E8%A1%8CKafka%E9%9B%86%E7%BE%A4/"/>
    <id>https://zjrongxiang.github.io/2018/06/25/2019-07-27-在Minikube上运行Kafka集群/</id>
    <published>2018-06-25T11:30:00.000Z</published>
    <updated>2020-03-22T03:41:26.016Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   Minikube集群启动</li><li>第一部分   Kubernetes中StatefulSet介绍</li><li>第三部分   部署Zookeeper集群</li><li>第四部分   部署Kafka集群</li><li>第五部分   总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Kafka和zookeeper是在两种典型的有状态的集群服务。首先kafka和zookeeper都需要存储盘来保存有状态信息，其次kafka和zookeeper每一个实例都需要有对应的实例Id(Kafka需要broker.id,zookeeper需要my.id)来作为集群内部每个成员的标识，集群内节点之间进行内部通信时需要用到这些标识。</p><p>对于这类服务的部署，需要解决两个大的问题，一个是状态保存，另一个是集群管理(多服务实例管理)。kubernetes中提的StatefulSet(1.5版本之前称为Petset)方便了有状态集群服务在上的部署和管理。具体来说是通过Init Container来做集群的初始化工 作，用 Headless Service来维持集群成员的稳定关系，用Persistent Volume和Persistent Volume Claim提供网络存储来持久化数据，从而支持有状态集群服务的部署。</p><p>StatefulSet 是Kubernetes1.9版本中稳定的特性，本文使用的环境为 Kubernetes 1.10.0。</p><h2 id="第一部分-Minikube集群启动"><a href="#第一部分-Minikube集群启动" class="headerlink" title="第一部分 Minikube集群启动"></a>第一部分 Minikube集群启动</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:~# minikube start</span><br><span class="line">There is a newer version of minikube available (v1.2.0).  Download it here:</span><br><span class="line">https://github.com/kubernetes/minikube/releases/tag/v1.2.0</span><br><span class="line"></span><br><span class="line">To disable this notification, run the following:</span><br><span class="line">minikube config set WantUpdateNotification false</span><br><span class="line">Starting local Kubernetes v1.10.0 cluster...</span><br><span class="line">Starting VM...</span><br><span class="line">Downloading Minikube ISO</span><br><span class="line"> 153.08 MB / 153.08 MB [============================================] 100.00% 0s</span><br><span class="line">Getting VM IP address...</span><br><span class="line">Moving files into cluster...</span><br><span class="line">Downloading kubeadm v1.10.0</span><br><span class="line">Downloading kubelet v1.10.0</span><br><span class="line">Finished Downloading kubeadm v1.10.0</span><br><span class="line">Finished Downloading kubelet v1.10.0</span><br><span class="line">Setting up certs...</span><br><span class="line">Connecting to cluster...</span><br><span class="line">Setting up kubeconfig...</span><br><span class="line">Starting cluster components...</span><br><span class="line">Kubectl is now configured to use the cluster.</span><br><span class="line">Loading cached images from config file.</span><br></pre></td></tr></table></figure><h2 id="第二部分-Kubernetes中StatefulSet介绍"><a href="#第二部分-Kubernetes中StatefulSet介绍" class="headerlink" title="第二部分 Kubernetes中StatefulSet介绍"></a>第二部分 Kubernetes中StatefulSet介绍</h2><p>使用Kubernetes来调度无状态的应用较为简单</p><p>StatefulSet 这个对象是专门用来部署用状态应用的，可以为Pod提供稳定的身份标识，包括hostname、启动顺序、DNS名称等。</p><p>在最新发布的 <a href="https://www.kubernetes.org.cn/tags/kubernetes1-5" target="_blank" rel="noopener">Kubernetes 1.5</a> 我们将过去的 PetSet 功能升级到了 Beta 版本，并重新命名为StatefulSet</p><h2 id="第三部分-部署Zookeeper集群"><a href="#第三部分-部署Zookeeper集群" class="headerlink" title="第三部分 部署Zookeeper集群"></a>第三部分 部署Zookeeper集群</h2><h2 id="第四部分-部署Kafka集群"><a href="#第四部分-部署Kafka集群" class="headerlink" title="第四部分 部署Kafka集群"></a>第四部分 部署Kafka集群</h2><h3 id="参考文献及材料"><a href="#参考文献及材料" class="headerlink" title="参考文献及材料"></a>参考文献及材料</h3><p>1、kubernetes 中 kafka 和 zookeeper 有状态集群服务部署实践 (一) <a href="https://cloud.tencent.com/developer/article/1005492" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1005492</a></p><p>2、<a href="https://cloud.tencent.com/developer/article/1005491" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1005491</a></p><p>3、<a href="https://www.bogotobogo.com/DevOps/Docker/Docker_Kubernetes_StatefulSet.php" target="_blank" rel="noopener">https://www.bogotobogo.com/DevOps/Docker/Docker_Kubernetes_StatefulSet.php</a></p><p><a href="https://technology.amis.nl/2018/04/19/15-minutes-to-get-a-kafka-cluster-running-on-kubernetes-and-start-producing-and-consuming-from-a-node-application/" target="_blank" rel="noopener">https://technology.amis.nl/2018/04/19/15-minutes-to-get-a-kafka-cluster-running-on-kubernetes-and-start-producing-and-consuming-from-a-node-application/</a></p><p>4、<a href="https://kubernetes.io/zh/docs/tutorials/stateful-application/basic-stateful-set/" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/tutorials/stateful-application/basic-stateful-set/</a></p><p>5、<a href="https://jimmysong.io/kubernetes-handbook/guide/using-statefulset.html" target="_blank" rel="noopener">https://jimmysong.io/kubernetes-handbook/guide/using-statefulset.html</a></p><p>6、<a href="https://www.cnblogs.com/00986014w/p/9561901.html" target="_blank" rel="noopener">Kubernetes部署Kafka集群</a></p><p><a href="https://blog.usejournal.com/kafka-on-kubernetes-a-good-fit-95251da55837" target="_blank" rel="noopener">https://blog.usejournal.com/kafka-on-kubernetes-a-good-fit-95251da55837</a></p><p><a href="https://www.cnblogs.com/cocowool/p/kubernetes_statefulset.html" target="_blank" rel="noopener">https://www.cnblogs.com/cocowool/p/kubernetes_statefulset.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   Minikube集群启动&lt;/li&gt;
&lt;li&gt;第一部分   Kubernetes中State
      
    
    </summary>
    
      <category term="Minikube Kafka" scheme="https://zjrongxiang.github.io/categories/Minikube-Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>使用Cloudera Quickstart Docker镜像快速部署hadoop集群</title>
    <link href="https://zjrongxiang.github.io/2018/06/25/2019-04-30-%E4%BD%BF%E7%94%A8Cloudera%20Quickstart%20Docker%E9%95%9C%E5%83%8F%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2hadoop%E9%9B%86%E7%BE%A4/"/>
    <id>https://zjrongxiang.github.io/2018/06/25/2019-04-30-使用Cloudera Quickstart Docker镜像快速部署hadoop集群/</id>
    <published>2018-06-25T11:30:00.000Z</published>
    <updated>2020-03-22T03:40:56.191Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   Docker镜像准备</li><li>第二部分   运行容器</li><li>第三部分   cloudera-manager管理</li><li>第四部分   组件使用测试</li><li>第五部分   总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>通常在个人笔记本上部署Hadoop测试集群（含生态圈各组件）是个很耗时的工作。Cloudera公司提供一个快速部署的Docker镜像，可以快速启动一个测试集群。</p><blockquote><p>测试环境为Ubuntu服务器</p></blockquote><h2 id="第一部分-Docker镜像准备"><a href="#第一部分-Docker镜像准备" class="headerlink" title="第一部分 Docker镜像准备"></a>第一部分 Docker镜像准备</h2><p>首先本机需要部署有docker环境，如果没有需要提前部署。</p><h3 id="1-1-拉取Docker镜像"><a href="#1-1-拉取Docker镜像" class="headerlink" title="1.1 拉取Docker镜像"></a>1.1 拉取Docker镜像</h3><p>可以从DockerHub上拉取cloudera/quickstart镜像。</p><blockquote><p>镜像项目地址为：<a href="https://hub.docker.com/r/cloudera/quickstart" target="_blank" rel="noopener">https://hub.docker.com/r/cloudera/quickstart</a></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker pull cloudera/quickstart:latest</span></span><br></pre></td></tr></table></figure><p>如果不具备联网环境，可以通过镜像介质包安装。介质可以在官网（需要注册用户）下载：<a href="https://www.cloudera.com/downloads/quickstart_vms/5-13.html" target="_blank" rel="noopener">https://www.cloudera.com/downloads/quickstart_vms/5-13.html</a></p><blockquote><p>由于墙的原因下载会很慢</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> wget https://downloads.cloudera.com/demo_vm/docker/cloudera-quickstart-vm-5.13.0-0-beta-docker.tar.gz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tar xzf cloudera-quickstart-vm-5.13.0-0-beta-docker.tar.gz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker import - cloudera/quickstart:latest &lt; cloudera-quickstart-vm-5.13.0-0-beta-docker/*.tar</span></span><br></pre></td></tr></table></figure><h3 id="1-2-检查镜像库"><a href="#1-2-检查镜像库" class="headerlink" title="1.2 检查镜像库"></a>1.2 检查镜像库</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker images|grep cloudera</span></span><br><span class="line">cloudera/quickstart   latest   4239cd2958c6   3 years ago         6.34GB</span><br></pre></td></tr></table></figure><p>说明镜像准备好了，下面基于镜像启动容器。</p><h2 id="第二部分-运行容器"><a href="#第二部分-运行容器" class="headerlink" title="第二部分 运行容器"></a>第二部分 运行容器</h2><h3 id="2-1-使用镜像启动容器"><a href="#2-1-使用镜像启动容器" class="headerlink" title="2.1 使用镜像启动容器"></a>2.1 使用镜像启动容器</h3><p>启动CDH集群的命令格式为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker run --hostname=quickstart.cloudera --privileged=<span class="literal">true</span> -t -i [OPTIONS] [IMAGE] /usr/bin/docker-quickstart</span></span><br></pre></td></tr></table></figure><p>官方提示的参数介绍如下：</p><table><thead><tr><th style="text-align:left">Option</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left">–hostname=quickstart.cloudera</td><td style="text-align:left">Required: Pseudo-distributed configuration assumes this hostname.容器主机名（/etc/hosts中指定hostname）。</td></tr><tr><td style="text-align:left">–privileged=true</td><td style="text-align:left">Required: For HBase, MySQL-backed Hive metastore, Hue, Oozie, Sentry, and Cloudera Manager.这是Hbase组件需要的模式。</td></tr><tr><td style="text-align:left">-t</td><td style="text-align:left">Required: Allocate a pseudoterminal. Once services are started, a Bash shell takes over. This switch starts a terminal emulator to run the services.</td></tr><tr><td style="text-align:left">-i</td><td style="text-align:left">Required: If you want to use the terminal, either immediately or connect to the terminal later.</td></tr><tr><td style="text-align:left">-p 8888</td><td style="text-align:left">Recommended: Map the Hue port in the guest to another port on the host.端口映射参数。</td></tr><tr><td style="text-align:left">-p [PORT]</td><td style="text-align:left">Optional: Map any other ports (for example, 7180 for Cloudera Manager, 80 for a guided tutorial).</td></tr><tr><td style="text-align:left">-d</td><td style="text-align:left">Optional: Run the container in the background.容器后台启动。</td></tr><tr><td style="text-align:left">–name</td><td style="text-align:left">容器的名字</td></tr><tr><td style="text-align:left">-v host_path:container_path</td><td style="text-align:left">主机上目录挂载到容器中目录上，主机上该放入任何东西，Docker容器中对于目录可以直接访问。</td></tr></tbody></table><p>当然还可以自定义其他docker启动参数。最后启动命令整理为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run -t -i -d \</span><br><span class="line">--name cdh \</span><br><span class="line">--hostname=quickstart.cloudera \</span><br><span class="line">--privileged=true \</span><br><span class="line">-v /data/CDH:/src \</span><br><span class="line">-p 8020:8020 -p 8022:8022 -p 7180:7180 -p 21050:21050 -p 50070:50070 -p 50075:50075 -p 50010:50010 -p 50020:50020 -p 8890:8890 -p 60010:60010 -p 10002:10002 -p 25010:25010 -p 25020:25020 -p 18088:18088 -p 8088:8088 -p 19888:19888 -p 7187:7187 -p 11000:11000 -p 8888:8888 cloudera/quickstart \</span><br><span class="line">/bin/bash -c '/usr/bin/docker-quickstart'</span><br></pre></td></tr></table></figure><blockquote><p>Cloudera 本身的 manager 是 7180 端口，提前配置端口映射。</p></blockquote><p>启动容器我们使用了-d后台启动参数，如果没有指定后台启动，终端将自动连接到容器，退出shell后容器会中止运行（可以通过使用Ctrl + P + Q命令退出，这样容器会继续保持运行）。</p><p>对于已经后台运行的容器，我们使用下面的命令进入容器shell：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker attach [CONTAINER HASH]</span></span><br></pre></td></tr></table></figure><h3 id="2-2-时钟同步问题"><a href="#2-2-时钟同步问题" class="headerlink" title="2.2 时钟同步问题"></a>2.2 时钟同步问题</h3><p>容器内部使用的时间时区为UTC，和主机（宿主机通常为CST（东八区））不同时区，会提示时钟同步问题。解决的办法是：</p><p>在环境变量中添加时区变量,并source生效:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vi /etc/profile</span></span><br><span class="line">文件末尾添加一行：TZ='Asia/Shanghai'; export TZ</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure><p>最后启动时钟同步服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@quickstart home]# service ntpd start</span><br><span class="line">Starting ntpd:                                             [  OK  ]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查服务状态</span></span><br><span class="line">[root@quickstart home]# service ntpd status</span><br><span class="line">ntpd (pid  13536) is running...</span><br></pre></td></tr></table></figure><p>这样完成时钟同步。</p><h3 id="2-3-使用集群服务"><a href="#2-3-使用集群服务" class="headerlink" title="2.3 使用集群服务"></a>2.3 使用集群服务</h3><p>这样集群的大部分服务组件均可使用。</p><h2 id="第三部分-cloudera-manager管理"><a href="#第三部分-cloudera-manager管理" class="headerlink" title="第三部分 cloudera-manager管理"></a>第三部分 cloudera-manager管理</h2><h3 id="3-1-启动管理服务"><a href="#3-1-启动管理服务" class="headerlink" title="3.1 启动管理服务"></a>3.1 启动管理服务</h3><p>CDH在该镜像中提供cloudera-manager组件，用户集群web管理界面，可以通过下面的命令启动。</p><blockquote><p>需要注意的是，启动后CDH会停止其他组件服务。</p></blockquote><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#./home/cloudera/cloudera-manager --express</span></span><br><span class="line">[QuickStart] Shutting down CDH services via init scripts...</span><br><span class="line">kafka-server: unrecognized service</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /etc/zookeeper/conf/zoo.cfg</span><br><span class="line">[QuickStart] Disabling CDH services on boot...</span><br><span class="line">error reading information on service kafka-server: No such file or directory</span><br><span class="line">[QuickStart] Starting Cloudera Manager server...</span><br><span class="line">[QuickStart] Waiting <span class="keyword">for</span> Cloudera Manager API...</span><br><span class="line">[QuickStart] Starting Cloudera Manager agent...</span><br><span class="line">[QuickStart] Configuring deployment...</span><br><span class="line">Submitted <span class="built_in">jobs</span>: 14</span><br><span class="line">[QuickStart] Deploying client configuration...</span><br><span class="line">Submitted <span class="built_in">jobs</span>: 16</span><br><span class="line">[QuickStart] Starting Cloudera Management Service...</span><br><span class="line">Submitted <span class="built_in">jobs</span>: 24</span><br><span class="line">[QuickStart] Enabling Cloudera Manager daemons on boot...</span><br><span class="line">________________________________________________________________________________</span><br><span class="line"></span><br><span class="line">Success! You can now <span class="built_in">log</span> into Cloudera Manager from the QuickStart VM<span class="string">'s browser:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    http://quickstart.cloudera:7180</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Username: cloudera</span></span><br><span class="line"><span class="string">    Password: cloudera</span></span><br></pre></td></tr></table></figure><p>集群控制台的地址为：<a href="http://quickstart.cloudera:7180，需要注意的是这里quickstart.cloudera是主机名，需要客户端hosts中配置，否则使用实IP或者容器端口映射后使用宿主机IP（例如：192.168.31.3）。" target="_blank" rel="noopener">http://quickstart.cloudera:7180，需要注意的是这里quickstart.cloudera是主机名，需要客户端hosts中配置，否则使用实IP或者容器端口映射后使用宿主机IP（例如：192.168.31.3）。</a></p><p>用户名和密码为：cloudera/cloudera,登录界面如下：</p><p><img src="\images\picture\cloudera-manager1.jpg" alt=""></p><p>登录后，下图是集群控制台：</p><p><img src="\images\picture\cloudera-manager2.jpg" alt=""></p><p>从管理界面上可以看到除了主机和 manager ，其他服务组件均未启动。</p><h3 id="3-2-启动集群组件服务"><a href="#3-2-启动集群组件服务" class="headerlink" title="3.2 启动集群组件服务"></a>3.2 启动集群组件服务</h3><p>在控制台上，我们按照顺序启动HDFS、Hive、Hue、Yarn服务。</p><blockquote><p>如果服务启动异常，可以尝试重启服务组件。注意需要先启动HDFS后启动Hive，否则需要重启Hive。</p></blockquote><h2 id="第四部分-组件使用测试"><a href="#第四部分-组件使用测试" class="headerlink" title="第四部分 组件使用测试"></a>第四部分 组件使用测试</h2><h3 id="4-1-HDFS组件使用"><a href="#4-1-HDFS组件使用" class="headerlink" title="4.1 HDFS组件使用"></a>4.1 HDFS组件使用</h3><p>我们使用HDFS的Python API与集群hdfs文件系统进行交互测试：</p><h4 id="4-1-1-查看文件系统"><a href="#4-1-1-查看文件系统" class="headerlink" title="4.1.1 查看文件系统"></a>4.1.1 查看文件系统</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hdfs.client <span class="keyword">import</span> Client</span><br><span class="line">client = Client(<span class="string">"http://192.168.31.3:50070"</span>, root=<span class="string">"/"</span>, timeout=<span class="number">100</span>)</span><br><span class="line">print(client.list(<span class="string">"/"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ['benchmarks', 'hbase', 'tmp', 'user', 'var']</span></span><br><span class="line"><span class="comment"># 返回一个list记录主目录</span></span><br></pre></td></tr></table></figure><h4 id="4-1-2-上传新增文件"><a href="#4-1-2-上传新增文件" class="headerlink" title="4.1.2 上传新增文件"></a>4.1.2 上传新增文件</h4><p>注意这里需要在宿主机（客户端机器）配置hosts文件：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192<span class="selector-class">.168</span><span class="selector-class">.31</span><span class="selector-class">.3</span>       <span class="selector-tag">quickstart</span><span class="selector-class">.cloudera</span></span><br></pre></td></tr></table></figure><p>然后执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">client.upload(<span class="string">"/tmp"</span>, <span class="string">"/root/jupyter/nohup.out"</span>)</span><br><span class="line"><span class="comment"># '/tmp/nohup.out'</span></span><br><span class="line"><span class="comment"># 返回路径信息</span></span><br></pre></td></tr></table></figure><h4 id="4-1-3-下载hdfs文件"><a href="#4-1-3-下载hdfs文件" class="headerlink" title="4.1.3 下载hdfs文件"></a>4.1.3 下载hdfs文件</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client.download(<span class="string">"/tmp/nohup.out"</span>, <span class="string">"/tmp"</span>)</span><br><span class="line"><span class="comment"># 返回路径'/tmp/nohup.out'</span></span><br></pre></td></tr></table></figure><h2 id="第五部分-总结"><a href="#第五部分-总结" class="headerlink" title="第五部分 总结"></a>第五部分 总结</h2><p>1、Cloudera 的 docker 版本分成两部分启动。(1)启动各组件启动,使用命令为： /usr/bin/docker-quickstart，(2) 启动Cloudera manager 管理服务，启动命令为：/home/cloudera/cloudera-manager。docker启动时选择启动一项。</p><h2 id="参考文献及材料"><a href="#参考文献及材料" class="headerlink" title="参考文献及材料"></a>参考文献及材料</h2><p>1、cloudera/quickstart镜像地址：<a href="https://hub.docker.com/r/cloudera/quickstart" target="_blank" rel="noopener">https://hub.docker.com/r/cloudera/quickstart</a></p><p>2、cloudera/quickstart镜像部署指引：<a href="https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html</a></p><p>3、<a href="https://www.cnblogs.com/piperck/p/9917118.html" target="_blank" rel="noopener">利用 Docker 搭建单机的 Cloudera CDH 以及使用实践</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   Docker镜像准备&lt;/li&gt;
&lt;li&gt;第二部分   运行容器&lt;/li&gt;
&lt;li&gt;第三部分
      
    
    </summary>
    
      <category term="hadoop" scheme="https://zjrongxiang.github.io/categories/hadoop/"/>
    
    
  </entry>
  
</feed>
