<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RongXiang</title>
  
  <subtitle>我的烂笔头</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zjrongxiang.github.io/"/>
  <updated>2021-06-23T15:01:47.391Z</updated>
  <id>https://zjrongxiang.github.io/</id>
  
  <author>
    <name>rong xiang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink Table API &amp; SQL编程总结</title>
    <link href="https://zjrongxiang.github.io/2021/06/14/2021-06-23-Flink%20Table%20API%20&amp;%20SQL%E7%BC%96%E7%A8%8B%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/06/14/2021-06-23-Flink Table API &amp; SQL编程总结/</id>
    <published>2021-06-14T04:42:00.000Z</published>
    <updated>2021-06-23T15:01:47.391Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分 <code>su</code>命令</p></li><li><p>第二部分 <code>su</code> 和 <code>su -</code>区别</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Apache Flink提供了两种顶层的关系型API，分别为Table API和SQL，Flink通过Table API&amp;SQL实现了批流统一。其中Table API是用于Scala和Java的语言集成查询API，它允许以非常直观的方式组合关系运算符（例如select，where和join）的查询。Flink SQL基于<a href="https://calcite.apache.org/" target="_blank" rel="noopener">Apache Calcite</a> 实现了标准的SQL，用户可以使用标准的SQL处理数据集。Table API和SQL与Flink的DataStream和DataSet API紧密集成在一起，用户可以实现相互转化，比如可以将DataStream或者DataSet注册为table进行操作数据。值得注意的是，<strong>Table API and SQL</strong>目前尚未完全完善，还在积极的开发中，所以并不是所有的算子操作都可以通过其实现。</p><h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>从Flink1.9开始，Flink为Table &amp; SQL API提供了两种planner,分别为Blink planner和old planner，其中old planner是在Flink1.9之前的版本使用。主要区别如下：</p><p><strong>尖叫提示</strong>：对于生产环境，目前推荐使用old planner.</p><ul><li><code>flink-table-common</code>: 通用模块，包含 Flink Planner 和 Blink Planner 一些共用的代码</li><li><code>flink-table-api-java</code>: java语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用)</li><li><code>flink-table-api-scala</code>: scala语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用)</li><li><code>flink-table-api-java-bridge</code>: java语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用)</li><li><code>flink-table-api-scala-bridge</code>: scala语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用)</li><li><code>flink-table-planner</code>:planner 和runtime. planner为Flink1,9之前的old planner(推荐使用)</li><li><code>flink-table-planner-blink</code>: 新的Blink planner.</li><li><code>flink-table-runtime-blink</code>: 新的Blink runtime.</li><li><code>flink-table-uber</code>: 将上述的API模块及old planner打成一个jar包，形如flink-table-*.jar，位与/lib目录下</li><li><code>flink-table-uber-blink</code>:将上述的API模块及Blink 模块打成一个jar包，形如fflink-table-blink-*.jar，位与/lib目录下</li></ul><h2 id="Blink-planner-amp-old-planner"><a href="#Blink-planner-amp-old-planner" class="headerlink" title="Blink planner &amp; old planner"></a>Blink planner &amp; old planner</h2><p>Blink planner和old planner有许多不同的特点，具体列举如下：</p><ul><li>Blink planner将批处理作业看做是流处理作业的特例。所以，不支持Table 与DataSet之间的转换，批处理的作业也不会被转成DataSet程序，而是被转为DataStream程序。</li><li>Blink planner不支持 <code>BatchTableSource</code>，使用的是有界的StreamTableSource。</li><li>Blink planner仅支持新的 <code>Catalog</code>，不支持<code>ExternalCatalog</code> (已过时)。</li><li>对于FilterableTableSource的实现，两种Planner是不同的。old planner会谓词下推到<code>PlannerExpression</code>(未来会被移除)，而Blink planner 会谓词下推到 <code>Expression</code>(表示一个产生计算结果的逻辑树)。</li><li>仅仅Blink planner支持key-value形式的配置，即通过Configuration进行参数设置。</li><li>关于PlannerConfig的实现，两种planner有所不同。</li><li>Blink planner 会将多个sink优化成一个DAG(仅支持TableEnvironment，StreamTableEnvironment不支持)，old planner总是将每一个sink优化成一个新的DAG，每一个DAG都是相互独立的。</li><li>old planner不支持catalog统计，Blink planner支持catalog统计。</li></ul><h2 id="第一部分-Flink-Table-amp-SQL程序的pom依赖"><a href="#第一部分-Flink-Table-amp-SQL程序的pom依赖" class="headerlink" title="第一部分 Flink Table &amp; SQL程序的pom依赖"></a>第一部分 Flink Table &amp; SQL程序的pom依赖</h2><p>根据使用的语言不同，可以选择下面的依赖，包括scala版和java版，如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- java版 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-java-bridge_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- scala版 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-scala-bridge_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>除此之外，如果需要在本地的IDE中运行Table API &amp; SQL的程序，则需要添加下面的pom依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Flink 1.9之前的old planner --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 新的Blink planner --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>另外，如果需要实现自定义的格式(比如和kafka交互)或者用户自定义函数，需要添加如下依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Table-API-amp-SQL的编程模板"><a href="#Table-API-amp-SQL的编程模板" class="headerlink" title="Table API &amp; SQL的编程模板"></a>Table API &amp; SQL的编程模板</h2><p>所有的Table API&amp;SQL的程序(无论是批处理还是流处理)都有着相同的形式，下面将给出通用的编程结构形式：</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个TableEnvironment对象，指定planner、处理模式(batch、streaming)</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 创建一个表</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"table1"</span>);</span><br><span class="line"><span class="comment">// 注册一个外部的表</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"outputTable"</span>);</span><br><span class="line"><span class="comment">// 通过Table API的查询创建一个Table 对象</span></span><br><span class="line"><span class="keyword">Table</span> tapiResult <span class="comment">= tableEnv.from(</span><span class="comment">"table1"</span><span class="comment">).select(...)</span>;</span><br><span class="line"><span class="comment">// 通过SQL查询的查询创建一个Table 对象</span></span><br><span class="line"><span class="keyword">Table</span> sqlResult  <span class="comment">= tableEnv.sqlQuery(</span><span class="comment">"SELECT ... FROM table1 ... "</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将结果写入TableSink</span></span><br><span class="line">tapiResult.insertInto(<span class="string">"outputTable"</span>);</span><br><span class="line"><span class="comment">// 执行</span></span><br><span class="line">tableEnv.execute(<span class="string">"java_job"</span>);</span><br></pre></td></tr></table></figure><p>注意：Table API &amp; SQL的查询可以相互集成，另外还可以在DataStream或者DataSet中使用Table API &amp; SQL的API，实现DataStreams、 DataSet与Table之间的相互转换。</p><h2 id="创建TableEnvironment"><a href="#创建TableEnvironment" class="headerlink" title="创建TableEnvironment"></a>创建TableEnvironment</h2><p>TableEnvironment是Table API &amp; SQL程序的一个入口，主要包括如下的功能：</p><ul><li>在内部的catalog中注册Table</li><li>注册catalog</li><li>加载可插拔模块</li><li>执行SQL查询</li><li>注册用户定义函数</li><li><code>DataStream</code> 、<code>DataSet</code>与Table之间的相互转换</li><li>持有对<code>ExecutionEnvironment</code> 、<code>StreamExecutionEnvironment</code>的引用</li></ul><p>一个Table必定属于一个具体的TableEnvironment，不可以将不同TableEnvironment的表放在一起使用(比如join，union等操作)。</p><p>TableEnvironment是通过调用 <code>BatchTableEnvironment.create()</code> 或者StreamTableEnvironment.create()的静态方法进行创建的。另外，默认两个planner的jar包都存在与classpath下，所有需要明确指定使用的planner。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="comment">// FLINK 流处理查询</span></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line">import org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings fsSettings = <span class="module-access"><span class="module"><span class="identifier">EnvironmentSettings</span>.</span></span><span class="keyword">new</span><span class="constructor">Instance()</span>.use<span class="constructor">OldPlanner()</span>.<span class="keyword">in</span><span class="constructor">StreamingMode()</span>.build<span class="literal">()</span>;</span><br><span class="line">StreamExecutionEnvironment fsEnv = <span class="module-access"><span class="module"><span class="identifier">StreamExecutionEnvironment</span>.</span></span>get<span class="constructor">ExecutionEnvironment()</span>;</span><br><span class="line">StreamTableEnvironment fsTableEnv = <span class="module-access"><span class="module"><span class="identifier">StreamTableEnvironment</span>.</span></span>create(fsEnv, fsSettings);</span><br><span class="line"><span class="comment">//或者TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="comment">// FLINK 批处理查询</span></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line">import org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line">import org.apache.flink.table.api.java.BatchTableEnvironment;</span><br><span class="line"></span><br><span class="line">ExecutionEnvironment fbEnv = <span class="module-access"><span class="module"><span class="identifier">ExecutionEnvironment</span>.</span></span>get<span class="constructor">ExecutionEnvironment()</span>;</span><br><span class="line">BatchTableEnvironment fbTableEnv = <span class="module-access"><span class="module"><span class="identifier">BatchTableEnvironment</span>.</span></span>create(fbEnv);</span><br><span class="line"></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="comment">// BLINK 流处理查询</span></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line">import org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">StreamExecutionEnvironment bsEnv = <span class="module-access"><span class="module"><span class="identifier">StreamExecutionEnvironment</span>.</span></span>get<span class="constructor">ExecutionEnvironment()</span>;</span><br><span class="line">EnvironmentSettings bsSettings = <span class="module-access"><span class="module"><span class="identifier">EnvironmentSettings</span>.</span></span><span class="keyword">new</span><span class="constructor">Instance()</span>.use<span class="constructor">BlinkPlanner()</span>.<span class="keyword">in</span><span class="constructor">StreamingMode()</span>.build<span class="literal">()</span>;</span><br><span class="line">StreamTableEnvironment bsTableEnv = <span class="module-access"><span class="module"><span class="identifier">StreamTableEnvironment</span>.</span></span>create(bsEnv, bsSettings);</span><br><span class="line"><span class="comment">// 或者 TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="comment">// BLINK 批处理查询</span></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line">import org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line">import org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings bbSettings = <span class="module-access"><span class="module"><span class="identifier">EnvironmentSettings</span>.</span></span><span class="keyword">new</span><span class="constructor">Instance()</span>.use<span class="constructor">BlinkPlanner()</span>.<span class="keyword">in</span><span class="constructor">BatchMode()</span>.build<span class="literal">()</span>;</span><br><span class="line">TableEnvironment bbTableEnv = <span class="module-access"><span class="module"><span class="identifier">TableEnvironment</span>.</span></span>create(bbSettings);</span><br></pre></td></tr></table></figure><h2 id="在catalog中创建表"><a href="#在catalog中创建表" class="headerlink" title="在catalog中创建表"></a>在catalog中创建表</h2><h3 id="临时表与永久表"><a href="#临时表与永久表" class="headerlink" title="临时表与永久表"></a>临时表与永久表</h3><p>表可以分为临时表和永久表两种，其中永久表需要一个catalog(比如Hive的Metastore)俩维护表的元数据信息，一旦永久表被创建，只要连接到该catalog就可以访问该表，只有显示删除永久表，该表才可以被删除。临时表的生命周期是Flink Session，这些表不能够被其他的Flink Session访问，这些表不属于任何的catalog或者数据库，如果与临时表相对应的数据库被删除了，该临时表也不会被删除。</p><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><h4 id="虚表-Virtual-Tables"><a href="#虚表-Virtual-Tables" class="headerlink" title="虚表(Virtual Tables)"></a>虚表(Virtual Tables)</h4><p>一个Table对象相当于SQL中的视图(虚表)，它封装了一个逻辑执行计划，可以通过一个catalog创建，具体如下：</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取一个TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// table对象，查询的结果集</span></span><br><span class="line"><span class="keyword">Table</span> projTable <span class="comment">= tableEnv.from(</span><span class="comment">"X"</span><span class="comment">).select(...)</span>;</span><br><span class="line"><span class="comment">// 注册一个表，名称为 "projectedTable"</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"projectedTable"</span>, projTable);</span><br></pre></td></tr></table></figure><h4 id="外部数据源表-Connector-Tables"><a href="#外部数据源表-Connector-Tables" class="headerlink" title="外部数据源表(Connector Tables)"></a>外部数据源表(Connector Tables)</h4><p>可以把外部的数据源注册成表，比如可以读取MySQL数据库数据、Kafka数据等</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tableEnvironment</span><br><span class="line">  .connect(...)</span><br><span class="line">  .<span class="keyword">with</span><span class="constructor">Format(<span class="operator">...</span>)</span></span><br><span class="line">  .<span class="keyword">with</span><span class="constructor">Schema(<span class="operator">...</span>)</span></span><br><span class="line">  .<span class="keyword">in</span><span class="constructor">AppendMode()</span></span><br><span class="line">  .create<span class="constructor">TemporaryTable(<span class="string">"MyTable"</span>)</span></span><br></pre></td></tr></table></figure><h3 id="扩展创建表的标识属性"><a href="#扩展创建表的标识属性" class="headerlink" title="扩展创建表的标识属性"></a>扩展创建表的标识属性</h3><p>表的注册总是包含三部分标识属性：catalog、数据库、表名。用户可以在内部设置一个catalog和一个数据库作为当前的catalog和数据库，所以对于catalog和数据库这两个标识属性是可选的，即如果不指定，默认使用的是“current catalog”和 “current database”。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tEnv = ...;</span><br><span class="line">tEnv.useCatalog(<span class="string">"custom_catalog"</span>);<span class="comment">//设置catalog</span></span><br><span class="line">tEnv.useDatabase(<span class="string">"custom_database"</span>);<span class="comment">//设置数据库</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= ...</span>;</span><br><span class="line"><span class="comment">// 注册一个名为exampleView的视图，catalog名为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为custom_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"exampleView"</span>, <span class="keyword">table</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为exampleView的视图，catalog的名为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为other_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"other_database.exampleView"</span>, <span class="keyword">table</span>);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 注册一个名为'View'的视图，catalog的名称为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为custom_database，'View'是保留关键字，需要使用``(反引号)</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"`View`"</span>, <span class="keyword">table</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为example.View的视图，catalog的名为custom_catalog，</span></span><br><span class="line"><span class="comment">// 数据库名为custom_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"`example.View`"</span>, <span class="keyword">table</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为'exampleView'的视图， catalog的名为'other_catalog'</span></span><br><span class="line"><span class="comment">// 数据库名为other_database' </span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"other_catalog.other_database.exampleView"</span>, <span class="keyword">table</span>);</span><br></pre></td></tr></table></figure><h2 id="查询表"><a href="#查询表" class="headerlink" title="查询表"></a>查询表</h2><h3 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h3><p>Table API是一个集成Scala与Java语言的查询API，与SQL相比，它的查询不是一个标准的SQL语句，而是由一步一步的操作组成的。如下展示了一个使用Table API实现一个简单的聚合查询。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"><span class="comment">//注册Orders表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询注册的表</span></span><br><span class="line"><span class="keyword">Table</span> orders <span class="comment">= tableEnv.from(</span><span class="comment">"Orders"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 计算操作</span></span><br><span class="line"><span class="keyword">Table</span> revenue <span class="comment">= orders</span></span><br><span class="line">  .filter(<span class="string">"cCountry === 'FRANCE'"</span>)</span><br><span class="line">  .groupBy(<span class="string">"cID, cName"</span>)</span><br><span class="line">  .select(<span class="string">"cID, cName, revenue.sum AS revSum"</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p>Flink SQL依赖于<a href="https://calcite.apache.org/" target="_blank" rel="noopener">Apache Calcite</a>，其实现了标准的SQL语法，如下案例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// 获取TableEnvironment</span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line">//注册Orders表</span><br><span class="line"></span><br><span class="line">// 计算逻辑同上面的Table API</span><br><span class="line">Table revenue = tableEnv.sqlQuery(</span><br><span class="line">    "<span class="keyword">SELECT</span> cID, cName, <span class="keyword">SUM</span>(revenue) <span class="keyword">AS</span> revSum <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">FROM</span> Orders <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">WHERE</span> cCountry = <span class="string">'FRANCE'</span> <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">GROUP</span> <span class="keyword">BY</span> cID, cName<span class="string">"</span></span><br><span class="line"><span class="string">  );</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 注册"</span>RevenueFrance<span class="string">"外部输出表</span></span><br><span class="line"><span class="string">// 计算结果插入"</span>RevenueFrance<span class="string">"表</span></span><br><span class="line"><span class="string">tableEnv.sqlUpdate(</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">INSERT</span> <span class="keyword">INTO</span> RevenueFrance <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">SELECT</span> cID, cName, <span class="keyword">SUM</span>(revenue) <span class="keyword">AS</span> revSum <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">FROM</span> Orders <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">WHERE</span> cCountry = <span class="string">'FRANCE'</span> <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">GROUP</span> <span class="keyword">BY</span> cID, cName<span class="string">"</span></span><br><span class="line"><span class="string">  );</span></span><br></pre></td></tr></table></figure><h2 id="输出表"><a href="#输出表" class="headerlink" title="输出表"></a>输出表</h2><p>一个表通过将其写入到TableSink，然后进行输出。TableSink是一个通用的支持多种文件格式(CSV、Parquet, Avro)和多种外部存储系统(JDBC, Apache HBase, Apache Cassandra, Elasticsearch)以及多种消息对列(Apache Kafka, RabbitMQ)的接口。</p><p>批处理的表只能被写入到 <code>BatchTableSink</code>,流处理的表需要指明AppendStreamTableSink、RetractStreamTableSink或者 <code>UpsertStreamTableSink</code></p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建输出表</span></span><br><span class="line">final Schema schema = <span class="keyword">new</span> <span class="constructor">Schema()</span></span><br><span class="line">    .field(<span class="string">"a"</span>, DataTypes.<span class="constructor">INT()</span>)</span><br><span class="line">    .field(<span class="string">"b"</span>, DataTypes.<span class="constructor">STRING()</span>)</span><br><span class="line">    .field(<span class="string">"c"</span>, DataTypes.<span class="constructor">LONG()</span>);</span><br><span class="line"></span><br><span class="line">tableEnv.connect(<span class="keyword">new</span> <span class="constructor">FileSystem(<span class="string">"/path/to/file"</span>)</span>)</span><br><span class="line">    .<span class="keyword">with</span><span class="constructor">Format(<span class="params">new</span> Csv()</span>.field<span class="constructor">Delimiter('|')</span>.derive<span class="constructor">Schema()</span>)</span><br><span class="line">    .<span class="keyword">with</span><span class="constructor">Schema(<span class="params">schema</span>)</span></span><br><span class="line">    .create<span class="constructor">TemporaryTable(<span class="string">"CsvSinkTable"</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算结果表</span></span><br><span class="line">Table result = ...</span><br><span class="line"><span class="comment">// 输出结果表到注册的TableSink</span></span><br><span class="line">result.insert<span class="constructor">Into(<span class="string">"CsvSinkTable"</span>)</span>;</span><br></pre></td></tr></table></figure><h2 id="Table-API-amp-SQL底层的转换与执行"><a href="#Table-API-amp-SQL底层的转换与执行" class="headerlink" title="Table API &amp; SQL底层的转换与执行"></a>Table API &amp; SQL底层的转换与执行</h2><p>上文提到了Flink提供了两种planner，分别为old planner和Blink planner，对于不同的planner而言，Table API &amp; SQL底层的执行与转换是有所不同的。</p><h4 id="Old-planner"><a href="#Old-planner" class="headerlink" title="Old planner"></a>Old planner</h4><p>根据是流处理作业还是批处理作业，Table API &amp;SQL会被转换成DataStream或者DataSet程序。一个查询在内部表示为一个逻辑查询计划，会被转换为两个阶段:</p><ul><li>1.逻辑查询计划优化</li><li>2.转换成DataStream或者DataSet程序</li></ul><p>上面的两个阶段只有下面的操作被执行时才会被执行：</p><ul><li>当一个表被输出到TableSink时，比如调用了Table.insertInto()方法</li><li>当执行更新查询时，比如调用TableEnvironment.sqlUpdate()方法</li><li>当一个表被转换为DataStream或者DataSet时</li></ul><p>一旦执行上述两个阶段，Table API &amp; SQL的操作会被看做是普通的DataStream或者DataSet程序，所以当<code>StreamExecutionEnvironment.execute()</code>或者<code>ExecutionEnvironment.execute()</code> 被调用时，会执行转换后的程序。</p><h4 id="Blink-planner"><a href="#Blink-planner" class="headerlink" title="Blink planner"></a>Blink planner</h4><p>无论是批处理作业还是流处理作业，如果使用的是Blink planner，底层都会被转换为DataStream程序。在一个查询在内部表示为一个逻辑查询计划，会被转换成两个阶段：</p><ul><li>1.逻辑查询计划优化</li><li>2.转换成DataStream程序</li></ul><p>对于<code>TableEnvironment</code> and <code>StreamTableEnvironment</code>而言，一个查询的转换是不同的</p><p>首先对于TableEnvironment，当TableEnvironment.execute()方法执行时，Table API &amp; SQL的查询才会被转换，因为TableEnvironment会将多个sink优化为一个DAG。</p><p>对于StreamTableEnvironment，转换发生的时间与old planner相同。</p><h2 id="与DataStream-amp-DataSet-API集成"><a href="#与DataStream-amp-DataSet-API集成" class="headerlink" title="与DataStream &amp; DataSet API集成"></a>与DataStream &amp; DataSet API集成</h2><p>对于Old planner与Blink planner而言，只要是流处理的操作，都可以与DataStream API集成，<strong>仅仅只有Old planner才可以与DataSet API集成</strong>，由于Blink planner的批处理作业会被转换成DataStream程序，所以不能够与DataSet API集成。值得注意的是，下面提到的table与DataSet之间的转换仅适用于Old planner。</p><p>Table API &amp; SQL的查询很容易与DataStream或者DataSet程序集成，并可以将Table API &amp; SQL的查询嵌入DataStream或者DataSet程序中。DataStream或者DataSet可以转换成表，反之，表也可以被转换成DataStream或者DataSet。</p><h3 id="从DataStream或者DataSet中注册临时表-视图"><a href="#从DataStream或者DataSet中注册临时表-视图" class="headerlink" title="从DataStream或者DataSet中注册临时表(视图)"></a>从DataStream或者DataSet中注册临时表(视图)</h3><p><strong>尖叫提示：</strong>只能将DataStream或者DataSet转换为临时表(视图)</p><p>下面演示DataStream的转换，对于DataSet的转换类似。</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 获取<span class="keyword">StreamTableEnvironment</span></span><br><span class="line"><span class="keyword">StreamTableEnvironment </span>tableEnv = ...<span class="comment">; </span></span><br><span class="line"><span class="symbol">DataStream</span>&lt;Tuple2&lt;Long, <span class="keyword">String&gt;&gt; </span><span class="keyword">stream </span>= ...</span><br><span class="line">// 将DataStream注册为一个名为myTable的视图，其中字段分别为<span class="string">"f0"</span>, <span class="string">"f1"</span></span><br><span class="line"><span class="symbol">tableEnv.createTemporaryView</span>(<span class="string">"myTable"</span>, <span class="keyword">stream);</span></span><br><span class="line"><span class="keyword">// </span>将DataStream注册为一个名为myTable2的视图,其中字段分别为<span class="string">"myLong"</span>, <span class="string">"myString"</span></span><br><span class="line"><span class="symbol">tableEnv.createTemporaryView</span>(<span class="string">"myTable2"</span>, <span class="keyword">stream, </span><span class="string">"myLong, myString"</span>)<span class="comment">;</span></span><br></pre></td></tr></table></figure><h3 id="将DataStream或者DataSet转化为Table对象"><a href="#将DataStream或者DataSet转化为Table对象" class="headerlink" title="将DataStream或者DataSet转化为Table对象"></a>将DataStream或者DataSet转化为Table对象</h3><p>可以直接将DataStream或者DataSet转换为Table对象，之后可以使用Table API进行查询操作。下面演示DataStream的转换，对于DataSet的转换类似。</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 获取<span class="keyword">StreamTableEnvironment</span></span><br><span class="line"><span class="keyword">StreamTableEnvironment </span>tableEnv = ...<span class="comment">; </span></span><br><span class="line"><span class="symbol">DataStream</span>&lt;Tuple2&lt;Long, <span class="keyword">String&gt;&gt; </span><span class="keyword">stream </span>= ...</span><br><span class="line">// 将DataStream转换为Table对象，默认的字段为<span class="string">"f0"</span>, <span class="string">"f1"</span></span><br><span class="line"><span class="symbol">Table</span> table1 = tableEnv.fromDataStream(<span class="keyword">stream);</span></span><br><span class="line"><span class="keyword">// </span>将DataStream转换为Table对象，默认的字段为<span class="string">"myLong"</span>, <span class="string">"myString"</span></span><br><span class="line"><span class="symbol">Table</span> table2 = tableEnv.fromDataStream(<span class="keyword">stream, </span><span class="string">"myLong, myString"</span>)<span class="comment">;</span></span><br></pre></td></tr></table></figure><h3 id="将表转换为DataStream或者DataSet"><a href="#将表转换为DataStream或者DataSet" class="headerlink" title="将表转换为DataStream或者DataSet"></a>将表转换为DataStream或者DataSet</h3><p>当将Table转为DataStream或者DataSet时，需要指定DataStream或者DataSet的数据类型。通常最方便的数据类型是row类型，Flink提供了很多的数据类型供用户选择，具体包括Row、POJO、样例类、Tuple和原子类型。</p><h4 id="将表转换为DataStream"><a href="#将表转换为DataStream" class="headerlink" title="将表转换为DataStream"></a>将表转换为DataStream</h4><p>一个流处理查询的结果是动态变化的，所以将表转为DataStream时需要指定一个更新模式，共有两种模式：<strong>Append Mode</strong>和<strong>Retract Mode</strong>。</p><ul><li><strong>Append Mode</strong></li></ul><p>如果动态表仅只有Insert操作，即之前输出的结果不会被更新，则使用该模式。如果更新或删除操作使用追加模式会失败报错</p><ul><li><strong>Retract Mode</strong></li></ul><p>始终可以使用此模式。返回值是boolean类型。它用true或false来标记数据的插入和撤回，返回true代表数据插入，false代表数据的撤回。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment. </span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 包含两个字段的表(String name, Integer age)</span></span><br><span class="line">Table table = ...</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为Row</span></span><br><span class="line">DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为定义好的TypeInformation</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(</span><br><span class="line">  Types.STRING(),</span><br><span class="line">  Types.INT());</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = </span><br><span class="line">  tableEnv.toAppendStream(table, tupleType);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用的模式为Retract Mode撤回模式，类型为Row</span></span><br><span class="line"><span class="comment">// 对于转换后的DataStream&lt;Tuple2&lt;Boolean, X&gt;&gt;，X表示流的数据类型，</span></span><br><span class="line"><span class="comment">// boolean值表示数据改变的类型，其中INSERT返回true，DELETE返回的是false</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = </span><br><span class="line">  tableEnv.toRetractStream(table, Row.class);</span><br></pre></td></tr></table></figure><h4 id="将表转换为DataSet"><a href="#将表转换为DataSet" class="headerlink" title="将表转换为DataSet"></a>将表转换为DataSet</h4><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取BatchTableEnvironment</span></span><br><span class="line">BatchTableEnvironment tableEnv = <span class="module-access"><span class="module"><span class="identifier">BatchTableEnvironment</span>.</span></span>create(env);</span><br><span class="line"><span class="comment">// 包含两个字段的表(String name, Integer age)</span></span><br><span class="line">Table table = ...</span><br><span class="line"><span class="comment">// 将表转为DataSet数据类型为Row</span></span><br><span class="line">DataSet&lt;Row&gt; dsRow = tableEnv.<span class="keyword">to</span><span class="constructor">DataSet(<span class="params">table</span>, Row.<span class="params">class</span>)</span>;</span><br><span class="line"><span class="comment">// 将表转为DataSet，通过TypeInformation定义Tuple2&lt;String, Integer&gt;数据类型</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(</span><br><span class="line">  Types.<span class="constructor">STRING()</span>,</span><br><span class="line">  Types.<span class="constructor">INT()</span>);</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = </span><br><span class="line">  tableEnv.<span class="keyword">to</span><span class="constructor">DataSet(<span class="params">table</span>, <span class="params">tupleType</span>)</span>;</span><br></pre></td></tr></table></figure><h3 id="表的Schema与数据类型之间的映射"><a href="#表的Schema与数据类型之间的映射" class="headerlink" title="表的Schema与数据类型之间的映射"></a>表的Schema与数据类型之间的映射</h3><p>表的Schema与数据类型之间的映射有两种方式：分别是基于字段下标位置的映射和基于字段名称的映射。</p><h4 id="基于字段下标位置的映射"><a href="#基于字段下标位置的映射" class="headerlink" title="基于字段下标位置的映射"></a>基于字段下标位置的映射</h4><p>该方式是按照字段的顺序进行一一映射，使用方式如下：</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, <span class="keyword">Integer</span>&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"和"f1"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，选取tuple的第一个元素，指定一个名为"myLong"的字段名</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"myLong"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，为tuple的第一个元素指定名为"myLong"，为第二个元素指定myInt的字段名</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"myLong, myInt"</span><span class="comment">)</span>;</span><br></pre></td></tr></table></figure><h4 id="基于字段名称的映射"><a href="#基于字段名称的映射" class="headerlink" title="基于字段名称的映射"></a>基于字段名称的映射</h4><p>基于字段名称的映射方式支持任意的数据类型包括POJO类型，可以很灵活地定义表Schema映射，所有的字段被映射成一个具体的字段名称，同时也可以使用”as”为字段起一个别名。其中Tuple元素的第一个元素为f0,第二个元素为f1，以此类推。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, <span class="keyword">Integer</span>&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"和"f1"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，选择tuple的第二个元素，指定一个名为"f1"的字段名</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"f1"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，交换字段的顺序</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"f1, f0"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，交换字段的顺序，并为f1起别名为"myInt"，为f0起别名为"myLong</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"f1 as myInt, f0 as myLong"</span><span class="comment">)</span>;</span><br></pre></td></tr></table></figure><h4 id="原子类型"><a href="#原子类型" class="headerlink" title="原子类型"></a>原子类型</h4><p>Flink将<code>Integer</code>, <code>Double</code>, <code>String</code>或者普通的类型称之为原子类型，一个数据类型为原子类型的DataStream或者DataSet可以被转成单个字段属性的表，这个字段的类型与DataStream或者DataSet的数据类型一致，这个字段的名称可以进行指定。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 数据类型为原子类型Long</span></span><br><span class="line">DataStream&lt;Long&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为myLong"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"myLong"</span><span class="comment">)</span>;</span><br></pre></td></tr></table></figure><h4 id="Tuple类型"><a href="#Tuple类型" class="headerlink" title="Tuple类型"></a>Tuple类型</h4><p>Tuple类型的DataStream或者DataSet都可以转为表，可以重新设定表的字段名(即根据tuple元素的位置进行一一映射，转为表之后，每个元素都有一个别名)，如果不为字段指定名称，则使用默认的名称(java语言默认的是f0,f1,scala默认的是_1),用户也可以重新排列字段的顺序，并为每个字段起一个别名。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">//Tuple2&lt;Long, String&gt;类型的DataStream</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为 "f0", "f1"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为 "myLong", "myString"(按照Tuple元素的顺序位置)</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"myLong, myString"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为 "f0", "f1"，并且交换顺序</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"f1, f0"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，只选择Tuple的第二个元素，指定字段名为"f1"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"f1"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，为Tuple的第二个元素指定别名为myString，为第一个元素指定字段名为myLong</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"f1 as 'myString', f0 as 'myLong'"</span><span class="comment">)</span>;</span><br></pre></td></tr></table></figure><h4 id="POJO类型"><a href="#POJO类型" class="headerlink" title="POJO类型"></a>POJO类型</h4><p>当将POJO类型的DataStream或者DataSet转为表时，如果不指定表名，则默认使用的是POJO字段本身的名称，原始字段名称的映射需要指定原始字段的名称，可以为其起一个别名，也可以调换字段的顺序，也可以只选择部分的字段。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">//数据类型为Person的POJO类型，字段包括"name"和"age"</span></span><br><span class="line">DataStream&lt;Person&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名称为"age", "name"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream)</span>;</span><br><span class="line"><span class="comment">//  将DataStream转为表，为"age"字段指定别名myAge, 为"name"字段指定别名myName</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"age as myAge, name as myName"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">//  将DataStream转为表，只选择一个name字段</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"name"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">//  将DataStream转为表，只选择一个name字段，并起一个别名myName</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"name as myName"</span><span class="comment">)</span>;</span><br></pre></td></tr></table></figure><h4 id="Row类型"><a href="#Row类型" class="headerlink" title="Row类型"></a>Row类型</h4><p>Row类型的DataStream或者DataSet转为表的过程中，可以根据字段的位置或者字段名称进行映射，同时也可以为字段起一个别名，或者只选择部分字段。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// Row类型的DataStream，通过RowTypeInfo指定两个字段"name"和"age"</span></span><br><span class="line">DataStream&lt;Row&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为原始字段名"name"和"age"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据位置映射，为第一个字段指定myName别名，为第二个字段指定myAge别名</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"myName, myAge"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，为name字段起别名myName，为age字段起别名myAge</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"name as myName, age as myAge"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，只选择name字段</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"name"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，只选择name字段，并起一个别名"myName"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"name as myName"</span><span class="comment">)</span>;</span><br></pre></td></tr></table></figure><h2 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h2><h3 id="Old-planner-1"><a href="#Old-planner-1" class="headerlink" title="Old planner"></a>Old planner</h3><p>Apache Flink利用Apache Calcite来优化和转换查询。当前执行的优化包括投影和过滤器下推，去相关子查询以及其他类型的查询重写。Old Planner目前不支持优化JOIN的顺序，而是按照查询中定义的顺序执行它们。</p><p>通过提供一个<code>CalciteConfig</code>对象，可以调整在不同阶段应用的优化规则集。这可通过调用<code>CalciteConfig.createBuilder()</code>方法来进行创建，并通过调用<code>tableEnv.getConfig.setPlannerConfig(calciteConfig)</code>方法将该对象传递给TableEnvironment。</p><h3 id="Blink-planner-1"><a href="#Blink-planner-1" class="headerlink" title="Blink planner"></a>Blink planner</h3><p>Apache Flink利用并扩展了Apache Calcite来执行复杂的查询优化。这包括一系列基于规则和基于成本的优化(cost_based)，例如：</p><ul><li>基于Apache Calcite的去相关子查询</li><li>投影裁剪</li><li>分区裁剪</li><li>过滤器谓词下推</li><li>过滤器下推</li><li>子计划重复数据删除以避免重复计算</li><li>特殊的子查询重写，包括两个部分：<ul><li>将IN和EXISTS转换为左半联接( left semi-join)</li><li>将NOT IN和NOT EXISTS转换为left anti-join</li></ul></li><li>调整join的顺序，需要启用 <code>table.optimizer.join-reorder-enabled</code></li></ul><p><strong>注意：</strong> IN / EXISTS / NOT IN / NOT EXISTS当前仅在子查询重写的结合条件下受支持。</p><p>查询优化器不仅基于计划，而且还可以基于数据源的统计信息以及每个操作的细粒度开销(例如io，cpu，网络和内存）,从而做出更加明智且合理的优化决策。</p><p>高级用户可以通过<code>CalciteConfig</code>对象提供自定义优化规则，通过调用tableEnv.getConfig.setPlannerConfig(calciteConfig)，将参数传递给TableEnvironment。</p><h3 id="查看执行计划"><a href="#查看执行计划" class="headerlink" title="查看执行计划"></a>查看执行计划</h3><p>SQL语言支持通过explain来查看某条SQL的执行计划，Flink Table API也可以通过调用explain()方法来查看具体的执行计划。该方法返回一个字符串用来描述三个部分计划，分别为：</p><ol><li>关系查询的抽象语法树，即未优化的逻辑查询计划，</li><li>优化的逻辑查询计划</li><li>实际执行计划</li></ol><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = <span class="module-access"><span class="module"><span class="identifier">StreamExecutionEnvironment</span>.</span></span>get<span class="constructor">ExecutionEnvironment()</span>;</span><br><span class="line">StreamTableEnvironment tEnv = <span class="module-access"><span class="module"><span class="identifier">StreamTableEnvironment</span>.</span></span>create(env);</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.from<span class="constructor">Elements(<span class="params">new</span> Tuple2&lt;&gt;(1, <span class="string">"hello"</span>)</span>);</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.from<span class="constructor">Elements(<span class="params">new</span> Tuple2&lt;&gt;(1, <span class="string">"hello"</span>)</span>);</span><br><span class="line">Table table1 = tEnv.from<span class="constructor">DataStream(<span class="params">stream1</span>, <span class="string">"count, word"</span>)</span>;</span><br><span class="line">Table table2 = tEnv.from<span class="constructor">DataStream(<span class="params">stream2</span>, <span class="string">"count, word"</span>)</span>;</span><br><span class="line">Table table = table1</span><br><span class="line">  .where(<span class="string">"LIKE(word, 'F%')"</span>)</span><br><span class="line">  .union<span class="constructor">All(<span class="params">table2</span>)</span>;</span><br><span class="line"><span class="comment">// 查看执行计划</span></span><br><span class="line">String explanation = tEnv.explain(table);</span><br><span class="line"><span class="module-access"><span class="module"><span class="identifier">System</span>.</span></span>out.println(explanation);</span><br></pre></td></tr></table></figure><p>执行计划的结果为：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">== 抽象语法树 ==</span><br><span class="line">LogicalUnion(<span class="keyword">all</span>=[<span class="keyword">true</span>])</span><br><span class="line">  LogicalFilter(condition=[<span class="keyword">LIKE</span>(<span class="meta">$1</span>, _UTF<span class="number">-16</span>L<span class="string">E'F%'</span>)])</span><br><span class="line">    FlinkLogicalDataStreamScan(id=[<span class="number">1</span>], fields=[count, word])</span><br><span class="line">  FlinkLogicalDataStreamScan(id=[<span class="number">2</span>], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== 优化的逻辑执行计划 ==</span><br><span class="line">DataStreamUnion(<span class="keyword">all</span>=[<span class="keyword">true</span>], <span class="keyword">union</span> <span class="keyword">all</span>=[count, word])</span><br><span class="line">  DataStreamCalc(<span class="keyword">select</span>=[count, word], <span class="keyword">where</span>=[<span class="keyword">LIKE</span>(word, _UTF<span class="number">-16</span>L<span class="string">E'F%'</span>)])</span><br><span class="line">    DataStreamScan(id=[<span class="number">1</span>], fields=[count, word])</span><br><span class="line">  DataStreamScan(id=[<span class="number">2</span>], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== 物理执行计划 ==</span><br><span class="line">Stage <span class="number">1</span> : Data Source</span><br><span class="line">content : collect elements <span class="keyword">with</span> CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage <span class="number">2</span> : Data Source</span><br><span class="line">content : collect elements <span class="keyword">with</span> CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage <span class="number">3</span> : <span class="keyword">Operator</span></span><br><span class="line">content : <span class="keyword">from</span>: (count, word)</span><br><span class="line">ship_strategy : REBALANCE</span><br><span class="line"></span><br><span class="line">Stage <span class="number">4</span> : <span class="keyword">Operator</span></span><br><span class="line">content : <span class="keyword">where</span>: (<span class="keyword">LIKE</span>(word, _UTF<span class="number">-16</span>L<span class="string">E'F%'</span>)), <span class="keyword">select</span>: (count, word)</span><br><span class="line">ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">Stage <span class="number">5</span> : <span class="keyword">Operator</span></span><br><span class="line">content : <span class="keyword">from</span>: (count, word)</span><br><span class="line">ship_strategy : REBALANCE</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了Flink TableAPI &amp;SQL，首先介绍了Flink Table API &amp;SQL的基本概念 ，然后介绍了构建Flink Table API &amp; SQL程序所需要的依赖，接着介绍了Flink的两种planner，还介绍了如何注册表以及DataStream、DataSet与表的相互转换，最后介绍了Flink的两种planner对应的查询优化并给出了一个查看执行计划的案例。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Su Command in Linux (Switch User)，链接：<a href="https://linuxize.com/post/su-command-in-linux/" target="_blank" rel="noopener">https://linuxize.com/post/su-command-in-linux/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分 &lt;code&gt;su&lt;/code&gt;命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux系统中的su命令</title>
    <link href="https://zjrongxiang.github.io/2021/06/14/2021-06-14-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84su%E5%91%BD%E4%BB%A4/"/>
    <id>https://zjrongxiang.github.io/2021/06/14/2021-06-14-Linux系统中的su命令/</id>
    <published>2021-06-14T04:42:00.000Z</published>
    <updated>2021-06-14T06:52:40.909Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分 <code>su</code>命令</p></li><li><p>第二部分 <code>su</code> 和 <code>su -</code>区别</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><code>Linux</code>操作系统是多用户多任务操作系统。多用户多任务就是可以在操作系统上建立多个用户，多个用户可以在同一时间内登录并执行各自不同的任务，互不影响。不同用户具有不同的权限，每个用户是在权限允许的范围内完成不同的任务，<code>Linux</code>正是通过这种权限的划分与管理，实现了多用户多任务的运行机制。</p><p>在日常运维中，<code>su</code>命令是最简单的用户切换命令，通过该命令可以实现任何用户身份的切换（包括从普通用户切换为 root 用户、从 root 用户切换为普通用户以及普通用户之间的切换）。普通用户之间切换以及普通用户切换至 root 用户，需要目标用户密钥，只有正确输入密钥，才能实现切换；从 root 用户切换至其他用户，无需知晓对方密钥，直接可切换成功。</p><h2 id="第一部分-su命令"><a href="#第一部分-su命令" class="headerlink" title="第一部分 su命令"></a>第一部分 <code>su</code>命令</h2><p><code>su</code> 命令的基本格式如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@VM-0-5-ubuntu:~# su [选项] 用户名</span><br></pre></td></tr></table></figure><p>参数选项：</p><ul><li><code>-</code>：当前用户不仅切换为指定用户的身份，同时所用的工作环境也切换为此用户的环境（包括 PATH 变量、MAIL 变量等）。使用 - 选项可省略用户名，默认会切换为 root 用户。</li><li><code>-l</code>：同<code>-</code>的使用类似，也就是在切换用户身份的同时，完整切换工作环境，但后面需要添加欲切换的使用者账号。</li><li><code>-p</code>：表示切换为指定用户的身份，但不改变当前的工作环境（不使用切换用户的配置文件）。</li><li><code>-m</code>：和 <code>-p</code> 一样；</li><li><code>-c</code> 命令：仅切换用户执行一次命令，执行后自动切换回来，该选项后通常会带有要执行的命令。</li></ul><h2 id="第二部分-su-和-su-区别"><a href="#第二部分-su-和-su-区别" class="headerlink" title="第二部分 su 和 su -区别"></a>第二部分 <code>su</code> 和 <code>su -</code>区别</h2><p>在实际运维使用中，经常踩的坑就是 <code>su</code> 和 <code>su -</code>的区别了。运维人员通常认为两者是相同，或者不知道 <code>su -</code>。</p><p>事实上，有<code>-</code>和没有 <code>-</code>是完全不同的，<code>-</code>选项表示在切换用户身份的同时，连当前使用的环境变量也切换成指定用户的。环境变量是用来定义操作系统环境的，因此如果系统环境没有随用户身份切换，很多命令无法正确执行。</p><p>初学者可以这样理解它们之间的区别，即有 - 选项，切换用户身份更彻底；反之，只切换了一部分。在不使用 <code>su -</code>的情况下，虽然用户身份成功切换，但环境变量依旧用的是原用户的，切换并不完整。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Su Command in Linux (Switch User)，链接：<a href="https://linuxize.com/post/su-command-in-linux/" target="_blank" rel="noopener">https://linuxize.com/post/su-command-in-linux/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分 &lt;code&gt;su&lt;/code&gt;命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>RestFul接口规范总结</title>
    <link href="https://zjrongxiang.github.io/2021/06/14/2021-06-24-RestFul%E6%8E%A5%E5%8F%A3%E8%A7%84%E8%8C%83%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/06/14/2021-06-24-RestFul接口规范总结/</id>
    <published>2021-06-14T04:42:00.000Z</published>
    <updated>2021-06-24T06:38:35.964Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分 <code>su</code>命令</p></li><li><p>第二部分 <code>su</code> 和 <code>su -</code>区别</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="一、协议"><a href="#一、协议" class="headerlink" title="一、协议"></a>一、协议</h2><p>API与用户的通信协议，总是使用<a href="https://www.ruanyifeng.com/blog/2014/02/ssl_tls.html" target="_blank" rel="noopener">HTTPs协议</a>。</p><h2 id="二、域名"><a href="#二、域名" class="headerlink" title="二、域名"></a>二、域名</h2><p>应该尽量将API部署在专用域名之下。</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; https:<span class="comment">//api.example.com</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>如果确定API很简单，不会有进一步扩展，可以考虑放在主域名下。</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; https:<span class="comment">//example.org/api/</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="三、版本（Versioning）"><a href="#三、版本（Versioning）" class="headerlink" title="三、版本（Versioning）"></a>三、版本（Versioning）</h2><p>应该将API的版本号放入URL。</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; https:<span class="comment">//api.example.com/v1/</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>另一种做法是，将版本号放在HTTP头信息中，但不如放入URL方便和直观。<a href="https://developer.github.com/v3/media/#request-specific-version" target="_blank" rel="noopener">Github</a>采用这种做法。</p><h2 id="四、路径（Endpoint）"><a href="#四、路径（Endpoint）" class="headerlink" title="四、路径（Endpoint）"></a>四、路径（Endpoint）</h2><p>路径又称”终点”（endpoint），表示API的具体网址。</p><p>在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的”集合”（collection），所以API中的名词也应该使用复数。</p><p>举例来说，有一个API提供动物园（zoo）的信息，还包括各种动物和雇员的信息，则它的路径应该设计成下面这样。</p><blockquote><ul><li><a href="https://api.example.com/v1/zoos" target="_blank" rel="noopener">https://api.example.com/v1/zoos</a></li><li><a href="https://api.example.com/v1/animals" target="_blank" rel="noopener">https://api.example.com/v1/animals</a></li><li><a href="https://api.example.com/v1/employees" target="_blank" rel="noopener">https://api.example.com/v1/employees</a></li></ul></blockquote><h2 id="五、HTTP动词"><a href="#五、HTTP动词" class="headerlink" title="五、HTTP动词"></a>五、HTTP动词</h2><p>对于资源的具体操作类型，由HTTP动词表示。</p><p>常用的HTTP动词有下面五个（括号里是对应的SQL命令）。</p><blockquote><ul><li>GET（SELECT）：从服务器取出资源（一项或多项）。</li><li>POST（CREATE）：在服务器新建一个资源。</li><li>PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。</li><li>PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。</li><li>DELETE（DELETE）：从服务器删除资源。</li></ul></blockquote><p>还有两个不常用的HTTP动词。</p><blockquote><ul><li>HEAD：获取资源的元数据。</li><li>OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。</li></ul></blockquote><p>下面是一些例子。</p><blockquote><ul><li>GET /zoos：列出所有动物园</li><li>POST /zoos：新建一个动物园</li><li>GET /zoos/ID：获取某个指定动物园的信息</li><li>PUT /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息）</li><li>PATCH /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息）</li><li>DELETE /zoos/ID：删除某个动物园</li><li>GET /zoos/ID/animals：列出某个指定动物园的所有动物</li><li>DELETE /zoos/ID/animals/ID：删除某个指定动物园的指定动物</li></ul></blockquote><h2 id="六、过滤信息（Filtering）"><a href="#六、过滤信息（Filtering）" class="headerlink" title="六、过滤信息（Filtering）"></a>六、过滤信息（Filtering）</h2><p>如果记录数量很多，服务器不可能都将它们返回给用户。API应该提供参数，过滤返回结果。</p><p>下面是一些常见的参数。</p><blockquote><ul><li>?limit=10：指定返回记录的数量</li><li>?offset=10：指定返回记录的开始位置。</li><li>?page=2&amp;per_page=100：指定第几页，以及每页的记录数。</li><li>?sortby=name&amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。</li><li>?animal_type_id=1：指定筛选条件</li></ul></blockquote><p>参数的设计允许存在冗余，即允许API路径和URL参数偶尔有重复。比如，GET /zoo/ID/animals 与 GET /animals?zoo_id=ID 的含义是相同的。</p><h2 id="七、状态码（Status-Codes）"><a href="#七、状态码（Status-Codes）" class="headerlink" title="七、状态码（Status Codes）"></a>七、状态码（Status Codes）</h2><p>服务器向用户返回的状态码和提示信息，常见的有以下一些（方括号中是该状态码对应的HTTP动词）。</p><blockquote><ul><li>200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。</li><li>201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。</li><li>202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务）</li><li>204 NO CONTENT - [DELETE]：用户删除数据成功。</li><li>400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。</li><li>401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。</li><li>403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。</li><li>404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。</li><li>406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。</li><li>410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。</li><li>422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。</li><li>500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。</li></ul></blockquote><p>状态码的完全列表参见<a href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html" target="_blank" rel="noopener">这里</a>。</p><h2 id="八、错误处理（Error-handling）"><a href="#八、错误处理（Error-handling）" class="headerlink" title="八、错误处理（Error handling）"></a>八、错误处理（Error handling）</h2><p>如果状态码是4xx，就应该向用户返回出错信息。一般来说，返回的信息中将error作为键名，出错信息作为键值即可。</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; &#123;</span><br><span class="line">&gt;     error: <span class="string">"Invalid API key"</span></span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="九、返回结果"><a href="#九、返回结果" class="headerlink" title="九、返回结果"></a>九、返回结果</h2><p>针对不同操作，服务器向用户返回的结果应该符合以下规范。</p><blockquote><ul><li>GET /collection：返回资源对象的列表（数组）</li><li>GET /collection/resource：返回单个资源对象</li><li>POST /collection：返回新生成的资源对象</li><li>PUT /collection/resource：返回完整的资源对象</li><li>PATCH /collection/resource：返回完整的资源对象</li><li>DELETE /collection/resource：返回一个空文档</li></ul></blockquote><h2 id="十、Hypermedia-API"><a href="#十、Hypermedia-API" class="headerlink" title="十、Hypermedia API"></a>十、Hypermedia API</h2><p>RESTful API最好做到Hypermedia，即返回结果中提供链接，连向其他API方法，使得用户不查文档，也知道下一步应该做什么。</p><p>比如，当用户向api.example.com的根目录发出请求，会得到这样一个文档。</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; &#123;<span class="string">"link"</span>: &#123;</span><br><span class="line">&gt;   <span class="string">"rel"</span>:   <span class="string">"collection https://www.example.com/zoos"</span>,</span><br><span class="line">&gt;   <span class="string">"href"</span>:  <span class="string">"https://api.example.com/zoos"</span>,</span><br><span class="line">&gt;   <span class="string">"title"</span>: <span class="string">"List of zoos"</span>,</span><br><span class="line">&gt;   <span class="string">"type"</span>:  <span class="string">"application/vnd.yourformat+json"</span></span><br><span class="line">&gt; &#125;&#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>上面代码表示，文档中有一个link属性，用户读取这个属性就知道下一步该调用什么API了。rel表示这个API与当前网址的关系（collection关系，并给出该collection的网址），href表示API的路径，title表示API的标题，type表示返回类型。</p><p>Hypermedia API的设计被称为<a href="https://en.wikipedia.org/wiki/HATEOAS" target="_blank" rel="noopener">HATEOAS</a>。Github的API就是这种设计，访问<a href="https://api.github.com/" target="_blank" rel="noopener">api.github.com</a>会得到一个所有可用API的网址列表。</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; &#123;</span><br><span class="line">&gt;   <span class="string">"current_user_url"</span>: <span class="string">"https://api.github.com/user"</span>,</span><br><span class="line">&gt;   <span class="string">"authorizations_url"</span>: <span class="string">"https://api.github.com/authorizations"</span>,</span><br><span class="line">&gt;   <span class="comment">// ...</span></span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>从上面可以看到，如果想获取当前用户的信息，应该去访问<a href="https://api.github.com/user" target="_blank" rel="noopener">api.github.com/user</a>，然后就得到了下面结果。</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; &#123;</span><br><span class="line">&gt;   <span class="string">"message"</span>: <span class="string">"Requires authentication"</span>,</span><br><span class="line">&gt;   <span class="string">"documentation_url"</span>: <span class="string">"https://developer.github.com/v3"</span></span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>上面代码表示，服务器给出了提示信息，以及文档的网址。</p><h2 id="十一、其他"><a href="#十一、其他" class="headerlink" title="十一、其他"></a>十一、其他</h2><p>（1）API的身份认证应该使用<a href="https://www.ruanyifeng.com/blog/2014/05/oauth_2_0.html" target="_blank" rel="noopener">OAuth 2.0</a>框架。</p><p>（2）服务器返回的数据格式，应该尽量使用JSON，避免使用XML。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Su Command in Linux (Switch User)，链接：<a href="https://linuxize.com/post/su-command-in-linux/" target="_blank" rel="noopener">https://linuxize.com/post/su-command-in-linux/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分 &lt;code&gt;su&lt;/code&gt;命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Lua语言系列--语言基础</title>
    <link href="https://zjrongxiang.github.io/2021/05/23/2021-05-27-Lua%E8%AF%AD%E8%A8%80%E7%B3%BB%E5%88%97--%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80/"/>
    <id>https://zjrongxiang.github.io/2021/05/23/2021-05-27-Lua语言系列--语言基础/</id>
    <published>2021-05-23T05:30:00.000Z</published>
    <updated>2021-06-21T13:28:14.097Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   <code>Lua</code>语言入门</li><li>第二部分  数值</li><li>第三部分  字符串</li><li>第四部分 表</li><li>第五部分 函数</li><li>第六部分 输入和输出</li><li>第七部分 知识补充</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="第一部分-Lua语言入门"><a href="#第一部分-Lua语言入门" class="headerlink" title="第一部分   Lua语言入门"></a>第一部分   <code>Lua</code>语言入门</h2><p>Lua是解释型语言。</p><h3 id="1-1-程序段"><a href="#1-1-程序段" class="headerlink" title="1.1 程序段"></a>1.1 程序段</h3><h3 id="1-2-语法规范"><a href="#1-2-语法规范" class="headerlink" title="1.2 语法规范"></a>1.2 语法规范</h3><p>Lua语言中标识符（名称）由任意字母、数值和下划线组成的字符串，但是不能以数值开头。</p><p>Lua中关键字（保留字）：</p><ul><li><p>逻辑运算关键字：and、 or、not</p></li><li><p>基本类型：function、table、nil</p></li><li><p>控制类：for、 while、do 、break、in、return、until、goto、repeat</p></li><li><p>逻辑变量：true、false</p></li><li><p>if控制类：if、then 、else、elseif</p></li></ul><ul><li>变量作用域：local</li></ul><p>Lua语言对于大小写敏感。</p><p>Lua语言使用连字符<code>--</code>作为单行注释。多行注释为：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--[[</span></span><br><span class="line"><span class="comment">print("多行注释")</span></span><br><span class="line"><span class="comment">--]]</span></span><br></pre></td></tr></table></figure><h3 id="1-3-全局变量"><a href="#1-3-全局变量" class="headerlink" title="1.3 全局变量"></a>1.3 全局变量</h3><p>Lua语言中，全局变量无需声明，可以直接使用。没有初始化的全局变量初始值为nil。</p><h3 id="1-4-类型和值"><a href="#1-4-类型和值" class="headerlink" title="1.4 类型和值"></a>1.4 类型和值</h3><p>Lua语言属于动态语言。</p><p>lua语言中有8种基本类型。</p><ul><li>nil（空）</li><li>boolean（布尔）</li><li>number（数值）</li><li>string（字符串）</li><li>userdata（用户数据）</li><li>function（函数）</li><li>thread（线程）</li><li>table（表）</li></ul><p>可以使用函数type来返回变量数据类型。注意type函数返回的是一个字符串。</p><h3 id="1-6-练习"><a href="#1-6-练习" class="headerlink" title="1.6 练习"></a>1.6 练习</h3><h2 id="第二部分-数值"><a href="#第二部分-数值" class="headerlink" title="第二部分  数值"></a>第二部分  数值</h2><h2 id="第三部分-字符串"><a href="#第三部分-字符串" class="headerlink" title="第三部分  字符串"></a>第三部分  字符串</h2><h2 id="第四部分-表"><a href="#第四部分-表" class="headerlink" title="第四部分 表"></a>第四部分 表</h2><h2 id="第五部分-函数"><a href="#第五部分-函数" class="headerlink" title="第五部分 函数"></a>第五部分 函数</h2><h2 id="第六部分-输入和输出"><a href="#第六部分-输入和输出" class="headerlink" title="第六部分 输入和输出"></a>第六部分 输入和输出</h2><h2 id="第七部分-知识补充"><a href="#第七部分-知识补充" class="headerlink" title="第七部分 知识补充"></a>第七部分 知识补充</h2><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   &lt;code&gt;Lua&lt;/code&gt;语言入门&lt;/li&gt;
&lt;li&gt;第二部分  数值&lt;/li&gt;
&lt;
      
    
    </summary>
    
      <category term="Nginx" scheme="https://zjrongxiang.github.io/categories/Nginx/"/>
    
    
  </entry>
  
  <entry>
    <title>Nginx使用说明</title>
    <link href="https://zjrongxiang.github.io/2021/05/23/2021-05-23-Nginx%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/"/>
    <id>https://zjrongxiang.github.io/2021/05/23/2021-05-23-Nginx使用说明/</id>
    <published>2021-05-23T05:30:00.000Z</published>
    <updated>2021-05-23T10:19:54.995Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   回本溯源</li><li>第二部分  <code>HDFS</code>大量小文件的危害</li><li>第三部分  小文件治理方案总结</li><li>第四部分 总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] <code>HDFS NameNode</code>内存全景，链接：<a href="https://tech.meituan.com/2016/08/26/namenode.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/08/26/namenode.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   回本溯源&lt;/li&gt;
&lt;li&gt;第二部分  &lt;code&gt;HDFS&lt;/code&gt;大量小文件的危害
      
    
    </summary>
    
      <category term="Nginx" scheme="https://zjrongxiang.github.io/categories/Nginx/"/>
    
    
  </entry>
  
  <entry>
    <title>Yarn资源调度的</title>
    <link href="https://zjrongxiang.github.io/2021/05/15/2021-05-15-%E6%8F%AD%E5%BC%80HDFS%E5%AD%98%E5%82%A8%E7%9A%84%E9%9D%A2%E7%BA%B1%20-%20%E5%89%AF%E6%9C%AC/"/>
    <id>https://zjrongxiang.github.io/2021/05/15/2021-05-15-揭开HDFS存储的面纱 - 副本/</id>
    <published>2021-05-15T05:30:00.000Z</published>
    <updated>2021-05-24T07:42:45.422Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   回本溯源</li><li>第二部分  <code>HDFS</code>大量小文件的危害</li><li>第三部分  小文件治理方案总结</li><li>第四部分 总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>yarn.resourcemanager.store.class : 有三种StateStore，分别是基于zookeeper, HDFS, leveldb, HA高可用集群必须用ZKRMStateStore</p><table><thead><tr><th style="text-align:left">存储</th><th style="text-align:left">yarn.resourcemanager.store.class</th></tr></thead><tbody><tr><td style="text-align:left">ZooKeeper</td><td style="text-align:left">org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</td></tr><tr><td style="text-align:left">FileSystem</td><td style="text-align:left">org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore</td></tr><tr><td style="text-align:left">LevelDB</td><td style="text-align:left">org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore</td></tr></tbody></table><p>By default the number of completed applications stored in state store is 10000. <a href="https://maprdocs.mapr.com/51/ReferenceGuide/Default-YARN-Parameters.html" target="_blank" rel="noopener">https://maprdocs.mapr.com/51/ReferenceGuide/Default-YARN-Parameters.html</a></p><p>Try to move/delete some completed applications</p><p>hadoop fs -mv /var/mapr/cluster/yarn/rm/system/FSRMStateRoot/RMAppRoot/* /path_to_local_dir</p><p>hadoop conf | grep yarn.resourcemanager.max-completed-applications</p><p><a href="https://www.programmersought.com/article/36321434084/" target="_blank" rel="noopener">https://www.programmersought.com/article/36321434084/</a></p><p><a href="https://issues.apache.org/jira/browse/YARN-7150" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/YARN-7150</a></p><p><a href="https://my.oschina.net/dabird/blog/3089265" target="_blank" rel="noopener">https://my.oschina.net/dabird/blog/3089265</a></p><p><a href="https://cloud.tencent.com/developer/article/1491079" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1491079</a></p><p><a href="https://my.oschina.net/dabird/blog/4273830" target="_blank" rel="noopener">https://my.oschina.net/dabird/blog/4273830</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] <code>HDFS NameNode</code>内存全景，链接：<a href="https://tech.meituan.com/2016/08/26/namenode.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/08/26/namenode.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   回本溯源&lt;/li&gt;
&lt;li&gt;第二部分  &lt;code&gt;HDFS&lt;/code&gt;大量小文件的危害
      
    
    </summary>
    
      <category term="HDFS" scheme="https://zjrongxiang.github.io/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>揭开HDFS存储的面纱</title>
    <link href="https://zjrongxiang.github.io/2021/05/15/2021-05-15-%E6%8F%AD%E5%BC%80HDFS%E5%AD%98%E5%82%A8%E7%9A%84%E9%9D%A2%E7%BA%B1/"/>
    <id>https://zjrongxiang.github.io/2021/05/15/2021-05-15-揭开HDFS存储的面纱/</id>
    <published>2021-05-15T05:30:00.000Z</published>
    <updated>2021-05-15T16:21:49.643Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   回本溯源</li><li>第二部分  <code>HDFS</code>大量小文件的危害</li><li>第三部分  小文件治理方案总结</li><li>第四部分 总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://blog.csdn.net/m0_37613244/article/details/109920466?utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control" target="_blank" rel="noopener">https://blog.csdn.net/m0_37613244/article/details/109920466?utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control</a></p><p>HDFS Namenode本地目录的存储结构和Datanode数据块存储目录结构，也就是hdfs-site.xml中配置的dfs.namenode.name.dir和dfs.namenode.data.dir</p><h2 id="第一部分-NameNode元数据"><a href="#第一部分-NameNode元数据" class="headerlink" title="第一部分 NameNode元数据"></a>第一部分 NameNode元数据</h2><h2 id="第二部分-DataNode数据"><a href="#第二部分-DataNode数据" class="headerlink" title="第二部分 DataNode数据"></a>第二部分 DataNode数据</h2><p>You need to look in your <strong>hdfs-default.xml</strong> configuration file for the <strong>dfs.data.dir</strong> setting. The default setting is: <strong>${hadoop.tmp.dir}/dfs/data</strong> and note that the ${hadoop.tmp.dir} is actually in core-default.xml described <a href="http://hadoop.apache.org/docs/r1.2.1/core-default.html" target="_blank" rel="noopener">here</a>.</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] <code>HDFS NameNode</code>内存全景，链接：<a href="https://tech.meituan.com/2016/08/26/namenode.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/08/26/namenode.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   回本溯源&lt;/li&gt;
&lt;li&gt;第二部分  &lt;code&gt;HDFS&lt;/code&gt;大量小文件的危害
      
    
    </summary>
    
      <category term="HDFS" scheme="https://zjrongxiang.github.io/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>Pyspark实现原理和源码分析</title>
    <link href="https://zjrongxiang.github.io/2021/05/06/2020-10-06-Pyspark%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/05/06/2020-10-06-Pyspark实现原理总结/</id>
    <published>2021-05-06T05:30:00.000Z</published>
    <updated>2021-05-16T09:40:11.720Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  常用快捷键</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://blog.csdn.net/oTengYue/article/details/105379628" target="_blank" rel="noopener">https://blog.csdn.net/oTengYue/article/details/105379628</a></p><p><a href="https://www.readfog.com/a/1631040025628086272" target="_blank" rel="noopener">https://www.readfog.com/a/1631040025628086272</a></p><p>spark为了保证核心架构的统一性，在核心架构外围封装了一层python，spark的核心架构功能包括计算资源的申请，task的管理和分配， driver与executor之间的通信，executor之间的通信，rdd的载体等都是在基于JVM的</p><p>spark的这种设计可以说是非常方便的去进行多种开发语言的扩展。但是也可以明显看出与在jvm内部运行的udf相比，在python worker中执行udf时，额外增加了数据在executor jvm和pythin worker之间序列化、反序列化、及通信IO等损耗，并且在程序运行上python相比java的具有一定的性能劣势。在计算逻辑比重比较大的spark任务中，使用自定义udf的pyspark程序会明显有更多的性能损耗。当然在spark sql 中使用内置udf会降低或除去上述描述中产生的性能差异。</p><p>程序模型提交命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@quickstart pysparkExample]# cat run.sh </span><br><span class="line">/usr/lib/spark/bin/spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--archives hdfs:///user/admin/python/python3.5.2.zip \</span><br><span class="line">--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./python3.5.2.zip/conda/bin/python \</span><br><span class="line">test.py</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">[root@quickstart pysparkExample]# ./run.sh</span><br><span class="line">21/05/16 00:02:32 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/172.17.0.2:8032</span><br><span class="line">21/05/16 00:02:32 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers</span><br><span class="line">21/05/16 00:02:32 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (2816 MB per container)</span><br><span class="line">21/05/16 00:02:32 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead</span><br><span class="line">21/05/16 00:02:32 INFO yarn.Client: Setting up container launch context for our AM</span><br><span class="line">21/05/16 00:02:32 INFO yarn.Client: Setting up the launch environment for our AM container</span><br><span class="line">21/05/16 00:02:32 INFO yarn.Client: Preparing resources for our AM container</span><br><span class="line">21/05/16 00:02:33 INFO yarn.YarnSparkHadoopUtil: getting token for namenode: hdfs://quickstart.cloudera:8020/user/admin/.sparkStaging/application_1621088965108_0002</span><br><span class="line">21/05/16 00:02:33 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 9 for admin on 172.17.0.2:8020</span><br><span class="line">21/05/16 00:02:34 INFO hive.metastore: Trying to connect to metastore with URI thrift://quickstart.cloudera:9083</span><br><span class="line">21/05/16 00:02:34 INFO hive.metastore: Opened a connection to metastore, current connections: 1</span><br><span class="line">21/05/16 00:02:34 INFO hive.metastore: Connected to metastore.</span><br><span class="line">21/05/16 00:02:34 INFO hive.metastore: Closed a connection to metastore, current connections: 0</span><br><span class="line">21/05/16 00:02:34 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/admin/python/python3.5.2.zip</span><br><span class="line">21/05/16 00:02:34 INFO yarn.Client: Uploading resource file:/home/pyspark/pysparkExample/test.py -&gt; hdfs://quickstart.cloudera:8020/user/admin/.sparkStaging/application_1621088965108_0002/test.py</span><br><span class="line">21/05/16 00:02:34 INFO yarn.Client: Uploading resource file:/tmp/spark-f65e84d5-0438-473c-9dff-03aeb95d4f18/__spark_conf__7787627711692930444.zip -&gt; hdfs://quickstart.cloudera:8020/user/admin/.sparkStaging/application_1621088965108_0002/__spark_conf__7787627711692930444.zip</span><br><span class="line">21/05/16 00:02:35 INFO spark.SecurityManager: Changing view acls to: root,admin</span><br><span class="line">21/05/16 00:02:35 INFO spark.SecurityManager: Changing modify acls to: root,admin</span><br><span class="line">21/05/16 00:02:35 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, admin); users with modify permissions: Set(root, admin)</span><br><span class="line">21/05/16 00:02:35 INFO yarn.Client: Submitting application 2 to ResourceManager</span><br><span class="line">21/05/16 00:02:35 INFO impl.YarnClientImpl: Submitted application application_1621088965108_0002</span><br><span class="line">21/05/16 00:02:36 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)</span><br><span class="line">21/05/16 00:02:36 INFO yarn.Client: </span><br><span class="line"> client token: Token &#123; kind: YARN_CLIENT_TOKEN, service:  &#125;</span><br><span class="line"> diagnostics: N/A</span><br><span class="line"> ApplicationMaster host: N/A</span><br><span class="line"> ApplicationMaster RPC port: -1</span><br><span class="line"> queue: root.admin</span><br><span class="line"> start time: 1621094555087</span><br><span class="line"> final status: UNDEFINED</span><br><span class="line"> tracking URL: http://quickstart.cloudera:8088/proxy/application_1621088965108_0002/</span><br><span class="line"> user: admin</span><br><span class="line">21/05/16 00:02:37 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)</span><br><span class="line">21/05/16 00:02:38 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)</span><br><span class="line">21/05/16 00:02:39 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)</span><br><span class="line">21/05/16 00:02:40 INFO yarn.Client: Application report for application_1621088965108_0002 (state: FINISHED)</span><br><span class="line">21/05/16 00:02:40 INFO yarn.Client: </span><br><span class="line"> client token: Token &#123; kind: YARN_CLIENT_TOKEN, service:  &#125;</span><br><span class="line"> diagnostics: N/A</span><br><span class="line"> ApplicationMaster host: 172.17.0.2</span><br><span class="line"> ApplicationMaster RPC port: 0</span><br><span class="line"> queue: root.admin</span><br><span class="line"> start time: 1621094555087</span><br><span class="line"> final status: SUCCEEDED</span><br><span class="line"> tracking URL: http://quickstart.cloudera:8088/proxy/application_1621088965108_0002/history/application_1621088965108_0002/1</span><br><span class="line"> user: admin</span><br><span class="line">21/05/16 00:02:40 INFO yarn.Client: Deleting staging directory .sparkStaging/application_1621088965108_0002</span><br><span class="line">21/05/16 00:02:40 INFO util.ShutdownHookManager: Shutdown hook called</span><br><span class="line">21/05/16 00:02:40 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f65e84d5-0438-473c-9dff-03aeb95d4f18</span><br></pre></td></tr></table></figure><p>yarn日志：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">21/05/15 16:19:29 INFO yarn.ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]</span><br><span class="line">21/05/15 16:19:30 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1621095402395_0001_000001</span><br><span class="line">21/05/15 16:19:30 INFO spark.SecurityManager: Changing view acls to: admin</span><br><span class="line">21/05/15 16:19:30 INFO spark.SecurityManager: Changing modify acls to: admin</span><br><span class="line">21/05/15 16:19:30 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)</span><br><span class="line">21/05/15 16:19:30 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread</span><br><span class="line">21/05/15 16:19:30 INFO yarn.ApplicationMaster: Waiting for spark context initialization</span><br><span class="line">21/05/15 16:19:30 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... </span><br><span class="line">21/05/15 16:19:31 INFO spark.SparkContext: Running Spark version 1.6.0</span><br><span class="line">21/05/15 16:19:31 INFO spark.SecurityManager: Changing view acls to: admin</span><br><span class="line">21/05/15 16:19:31 INFO spark.SecurityManager: Changing modify acls to: admin</span><br><span class="line">21/05/15 16:19:31 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)</span><br><span class="line">21/05/15 16:19:31 INFO util.Utils: Successfully started service 'sparkDriver' on port 46545.</span><br><span class="line">21/05/15 16:19:31 INFO slf4j.Slf4jLogger: Slf4jLogger started</span><br><span class="line">21/05/15 16:19:31 INFO Remoting: Starting remoting</span><br><span class="line">21/05/15 16:19:31 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.17.0.2:35921]</span><br><span class="line">21/05/15 16:19:31 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@172.17.0.2:35921]</span><br><span class="line">21/05/15 16:19:31 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 35921.</span><br><span class="line">21/05/15 16:19:31 INFO spark.SparkEnv: Registering MapOutputTracker</span><br><span class="line">21/05/15 16:19:31 INFO spark.SparkEnv: Registering BlockManagerMaster</span><br><span class="line">21/05/15 16:19:31 INFO storage.DiskBlockManager: Created local directory at /yarn/nm/usercache/admin/appcache/application_1621095402395_0001/blockmgr-edbfee4f-522d-4c06-81a0-5b83b750e88a</span><br><span class="line">21/05/15 16:19:31 INFO storage.MemoryStore: MemoryStore started with capacity 491.7 MB</span><br><span class="line">21/05/15 16:19:31 INFO spark.SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">21/05/15 16:19:31 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter</span><br><span class="line">21/05/15 16:19:31 INFO util.Utils: Successfully started service 'SparkUI' on port 43419.</span><br><span class="line">21/05/15 16:19:31 INFO ui.SparkUI: Started SparkUI at http://172.17.0.2:43419</span><br><span class="line">21/05/15 16:19:31 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler</span><br><span class="line">21/05/15 16:19:31 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40989.</span><br><span class="line">21/05/15 16:19:31 INFO netty.NettyBlockTransferService: Server created on 40989</span><br><span class="line">21/05/15 16:19:31 INFO storage.BlockManager: external shuffle service port = 7337</span><br><span class="line">21/05/15 16:19:31 INFO storage.BlockManagerMaster: Trying to register BlockManager</span><br><span class="line">21/05/15 16:19:31 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.2:40989 with 491.7 MB RAM, BlockManagerId(driver, 172.17.0.2, 40989)</span><br><span class="line">21/05/15 16:19:31 INFO storage.BlockManagerMaster: Registered BlockManager</span><br><span class="line">21/05/15 16:19:32 INFO scheduler.EventLoggingListener: Logging events to hdfs://quickstart.cloudera:8020/user/spark/applicationHistory/application_1621095402395_0001_1</span><br><span class="line">21/05/15 16:19:32 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8</span><br><span class="line">21/05/15 16:19:32 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done</span><br><span class="line">21/05/15 16:19:32 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@172.17.0.2:46545)</span><br><span class="line">21/05/15 16:19:32 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/172.17.0.2:8030</span><br><span class="line">21/05/15 16:19:32 INFO yarn.YarnRMClient: Registering the ApplicationMaster</span><br><span class="line">21/05/15 16:19:32 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals</span><br><span class="line">21/05/15 16:19:32 INFO yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0</span><br><span class="line">21/05/15 16:19:32 INFO spark.SparkContext: Invoking stop() from shutdown hook</span><br><span class="line">21/05/15 16:19:32 INFO ui.SparkUI: Stopped Spark web UI at http://172.17.0.2:43419</span><br><span class="line">21/05/15 16:19:32 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors</span><br><span class="line">21/05/15 16:19:32 INFO cluster.YarnClusterSchedulerBackend: Asking each executor to shut down</span><br><span class="line">21/05/15 16:19:32 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line">21/05/15 16:19:32 INFO storage.MemoryStore: MemoryStore cleared</span><br><span class="line">21/05/15 16:19:32 INFO storage.BlockManager: BlockManager stopped</span><br><span class="line">21/05/15 16:19:32 INFO storage.BlockManagerMaster: BlockManagerMaster stopped</span><br><span class="line">21/05/15 16:19:32 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span><br><span class="line">21/05/15 16:19:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.</span><br><span class="line">21/05/15 16:19:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.</span><br><span class="line">21/05/15 16:19:32 INFO spark.SparkContext: Successfully stopped SparkContext</span><br><span class="line">21/05/15 16:19:32 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED</span><br><span class="line">21/05/15 16:19:32 INFO Remoting: Remoting shut down</span><br><span class="line">21/05/15 16:19:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.</span><br><span class="line">21/05/15 16:19:32 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.</span><br><span class="line">21/05/15 16:19:32 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1621095402395_0001</span><br><span class="line">21/05/15 16:19:32 INFO util.ShutdownHookManager: Shutdown hook called</span><br><span class="line">21/05/15 16:19:32 INFO util.ShutdownHookManager: Deleting directory /yarn/nm/usercache/admin/appcache/application_1621095402395_0001/spark-76c43a00-f562-4e4e-be1a-be3a2fcefc21/pyspark-b56e6390-71a2-4475-8ffe-4164798ab6c4</span><br><span class="line">21/05/15 16:19:32 INFO util.ShutdownHookManager: Deleting directory /yarn/nm/usercache/admin/appcache/application_1621095402395_0001/spark-76c43a00-f562-4e4e-be1a-be3a2fcefc21</span><br></pre></td></tr></table></figure><p><a href="http://sharkdtu.com/posts/pyspark-internal.html" target="_blank" rel="noopener">http://sharkdtu.com/posts/pyspark-internal.html</a></p><p><a href="https://cloud.tencent.com/developer/article/1589011" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1589011</a></p><p><a href="https://cloud.tencent.com/developer/article/1558621" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1558621</a></p><p><a href="https://www.nativex.com/cn/blog/2019-12-27-2/" target="_blank" rel="noopener">https://www.nativex.com/cn/blog/2019-12-27-2/</a></p><p>但是在大数据场景下，JVM和Python进程间频繁的数据通信导致其性能损耗较多，恶劣时还可能会直接卡死，所以建议对于大规模机器学习或者Streaming应用场景还是慎用PySpark，尽量使用原生的Scala/Java编写应用程序，对于中小规模数据量下的简单离线任务，可以使用PySpark快速部署提交。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Job Scheduling，链接：<a href="https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  常用快捷键&lt;/li&gt;
&lt;li&gt;参考文献及资料&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;背景&quot;&gt;
      
    
    </summary>
    
      <category term="spark" scheme="https://zjrongxiang.github.io/categories/spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark中的动态伸缩和反压机制</title>
    <link href="https://zjrongxiang.github.io/2021/05/02/2021-05-02-Spark%E4%B8%AD%E7%9A%84%E5%8A%A8%E6%80%81%E4%BC%B8%E7%BC%A9%E5%92%8C%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6/"/>
    <id>https://zjrongxiang.github.io/2021/05/02/2021-05-02-Spark中的动态伸缩和反压机制/</id>
    <published>2021-05-02T05:30:00.000Z</published>
    <updated>2021-05-02T14:11:27.463Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  常用快捷键</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://fares.codes/posts/dynamic-scaling-and-backpressure/" target="_blank" rel="noopener">https://fares.codes/posts/dynamic-scaling-and-backpressure/</a></p><h2 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h2><ul><li>建议采用以下做法以实现更好的自动缩放比例：<ul><li>最好从相当大的集群和数量的执行程序开始，并在必要时进行缩减。（执行程序映射到<a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">YARN容器</a>。）</li><li>执行者的数量应至少等于接收者的数量。</li><li>设置每个执行器的核心数，以使执行器具有一些多余的容量，这些容量超出了运行接收器所需的容量。</li><li>内核总数必须大于接收器数量；否则，应用程序将无法处理收到的数据。</li></ul></li><li>设置<code>spark.streaming.backpressure.enabled</code>为<code>true</code>，则Spark Streaming可以控制接收速率（基于当前的批处理调度延迟和处理时间），以便系统仅以其可以处理的速度接收数据。</li><li>为了获得最佳性能，请考虑使用Kryo序列化程序在Spark数据的序列化表示和反序列化表示之间进行转换。这不是Spark的默认设置，但是您可以显式更改它：将<code>spark.serializer</code>属性设置 为<code>org.apache.spark.serializer.KryoSerializer</code>。</li><li>开发人员可以通过在不再需要DStream时取消缓存它们来减少内存消耗。</li></ul><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Job Scheduling，链接：<a href="https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  常用快捷键&lt;/li&gt;
&lt;li&gt;参考文献及资料&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;背景&quot;&gt;
      
    
    </summary>
    
      <category term="spark" scheme="https://zjrongxiang.github.io/categories/spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark on Yarn任务动态伸缩机制介绍</title>
    <link href="https://zjrongxiang.github.io/2021/05/02/2021-05-02-Spark%20on%20Yarn%E4%BB%BB%E5%8A%A1%E5%8A%A8%E6%80%81%E4%BC%B8%E7%BC%A9%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D/"/>
    <id>https://zjrongxiang.github.io/2021/05/02/2021-05-02-Spark on Yarn任务动态伸缩机制介绍/</id>
    <published>2021-05-02T05:30:00.000Z</published>
    <updated>2021-05-03T17:39:04.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  配置实现</li><li>第二部分 动态配置原理和源码分析</li><li>第三部分 总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><code>Spark</code>默认使用的是资源预分配的模式。即在任务运行之前，需要提前指定任务运行需要的资源量。但是在实际线上生产环境使用过程就存在资源浪费和不足的问题，特别是<code>Spark Streaming</code>类型的任务。例如很多日志数据在一天中量并不是均匀分布的，而是一个“双驼峰”。对于预分配模式，就存在日志峰值期间，运算资源不足导致数据处理的延迟，而在日志低峰时期存在资源闲置却无法释放（特别是资源管理器粗粒度模式）。使得生产线上环境资源未能高效使用。</p><p><img src="D:\myblog\source\_posts\images\picture\spark DRA\dynamic-allocation-in-spark-19-638.jpg" alt="dynamic-allocation-in-spark-19-638"></p><p><code>Spark</code>在<code>Spark 1.2</code>版本后，对于<code>Spark On Yarn</code>模式，开始支持动态资源分配（<code>Dynamic Resource Allocation</code>，后文我们也简称<code>DRA</code>）。该机制下<code>Spark Core</code>和<code>Spark Streaming</code>任务就可以根据<code>Application</code>的负载情况，动态的增加和减少<code>Executors</code>。</p><p><img src="D:\myblog\source\_posts\images\picture\spark DRA\dynamic-allocation-in-spark-20-638.jpg" alt="dynamic-allocation-in-spark-20-638"></p><h2 id="第一部分-配置实现"><a href="#第一部分-配置实现" class="headerlink" title="第一部分 配置实现"></a>第一部分 配置实现</h2><p>对于<code>Spark on Yarn</code>模式需要提前配置Yarn服务，主要是配置<code>External shuffle service</code>（<code>Spark 1.2</code>开始引入）。<code>Spark</code>计算需要<code>shuffle</code>时候，每个<code>Executor</code> 需要把上一个 <code>stage</code> 的 <code>mapper</code> 输出写入磁盘，然后作为 <code>server</code> 等待下一个<code>stage</code> 的<code>reducer</code> 来获取 map 的输出。因此如果 <code>Executor</code> 在 <code>map</code> 阶段完成后被回收，<code>reducer</code> 将无法找到 <code>block</code>的位置。所以开启 <code>Dynamic Resource Allocation</code> 时，必须开启 <code>External shuffle service</code>。这样，mapper 的输出位置（元数据信息）将会由 <code>External shuffle service</code>（长期运行的守护进程） 来登记保存，<code>Executor</code> 不需要再保留状态信息，可以安全回收。</p><h3 id="1-1-Yarn服务配置"><a href="#1-1-Yarn服务配置" class="headerlink" title="1.1 Yarn服务配置"></a>1.1 Yarn服务配置</h3><p>首先需要对<code>Yarn</code>的<code>NodeManager</code>服务进行配置，使其支持<code>Spark</code>的<code>Shuffle Service</code>。</p><ul><li><p>修改每台<code>NodeManager</code>上的配置文件<code>yarn-site.xml</code>：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--修改和增加--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle,spark_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">　<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">　<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">　<span class="tag">&lt;<span class="name">name</span>&gt;</span>spark.shuffle.service.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">　<span class="tag">&lt;<span class="name">value</span>&gt;</span>7337<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置服务依赖包。将<code>$SPARK_HOME/lib/spark-1.6.0-yarn-shuffle.jar</code>（注意实际版本号）复制到每台<code>NodeManager</code>的<code>${HADOOP_HOME}/share/hadoop/yarn/lib/</code>下。</p></li><li>重启所有<code>NodeManager</code>生效配置调整。</li></ul><h3 id="1-2-Spark-core-任务配置"><a href="#1-2-Spark-core-任务配置" class="headerlink" title="1.2 Spark core 任务配置"></a>1.2 Spark core 任务配置</h3><h4 id="1-2-1-配置方法"><a href="#1-2-1-配置方法" class="headerlink" title="1.2.1 配置方法"></a>1.2.1 配置方法</h4><p>通常配置<code>Saprk</code>应用任务的参数有三种方式：</p><ul><li><p>修改配置文件<code>spark-defaults.conf</code>，全局生效；</p><p>配置文件位置：<code>$SPARK_HOME/conf/spark-defaults.conf</code>，具体参数如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//启用External shuffle Service服务</span><br><span class="line">spark.shuffle.service.enabled true</span><br><span class="line">//Shuffle Service服务端口，必须和yarn-site中的一致</span><br><span class="line">spark.shuffle.service.port 7337</span><br><span class="line">//开启动态资源分配</span><br><span class="line">spark.dynamicAllocation.enabled true</span><br><span class="line">//每个Application最小分配的executor数</span><br><span class="line">spark.dynamicAllocation.minExecutors 1</span><br><span class="line">//每个Application最大并发分配的executor数</span><br><span class="line">spark.dynamicAllocation.maxExecutors 30</span><br><span class="line">spark.dynamicAllocation.schedulerBacklogTimeout 1s</span><br><span class="line">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s</span><br></pre></td></tr></table></figure></li><li><p>spark-submit 命令配置，个性化生效；</p><p>参考下面的案例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master yarn-cluster \</span><br><span class="line">    --driver-cores 2 \</span><br><span class="line">    --driver-memory 2G \</span><br><span class="line">    --num-executors 10 \</span><br><span class="line">    --executor-cores 5 \</span><br><span class="line">    --executor-memory 2G \</span><br><span class="line">    --conf spark.dynamicAllocation.enabled=true \</span><br><span class="line">    --conf spark.shuffle.service.enabled=true \</span><br><span class="line">    --conf spark.dynamicAllocation.minExecutors=5 \</span><br><span class="line">    --conf spark.dynamicAllocation.maxExecutors=30 \</span><br><span class="line">    --conf spark.dynamicAllocation.initialExecutors=10 </span><br><span class="line">    --class com.spark.sql.jdbc.SparkDFtoOracle2 \</span><br><span class="line">    Spark-hive-sql-Dataframe-0.0.1-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure></li><li><p>代码中配置，个性化生效；</p><p>参考下面的<code>scala</code>代码案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.set(<span class="string">"spark.dynamicAllocation.enabled"</span>, <span class="literal">true</span>);</span><br><span class="line">conf.set(<span class="string">"spark.shuffle.service.enabled"</span>, <span class="literal">true</span>);</span><br><span class="line">conf.set(<span class="string">"spark.dynamicAllocation.minExecutors"</span>, <span class="string">"5"</span>);</span><br><span class="line">conf.set(<span class="string">"spark.dynamicAllocation.maxExecutors"</span>, <span class="string">"30"</span>);</span><br><span class="line">conf.set(<span class="string">"spark.dynamicAllocation.initialExecutors"</span>, <span class="string">"10"</span>);</span><br></pre></td></tr></table></figure></li></ul><p>接下来我们介绍详细的参数含义。</p><h4 id="1-2-2-配置说明"><a href="#1-2-2-配置说明" class="headerlink" title="1.2.2 配置说明"></a>1.2.2 配置说明</h4><table><thead><tr><th style="text-align:left">Property Name</th><th style="text-align:left">Default</th><th style="text-align:left">Meaning</th><th style="text-align:left">Since Version</th></tr></thead><tbody><tr><td style="text-align:left"><code>spark.dynamicAllocation.enabled</code></td><td style="text-align:left">false</td><td style="text-align:left">Whether to use dynamic resource allocation, which scales the number of executors registered with this application up and down based on the workload. For more detail, see the description <a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation" target="_blank" rel="noopener">here</a>.  This requires <code>spark.shuffle.service.enabled</code> or <code>spark.dynamicAllocation.shuffleTracking.enabled</code> to be set. The following configurations are also relevant: <code>spark.dynamicAllocation.minExecutors</code>, <code>spark.dynamicAllocation.maxExecutors</code>, and <code>spark.dynamicAllocation.initialExecutors</code> <code>spark.dynamicAllocation.executorAllocationRatio</code></td><td style="text-align:left">1.2.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.executorIdleTimeout</code></td><td style="text-align:left">60s</td><td style="text-align:left">If dynamic allocation is enabled and an executor has been idle for more than this duration, the executor will be removed. For more detail, see this <a href="https://spark.apache.org/docs/latest/job-scheduling.html#resource-allocation-policy" target="_blank" rel="noopener">description</a>.</td><td style="text-align:left">1.2.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.cachedExecutorIdleTimeout</code></td><td style="text-align:left">infinity</td><td style="text-align:left">If dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration, the executor will be removed. For more details, see this <a href="https://spark.apache.org/docs/latest/job-scheduling.html#resource-allocation-policy" target="_blank" rel="noopener">description</a>.</td><td style="text-align:left">1.4.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.initialExecutors</code></td><td style="text-align:left"><code>spark.dynamicAllocation.minExecutors</code></td><td style="text-align:left">Initial number of executors to run if dynamic allocation is enabled.  If <code>--num-executors</code> (or <code>spark.executor.instances</code>) is set and larger than this value, it will be used as the initial number of executors.</td><td style="text-align:left">1.3.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.maxExecutors</code></td><td style="text-align:left">infinity</td><td style="text-align:left">Upper bound for the number of executors if dynamic allocation is enabled.</td><td style="text-align:left">1.2.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.minExecutors</code></td><td style="text-align:left">0</td><td style="text-align:left">Lower bound for the number of executors if dynamic allocation is enabled.</td><td style="text-align:left">1.2.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.executorAllocationRatio</code></td><td style="text-align:left">1</td><td style="text-align:left">By default, the dynamic allocation will request enough executors to maximize the parallelism according to the number of tasks to process. While this minimizes the latency of the job, with small tasks this setting can waste a lot of resources due to executor allocation overhead, as some executor might not even do any work. This setting allows to set a ratio that will be used to reduce the number of executors w.r.t. full parallelism. Defaults to 1.0 to give maximum parallelism. 0.5 will divide the target number of executors by 2 The target number of executors computed by the dynamicAllocation can still be overridden by the <code>spark.dynamicAllocation.minExecutors</code> and <code>spark.dynamicAllocation.maxExecutors</code> settings</td><td style="text-align:left">2.4.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.schedulerBacklogTimeout</code></td><td style="text-align:left">1s</td><td style="text-align:left">If dynamic allocation is enabled and there have been pending tasks backlogged for more than this duration, new executors will be requested. For more detail, see this <a href="https://spark.apache.org/docs/latest/job-scheduling.html#resource-allocation-policy" target="_blank" rel="noopener">description</a>.</td><td style="text-align:left">1.2.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.sustainedSchedulerBacklogTimeout</code></td><td style="text-align:left"><code>schedulerBacklogTimeout</code></td><td style="text-align:left">Same as <code>spark.dynamicAllocation.schedulerBacklogTimeout</code>, but used only for subsequent executor requests. For more detail, see this <a href="https://spark.apache.org/docs/latest/job-scheduling.html#resource-allocation-policy" target="_blank" rel="noopener">description</a>.</td><td style="text-align:left">1.2.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.shuffleTracking.enabled</code></td><td style="text-align:left"><code>false</code></td><td style="text-align:left">Experimental. Enables shuffle file tracking for executors, which allows dynamic allocation without the need for an external shuffle service. This option will try to keep alive executors that are storing shuffle data for active jobs.</td><td style="text-align:left">3.0.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.shuffleTracking.timeout</code></td><td style="text-align:left"><code>infinity</code></td><td style="text-align:left">When shuffle tracking is enabled, controls the timeout for executors that are holding shuffle data. The default value means that Spark will rely on the shuffles being garbage collected to be able to release executors. If for some reason garbage collection is not cleaning up shuffles quickly enough, this option can be used to control when to time out executors even when they are storing shuffle data.</td><td style="text-align:left">3.0.0</td></tr></tbody></table><h3 id="1-3-Spark-Streaming-任务配置"><a href="#1-3-Spark-Streaming-任务配置" class="headerlink" title="1.3 Spark Streaming 任务配置"></a>1.3 Spark Streaming 任务配置</h3><p>对于Spark Streaming 流处理任务，Spark官方并未在文档中给出介绍。<code>Dynamic Resource Allocation</code>配置指引如下：</p><ul><li><p>必要配置（Spark 3.0.0）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启Spark Streaming流处理动态资源分配参数开关（默认关闭）</span></span><br><span class="line">spark.streaming.dynamicAllocation.enabled=true</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置最大和最小的Executor数量</span></span><br><span class="line">spark.streaming.dynamicAllocation.minExecutors=1（必须正整数）</span><br><span class="line">spark.streaming.dynamicAllocation.maxExecutors=50（必须正整数，默认Int.MaxValue，即无限大）</span><br></pre></td></tr></table></figure></li><li><p>可选配置（Spark 3.0.0）</p><p>这些参数可以不用配置，都已经提供了一个较为合理的默认值。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.streaming.dynamicAllocation.scalingUpRatio（必须正数，默认0.9）</span><br><span class="line">spark.streaming.dynamicAllocation.scalingInterval（单位秒，默认60）</span><br><span class="line">spark.streaming.dynamicAllocation.scalingDownRatio（必须正数，默认0.3）</span><br></pre></td></tr></table></figure></li></ul><h2 id="第二部分-动态配置原理和源码分析"><a href="#第二部分-动态配置原理和源码分析" class="headerlink" title="第二部分 动态配置原理和源码分析"></a>第二部分 动态配置原理和源码分析</h2><p>介绍完使用配置后，接下来将详细介绍实现原理。以便理解各参数的含义和参数调优。</p><h3 id="2-1-Spark-Core任务"><a href="#2-1-Spark-Core任务" class="headerlink" title="2.1 Spark Core任务"></a>2.1 Spark Core任务</h3><p>为了动态伸缩Spark任务的计算资源（Executor为基本分配单位），首先需要确定的度量是任务的繁忙程度。<code>DRA</code>机制将Spark任务是否有挂起任务(pending task)作为判断标准，一旦有挂起任务表示当前的Executor数量不够支撑所有的task并行运行，所以会申请增加资源。</p><h4 id="2-1-1-资源请求（Request）策略"><a href="#2-1-1-资源请求（Request）策略" class="headerlink" title="2.1.1 资源请求（Request）策略"></a>2.1.1 资源请求（Request）策略</h4><p>当Spark任务开启<code>DRA</code>机制，<code>SparkContext</code>会启动后台<code>ExecutorAllocationManager</code>，用来管理集群的Executors。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//package org.apache.spark SparkContext.scala</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dynamicAllocationEnabled = <span class="type">Utils</span>.isDynamicAllocationEnabled(_conf)</span><br><span class="line">    _executorAllocationManager =</span><br><span class="line">      <span class="keyword">if</span> (dynamicAllocationEnabled) &#123;</span><br><span class="line">        schedulerBackend <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> b: <span class="type">ExecutorAllocationClient</span> =&gt;</span><br><span class="line">            <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ExecutorAllocationManager</span>(</span><br><span class="line">              schedulerBackend.asInstanceOf[<span class="type">ExecutorAllocationClient</span>], listenerBus, _conf,</span><br><span class="line">              cleaner = cleaner, resourceProfileManager = resourceProfileManager))</span><br><span class="line">          <span class="keyword">case</span> _ =&gt;</span><br><span class="line">            <span class="type">None</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line">    _executorAllocationManager.foreach(_.start())</span><br></pre></td></tr></table></figure><p>Start()方法将<code>ExecutorAllocationListener</code>加入到<code>listenerBus</code>中，<code>ExecutorAllocationListener</code>通过监听<code>listenerBus</code>里的事件，动态添加，删除<code>Executor</code>。并且通过<code>Thread</code>不断添加<code>Executor</code>，遍历<code>Executor</code>，将超时的<code>Executor</code>杀掉并移除。</p><p>Spark会周期性（<code>intervalMillis</code>=100毫秒）计算实际需要的Executor的最大数量<code>maxNeeded</code>。公式如下。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> maxNeeded = math.ceil(numRunningOrPendingTasks * executorAllocationRatio /</span><br><span class="line">      tasksPerExecutor).toInt</span><br></pre></td></tr></table></figure><p>逻辑代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateAndSyncNumExecutorsTarget</span></span>(now: <span class="type">Long</span>): <span class="type">Int</span> = synchronized &#123;</span><br><span class="line">  <span class="keyword">if</span> (initializing) &#123;</span><br><span class="line">    <span class="number">0</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> updatesNeeded = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">ExecutorAllocationManager</span>.<span class="type">TargetNumUpdates</span>]</span><br><span class="line">    numExecutorsTargetPerResourceProfileId.foreach &#123; <span class="keyword">case</span> (rpId, targetExecs) =&gt;</span><br><span class="line">      <span class="keyword">val</span> maxNeeded = maxNumExecutorsNeededPerResourceProfile(rpId)</span><br><span class="line">      <span class="keyword">if</span> (maxNeeded &lt; targetExecs) &#123;</span><br><span class="line">        decrementExecutorsFromTarget(maxNeeded, rpId, updatesNeeded)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (addTime != <span class="type">NOT_SET</span> &amp;&amp; now &gt;= addTime) &#123;</span><br><span class="line">        addExecutorsToTarget(maxNeeded, rpId, updatesNeeded)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    doUpdateRequest(updatesNeeded.toMap, now)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>当集群中有<code>Executor</code>出现<code>pending task</code>，计算判断条件<code>maxNeeded &gt; targetExecs</code>，并且等待时间超过<code>schedulerBacklogTimeout</code>(默认<code>1s</code>)，则会触发方法<code>addExecutorsToTarget(maxNeeded, rpId, updatesNeeded)</code>。对于首次增加Executor。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.dynamicAllocation.schedulerBacklogTimeout = 1s（秒）</span><br></pre></td></tr></table></figure><ul><li>后续按照周期性时间<code>sustainedSchedulerBacklogTimeout</code>来检测pending task，一旦出现pending task，即触发增加Executor。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = 1s(秒)</span><br></pre></td></tr></table></figure><p>每次（轮）触发增加<code>Executor</code>资源请求，增加的数量翻倍，即是一个指数数列（2的n次方），例如：<code>1、2、4、8</code>。</p><h4 id="2-1-2-资源释放（Remove）策略"><a href="#2-1-2-资源释放（Remove）策略" class="headerlink" title="2.1.2 资源释放（Remove）策略"></a>2.1.2 资源释放（Remove）策略</h4><p>对于移除策略如下：</p><ul><li>如果Executor闲置（<code>maxNeeded &lt; targetExecs</code>）时间超过以下参数，并且executor中没有cache（数据缓存在内存），则spark应用将会释放该Executor。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.dynamicAllocation.executorIdleTimeout（单位为秒） 默认60s</span><br></pre></td></tr></table></figure><ul><li>如果空闲Executor中有cache，那么这个超时参数为：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.dynamicAllocation.cachedExecutorIdleTimeout 默认值：Integer.MAX_VALUE（即永不超时）</span><br></pre></td></tr></table></figure><p>对于Executor的退出，设计上需要考虑状态的问题，主要：</p><ul><li><p>需要移除的<code>Executor</code>存在<code>cache</code>。</p><p>如果需要移除的<code>Executor</code>含有<code>RDD cache</code>。这时候超时时间为整型最大值（相当于无限）。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">val</span> <span class="type">DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT</span> =</span><br><span class="line">  <span class="type">ConfigBuilder</span>(<span class="string">"spark.dynamicAllocation.cachedExecutorIdleTimeout"</span>)</span><br><span class="line">    .version(<span class="string">"1.4.0"</span>)</span><br><span class="line">    .timeConf(<span class="type">TimeUnit</span>.<span class="type">SECONDS</span>)</span><br><span class="line">    .checkValue(_ &gt;= <span class="number">0</span>L, <span class="string">"Timeout must be &gt;= 0."</span>)</span><br><span class="line">    .createWithDefault(<span class="type">Integer</span>.<span class="type">MAX_VALUE</span>)</span><br></pre></td></tr></table></figure></li><li><p>Shuffle状态的保存问题。如果需要移除的Executor包含了Shuffle状态数据（在shuffle期间，Spark executor先要将map的输出写入到磁盘，然后该executor充当一个文件服务器，将这些文件共享给其他的executor访问）。需要提前启动<code>External shuffle service</code>，由专门外置服务提供存储，Executor中不再负责保存，架构上功能解耦。</p></li></ul><p>另外添加和移除Executor之后，需要告知<code>DAGSchedule</code>进行相关信息更新。</p><h4 id="2-1-3-配置建议"><a href="#2-1-3-配置建议" class="headerlink" title="2.1.3 配置建议"></a>2.1.3 配置建议</h4><p>Spark的动态伸缩机制的几点建议：</p><ul><li>给Executor数量设置一个合理的伸缩区间，即<code>[minExecutors-maxExecutors]</code>区间值。</li><li>配置资源粒度较小的Executor，例如CPU数量为3-4个。动态伸缩的最小伸缩单位是单个Executor，如果出现资源伸缩，特别是Executor数目下降后业务量突增，新申请资源未就绪，已有的Executor就可能由于任务过载而导致集群崩溃。</li><li>如果程序中有shuffle,例如(reduce<em>,groupBy</em>),建议设置一个合理的并行数，避免杀掉过多的Executors。</li><li>对于每个Stage持续时间很短的应用，不适合动态伸缩机制。这样会频繁增加和移除Executors，造成系统颠簸。特别是在 Spark on Yarn模式下资源的申请处理速度并不快。</li></ul><h3 id="2-2-Spark-Streaming-任务"><a href="#2-2-Spark-Streaming-任务" class="headerlink" title="2.2 Spark Streaming 任务"></a>2.2 Spark Streaming 任务</h3><p>Spark Streaming任务可以看成连续运行的微（micro-batch）批任务，如果直接套用Spark Core的动态伸缩机制就水土不服了。一般一个微批任务较短（默认60秒），实际线上任务可能更小，动态伸缩的反应时间较长（特别是on Yarn模式），一个微批任务结束，动态伸缩策略还没生效。所以针对Spark Streaming任务，项目组设计新的动态机制（Spark 2.0.0 版本引入）。</p><p>提案：<a href="https://issues.apache.org/jira/browse/SPARK-12133" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-12133</a></p><h4 id="2-2-1-源码分析"><a href="#2-2-1-源码分析" class="headerlink" title="2.2.1 源码分析"></a>2.2.1 源码分析</h4><p>Spark Streaming任务会统计微批任务运行时间的延迟时间，最朴素的想法就是按照这个度量指标来作为动态伸缩的触发指标。这部分源码在<code>org.apache.spark.streaming.scheduler</code>中：</p><ul><li><p>周期性计算微批运行完成的平均时间，然后和<code>batch interval</code>进行比较；</p><p>这里的周期大小由参数<code>spark.streaming.dynamicAllocation.scalingInterval</code>决定，大小为<code>scalingIntervalSecs * 1000</code>。例如默认值为：60*1000毫秒，即60秒。</p><p>通过<code>streamingListener</code>计算微批平均处理时间（<code>averageBatchProcTime</code>），然后计算微批处理率（ratio，微批平均处理时间/微批处理周期）。</p><p>然后和参数值上限（<code>scalingUpRatio</code>）和下限（<code>scalingDownRatio</code>）进行比较。详细控制函数如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">manageAllocation</span></span>(): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">   logInfo(<span class="string">s"Managing executor allocation with ratios = [<span class="subst">$scalingUpRatio</span>, <span class="subst">$scalingDownRatio</span>]"</span>)</span><br><span class="line">   <span class="keyword">if</span> (batchProcTimeCount &gt; <span class="number">0</span>) &#123;</span><br><span class="line">     <span class="keyword">val</span> averageBatchProcTime = batchProcTimeSum / batchProcTimeCount</span><br><span class="line">     <span class="keyword">val</span> ratio = averageBatchProcTime.toDouble / batchDurationMs</span><br><span class="line">     logInfo(<span class="string">s"Average: <span class="subst">$averageBatchProcTime</span>, ratio = <span class="subst">$ratio</span>"</span> )</span><br><span class="line">     <span class="keyword">if</span> (ratio &gt;= scalingUpRatio) &#123;</span><br><span class="line">       logDebug(<span class="string">"Requesting executors"</span>)</span><br><span class="line">       <span class="keyword">val</span> numNewExecutors = math.max(math.round(ratio).toInt, <span class="number">1</span>)</span><br><span class="line">       requestExecutors(numNewExecutors)</span><br><span class="line">     &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ratio &lt;= scalingDownRatio) &#123;</span><br><span class="line">       logDebug(<span class="string">"Killing executors"</span>)</span><br><span class="line">       killExecutor()</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   batchProcTimeSum = <span class="number">0</span></span><br><span class="line">   batchProcTimeCount = <span class="number">0</span></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li><li><p>增加Executor数量；如果<code>ratio &gt;= scalingUpRatio</code>，然后按照下面的公司增加数量：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> numNewExecutors = math.max(math.round(ratio).toInt, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>例如<code>ratio=1.6&gt;0.9(scalingUpRatio)</code>，这时候说明有大量微批任务出现了延迟，按照公式计算<code>numNewExecutors=2</code>。接下来会调用<code>requestExecutors(numNewExecutors)</code>方法去申请2个Executor。</p></li><li><p>减少Executor数量；如果<code>ratio &lt;= scalingDownRatio</code>，这直接调用<code>killExecutor()</code>方法（方法中判断没有receiver运行的Executor）去kill Executor。</p></li></ul><h4 id="2-2-2-配置建议"><a href="#2-2-2-配置建议" class="headerlink" title="2.2.2 配置建议"></a>2.2.2 配置建议</h4><p>Spark Streaming动态资源分配起作用前，需要至少完成一个Batch处理(<code>batchProcTimeCount &gt; 0</code>)。</p><ul><li><p>Spark Core和Spark Streaming的动态配置开关配置是分别设置的。</p><p>如果两个配置开关同时配置为true，会抛出错误。建议如下配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.dynamicAllocation.enabled=false （默认是false，可以不配置）</span><br><span class="line">spark.streaming.dynamicAllocation.enabled=true</span><br></pre></td></tr></table></figure></li></ul><h2 id="第三部分-总结"><a href="#第三部分-总结" class="headerlink" title="第三部分 总结"></a>第三部分 总结</h2><h3 id="3-1-对比"><a href="#3-1-对比" class="headerlink" title="3.1 对比"></a>3.1 对比</h3><p>Spark Core中动态伸缩机制是基于空闲时间来控制回收Executor。而在Spark Streaming中，一个Executor每隔很短的时间都会有一批作业被调度，所以在streaming里面是基于平均每批作业处理的时间。</p><h3 id="3-2-Structed-Streaming任务动态伸缩"><a href="#3-2-Structed-Streaming任务动态伸缩" class="headerlink" title="3.2 Structed Streaming任务动态伸缩"></a>3.2 <code>Structed Streaming</code>任务动态伸缩</h3><p>在spark Streaming中，最小的可能延迟受限于每批的调度间隔以及任务启动时间。所以这不能满足更低延迟的需求。如果能够连续的处理，尤其是简单的处理而没有任何的阻塞操作。这种连续处理的架构可以使得端到端延迟最低降低到<code>1ms</code>级别，而不是目前的<code>10-100ms</code>级别，这就是Spark 2.2.0版本引入新的Spark流处理框架：<code>Structed Streaming</code>。</p><blockquote><p><a href="https://issues.apache.org/jira/browse/SPARK-20928" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-20928</a></p></blockquote><p>当然项目组自然也会考虑该框架的资源伸缩机制（未完成）</p><blockquote><p><a href="https://issues.apache.org/jira/browse/SPARK-24815" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-24815</a></p></blockquote><p>后续趋势上看，Spark项目会将更多精力放在<code>Structed Streaming</code>。</p><h3 id="3-3-Spark-Streaming-背压机制"><a href="#3-3-Spark-Streaming-背压机制" class="headerlink" title="3.3 Spark Streaming 背压机制"></a>3.3 Spark <strong>Streaming</strong> 背压机制</h3><p>为了应对Spark Streaming处理数据波动，除了资源动态伸缩机制，在Spark 1.5版本项目在Spark Streaming 中引入了的背压（<code>Backpressure</code>）机制。</p><p>Spark Streaming任务中，当batch的处理时间大于batch interval时，意味着数据处理速度跟不上数据接收速度。这时候在数据接收端(Receiver)Executor就会开始积压数据。如果数据存储采用MEMORY_ONLY模式（内存）就会导致OOM，采用MEMORY_AND_DISK多余的数据保存到磁盘上，增加数据IO时间。</p><p>背压（<code>Backpressure</code>）机制，通过动态控制数据接收速率来适配集群数据处理能力。这是被动防守型的应对，将数据缓存在Kafka消息层。如果数据持续保持高量级，就需要主动启停任务来增加计算资源。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Job Scheduling，链接：<a href="https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup</a></p><p>2、About Spark Streaming，链接：<a href="https://www.turbofei.wang/spark/2019/05/26/about-spark-streaming" target="_blank" rel="noopener">https://www.turbofei.wang/spark/2019/05/26/about-spark-streaming</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  配置实现&lt;/li&gt;
&lt;li&gt;第二部分 动态配置原理和源码分析&lt;/li&gt;
&lt;li&gt;第三部分 总
      
    
    </summary>
    
      <category term="spark" scheme="https://zjrongxiang.github.io/categories/spark/"/>
    
    
  </entry>
  
  <entry>
    <title>suse操作系统rpm命令</title>
    <link href="https://zjrongxiang.github.io/2021/04/15/2021-05-30-suse%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9Frpm%E5%91%BD%E4%BB%A4/"/>
    <id>https://zjrongxiang.github.io/2021/04/15/2021-05-30-suse操作系统rpm命令/</id>
    <published>2021-04-15T13:30:00.000Z</published>
    <updated>2021-06-03T00:50:27.051Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>RPM 安装操作</p><p>命令：</p><p>rpm -i 需要安装的包文件名</p><p>举例如下：</p><p>rpm -i example.rpm 安装 example.rpm 包；</p><p>rpm -iv example.rpm 安装 example.rpm 包并在安装过程中显示正在安装的文件信息；</p><p>rpm -ivh example.rpm 安装 example.rpm 包并在安装过程中显示正在安装的文件信息及安装进度；</p><p>RPM 查询操作</p><p>命令：</p><p>rpm -q …</p><p>附加查询命令：</p><p>a 查询所有已经安装的包以下两个附加命令用于查询安装包的信息；</p><p>i 显示安装包的信息；</p><p>l 显示安装包中的所有文件被安装到哪些目录下；</p><p>s 显示安装版中的所有文件状态及被安装到哪些目录下；以下两个附加命令用于指定需要查询的是安装包还是已安装后的文件；</p><p>p 查询的是安装包的信息；</p><p>f 查询的是已安装的某文件信息；</p><p>举例如下：</p><p>rpm -qa | grep tomcat4 查看 tomcat4 是否被安装；</p><p>rpm -qip example.rpm 查看 example.rpm 安装包的信息；</p><p>rpm -qif /bin/df 查看/bin/df 文件所在安装包的信息；</p><p>rpm -qlf /bin/df 查看/bin/df 文件所在安装包中的各个文件分别被安装到哪个目录下；</p><p>RPM 卸载操作</p><p>命令：</p><p>rpm -e 需要卸载的安装包</p><p>在卸载之前，通常需要使用rpm -q …命令查出需要卸载的安装包名称。</p><p>举例如下：</p><p>rpm -e tomcat4 卸载 tomcat4 软件包</p><p>RPM 升级操作</p><p>命令：</p><p>rpm -U 需要升级的包</p><p>举例如下：</p><p>rpm -Uvh example.rpm 升级 example.rpm 软件包</p><p>RPM 验证操作</p><p>命令：</p><p>rpm -V 需要验证的包</p><p>举例如下：</p><p>rpm -Vf /etc/tomcat4/tomcat4.conf</p><p>输出信息类似如下：</p><p>S.5….T c /etc/tomcat4/tomcat4.conf</p><p>其中，S 表示文件大小修改过，T 表示文件日期修改过。限于篇幅，更多的验证信息请您参考rpm 帮助文件：man rpm</p><p>RPM 的其他附加命令</p><p>–force 强制操作 如强制安装删除等；</p><p>–requires 显示该包的依赖关系；</p><p>–nodeps 忽略依赖关系并继续操作；</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] Orange官网，链接：<a href="http://orange.sumory.com/" target="_blank" rel="noopener">http://orange.sumory.com/</a></p><p>[2] Orange网关官网docker，链接：<a href="https://hub.docker.com/r/syhily/orange" target="_blank" rel="noopener">https://hub.docker.com/r/syhily/orange</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;RPM 安装操作&lt;/p&gt;
&lt;p&gt;命令：&lt;/p&gt;
&lt;p&gt;rpm -i 需要安装的包文件名&lt;/p&gt;
&lt;p&gt;举例如下：&lt;/p&gt;
&lt;p&gt;rpm -i
      
    
    </summary>
    
      <category term="orange" scheme="https://zjrongxiang.github.io/categories/orange/"/>
    
    
      <category term="orange" scheme="https://zjrongxiang.github.io/tags/orange/"/>
    
  </entry>
  
  <entry>
    <title>orange网关原理的源码分析</title>
    <link href="https://zjrongxiang.github.io/2021/04/15/2021-05-29-orange%E7%BD%91%E5%85%B3%E5%8E%9F%E7%90%86%E7%9A%84%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    <id>https://zjrongxiang.github.io/2021/04/15/2021-05-29-orange网关原理的源码分析/</id>
    <published>2021-04-15T13:30:00.000Z</published>
    <updated>2021-06-17T12:17:33.339Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Orange v0.6.0版本是一个重构版本， 着重为了解决之前版本在有大量规则配置时性能损耗的问题。 基本的设计思路是将原来的规则细分成两层， 第一层叫做<a href="http://orange.sumory.com/docs/concept/selector/" target="_blank" rel="noopener">selector</a>, 用于将流量进行第一步划分， 在进入某个selector后才按照之前的设计进行<a href="http://orange.sumory.com/docs/concept/rule/" target="_blank" rel="noopener">规则</a>匹配， 匹配到后进行相关处理。</p><p><a href="https://lengrongfu.github.io/2019/05/21/orange-%E5%8E%9F%E7%90%86/" target="_blank" rel="noopener">https://lengrongfu.github.io/2019/05/21/orange-%E5%8E%9F%E7%90%86/</a></p><p><a href="https://book.aikaiyuan.com/openresty/understanding-orange.html" target="_blank" rel="noopener">https://book.aikaiyuan.com/openresty/understanding-orange.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/67481992" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/67481992</a></p><h2 id="网关优化项目"><a href="#网关优化项目" class="headerlink" title="网关优化项目"></a>网关优化项目</h2><p><a href="https://github.com/starjiang/xorange" target="_blank" rel="noopener">https://github.com/starjiang/xorange</a></p><h2 id="orange-中设计概念"><a href="#orange-中设计概念" class="headerlink" title="orange 中设计概念"></a>orange 中设计概念</h2><h3 id="orange-缓存"><a href="#orange-缓存" class="headerlink" title="orange 缓存"></a>orange 缓存</h3><p>在<code>nginx.conf</code>文件中：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">lua_code_cache</span> <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line"><span class="attribute">lua_shared_dict</span> orange_data <span class="number">20m</span>; <span class="comment"># should not removed. used for orange data, e.g. plugins configurations..</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">lua_shared_dict</span> status <span class="number">1m</span>; <span class="comment"># used for global statistic, see plugin: stat</span></span><br><span class="line"><span class="attribute">lua_shared_dict</span> waf_status <span class="number">1m</span>; <span class="comment"># used for waf statistic, see plugin: waf</span></span><br><span class="line"><span class="attribute">lua_shared_dict</span> monitor <span class="number">10m</span>; <span class="comment"># used for url monitor statistic, see plugin: monitor</span></span><br><span class="line"><span class="attribute">lua_shared_dict</span> rate_limit <span class="number">10m</span>; <span class="comment"># used for rate limiting count, see plugin: rate_limiting</span></span><br><span class="line"><span class="attribute">lua_shared_dict</span> property_rate_limiting <span class="number">10m</span>; <span class="comment"># used for rate limiting count, see plugin: rate_limiting</span></span><br><span class="line"><span class="attribute">lua_shared_dict</span> consul_upstream <span class="number">5m</span>; <span class="comment"># used for consul_upstream, see plugin consul_balancer</span></span><br><span class="line"><span class="attribute">lua_shared_dict</span> consul_upstream_watch <span class="number">5m</span>; <span class="comment"># used for consul_upstream_watch, consul_balancer</span></span><br></pre></td></tr></table></figure><p>这些配置是插件缓存数据。</p><h2 id="如何开发一个插件"><a href="#如何开发一个插件" class="headerlink" title="如何开发一个插件"></a>如何开发一个插件</h2><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] Orange官网，链接：<a href="http://orange.sumory.com/" target="_blank" rel="noopener">http://orange.sumory.com/</a></p><p>[2] Orange网关官网docker，链接：<a href="https://hub.docker.com/r/syhily/orange" target="_blank" rel="noopener">https://hub.docker.com/r/syhily/orange</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;Orange v0.6.0版本是一个重构版本， 着重为了解决之前版本在有大量规则配置时性能损耗的问题。 基本的设计思路是将原来的规则细分成两
      
    
    </summary>
    
      <category term="orange" scheme="https://zjrongxiang.github.io/categories/orange/"/>
    
    
      <category term="orange" scheme="https://zjrongxiang.github.io/tags/orange/"/>
    
  </entry>
  
  <entry>
    <title>使用spark streaming将Kafka汇入Mysql实践</title>
    <link href="https://zjrongxiang.github.io/2021/04/15/2021-04-20-%E4%BD%BF%E7%94%A8spark%20streaming%E5%B0%86Kafka%E6%B1%87%E5%85%A5Mysql%E5%AE%9E%E8%B7%B5/"/>
    <id>https://zjrongxiang.github.io/2021/04/15/2021-04-20-使用spark streaming将Kafka汇入Mysql实践/</id>
    <published>2021-04-15T13:30:00.000Z</published>
    <updated>2021-04-20T00:30:15.767Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="http://www.biancheng666.com/article_147327.html" target="_blank" rel="noopener">http://www.biancheng666.com/article_147327.html</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://www.biancheng666.com/article_147327.html&quot; target=&quot;_bla
      
    
    </summary>
    
      <category term="spark" scheme="https://zjrongxiang.github.io/categories/spark/"/>
    
    
      <category term="spark" scheme="https://zjrongxiang.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Kafka中序列化和反序列化总结</title>
    <link href="https://zjrongxiang.github.io/2021/04/15/2021-04-15-Kafka%E4%B8%AD%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/04/15/2021-04-15-Kafka中序列化和反序列化总结/</id>
    <published>2021-04-15T13:30:00.000Z</published>
    <updated>2021-05-06T14:49:23.075Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://www.vijaykonnackal.com/protobuf-kafka-message/" target="_blank" rel="noopener">https://www.vijaykonnackal.com/protobuf-kafka-message/</a></p><p><a href="https://blog.csdn.net/weixin_26717681/article/details/108499713#t6" target="_blank" rel="noopener">https://blog.csdn.net/weixin_26717681/article/details/108499713#t6</a></p><p><a href="https://codingharbour.com/apache-kafka/how-to-use-protobuf-with-apache-kafka-and-schema-registry/" target="_blank" rel="noopener">https://codingharbour.com/apache-kafka/how-to-use-protobuf-with-apache-kafka-and-schema-registry/</a></p><p><a href="https://codingharbour.com/apache-kafka/how-to-use-protobuf-with-apache-kafka-and-schema-registry/" target="_blank" rel="noopener">https://codingharbour.com/apache-kafka/how-to-use-protobuf-with-apache-kafka-and-schema-registry/</a></p><p><a href="https://data-flair.training/blogs/kafka-serialization-and-deserialization/" target="_blank" rel="noopener">https://data-flair.training/blogs/kafka-serialization-and-deserialization/</a></p><p><a href="https://blog.csdn.net/weixin_40929150/article/details/88775559" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40929150/article/details/88775559</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p><a href="https://blog.csdn.net/shirukai/article/details/82152172" target="_blank" rel="noopener">https://blog.csdn.net/shirukai/article/details/82152172</a></p><p><a href="https://shirukai.github.io/blog/kafka-custom-message-serialization-and-deserialization-mode.html" target="_blank" rel="noopener">https://shirukai.github.io/blog/kafka-custom-message-serialization-and-deserialization-mode.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.vijaykonnackal.com/protobuf-kafka-message/&quot; targe
      
    
    </summary>
    
      <category term="Kafka" scheme="https://zjrongxiang.github.io/categories/Kafka/"/>
    
    
      <category term="Kafka" scheme="https://zjrongxiang.github.io/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Nginx常见使用场景总结</title>
    <link href="https://zjrongxiang.github.io/2021/04/15/2021-05-30-Nginx%E5%B8%B8%E8%A7%81%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/04/15/2021-05-30-Nginx常见使用场景总结/</id>
    <published>2021-04-15T13:30:00.000Z</published>
    <updated>2021-06-01T11:51:22.365Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul><li>HTTP服务器（含动静分离）</li><li>负载均衡</li><li>反向代理</li><li>正向代理</li><li>跨域请求</li></ul><h2 id="第一部分-HTTP服务器（含动静分离）"><a href="#第一部分-HTTP服务器（含动静分离）" class="headerlink" title="第一部分 HTTP服务器（含动静分离）"></a>第一部分 HTTP服务器（含动静分离）</h2><p>Nginx本身是一个静态资源的服务器，当只有静态资源的时候，就可以使用Nginx来做服务器，如下，我们使用Nginx来部署一个打包好的vue项目<strong></strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#vue项目</span></span><br><span class="line">server</span><br><span class="line">&#123;</span><br><span class="line">     listen 8081; #监听端口</span><br><span class="line">     server_name 209.250.235.145; </span><br><span class="line">     root /app/vue/dist/; # 我们的资源在服务器中的路径</span><br><span class="line">     index index.html; #指定资源的入口文件</span><br><span class="line">&#125;复制代码</span><br></pre></td></tr></table></figure><p>完成后我们nginx -s reload一下，然后访问209.250.235.145:8081，只要路径没错静态资源就访问的到了</p><h2 id="第二部分-正向代理"><a href="#第二部分-正向代理" class="headerlink" title="第二部分 正向代理"></a>第二部分 正向代理</h2><h2 id="第三部分-反向代理"><a href="#第三部分-反向代理" class="headerlink" title="第三部分 反向代理"></a>第三部分 反向代理</h2><p>反向代理应该是Nginx做的最多的一件事了，什么是反向代理呢，以下是百度百科的说法：反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。简单来说就是真实的服务器不能直接被外部网络访问，所以需要一台代理服务器，而代理服务器能被外部网络访问的同时又跟真实服务器在同一个网络环境，当然也可能是同一台服务器，端口不同而已。 下面贴上一段简单的实现反向代理的代码</p><p>server{</p><p>listen80;</p><p>server_namelocalhost;</p><p>client_max_body_size1024M;</p><p>location/{</p><p>proxy_pass<a href="http://localhost:8080" target="_blank" rel="noopener">http://localhost:8080</a>;</p><p>proxy_set_headerHost$host:$server_port;</p><p>}</p><p>}</p><p>保存配置文件后启动Nginx，这样当我们访问localhost的时候，就相当于访问localhost:8080了</p><h2 id="第四部分-负载均衡"><a href="#第四部分-负载均衡" class="headerlink" title="第四部分 负载均衡"></a>第四部分 负载均衡</h2><p>在线上生产环境，为了承载较大流量，通常需要以集群方式并发处理，这就需要有代理服务对流量进行智能负载。通过算法将流量合理的分配给集群中各个处理节点。实现方式有硬件和软件两种，硬件常见的是F5专用设备，成本较高。如果流量不大可以由软件来实现。</p><p>而Nginx就是常见的软负载组件。利用<code>upstream</code>定义集群服务器。负载均衡配置一般都需要同时配置反向代理，通过反向代理跳转到负载均衡。</p><p>Nginx目前支持自带3种负载均衡策略，还有2种常用的第三方策略。</p><h3 id="4-1-配置"><a href="#4-1-配置" class="headerlink" title="4.1 配置"></a>4.1 配置</h3><p>首先需要在http节点中，配置upstream，例如：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">upstream</span> upstreamTest &#123; </span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.1:9200</span>; </span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.2:9200</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将server节点下的location节点中的proxy_pass配置为：http:// + upstream名称，即</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">location</span> / &#123; </span><br><span class="line">            <span class="attribute">root</span>  html; </span><br><span class="line">            <span class="attribute">index</span>  index.html index.htm; </span><br><span class="line">            <span class="attribute">proxy_pass</span> http://upstreamTest; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-2-负载模式"><a href="#4-2-负载模式" class="headerlink" title="4.2 负载模式"></a>4.2 负载模式</h3><h4 id="4-2-1-轮询-（round-robin）（默认方式）"><a href="#4-2-1-轮询-（round-robin）（默认方式）" class="headerlink" title="4.2.1 轮询 （round-robin）（默认方式）"></a>4.2.1 轮询 （round-robin）（默认方式）</h4><p>轮询为负载均衡中最为朴素的算法，不需要配置额外参数。假设共有<code>N</code>台服务器，算法将遍历服务器节点列表，并按节点次序每轮选择一台服务器处理请求。当所有节点均被调用过一次后，算法将从第一个节点开始重新一轮遍历。如果列表中服务有下宕的，算法能主动将服务器从轮询列表中去除。</p><p>这个算法前提需要服务器列表中每台服务器的处理能力是均衡的，否则会有分配不均的问题。</p><p>配置案例：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">upstream</span> upstreamTest &#123; </span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.1:9200</span>; </span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.2:9200</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-2-2-加权轮询"><a href="#4-2-2-加权轮询" class="headerlink" title="4.2.2 加权轮询"></a>4.2.2 加权轮询</h4><p>但后端负载集群性能不均的时候，可以通过加权方式分配流量，这就是加权轮询。例如：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">upstream</span> upstreamTest &#123; </span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.1:9200</span> weight=<span class="number">5</span>; </span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.2:9200</span> weight=<span class="number">10</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的配置给每一台服务指定了<code>weight</code>值，<code>weight</code> 的值越大意味着该服务器的性能越好，可以承载更多的请求。也可以从概率角度去理解，<code>192.168.88.2</code>的流量分配概率比<code>192.168.88.1</code>大一倍。</p><h4 id="4-2-3-IP-哈希（IP-hash）"><a href="#4-2-3-IP-哈希（IP-hash）" class="headerlink" title="4.2.3 IP 哈希（IP hash）"></a>4.2.3 IP 哈希（IP hash）</h4><p>轮询和加权轮询，每次访问后端是随机不同的机器，对于一些场景就不太适应。当程序有状态的时候，例如采用了session保存数据，把登录信息保存到了session中，那么跳转到另外一台服务器的时候就需要重新登录。所以这时候需要原IP客户端固定访问同一台服务器。</p><p>ip hash函数将每个请求按访问ip的hash结果分配，同一个IP客户端访问的负载后端服务不变。配置如下：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">upstream</span> upstreamTest &#123;</span><br><span class="line">      ip_hash; </span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.1:9200</span>; </span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.2:9200</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-2-4-fair（第三方）"><a href="#4-2-4-fair（第三方）" class="headerlink" title="4.2.4 fair（第三方）"></a>4.2.4 fair（第三方）</h4><p>对于上面的负载算法没有动态的考虑服务器的性能变化。<code>fair</code>算法根据负载后端服务器的响应时间来动态分配请求，响应时间短优先分配流量。配置参考：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">upstream</span> upstreamTest &#123;</span><br><span class="line">      fair;</span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.1:9200</span>; </span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.2:9200</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-2-5-url-hash（第三方）"><a href="#4-2-5-url-hash（第三方）" class="headerlink" title="4.2.5 url_hash（第三方）"></a>4.2.5 url_hash（第三方）</h4><p>按訪问url的hash结果来分配请求，使每一个url定向到同一个后端服务器。后端服务器为缓存时有效。静态资源缓存,节约存储，加快速度。配置参考：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">upstream</span> upstreamTest &#123;</span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.1:9200</span>; </span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.2:9200</span>;</span><br><span class="line">      <span class="attribute">hash</span> <span class="variable">$request_uri</span>; </span><br><span class="line">      <span class="attribute">hash_method</span> crc32;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中<code>hash_method crc32</code>配置为指定<code>hash</code>算法为<code>crc32</code>。</p><h3 id="4-3-补充"><a href="#4-3-补充" class="headerlink" title="4.3 补充"></a>4.3 补充</h3><p>upstream还能够为每一个设备设置状态值，这些状态值的含义分别例如以下：</p><ul><li><p>down</p><p>后端节点不参与负载；</p></li><li><p>max_fails和fail_timeout </p><p>Nginx基于连接探测，如果发现后端异常，在单位周期为fail_timeout设置的时间，中达到max_fails次数，这个周期次数内，如果后端同一个节点不可用，那么接将把节点标记为不可用，并等待下一个周期（同样时常为fail_timeout）再一次去请求，判断是否连接是否成功。如果成功，将恢复之前的轮询方式，如果不可用将在下一个周期(fail_timeout)再试一次。</p></li><li><p>backup</p><p>backup 不能和ip_hash一起使用，backup 参数是指当所有非备机都宕机或者不可用的情况下，就只能使用带backup标准的备机。</p></li><li><p>max_conns</p><p>允许最大连接数。</p></li><li><p>slow_start</p><p>当节点恢复，不立即加入</p></li></ul><p>例如下面的案例：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">upstream</span> upstreamTest &#123;</span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.1:9200</span> down; </span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.2:9200</span> backup;</span><br><span class="line">      <span class="attribute">server</span> <span class="number">192.168.88.3:9200</span> max_fails=<span class="number">2</span> fail_timeout=<span class="number">60s</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如上配置表明如果后端节点60秒内出现2次不可用情况，判定节点不可用。判定不可用后10秒内请求不会转发到此节点，直到60秒后重新检测节点健康情况。</p><h2 id="第五部分-跨域请求"><a href="#第五部分-跨域请求" class="headerlink" title="第五部分 跨域请求"></a>第五部分 跨域请求</h2><p>前后端分离的项目中由于前后端项目分别部署到不同的服务器上，我们首先遇到的问题就是跨域，在这个场景我们下nginx可以帮助我们很好地解决这个问题</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#跨域请求server</span></span><br><span class="line"><span class="attr">server&#123;</span></span><br><span class="line"><span class="attr">listen</span> <span class="string">9000;</span></span><br><span class="line"><span class="attr">server_name</span> <span class="string">209.250.235.145;</span></span><br><span class="line"><span class="attr">root</span> <span class="string">/app/crossDomain/;</span></span><br><span class="line"><span class="attr">index</span> <span class="string">index.html;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">location</span> <span class="string">/douban/ &#123; #添加访问目录为/apis的代理配置</span></span><br><span class="line"><span class="attr">rewrite</span>   <span class="string">^/douban/(.*)$ /$1 break;</span></span><br><span class="line"><span class="attr">proxy_pass</span>   <span class="string">https://m.douban.com;</span></span><br><span class="line">   <span class="attr">&#125;</span></span><br><span class="line"><span class="attr">&#125;</span></span><br><span class="line"><span class="attr">复制代码</span></span><br></pre></td></tr></table></figure><p>在我的服务器下我写了一个</p><p>index.html请求豆瓣接口，模拟跨域</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">nginxClick</span><span class="params">()</span>&#123;</span></span><br><span class="line">$.ajax(&#123;</span><br><span class="line">ur<span class="variable">l:</span> <span class="string">'/douban/rexxar/api/v2/muzzy/columns/10018/items?start=0&amp;count=3'</span>,</span><br><span class="line">dataType: <span class="string">'json'</span>,</span><br><span class="line"><span class="built_in">type</span>: <span class="string">'get'</span>,</span><br><span class="line">dat<span class="variable">a:</span> <span class="string">""</span>,</span><br><span class="line">succes<span class="variable">s:</span>(<span class="keyword">res</span>)=&gt;&#123;</span><br><span class="line">    console.<span class="built_in">log</span>(<span class="keyword">res</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br><span class="line">复制代码</span><br></pre></td></tr></table></figure><p>当我们访问点击请求时，匹配到location下的/douban/</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rewrite   ^<span class="regexp">/douban/</span>(.*)<span class="variable">$ </span>/<span class="variable">$1</span> <span class="keyword">break</span>;复制代码</span><br></pre></td></tr></table></figure><p>这段配置将请求路径重写为/rexxar/api/v2/muzzy/columns/10018/items?start=0&amp;count=3，其中$1代表正则模糊匹配到的第一个参数，</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proxy_pass   <span class="string">https:</span><span class="comment">//m.douban.com;复制代码</span></span><br></pre></td></tr></table></figure><p>这段配置是将请求域名代理到豆瓣的域名下面，所以从本地服务器发出去的请求将被重新重写为：</p><p><a href="https://m.douban.com/rexxar/api/v2/muzzy/columns/10018/items?start=0&amp;count=3，我们就能拿到豆瓣api提供的数据。详情可以看看这篇[文章](https://www.jianshu.com/p/10ecc107b5ee)" target="_blank" rel="noopener">https://m.douban.com/rexxar/api/v2/muzzy/columns/10018/items?start=0&amp;count=3，我们就能拿到豆瓣api提供的数据。详情可以看看这篇[文章](https://www.jianshu.com/p/10ecc107b5ee)</a></p><p>演示地址：<a href="http://209.250.235.145:9000/" target="_blank" rel="noopener">http://209.250.235.145:9000/</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] Orange官网，链接：<a href="http://orange.sumory.com/" target="_blank" rel="noopener">http://orange.sumory.com/</a></p><p>[2] Orange网关官网docker，链接：<a href="https://hub.docker.com/r/syhily/orange" target="_blank" rel="noopener">https://hub.docker.com/r/syhily/orange</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;HTTP服务器（含动静分离）&lt;/li&gt;
&lt;li&gt;负载均衡&lt;/li&gt;
&lt;li&gt;反向代理&lt;/li&gt;
&lt;li&gt;正向代理&lt;/li&gt;
&lt;l
      
    
    </summary>
    
      <category term="Nginx" scheme="https://zjrongxiang.github.io/categories/Nginx/"/>
    
    
      <category term="Nginx" scheme="https://zjrongxiang.github.io/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>orange网关使用手册</title>
    <link href="https://zjrongxiang.github.io/2021/04/15/2021-04-19-orange%E7%BD%91%E5%85%B3%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"/>
    <id>https://zjrongxiang.github.io/2021/04/15/2021-04-19-orange网关使用手册/</id>
    <published>2021-04-15T13:30:00.000Z</published>
    <updated>2021-06-20T08:36:48.251Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Orange是一个基于OpenResty的API Gateway，提供API及自定义<a href="http://orange.sumory.com/docs/rule.html" target="_blank" rel="noopener">规则</a>的监控和管理，如访问统计、流量切分、API重定向、API鉴权、WEB防火墙等功能。Orange可用来替代前置机中广泛使用的Nginx/OpenResty， 在应用服务上无痛前置一个功能丰富的网关系统。</p><p>流量网关  业务网关；</p><h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><p>内置插件以HTTP Restful形式开放全部API，详细可查看<a href="http://orange.sumory.com/docs/api/" target="_blank" rel="noopener">API文档</a></p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><ul><li>现实中由于用户的业务系统多种多样，对于复杂应用，Orange并不是一个开箱即用的组件，需要调整一些配置才能集成到现有系统中。</li><li>Orange提供的的配置文件和示例都是最简配置，用户使用时请根据具体项目或业务需要自行调整，这些调整可能包括但不限于:<ul><li>使用的各个shared dict的大小， 如ngx.shared.status</li><li>nginx.conf配置文件中各个server、location的配置及其权限控制，比如orange dashboard/API的server应该只对内部有权限的机器开放访问</li><li>根据不同业务而设置的不同nginx配置，如timeout、keepalive、gzip、log、connections等等</li></ul></li></ul><h3 id="Licence"><a href="#Licence" class="headerlink" title="Licence"></a>Licence</h3><p>Orange采用MIT协议开源</p><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><ul><li>Orange的插件模式参考了<a href="https://getkong.org/" target="_blank" rel="noopener">Kong</a>，Kong是一个功能比较全面的API Gateway实现，推荐关注。</li><li>Orange与Kong的不同(刨除基础设计，如存储、API)主要体现在针对”插件”和”API”的组织方式， Orange在流量筛选和变量提取方面相对来说更灵活一些。</li></ul><h2 id="第一部分-Nginx知识准备"><a href="#第一部分-Nginx知识准备" class="headerlink" title="第一部分 Nginx知识准备"></a>第一部分 <code>Nginx</code>知识准备</h2><h3 id="1-1-Nginx配置文件"><a href="#1-1-Nginx配置文件" class="headerlink" title="1.1 Nginx配置文件"></a>1.1 Nginx配置文件</h3><p>nginx 的配置文件结构中 HTTP 配置主要包括三个区块，结构如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Global: nginx 运行相关</span><br><span class="line">Events: 与用户的网络连接相关</span><br><span class="line">http</span><br><span class="line">    http Global: 代理，缓存，日志，以及第三方模块的配置</span><br><span class="line">    server</span><br><span class="line">        server Global: 虚拟主机相关</span><br><span class="line">        location: 地址定向，数据缓存，应答控制，以及第三方模块的配置</span><br></pre></td></tr></table></figure><p>从上面展示的 nginx 结构中可以看出 location 属于请求级别配置，这也是我们最常用的配置。</p><h3 id="1-2-location介绍"><a href="#1-2-location介绍" class="headerlink" title="1.2  location介绍"></a>1.2  location介绍</h3><h4 id="1-2-1-location-语法"><a href="#1-2-1-location-语法" class="headerlink" title="1.2.1 location 语法"></a>1.2.1 location 语法</h4><p>Location 块通过指定模式来与客户端请求的URI相匹配。Location基本语法：</p><ul><li>匹配 URI 类型，有四种参数可选，当然也可以不带参数。</li><li>命名location，用@来标识，类似于定义goto语句块。</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">location [ = | ~ | ~* | ^~ | !~ | !~* ] /uri/&#123; … &#125;</span><br><span class="line">location @/name/ &#123; … &#125;</span><br></pre></td></tr></table></figure><p>各类参数含义：</p><ul><li><code>=</code>表示请求字符串与其精准匹配，成功则立即处理，nginx停止搜索其他匹配；</li><li>~ 表示区分大小写正则匹配；</li><li>~* 表示不区分大小写正则匹配；</li><li>^~ 表示URI以某个常规字符串开头，并要求一旦匹配到就会立即处理，不再去匹配其他的正则 URI，一般用来匹配目录；</li><li>!~ 表示区分大小写正则不匹配；</li><li>!~* 表示不区分大小写正则不匹配；</li><li>/ 通用匹配，任何请求都会匹配到；</li><li><code>@</code> 定义一个命名的 location，@ 定义的locaiton名字一般用在内部定向，例如error_page, try_files命令中。它的功能类似于编程中的goto。</li></ul><h4 id="1-2-2-location匹配顺序"><a href="#1-2-2-location匹配顺序" class="headerlink" title="1.2.2 location匹配顺序"></a>1.2.2 location匹配顺序</h4><p>nginx有两层指令来匹配请求 URI 。第一个层次是 server 指令，它通过域名、ip 和端口来做第一层级匹配，当找到匹配的 server 后就进入此 server 的 location 匹配。</p><p>location 的匹配并不完全按照其在配置文件中出现的顺序来匹配，请求URI 会按如下规则进行匹配：</p><ol><li>先精准匹配 <strong><code>=</code></strong> ，精准匹配成功则会立即停止其他类型匹配；</li><li>没有精准匹配成功时，进行前缀匹配。先查找带有 <strong><code>^~</code></strong> 的前缀匹配，带有 <strong><code>^~</code></strong> 的前缀匹配成功则立即停止其他类型匹配，普通前缀匹配（不带参数 <strong><code>^~</code></strong> ）成功则会暂存，继续查找正则匹配；</li><li><strong><code>=</code></strong> 和 <strong><code>^~</code></strong> 均未匹配成功前提下，查找正则匹配 <strong><code>~</code></strong> 和 <strong><code>~\*</code></strong> 。当同时有多个正则匹配时，按其在配置文件中出现的先后顺序优先匹配，命中则立即停止其他类型匹配；</li><li>所有正则匹配均未成功时，返回步骤 2 中暂存的普通前缀匹配（不带参数 <strong><code>^~</code></strong> ）结果</li></ol><p>以上规则简单总结就是优先级从高到低依次为（<strong>序号越小优先级越高</strong>）：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>. <span class="keyword">location</span> <span class="title">=    # 精准匹配</span></span><br><span class="line"><span class="title">2</span>. <span class="keyword">location</span> <span class="title">^~   # 带参前缀匹配</span></span><br><span class="line"><span class="title">3</span>. <span class="keyword">location</span> <span class="title">~    # 正则匹配（区分大小写）</span></span><br><span class="line"><span class="title">4</span>. <span class="keyword">location</span> <span class="title">~*   # 正则匹配（不区分大小写）</span></span><br><span class="line"><span class="title">5</span>. <span class="keyword">location</span> <span class="title">/a</span>   <span class="comment"># 普通前缀匹配，优先级低于带参数前缀匹配。</span></span><br><span class="line"><span class="number">6</span>. <span class="keyword">location</span> <span class="title">/    # 任何没有匹配成功的，都会匹配这里处理</span></span><br></pre></td></tr></table></figure><h3 id="1-2-网关中流量筛选"><a href="#1-2-网关中流量筛选" class="headerlink" title="1.2 网关中流量筛选"></a>1.2 网关中流量筛选</h3><h4 id="1-2-1-orange中流量选择器"><a href="#1-2-1-orange中流量选择器" class="headerlink" title="1.2.1 orange中流量选择器"></a>1.2.1 orange中流量选择器</h4><p>orange本质是使用web的方式动态配置nginx，就需要能过滤流量。实现方式是：流量选择器，如下图：</p><p><img src="images\picture\orangeUse\nginx_1.png" alt="nginx_1"></p><ul><li><p>名称，定义流量选择器名称；</p></li><li><p>类型，可选参数有：全流量、自定义流量；</p><ul><li>全流量匹配就是对原始流量不过滤。</li><li>自定义流量，需要设置匹配方式与条件，符合条件的请求才会被进行流量管理。</li></ul></li><li><p>规则，自定义流量开启参数。可选参数有：单一条件匹配、and匹配、or匹配、复杂匹配。</p><ul><li><p>单一条件匹配，单个条件，只能配置一个条件；</p></li><li><p>and匹配，多个条件以且的逻辑过滤；</p></li><li><p>or匹配，多个条件以或的逻辑过滤；</p></li><li><p>复杂匹配，即自定义条件之间逻辑关系；</p><p>按照表达式对所有条件求值，表达式不能为空。表达式中每个值的格式为<code>v[index]</code>, 比如v[1]对应的就是第一个条件的值。</p><p>例如我们编写了3个条件，表达式为：(v[1] or v[2]) and v[3]。即前两个条件至少一个为真并且第三个条件为真时，规则为真。3个条件按照顺序分别对应：v[1] 、v[2]、v[3]。</p></li></ul></li><li><p>条件编写，一条完整的调优有三个要素：条件类型、匹配类型、正则表达式。</p><p>条件类型有：</p><ul><li><p>Random，</p></li><li><p>URI</p><p>根据你请求路径中的 uri 来进行匹配，在接入网关的时候，前端几乎不用做任何更改。</p><p>在选择器中，推荐使用 uri 中的前缀来进行匹配，而在规则中，则使用具体路径来进行匹配。</p></li><li><p>Header，K/V类型</p><p>根据 <code>http</code> 请求头中的字段值来匹配。这个类型的name非空。</p></li><li><p>Query，K/V类型</p><p>根据 <code>uri</code> 中的查询参数来进行匹配，比如 <code>/test?a=1&amp;b=2</code> ，那么可以选择该匹配方式。</p></li><li><p>Cookie</p><p>Cookie是用于维持服务端会话状态的，通常由服务端写入，在后续请求中，供服务端读取。</p></li><li><p>Postparams，K/V类型</p></li><li><p>IP</p><p>根据 http 调用方的 ip 来进行匹配。尤其是在 waf 插件里面，如果发现一个 ip 地址有攻击，可以新增一条匹配条件，填上该 ip ，拒绝该 ip 的访问。</p></li><li><p>UserAgent</p><p>User-Agent会告诉网站服务器，访问者是通过什么工具来请求的。包含了一个特征字符串，用来让网络协议的对端来识别发起请求的用户代理软件的应用类型、操作系统、软件开发商以及版本号。例如火狐浏览器发起的请求，User-Agent字段为：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0</span><br></pre></td></tr></table></figure></li><li><p>Host</p><p>根据 http 调用方的 host 来进行匹配。尤其是在 waf 插件里面，如果发现一个 host 地址有攻击，可以新增一条匹配条件，填上该 host ，拒绝该 host 的访问。</p></li><li><p>Referer</p><p>HTTP 协议在请求（request）的头信息里面，设计了一个<code>Referer</code>字段，给出”引荐网页”的 URL。这个字段是可选的。客户端发送请求的时候，自主决定是否加上该字段。</p></li><li><p>HttpMethod</p><p>HTTP 请求可以使用多种请求方法。HTTP1.0 定义了三种请求方法： GET, POST 和 HEAD方法。HTTP1.1 新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT 方法。一共有8种请求方法。</p></li></ul><p>匹配的类型有：</p><ul><li>Match，正则匹配</li><li>Not Match，正则不匹配</li><li><code>=</code>，精确相等</li><li><code>!=</code>，精确不相等</li><li><code>&gt;</code></li><li><code>&gt;=</code></li><li><code>&lt;</code></li><li><code>&lt;=</code></li><li><code>%</code></li></ul><p>表达式，这里填写正则表达式或者匹配值。</p></li><li><p>处理，两个参数：继续后续选择器、略过后续选择器；</p><ul><li>继续后续选择器，流量继续被其他选择器过滤。</li><li>略过后续选择器，流量终止后面的选择器过滤。</li></ul></li><li><p>是否开启，是否生效选择器。</p></li><li><p>时候记录日志，过滤结果是否记录日志。</p></li></ul><p>在多个插件里（比如URL重定向插件、WAF插件、自定义监控插件）都使用了选择器来将流量进行第一步划分， 在流量被一个选择器命中后才会进入它内部的规则过滤。</p><p>使用选择器的目的是减少不必要的规则判断，进一步提高性能。</p><h4 id="1-2-2-orange中的流量规则"><a href="#1-2-2-orange中的流量规则" class="headerlink" title="1.2.2 orange中的流量规则"></a>1.2.2 orange中的流量规则</h4><p>流量规则必须归属一个流量选择器，用来进一步过滤被流量选择器过滤后的流量。</p><p>一个流量选择器，可以有多个所属规则器。</p><p>下图是一个规则器，不同的插件的规则器不相同，我们后续在插件中分别介绍。</p><h3 id="1-3-变量提取"><a href="#1-3-变量提取" class="headerlink" title="1.3 变量提取"></a>1.3 变量提取</h3><p>变量提取模块是很多orange插件都使用到的一个概念， 它主要用来从请求中提取出各种信息， 比如query string, form表单里的字段， 某个header头等等。</p><p>它支持两种提取方式：</p><ul><li>索引式提取</li><li>模板式提取</li></ul><h4 id="1-3-1-索引式提取"><a href="#1-3-1-索引式提取" class="headerlink" title="1.3.1 索引式提取"></a>1.3.1 索引式提取</h4><p>顾名思义， 索引式提取的含义是将提取出的所有变量按照顺序保存在一个数组中， 后续以下标的方式来使用。</p><p>比如我们在”变量提取模块”提取了三个值：</p><p><img src="http://orange.sumory.com/images/usages/extractor_1.png" alt="img"></p><p>那么之后就可以通过\${1}、​\${2}、​\${3}来使用， 其中</p><ul><li>${1}指的是header头里的app_version字段， 如果header没有此字段， 则赋一个默认值<code>v0.1</code></li><li>${2}指的是query中的uid字段</li><li>${3}指的是query中age字段， 若无则给予默认值18</li></ul><h4 id="1-3-2-模板式提取"><a href="#1-3-2-模板式提取" class="headerlink" title="1.3.2 模板式提取"></a>1.3.2 模板式提取</h4><p>模板时提取主要为了解决索引式提取必须要按序使用的问题， 并且当需要从uri中提取多个值时索引式提取方式并不友好。</p><p>如以下示例， 我们提取了四个值：</p><p><img src="http://orange.sumory.com/images/usages/extractor_2.png" alt="img"></p><p>则之后我们可以通过以下方式来使用：</p><ul><li><p>指的是从query中提取出的uid字段</p></li><li><p>指的是从query中提取出的age字段, 若无则给予默认值18</p></li><li>指的是从格式为<br><br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">^<span class="regexp">/start/</span>(.*)<span class="regexp">/(.*)/</span>end</span><br></pre></td></tr></table></figure><p>的URI中提取出的第1个分段值</p><ul><li>比如， 如果URI为/start/abc/123/end, 则此时值为abc</li><li>如果URI为/start/momo/sharp/end, 则此时值为momo</li></ul></li><li>指的是从格式为<br><br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">^<span class="regexp">/start/</span>(.*)<span class="regexp">/(.*)/</span>end</span><br></pre></td></tr></table></figure><p>的URI中提取出的第2个分段值</p><ul><li>比如， 若URI为/start/abc/123/end, 则此时值为123</li><li>如果URI为/start/momo/sharp/end, 则此时值为sharp</li></ul></li></ul><p>注意， 若从URI中提取， 仍然要根据顺序来使用， 如、、.</p><p>设计原理:<a href="https://github.com/orlabs/orange/issues/15" target="_blank" rel="noopener">https://github.com/orlabs/orange/issues/15</a></p><h2 id="第二部分-核心组件介绍"><a href="#第二部分-核心组件介绍" class="headerlink" title="第二部分 核心组件介绍"></a>第二部分 核心组件介绍</h2><p>最新稳定版本<code>0.8.1</code>，该版本对第三方组件进行去除。这里核心组件主要是<code>0.6.4</code>版本中组件。</p><h3 id="2-1-全局统计"><a href="#2-1-全局统计" class="headerlink" title="2.1 全局统计"></a>2.1 全局统计</h3><p>可统计API访问情况、Nginx连接情况、流量统计、QPS、应用版本、服务器信息等。如下图：</p><p><img src="images\picture\orangeUse\status.png" alt="status"></p><h3 id="2-2-自定义监控"><a href="#2-2-自定义监控" class="headerlink" title="2.2 自定义监控"></a>2.2 自定义监控</h3><p>可根据配置的规则筛选出流量并监控，统计其各个指标。当前的监控指标有：</p><ul><li>请求总次数： 分别统计200/300/400/500区间的请求数</li><li>QPS</li><li>请求总耗时/平均请求耗时</li><li>总流量/平均请求流量</li></ul><p>案例：</p><p>例如筛选出指定流量，进行监控。下面是监控视图：</p><p><img src="images\picture\orangeUse\monitor.png" alt="monitor"></p><h3 id="2-3-URL重定向（redirect）"><a href="#2-3-URL重定向（redirect）" class="headerlink" title="2.3 URL重定向（redirect）"></a>2.3 URL重定向（redirect）</h3><p>网关实现的重定向主要是：当客户端向网关请求URL资源的时候，网关通知客户端实际的资源地址，然后客户端向实际的URL请求资源。重定向是指当浏览器请求一个URL时，服务器返回一个重定向指令，告诉浏览器地址已经变了，麻烦使用新的URL再重新发送新请求。</p><p>网关实现了通过UI配置各种rewrite策略，省去手写nginx rewrite和重启。redirect是浏览器和服务器发生两次请求，也就是服务器命令客户端“去访问某个页面”。</p><h4 id="2-3-1-案例"><a href="#2-3-1-案例" class="headerlink" title="2.3.1 案例"></a>2.3.1 案例</h4><p>我们使用重定向来代理网关的官网(<a href="http://orange.sumory.com/)。" target="_blank" rel="noopener">http://orange.sumory.com/)。</a></p><p>首先，添加选择器：</p><p><img src="images\picture\orangeUse\URL重定向.PNG" alt="URL重定向"></p><p>然后在选择器中创建新的规则：</p><p><img src="images\picture\orangeUse\url重定向rule.png" alt="url重定向rule"></p><p>配置完成后，当我们访问<code>192.168.52.137:8888/to_orange</code>时候，会自动跳转为：<code>http://orange.sumory.com/</code>。</p><h4 id="2-3-2-参数说明"><a href="#2-3-2-参数说明" class="headerlink" title="2.3.2 参数说明"></a>2.3.2 参数说明</h4><p>重定向有两种：一种是302响应，称为临时重定向，一种是301响应，称为永久重定向。两者的区别是，如果服务器发送301永久重定向响应，浏览器会缓存<code>/hi</code>到<code>/hello</code>这个重定向的关联，下次请求<code>/hi</code>的时候，浏览器就直接发送<code>/hello</code>请求了。</p><h3 id="2-4-URI-重写-Rewrite"><a href="#2-4-URI-重写-Rewrite" class="headerlink" title="2.4 URI 重写(Rewrite)"></a>2.4 URI 重写(Rewrite)</h3><p>Url重写主要用于站内请求的重写。rewrite则是服务器内部的一个接管，在服务器内部告诉“某个页面请帮我处理这个用户的请求”，浏览器和服务器只发生一次交互，浏览器不知道是该页面做的响应，浏览器只是向服务器发出一个请求。</p><p>URL重写用于将页面映射到本站另一页面，若重写到另一网络主机（域名），则按重定向处理。</p><p>rewrite是把一个地址重写成另一个地址。地址栏不跳转。相当于给另一个地址加了一个别名一样。</p><h4 id="2-4-1-案例"><a href="#2-4-1-案例" class="headerlink" title="2.4.1 案例"></a>2.4.1 案例</h4><p>我们使用重写（rewrite）来重写上面案例中地址。</p><p>首先添加选择器：</p><p><img src="D:\myblog\source\_posts\images\picture\orangeUse\rewrite.png" alt="rewrite"></p><p>然后在选择器中创建新的规则：</p><p><img src="images\picture\orangeUse\rewirte_reg.png" alt="rewirte_reg"></p><p>配置完成后，当我们访问<code>http://192.168.52.137:8888/to_orange_test</code>时候，流量被映射到本站另一个地址<code>http://192.168.52.137:8888/to_orange</code>。而后面地址被重定向到<code>http://orange.sumory.com/</code>。</p><h4 id="2-4-2-参数说明"><a href="#2-4-2-参数说明" class="headerlink" title="2.4.2 参数说明"></a>2.4.2 参数说明</h4><h3 id="2-5-HTTP-Basic-Authorization"><a href="#2-5-HTTP-Basic-Authorization" class="headerlink" title="2.5 HTTP Basic Authorization"></a>2.5 HTTP Basic Authorization</h3><p><code>basic auth</code>是最简单权限认证方式，密钥被<code>base64</code>加密，但是网络传输是非加密的，一旦被截取，解密是容易的。所以通常用于安全的内部网络。</p><h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><h4 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h4><h3 id="2-6-HTTP-Key-Auth"><a href="#2-6-HTTP-Key-Auth" class="headerlink" title="2.6 HTTP Key Auth"></a>2.6 HTTP Key Auth</h3><h4 id="案例-1"><a href="#案例-1" class="headerlink" title="案例"></a>案例</h4><h4 id="参数说明-1"><a href="#参数说明-1" class="headerlink" title="参数说明"></a>参数说明</h4><h3 id="2-7-Signature-Auth"><a href="#2-7-Signature-Auth" class="headerlink" title="2.7 Signature Auth"></a>2.7 Signature Auth</h3><h4 id="案例-2"><a href="#案例-2" class="headerlink" title="案例"></a>案例</h4><h4 id="参数说明-2"><a href="#参数说明-2" class="headerlink" title="参数说明"></a>参数说明</h4><p><a href="https://www.cnblogs.com/Sinte-Beuve/p/12093307.html" target="_blank" rel="noopener">https://www.cnblogs.com/Sinte-Beuve/p/12093307.html</a></p><h3 id="2-8-Rate-Limiting-访问限速"><a href="#2-8-Rate-Limiting-访问限速" class="headerlink" title="2.8 Rate Limiting 访问限速"></a>2.8 Rate Limiting 访问限速</h3><h4 id="案例-3"><a href="#案例-3" class="headerlink" title="案例"></a>案例</h4><h4 id="参数说明-3"><a href="#参数说明-3" class="headerlink" title="参数说明"></a>参数说明</h4><h3 id="2-9-Rate-Limiting-防刷"><a href="#2-9-Rate-Limiting-防刷" class="headerlink" title="2.9 Rate Limiting 防刷"></a>2.9 Rate Limiting 防刷</h3><h4 id="案例-4"><a href="#案例-4" class="headerlink" title="案例"></a>案例</h4><h4 id="参数说明-4"><a href="#参数说明-4" class="headerlink" title="参数说明"></a>参数说明</h4><h3 id="2-10-WAF-防火墙"><a href="#2-10-WAF-防火墙" class="headerlink" title="2.10 WAF 防火墙"></a>2.10 WAF 防火墙</h3><h4 id="案例-5"><a href="#案例-5" class="headerlink" title="案例"></a>案例</h4><h4 id="参数说明-5"><a href="#参数说明-5" class="headerlink" title="参数说明"></a>参数说明</h4><h3 id="2-11-代理-amp-分流-amp-ABTesting"><a href="#2-11-代理-amp-分流-amp-ABTesting" class="headerlink" title="2.11 代理 &amp; 分流 &amp; ABTesting"></a>2.11 代理 &amp; 分流 &amp; ABTesting</h3><p>当前divide分流插件是静态的，需要提前在nginx.conf里配置upstream，但是这样不利于灵活管理，能否实现动态配置upstream。</p><p>分流插件，可分为三个使用场景：</p><ul><li>作为proxy，如代理后端的多个HTTP应用</li><li>用于AB测试</li><li>用于动态分流，API版本控制等</li></ul><p><a href="https://book.aikaiyuan.com/openresty/orange-divide.html#%E8%AF%95%E9%AA%8C%E7%8E%AF%E5%A2%83" target="_blank" rel="noopener">https://book.aikaiyuan.com/openresty/orange-divide.html#%E8%AF%95%E9%AA%8C%E7%8E%AF%E5%A2%83</a></p><p><a href="http://bbs.orchina.org/topic/160/view" target="_blank" rel="noopener">http://bbs.orchina.org/topic/160/view</a></p><h4 id="案例-6"><a href="#案例-6" class="headerlink" title="案例"></a>案例</h4><p>我们使用该插件代理elasticsearch集群节点。</p><p>我们对es进行负载配置</p><p><a href="http://www.ttlsa.com/nginx/nginx-elasticsearch/" target="_blank" rel="noopener">http://www.ttlsa.com/nginx/nginx-elasticsearch/</a></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">upstream es_upstream &#123;</span><br><span class="line">       <span class="built_in"> server </span>192.168.31.3:9200;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p><a href="https://github.com/orlabs/orange/issues/136" target="_blank" rel="noopener">https://github.com/orlabs/orange/issues/136</a></p><h3 id="2-12-KV-Store"><a href="#2-12-KV-Store" class="headerlink" title="2.12 KV Store"></a>2.12 KV Store</h3><h2 id="第三部分-第三方插件"><a href="#第三部分-第三方插件" class="headerlink" title="第三部分 第三方插件"></a>第三部分 第三方插件</h2><p><a href="https://zhjwpku.com/2017/11/14/orange-balancer-plugin-tutorial.html" target="_blank" rel="noopener">https://zhjwpku.com/2017/11/14/orange-balancer-plugin-tutorial.html</a></p><ul><li>The <code>balancer</code> plugin migrated to<code>v0.9.0-dev</code> due to conflicts with existing features.</li><li>The <code>dynamic_upstream</code> plugin migrated to<code>v0.9.0-dev</code> due to conflicts with existing features.</li><li>The <code>consul_balancer</code> plugin migrated to<code>v0.9.0-dev</code> due to conflict with existing functions.</li><li>The <code>persist</code> plugin migrated to<code>v0.9.0-dev</code> due to conflicts with existing features.</li></ul><h3 id="3-1-node插件"><a href="#3-1-node插件" class="headerlink" title="3.1 node插件"></a>3.1 node插件</h3><p>该插件主要用户网关集群管理。</p><p>node plugin（容器集群节点管理插件）</p><ul><li>新增集群节点注册命令 <code>orange register</code></li><li>通过 dashboard 面板同步节点配置信息</li><li>配合 persist 插件，可以查看历史统计信息</li></ul><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">influxdb:/usr/local/orange/conf # opm install ledgetech/lua-resty-http</span><br><span class="line"></span><br><span class="line">* Fetching ledgetech/lua-resty-http  </span><br><span class="line">  Downloading https:<span class="comment">//opm.openresty.org/api/pkg/tarball/ledgetech/lua-resty-http-0.14.opm.tar.gz</span></span><br><span class="line">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">  <span class="number">100</span> <span class="number">20862</span>  <span class="number">100</span> <span class="number">20862</span>    <span class="number">0</span>     <span class="number">0</span>   <span class="number">7545</span>      <span class="number">0</span>  <span class="number">0</span>:<span class="number">00</span>:<span class="number">02</span>  <span class="number">0</span>:<span class="number">00</span>:<span class="number">02</span> --:--:--  <span class="number">7545</span></span><br><span class="line">  Package ledgetech/lua-resty-http <span class="number">0.14</span> installed successfully under /usr/local/openresty/site/ .</span><br></pre></td></tr></table></figure><p><a href="https://github.com/orlabs/orange/issues/353" target="_blank" rel="noopener">https://github.com/orlabs/orange/issues/353</a></p><h3 id="3-2-headers-插件"><a href="#3-2-headers-插件" class="headerlink" title="3.2 headers 插件"></a>3.2 headers 插件</h3><p>用于修改请求头。</p><h3 id="3-3-balancer插件"><a href="#3-3-balancer插件" class="headerlink" title="3.3 balancer插件"></a>3.3 balancer插件</h3><p>用户负载多个upstream</p><p><a href="https://zhjwpku.com/2017/11/14/orange-balancer-plugin-tutorial.html" target="_blank" rel="noopener">https://zhjwpku.com/2017/11/14/orange-balancer-plugin-tutorial.html</a></p><h3 id="3-4-Consul-Upstream"><a href="#3-4-Consul-Upstream" class="headerlink" title="3.4 Consul Upstream"></a>3.4 Consul Upstream</h3><h3 id="3-4-Dynamic-Upstream"><a href="#3-4-Dynamic-Upstream" class="headerlink" title="3.4 Dynamic Upstream"></a>3.4 Dynamic Upstream</h3><h4 id="案例-7"><a href="#案例-7" class="headerlink" title="案例"></a>案例</h4><h4 id="参数说明-6"><a href="#参数说明-6" class="headerlink" title="参数说明"></a>参数说明</h4><h3 id="3-6-HTTP-Jwt-Auth"><a href="#3-6-HTTP-Jwt-Auth" class="headerlink" title="3.6 HTTP Jwt Auth"></a>3.6 HTTP Jwt Auth</h3><h3 id="3-7-HTTP-Hmac-Auth"><a href="#3-7-HTTP-Hmac-Auth" class="headerlink" title="3.7 HTTP Hmac Auth"></a>3.7 HTTP Hmac Auth</h3><h3 id="3-8-持久日志"><a href="#3-8-持久日志" class="headerlink" title="3.8  持久日志"></a>3.8  持久日志</h3><h2 id="第四部分-插件的优先级"><a href="#第四部分-插件的优先级" class="headerlink" title="第四部分 插件的优先级"></a>第四部分 插件的优先级</h2><p>orange中所有插件都是继承基本组件的，文件<code>plugins\base_handler.lua</code>中定义如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">local</span> Object = <span class="built_in">require</span>(<span class="string">"orange.lib.classic"</span>)</span><br><span class="line"><span class="keyword">local</span> BasePlugin = Object:extend()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">BasePlugin:new</span><span class="params">(name)</span></span></span><br><span class="line">    self._name = name</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">BasePlugin:get_name</span><span class="params">()</span></span></span><br><span class="line">    <span class="keyword">return</span> self._name</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">BasePlugin:init_worker</span><span class="params">()</span></span></span><br><span class="line">    ngx.<span class="built_in">log</span>(ngx.DEBUG, <span class="string">" executing plugin \""</span>, self._name, <span class="string">"\": init_worker"</span>)</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">BasePlugin:redirect</span><span class="params">()</span></span></span><br><span class="line">    ngx.<span class="built_in">log</span>(ngx.DEBUG, <span class="string">" executing plugin \""</span>, self._name, <span class="string">"\": redirect"</span>)</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">BasePlugin:rewrite</span><span class="params">()</span></span></span><br><span class="line">    ngx.<span class="built_in">log</span>(ngx.DEBUG, <span class="string">" executing plugin \""</span>, self._name, <span class="string">"\": rewrite"</span>)</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">BasePlugin:access</span><span class="params">()</span></span></span><br><span class="line">    ngx.<span class="built_in">log</span>(ngx.DEBUG, <span class="string">" executing plugin \""</span>, self._name, <span class="string">"\": access"</span>)</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">BasePlugin:balancer</span><span class="params">()</span></span></span><br><span class="line">    ngx.<span class="built_in">log</span>(ngx.DEBUG, <span class="string">" executing plugin \""</span>, self._name, <span class="string">"\": balancer"</span>)</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">BasePlugin:header_filter</span><span class="params">()</span></span></span><br><span class="line">    ngx.<span class="built_in">log</span>(ngx.DEBUG, <span class="string">" executing plugin \""</span>, self._name, <span class="string">"\": header_filter"</span>)</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">BasePlugin:body_filter</span><span class="params">()</span></span></span><br><span class="line">    ngx.<span class="built_in">log</span>(ngx.DEBUG, <span class="string">" executing plugin \""</span>, self._name, <span class="string">"\": body_filter"</span>)</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">BasePlugin:log</span><span class="params">()</span></span></span><br><span class="line">    ngx.<span class="built_in">log</span>(ngx.DEBUG, <span class="string">" executing plugin \""</span>, self._name, <span class="string">"\": log"</span>)</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> BasePlugin</span><br></pre></td></tr></table></figure><h2 id="第五部分-Dashboard-用户管理"><a href="#第五部分-Dashboard-用户管理" class="headerlink" title="第五部分 Dashboard 用户管理"></a>第五部分 Dashboard 用户管理</h2><p>从v0.1.1版本开始，Orange Dashboard支持授权验证（默认未开启）。当配置开启后，只有通过成功登录的账户才能登陆展现Dashboard。</p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>这部分功能在配置文件<code>conf/orange.conf</code>中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">"dashboard": &#123;</span><br><span class="line">   "auth": false,//是否开启用户鉴权，默认为false</span><br><span class="line">   "session_secret": "y0ji4pdj61aaf3f11c2e65cd2263d3e7e5",//用于加密cookie的盐</span><br><span class="line">   "whitelist": [</span><br><span class="line">         "^/auth/login$",</span><br><span class="line">         "^/error/$"</span><br><span class="line">     ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Dashboard 用户的用户信息存储在Mysql的<code>dashboard_user</code>表中。</p><p>默认系统管理员用户名和密钥如下，首次登陆后可以修改和添加其他用户：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">用户名：admin</span><br><span class="line">密码：orange_admin</span><br></pre></td></tr></table></figure><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] Orange官网，链接：<a href="http://orange.sumory.com/" target="_blank" rel="noopener">http://orange.sumory.com/</a></p><p>[2] Orange网关官网docker，链接：<a href="https://hub.docker.com/r/syhily/orange" target="_blank" rel="noopener">https://hub.docker.com/r/syhily/orange</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;Orange是一个基于OpenResty的API Gateway，提供API及自定义&lt;a href=&quot;http://orange.sumor
      
    
    </summary>
    
      <category term="orange" scheme="https://zjrongxiang.github.io/categories/orange/"/>
    
    
      <category term="orange" scheme="https://zjrongxiang.github.io/tags/orange/"/>
    
  </entry>
  
  <entry>
    <title>sparkSql使用总结</title>
    <link href="https://zjrongxiang.github.io/2021/04/15/2021-06-06-sparkSql%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/04/15/2021-06-06-sparkSql使用总结/</id>
    <published>2021-04-15T13:30:00.000Z</published>
    <updated>2021-06-06T00:54:58.880Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://blog.csdn.net/weixin_40035337/article/details/108018058" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40035337/article/details/108018058</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] Orange官网，链接：<a href="http://orange.sumory.com/" target="_blank" rel="noopener">http://orange.sumory.com/</a></p><p>[2] Orange网关官网docker，链接：<a href="https://hub.docker.com/r/syhily/orange" target="_blank" rel="noopener">https://hub.docker.com/r/syhily/orange</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_40035337/article/details/1080180
      
    
    </summary>
    
      <category term="orange" scheme="https://zjrongxiang.github.io/categories/orange/"/>
    
    
      <category term="orange" scheme="https://zjrongxiang.github.io/tags/orange/"/>
    
  </entry>
  
  <entry>
    <title>orange网关原理的源码分析</title>
    <link href="https://zjrongxiang.github.io/2021/04/15/2021-05-30-orange%E7%BD%91%E5%85%B3%E7%94%9F%E4%BA%A7%E7%BB%B4%E6%8A%A4%E6%89%8B%E5%86%8C/"/>
    <id>https://zjrongxiang.github.io/2021/04/15/2021-05-30-orange网关生产维护手册/</id>
    <published>2021-04-15T13:30:00.000Z</published>
    <updated>2021-06-14T17:22:35.449Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Orange v0.6.0版本是一个重构版本， 着重为了解决之前版本在有大量规则配置时性能损耗的问题。 基本的设计思路是将原来的规则细分成两层， 第一层叫做<a href="http://orange.sumory.com/docs/concept/selector/" target="_blank" rel="noopener">selector</a>, 用于将流量进行第一步划分， 在进入某个selector后才按照之前的设计进行<a href="http://orange.sumory.com/docs/concept/rule/" target="_blank" rel="noopener">规则</a>匹配， 匹配到后进行相关处理。</p><p><a href="https://lengrongfu.github.io/2019/05/21/orange-%E5%8E%9F%E7%90%86/" target="_blank" rel="noopener">https://lengrongfu.github.io/2019/05/21/orange-%E5%8E%9F%E7%90%86/</a></p><p><a href="https://book.aikaiyuan.com/openresty/understanding-orange.html" target="_blank" rel="noopener">https://book.aikaiyuan.com/openresty/understanding-orange.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/67481992" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/67481992</a></p><h3 id="生产单机部署"><a href="#生产单机部署" class="headerlink" title="生产单机部署"></a>生产单机部署</h3><h3 id="生产集群部署"><a href="#生产集群部署" class="headerlink" title="生产集群部署"></a>生产集群部署</h3><h3 id="配置更新"><a href="#配置更新" class="headerlink" title="配置更新"></a>配置更新</h3><h3 id="日志切割"><a href="#日志切割" class="headerlink" title="日志切割"></a>日志切割</h3><p><a href="https://jingsam.github.io/2019/01/15/nginx-access-log.html" target="_blank" rel="noopener">https://jingsam.github.io/2019/01/15/nginx-access-log.html</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] Orange官网，链接：<a href="http://orange.sumory.com/" target="_blank" rel="noopener">http://orange.sumory.com/</a></p><p>[2] Orange网关官网docker，链接：<a href="https://hub.docker.com/r/syhily/orange" target="_blank" rel="noopener">https://hub.docker.com/r/syhily/orange</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;Orange v0.6.0版本是一个重构版本， 着重为了解决之前版本在有大量规则配置时性能损耗的问题。 基本的设计思路是将原来的规则细分成两
      
    
    </summary>
    
      <category term="orange" scheme="https://zjrongxiang.github.io/categories/orange/"/>
    
    
      <category term="orange" scheme="https://zjrongxiang.github.io/tags/orange/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch中Mapping总结</title>
    <link href="https://zjrongxiang.github.io/2021/04/12/2021-04-12-Elasticsearch%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Elasticsearch%E4%B8%ADMapping%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/04/12/2021-04-12-Elasticsearch系列文章-Elasticsearch中Mapping总结/</id>
    <published>2021-04-12T14:30:00.000Z</published>
    <updated>2021-05-01T14:27:13.480Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   版本升级指引</li><li>第二部分   升级方法和具体步骤</li><li>总结</li><li>参考文献及资料 </li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="第一部分-Mapping"><a href="#第一部分-Mapping" class="headerlink" title="第一部分 Mapping"></a>第一部分 Mapping</h2><p>Mapping在Elasticsearch中作用类似于传统数据库中的表结构定义（schema），即Index的元数据信息。主要作用有：</p><ul><li>定义索引（Index）中字段名称；</li><li>定义字段的数据类型；</li><li>定义倒排索引的配置；</li></ul><p>在<code>Elasticsearch 7.x</code>版本前，单个index中支持多个type。每个index都会对应有自己的mapping。这时候index可以类比关系型数据库中库，而不同type类比库中表。</p><p>在索引中定义太多字段可能会导致索引膨胀，出现内存不足和难以恢复的情况，下面有几个设置：</p><ul><li>index.mapping.total_fields.limit：一个索引中能定义的字段的最大数量，默认是 1000</li><li>index.mapping.depth.limit：字段的最大深度，以内部对象的数量来计算，默认是20</li><li>index.mapping.nested_fields.limit：索引中嵌套字段的最大数量，默认是50</li></ul><h3 id="1-1-Elasticsearch中数据类型"><a href="#1-1-Elasticsearch中数据类型" class="headerlink" title="1.1 Elasticsearch中数据类型"></a>1.1 Elasticsearch中数据类型</h3><h4 id="1-1-1-基本类型"><a href="#1-1-1-基本类型" class="headerlink" title="1.1.1 基本类型"></a>1.1.1 基本类型</h4><ul><li><p>text类型</p><p>该类型的字段将经过分词器分词，用于全文检索；</p></li><li><p>keyword类型</p><p>不经过分词器，用于精确检索（只能检索该字段的完整值），用于过滤（filtering）</p></li><li><p>数值类型</p><p>和编程语言相同，主要有：</p><p>long：有符号64-bit integer：-2^63 ~ 2^63 - 1</p><p>integer：有符号32-bit integer，-2^31 ~ 2^31 - 1</p><p>short：有符号16-bit integer，-32768 ~ 32767</p><p>byte： 有符号8-bit integer，-128 ~ 127</p><p>double：64-bit IEEE 754 浮点数</p><p>float：32-bit IEEE 754 浮点数</p><p>half_float：16-bit IEEE 754 浮点数</p><p>scaled_float</p></li><li><p>布尔类型</p><p>逻辑型，即true/false</p></li><li><p>日期类型</p><p>由于Json没有date类型，所以es通过识别字符串是否符合format定义的格式来判断是否为date类型。ormat默认为：<code>strict_date_optional_time||epoch_millis</code></p><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html</a></p></li><li><p>二进制类型</p><p>该类型的字段把值当做经过 base64 编码的字符串，默认不存储，且不可搜索</p></li><li><p>范围类型</p><p>范围类型表示值是一个范围，而不是一个具体的值</p><p>譬如 age 的类型是 integer_range，那么值可以是  {“gte” : 10, “lte” : 20}；搜索 “term” : {“age”: 15} 可以搜索该值；搜索 “range”: {“age”: {“gte”:11, “lte”: 15}} 也可以搜索到</p><p>range参数 relation 设置匹配模式</p><ul><li>INTERSECTS ：默认的匹配模式，只要搜索值与字段值有交集即可匹配到</li><li>WITHIN：字段值需要完全包含在搜索值之内，也就是字段值是搜索值的子集才能匹配</li><li>CONTAINS：与WITHIN相反，只搜索字段值包含搜索值的文档</li></ul><p>integer_range</p><p>float_range</p><p>long_range</p><p>double_range</p><p>date_range：64-bit 无符号整数，时间戳（单位：毫秒）</p><p>ip_range：IPV4 或 IPV6 格式的字符串</p></li></ul><h4 id="1-1-2-复杂类型"><a href="#1-1-2-复杂类型" class="headerlink" title="1.1.2 复杂类型"></a>1.1.2 复杂类型</h4><ul><li><p>数组类型</p><p>字符串数组 [ “one”, “two” ]</p><p>整数数组 [ 1, 2 ]</p><p>数组的数组  [ 1, [ 2, 3 ]]，相当于 [ 1, 2, 3 ]</p><p>Object对象数组 [ { “name”: “Mary”, “age”: 12 }, { “name”: “John”, “age”: 10 }]</p><p>同一个数组只能存同类型的数据，不能混存，譬如 [ 10, “some string” ] 是错误的</p><p>数组中的 null 值将被 null_value 属性设置的值代替或者被忽略</p><p>空数组 [] 被当做 missing field 处理</p></li><li><p>对象类型</p><p>对象类型可能有内部对象</p><p>被索引的形式为：manager.name.first</p></li><li><p>嵌套类型</p></li><li><p>地理位置数据类型</p></li><li><p>专用数据类型</p><ul><li>记录IP地址 ip</li><li>实现自动补全 completion</li><li>记录分词数 token_count</li><li>记录字符串hash值 murmur3</li><li>Percolator</li></ul></li></ul><h3 id="1-2-多字段特性（multi-fields）"><a href="#1-2-多字段特性（multi-fields）" class="headerlink" title="1.2 多字段特性（multi-fields）"></a>1.2 多字段特性（multi-fields）</h3><ul><li>允许对同一个字段采用不同的配置，比如分词，常见例子如对人名实现拼音搜索，只需要在人名中新增一个<strong>子字段</strong>为 pinyin 即可</li><li>通过参数 fields 设置</li></ul><h3 id="1-3-不可修改性"><a href="#1-3-不可修改性" class="headerlink" title="1.3 不可修改性"></a>1.3 不可修改性</h3><p>Mapping中的字段类型一旦设定后，禁止直接修改，原因是Lucence实现的倒排索引生成后不允许修改，除非重新建立新的索引，然后做reindex操作。但是允许新增字段。通过dynamic参数来控制字段的新增：</p><ul><li>true(默认）允许自动新增字段</li><li>false不允许自动新增字段，但是文档可以正常写入，但无法对字段进行查询等操作</li><li>strict 文档不能写入，报错</li></ul><h2 id="第二部分-设置Mapping"><a href="#第二部分-设置Mapping" class="headerlink" title="第二部分 设置Mapping"></a>第二部分 设置Mapping</h2><p>在创建一个索引的时候，可以对 <code>dynamic</code> 进行设置，可以设成 <code>false</code>、<code>true</code> 或者 <code>strict</code>。</p><h2 id="第三部分-Dynamic-Mapping"><a href="#第三部分-Dynamic-Mapping" class="headerlink" title="第三部分 Dynamic Mapping"></a>第三部分 Dynamic Mapping</h2><p>Dynamic Mapping 机制使我们不需要手动定义 Mapping，ES 会<strong>自动根据文档信息来判断字段合适的类型</strong>，但是有时候也会推算的不对，比如地理位置信息有可能会判断为 <code>Text</code>，当类型如果设置不对时，会导致一些功能无法正常工作，比如 Range 查询。</p><h3 id="3-1-类型自动推断"><a href="#3-1-类型自动推断" class="headerlink" title="3.1 类型自动推断"></a>3.1 类型自动推断</h3><h3 id="3-2-更新mapping"><a href="#3-2-更新mapping" class="headerlink" title="3.2 更新mapping"></a>3.2 更新mapping</h3><p>如果是新增加的字段，根据 Dynamic 的设置分为以下三种状况：</p><ul><li>当 Dynamic 设置为 <code>true</code> 时，一旦有新增字段的文档写入，Mapping 也同时被更新。</li><li>当 Dynamic 设置为 <code>false</code> 时，索引的 Mapping 是不会被更新的，新增字段的数据无法被索引，也就是无法被搜索，但是信息会出现在 <code>_source</code> 中。</li><li>当 Dynamic 设置为 <code>strict</code> 时，文档写入会失败。</li></ul><p>另外一种是字段已经存在，这种情况下，ES 是不允许修改字段的类型的，因为 ES 是根据 Lucene 实现的倒排索引，一旦生成后就不允许修改，如果希望改变字段类型，必须使用 Reindex API 重建索引。</p><p>不能修改的原因是如果修改了字段的数据类型，会导致已被索引的无法被搜索，但是如果是增加新的字段，就不会有这样的影响。</p><h2 id="第四部分-Index-Template"><a href="#第四部分-Index-Template" class="headerlink" title="第四部分 Index Template"></a>第四部分 Index Template</h2><p>### </p><h2 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h2><p>1、Elasticsearch官网  链接：<a href="https://www.elastic.co/cn/" target="_blank" rel="noopener">https://www.elastic.co/cn/</a></p><p><a href="https://www.cnblogs.com/wupeixuan/p/12514843.html" target="_blank" rel="noopener">https://www.cnblogs.com/wupeixuan/p/12514843.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   版本升级指引&lt;/li&gt;
&lt;li&gt;第二部分   升级方法和具体步骤&lt;/li&gt;
&lt;li&gt;总结&lt;
      
    
    </summary>
    
      <category term="Elasticsearch" scheme="https://zjrongxiang.github.io/categories/Elasticsearch/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark研发各类报错汇总</title>
    <link href="https://zjrongxiang.github.io/2021/03/21/2021-03-29-Spark%E7%A0%94%E5%8F%91%E5%90%84%E7%B1%BB%E6%8A%A5%E9%94%99%E6%B1%87%E6%80%BB/"/>
    <id>https://zjrongxiang.github.io/2021/03/21/2021-03-29-Spark研发各类报错汇总/</id>
    <published>2021-03-21T13:30:00.000Z</published>
    <updated>2021-04-12T13:00:11.411Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>主要记录日常的爬坑记录，以Spark相关为主。</p><h2 id="第一部分-报错关键字：System-memory-must-be-at-least"><a href="#第一部分-报错关键字：System-memory-must-be-at-least" class="headerlink" title="第一部分 报错关键字：System memory  must be at least "></a>第一部分 报错关键字：System memory <em> must be at least </em></h2><h3 id="1-1-报错背景"><a href="#1-1-报错背景" class="headerlink" title="1.1 报错背景"></a>1.1 报错背景</h3><p>本地ideal研发环境（windows）运行spark程序调试，报错如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">21/03/29 12:28:36 ERROR SparkContext: Error initializing SparkContext. </span><br><span class="line">java.lang.IllegalArgumentException: System memory 259522560 must be at least 471859200. Please increase heap size using the --driver-memory option or spark.driver.memory in Spark configuration.</span><br></pre></td></tr></table></figure><h3 id="1-2-原因分析"><a href="#1-2-原因分析" class="headerlink" title="1.2 原因分析"></a>1.2 原因分析</h3><p>从报错内容上看SparkContext没有初始化成功，错误示内存资源不够，需要至少<code>471859200</code>。</p><h4 id="1-2-1-spark本地运行机制"><a href="#1-2-1-spark本地运行机制" class="headerlink" title="1.2.1 spark本地运行机制"></a>1.2.1 spark本地运行机制</h4><p>Spark在本地运行（local）原理是使用线程模拟进程，所以整个集群启动在一个进程中。</p><h4 id="1-2-2-源码分析"><a href="#1-2-2-源码分析" class="headerlink" title="1.2.2 源码分析"></a>1.2.2 源码分析</h4><p>上源码(Spark 3.1.0，<code>org.apache.spark.memory.UnifiedMemoryManager</code>)：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UnifiedMemoryManager</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Set aside a fixed amount of memory for non-storage, non-execution purposes.</span></span><br><span class="line">  <span class="comment">// This serves a function similar to `spark.memory.fraction`, but guarantees that we reserve</span></span><br><span class="line">  <span class="comment">// sufficient memory for the system even for small heaps. E.g. if we have a 1GB JVM, then</span></span><br><span class="line">  <span class="comment">// the memory used for execution and storage will be (1024 - 300) * 0.6 = 434MB by default.</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">RESERVED_SYSTEM_MEMORY_BYTES</span> = <span class="number">300</span> * <span class="number">1024</span> * <span class="number">1024</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(conf: <span class="type">SparkConf</span>, numCores: <span class="type">Int</span>): <span class="type">UnifiedMemoryManager</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> maxMemory = getMaxMemory(conf)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">UnifiedMemoryManager</span>(</span><br><span class="line">      conf,</span><br><span class="line">      maxHeapMemory = maxMemory,</span><br><span class="line">      onHeapStorageRegionSize =</span><br><span class="line">        (maxMemory * conf.get(config.<span class="type">MEMORY_STORAGE_FRACTION</span>)).toLong,</span><br><span class="line">      numCores = numCores)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return the total amount of memory shared between execution and storage, in bytes.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getMaxMemory</span></span>(conf: <span class="type">SparkConf</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> systemMemory = conf.get(<span class="type">TEST_MEMORY</span>)</span><br><span class="line">    <span class="keyword">val</span> reservedMemory = conf.getLong(<span class="type">TEST_RESERVED_MEMORY</span>.key,</span><br><span class="line">      <span class="keyword">if</span> (conf.contains(<span class="type">IS_TESTING</span>)) <span class="number">0</span> <span class="keyword">else</span> <span class="type">RESERVED_SYSTEM_MEMORY_BYTES</span>)</span><br><span class="line">    <span class="keyword">val</span> minSystemMemory = (reservedMemory * <span class="number">1.5</span>).ceil.toLong</span><br><span class="line">    <span class="keyword">if</span> (systemMemory &lt; minSystemMemory) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s"System memory <span class="subst">$systemMemory</span> must "</span> +</span><br><span class="line">        <span class="string">s"be at least <span class="subst">$minSystemMemory</span>. Please increase heap size using the --driver-memory "</span> +</span><br><span class="line">        <span class="string">s"option or <span class="subst">$&#123;config.DRIVER_MEMORY.key&#125;</span> in Spark configuration."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// SPARK-12759 Check executor memory to fail fast if memory is insufficient</span></span><br><span class="line">    <span class="keyword">if</span> (conf.contains(config.<span class="type">EXECUTOR_MEMORY</span>)) &#123;</span><br><span class="line">      <span class="keyword">val</span> executorMemory = conf.getSizeAsBytes(config.<span class="type">EXECUTOR_MEMORY</span>.key)</span><br><span class="line">      <span class="keyword">if</span> (executorMemory &lt; minSystemMemory) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s"Executor memory <span class="subst">$executorMemory</span> must be at least "</span> +</span><br><span class="line">          <span class="string">s"<span class="subst">$minSystemMemory</span>. Please increase executor memory using the "</span> +</span><br><span class="line">          <span class="string">s"--executor-memory option or <span class="subst">$&#123;config.EXECUTOR_MEMORY.key&#125;</span> in Spark configuration."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> usableMemory = systemMemory - reservedMemory</span><br><span class="line">    <span class="keyword">val</span> memoryFraction = conf.get(config.<span class="type">MEMORY_FRACTION</span>)</span><br><span class="line">    (usableMemory * memoryFraction).toLong</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>主要功能点：</p><ul><li><p><code>systemMemory</code>变量定义了系统内存资源。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.internal.config</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> <span class="type">TEST_MEMORY</span> = <span class="type">ConfigBuilder</span>(<span class="string">"spark.testing.memory"</span>)</span><br><span class="line">  .version(<span class="string">"1.6.0"</span>)</span><br><span class="line">  .longConf</span><br><span class="line">  .createWithDefault(<span class="type">Runtime</span>.getRuntime.maxMemory)</span><br></pre></td></tr></table></figure><p>其中默认值为<code>Runtime.getRuntime.maxMemory</code>，这个值为java虚拟机（JVM）能够从操作系统获取的最大内存资源，如果启动虚拟机时候没有配置<code>-Xmx</code>参数，那么就是<code>256M=256*1024*1024 beytes</code>。</p></li><li><p><code>reservedMemory</code>变量为系统保留内存资源。优先使用<code>TEST_RESERVED_MEMORY</code>的值，默认值是个表达式，如果<code>IS_TESTING=True</code>（测试模式）则值为0，否则为：<code>RESERVED_SYSTEM_MEMORY_BYTES=300M</code>。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">val</span> <span class="type">TEST_RESERVED_MEMORY</span> = <span class="type">ConfigBuilder</span>(<span class="string">"spark.testing.reservedMemory"</span>)</span><br><span class="line">    .version(<span class="string">"1.6.0"</span>)</span><br><span class="line">    .longConf</span><br><span class="line">    .createOptional</span><br><span class="line">  <span class="keyword">val</span> <span class="type">IS_TESTING</span> = <span class="type">ConfigBuilder</span>(<span class="string">"spark.testing"</span>)</span><br><span class="line">    .version(<span class="string">"1.0.1"</span>)</span><br><span class="line">    .booleanConf</span><br><span class="line">    .createOptional</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">RESERVED_SYSTEM_MEMORY_BYTES</span> = <span class="number">300</span> * <span class="number">1024</span> * <span class="number">1024</span></span><br></pre></td></tr></table></figure></li><li><p><code>minSystemMemory</code>变量为系统最小内存资源。定义为<code>reservedMemory</code>的1.5倍。</p></li></ul><p>从报错信息看，明显是触发下面的代码逻辑：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (systemMemory &lt; minSystemMemory) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s"System memory <span class="subst">$systemMemory</span> must "</span> +</span><br><span class="line">        <span class="string">s"be at least <span class="subst">$minSystemMemory</span>. Please increase heap size using the --driver-memory "</span> +</span><br><span class="line">        <span class="string">s"option or <span class="subst">$&#123;config.DRIVER_MEMORY.key&#125;</span> in Spark configuration."</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>spark应用中未进行相关参数配置，<code>reservedMemory</code>值为300M，那么<code>minSystemMemory</code>值为450M，而应用程序为设置JVM参数，<code>systemMemory</code>默认是256M。显然触发<code>systemMemory &lt; minSystemMemory</code>条件。</p><p>可以使用下面的命令查看java环境的默认<code>Xmx</code>的默认值。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">In Windows:</span></span><br><span class="line">java -XX:+PrintFlagsFinal -version | findstr /i "HeapSize PermSize ThreadStackSize"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">In Linux:</span></span><br><span class="line">java -XX:+PrintFlagsFinal -version | grep -iE 'HeapSize|PermSize|ThreadStackSize'</span><br></pre></td></tr></table></figure><h3 id="1-3-修复方法"><a href="#1-3-修复方法" class="headerlink" title="1.3 修复方法"></a>1.3 修复方法</h3><p>根据上面的源码分析，我们有下面的修复方法。</p><h4 id="1-3-1-生产环境"><a href="#1-3-1-生产环境" class="headerlink" title="1.3.1 生产环境"></a>1.3.1 生产环境</h4><p>生产代码按照源码分析要求jvm虚拟机至少是450M以上的内存。</p><ul><li><p>配置<code>-Xmx</code>参数，使得其远大于450M。例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Xmx1024m</span><br></pre></td></tr></table></figure></li></ul><h4 id="1-3-2-测试调试"><a href="#1-3-2-测试调试" class="headerlink" title="1.3.2 测试调试"></a>1.3.2 测试调试</h4><ul><li><p>配置<code>spark.testing.memory</code>参数</p><p>这时候<code>systemMemory</code>=<code>spark.testing.memory</code>参数的值，例如：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        .set(<span class="string">"spark.testing.memory"</span>,<span class="string">"2147480000"</span>)</span><br><span class="line"><span class="comment">//2147480000=2G</span></span><br></pre></td></tr></table></figure></li><li><p>开启测试模式（<code>IS_TESTING=True</code>）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        .set(<span class="string">"spark.testing"</span>,<span class="string">"true"</span>)</span><br></pre></td></tr></table></figure><p>这时候<code>minSystemMemory</code>即为0。这时候异常条件不会触发。</p></li><li><p>指定<code>spark.testing.reservedMemory</code>参数的值（尽可能的小）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        .set(<span class="string">"spark.testing.reservedMemory"</span>,<span class="string">"0"</span>)</span><br></pre></td></tr></table></figure><p>上面的配置下，<code>minSystemMemory</code>的值也为0。</p></li></ul><h3 id="1-4-总结"><a href="#1-4-总结" class="headerlink" title="1.4 总结"></a>1.4 总结</h3><p>Spark运行对内存资源进行了门槛限制，如果降低这个限制必须要特意显示配置测试相关的指标配置。</p><h2 id="第二部分-报错关键字：The-maximum-recommended-task-size-is-100-KB"><a href="#第二部分-报错关键字：The-maximum-recommended-task-size-is-100-KB" class="headerlink" title="第二部分 报错关键字：The maximum recommended task size is 100 KB"></a>第二部分 报错关键字：The maximum recommended task size is 100 KB</h2><h3 id="1-1-报错背景-1"><a href="#1-1-报错背景-1" class="headerlink" title="1.1 报错背景"></a>1.1 报错背景</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Stage 0 contains a task of very large size (183239 KB). The maximum recommended task size is 100 KB.</span><br></pre></td></tr></table></figure><h3 id="1-2-原因分析-1"><a href="#1-2-原因分析-1" class="headerlink" title="1.2 原因分析"></a>1.2 原因分析</h3><p>此错误消息意味着将一些较大的对象从driver端发送到executors。</p><p>spark rpc传输序列化数据是有大小的限制，默认大小是128M（即131072K, 134217728 字节）。所以需要修改spark.rpc.message.maxSize配置的值。</p><h3 id="1-3-修复方法-1"><a href="#1-3-修复方法-1" class="headerlink" title="1.3 修复方法"></a>1.3 修复方法</h3><p>在Dirver程序中，修改spark.rpc.message.maxSize 值，例如，增大到1024M：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.rpc.message.maxSize=1024</span><br></pre></td></tr></table></figure><h3 id="1-4-总结-1"><a href="#1-4-总结-1" class="headerlink" title="1.4 总结"></a>1.4 总结</h3><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p><a href="https://blog.csdn.net/wangshuminjava/article/details/79792961" target="_blank" rel="noopener">https://blog.csdn.net/wangshuminjava/article/details/79792961</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;主要记录日常的爬坑记录，以Spark相关为主。&lt;/p&gt;
&lt;h2 id=&quot;第一部分-报错关键字：System-memory-must-be-a
      
    
    </summary>
    
      <category term="Spark" scheme="https://zjrongxiang.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://zjrongxiang.github.io/tags/Spark/"/>
    
  </entry>
  
</feed>
