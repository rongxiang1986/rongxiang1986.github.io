<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RongXiang</title>
  
  <subtitle>我的烂笔头</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zjrongxiang.github.io/"/>
  <updated>2021-08-14T14:30:28.980Z</updated>
  <id>https://zjrongxiang.github.io/</id>
  
  <author>
    <name>rong xiang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>kubernetes常用命令汇总</title>
    <link href="https://zjrongxiang.github.io/2021/08/02/2021-08-14-kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%B1%87%E6%80%BB/"/>
    <id>https://zjrongxiang.github.io/2021/08/02/2021-08-14-kubernetes常用命令汇总/</id>
    <published>2021-08-02T13:30:00.000Z</published>
    <updated>2021-08-14T14:30:28.980Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  环境依赖</li><li>第二部分 交互接口</li><li>第三部分 任务提交</li><li>参考文献及资料</li></ul><h2 id="第一部分-kubectl-get-获取资源列表"><a href="#第一部分-kubectl-get-获取资源列表" class="headerlink" title="第一部分 kubectl get - 获取资源列表"></a>第一部分 kubectl get - 获取资源列表</h2><h3 id="1-1-获取集群所有pods信息"><a href="#1-1-获取集群所有pods信息" class="headerlink" title="1.1 获取集群所有pods信息"></a>1.1 获取集群所有pods信息</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="builtin-name">get</span> pods</span><br></pre></td></tr></table></figure><h3 id="1-2-查看当前所有service"><a href="#1-2-查看当前所有service" class="headerlink" title="1.2 查看当前所有service"></a>1.2 查看当前所有service</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="builtin-name">get</span> services</span><br></pre></td></tr></table></figure><h2 id="第二部分-kubectl-describe-查看资源详细信息"><a href="#第二部分-kubectl-describe-查看资源详细信息" class="headerlink" title="第二部分 kubectl describe - 查看资源详细信息"></a>第二部分 kubectl describe - 查看资源详细信息</h2><h2 id="第三部分-kubectl-logs-查看pod日志"><a href="#第三部分-kubectl-logs-查看pod日志" class="headerlink" title="第三部分 kubectl logs - 查看pod日志"></a>第三部分 kubectl logs - 查看pod日志</h2><h2 id="第四部分-kubectl-delete-删除一个资源"><a href="#第四部分-kubectl-delete-删除一个资源" class="headerlink" title="第四部分 kubectl delete - 删除一个资源"></a>第四部分 kubectl delete - 删除一个资源</h2><h2 id="第五部分-kubectl-proxy-启动代理"><a href="#第五部分-kubectl-proxy-启动代理" class="headerlink" title="第五部分 kubectl proxy - 启动代理"></a>第五部分 kubectl proxy - 启动代理</h2><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、  官网地址，链接:<a href="https://tomcat.apache.org/" target="_blank" rel="noopener">https://tomcat.apache.org/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;
      
    
    </summary>
    
      <category term="kubernetes" scheme="https://zjrongxiang.github.io/categories/kubernetes/"/>
    
    
      <category term="kubernetes" scheme="https://zjrongxiang.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>SpringCloud系列文章（nacos服务注册总结）</title>
    <link href="https://zjrongxiang.github.io/2021/07/31/2021-08-10-SpringCloud%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88nacos%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E6%80%BB%E7%BB%93%EF%BC%89/"/>
    <id>https://zjrongxiang.github.io/2021/07/31/2021-08-10-SpringCloud系列文章（nacos服务注册总结）/</id>
    <published>2021-07-31T05:30:00.000Z</published>
    <updated>2021-08-15T08:48:21.378Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分 开发、测试环境项目部署</li><li>第二部分 项目打包</li><li>第三部分 生产环境部署</li><li>第四部分  总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>而 Nacos 作为微服务核心的服务注册与发现中心，让大家在 Eureka 和 Consule 之外有了新的选择，开箱即用，上手简洁，暂时也没发现有太大的坑。</p><h2 id="第一部分-Nacos的部署"><a href="#第一部分-Nacos的部署" class="headerlink" title="第一部分 Nacos的部署"></a>第一部分 <code>Nacos</code>的部署</h2><p>Nacos支持三种部署模式</p><p>1、单机模式：可用于测试和单机使用，生产环境切忌使用单机模式（满足不了高可用）</p><p>2、集群模式：可用于生产环境，确保高可用</p><p>3、多集群模式：可用于多数据中心场景</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、<code>vue</code>项目文档，链接：<a href="https://www.cnblogs.com/crazymakercircle/p/14231815.html" target="_blank" rel="noopener">https://www.cnblogs.com/crazymakercircle/p/14231815.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分 开发、测试环境项目部署&lt;/li&gt;
&lt;li&gt;第二部分 项目打包&lt;/li&gt;
&lt;li&gt;第三部分 生产
      
    
    </summary>
    
      <category term="Vue" scheme="https://zjrongxiang.github.io/categories/Vue/"/>
    
    
  </entry>
  
  <entry>
    <title>Vue学习笔记</title>
    <link href="https://zjrongxiang.github.io/2021/07/31/2021-07-31-Vue%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://zjrongxiang.github.io/2021/07/31/2021-07-31-Vue学习笔记/</id>
    <published>2021-07-31T05:30:00.000Z</published>
    <updated>2021-07-31T15:28:53.446Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分 开发、测试环境项目部署</li><li>第二部分 项目打包</li><li>第三部分 生产环境部署</li><li>第四部分  总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="第一部分-第一个案例"><a href="#第一部分-第一个案例" class="headerlink" title="第一部分 第一个案例"></a>第一部分 第一个案例</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"app"</span>&gt;</span>&#123;&#123;message&#125;&#125;<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://cdn.jsdelivr.net/npm/vue@2.6.14"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="actionscript">  <span class="keyword">var</span> vm = <span class="keyword">new</span> Vue(&#123;</span></span><br><span class="line"><span class="actionscript">    el: <span class="string">"#app"</span>,</span></span><br><span class="line"><span class="undefined">    data: &#123;</span></span><br><span class="line"><span class="actionscript">      message: <span class="string">"Hello"</span></span></span><br><span class="line"><span class="undefined">    &#125;</span></span><br><span class="line"><span class="undefined">  &#125;);</span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="第二部分-组件"><a href="#第二部分-组件" class="headerlink" title="第二部分 组件"></a>第二部分 组件</h2><h3 id="2-1-v-bind"><a href="#2-1-v-bind" class="headerlink" title="2.1 v-bind"></a>2.1 v-bind</h3><p>绑定</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"app"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">span</span> <span class="attr">v-bind:title</span>=<span class="string">"messageTest"</span>&gt;</span></span><br><span class="line">  鼠标悬停</span><br><span class="line">  <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://cdn.jsdelivr.net/npm/vue@2.6.14"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined"></span></span><br><span class="line"><span class="actionscript">  <span class="keyword">var</span> vm = <span class="keyword">new</span> Vue(&#123;</span></span><br><span class="line"><span class="actionscript">    el: <span class="string">"#app"</span>,</span></span><br><span class="line"><span class="undefined">    data: &#123;</span></span><br><span class="line"><span class="actionscript">      <span class="comment">//messageTest: "页面加载与"+ new Date().toLocaleString()</span></span></span><br><span class="line"><span class="actionscript">      messageTest: <span class="string">"test"</span></span></span><br><span class="line"><span class="undefined">    &#125;</span></span><br><span class="line"><span class="undefined">  &#125;);</span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-2-判断循环（v-if、v-else、v-else-if）"><a href="#2-2-判断循环（v-if、v-else、v-else-if）" class="headerlink" title="2.2 判断循环（v-if、v-else、v-else-if）"></a>2.2 判断循环（v-if、v-else、v-else-if）</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"app"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">h1</span> <span class="attr">v-if</span>=<span class="string">"ok"</span>&gt;</span>Yes<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">h1</span> <span class="attr">v-else</span>&gt;</span>No<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://cdn.jsdelivr.net/npm/vue@2.6.14"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="actionscript">  <span class="keyword">var</span> vm = <span class="keyword">new</span> Vue(&#123;</span></span><br><span class="line"><span class="actionscript">    el: <span class="string">"#app"</span>,</span></span><br><span class="line"><span class="undefined">    data: &#123;</span></span><br><span class="line"><span class="actionscript">      ok: <span class="literal">true</span></span></span><br><span class="line"><span class="undefined">    &#125;</span></span><br><span class="line"><span class="undefined">  &#125;);</span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>表达式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"app"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">h1</span> <span class="attr">v-if</span>=<span class="string">"type==='A'"</span>&gt;</span>A<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">h1</span> <span class="attr">v-else-if</span>=<span class="string">"type==='B'"</span>&gt;</span>B<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">h1</span> <span class="attr">v-else</span>&gt;</span>C<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://cdn.jsdelivr.net/npm/vue@2.6.14"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="actionscript">  <span class="keyword">var</span> vm = <span class="keyword">new</span> Vue(&#123;</span></span><br><span class="line"><span class="actionscript">    el: <span class="string">"#app"</span>,</span></span><br><span class="line"><span class="undefined">    data: &#123;</span></span><br><span class="line"><span class="actionscript">      type: <span class="string">"A"</span></span></span><br><span class="line"><span class="undefined">    &#125;</span></span><br><span class="line"><span class="undefined">  &#125;);</span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-3-Vue-component"><a href="#2-3-Vue-component" class="headerlink" title="2.3 Vue.component"></a>2.3 Vue.component</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;div id=&quot;app&quot;&gt;</span><br><span class="line">  &lt;rongxiang v-for=&quot;item in items&quot; v-bind:mid=&quot;item&quot;&gt;&lt;/rongxiang&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue@2.6.14&quot;&gt;&lt;/script&gt;</span><br><span class="line">&lt;script&gt;</span><br><span class="line"></span><br><span class="line">  Vue.component(&quot;rongxiang&quot;,&#123;</span><br><span class="line">    props: [&quot;mid&quot;],</span><br><span class="line">    template: &apos;&lt;li&gt;&#123;&#123;mid&#125;&#125;&lt;/li&gt;&apos;</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  var vm = new Vue(&#123;</span><br><span class="line">    el: &quot;#app&quot;,</span><br><span class="line">    data: &#123;items: [&quot;java&quot;,&quot;python&quot;,&quot;lua&quot;,&quot;scala&quot;]&#125;,</span><br><span class="line"></span><br><span class="line">  &#125;);</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure><h2 id="第三部分-通信Axios"><a href="#第三部分-通信Axios" class="headerlink" title="第三部分 通信Axios"></a>第三部分 通信Axios</h2><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、<code>vue</code>项目文档，链接：<a href="https://vuejs.org/" target="_blank" rel="noopener">https://vuejs.org/</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分 开发、测试环境项目部署&lt;/li&gt;
&lt;li&gt;第二部分 项目打包&lt;/li&gt;
&lt;li&gt;第三部分 生产
      
    
    </summary>
    
      <category term="Vue" scheme="https://zjrongxiang.github.io/categories/Vue/"/>
    
    
  </entry>
  
  <entry>
    <title>OpenResty简介及原理</title>
    <link href="https://zjrongxiang.github.io/2021/06/14/2021-07-17-OpenResty%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/06/14/2021-07-17-OpenResty简介及原理总结/</id>
    <published>2021-06-14T04:42:00.000Z</published>
    <updated>2021-07-28T15:30:00.473Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分 <code>su</code>命令</p></li><li><p>第二部分 <code>su</code> 和 <code>su -</code>区别</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Nginx有很多的特性和好处，但是在Nginx上开发成了一个难题,Nginx模块需要用C开发，而且必须符合一系列复杂的规则，最重要的用C开发模块必须要熟悉Nginx的源代码，使得开发者对其望而生畏。为了开发人员方便，所以有了一种整合了Nginx和lua的框架，那就是OpenResty，它帮我们实现了可以用lua的规范开发，随着系统架构的不断升级、优化，OpenResty在被广泛的应用。</p><h2 id="2-OpenResty概念"><a href="#2-OpenResty概念" class="headerlink" title="2.OpenResty概念"></a>2.OpenResty概念</h2><p>OpenResty(又称：ngx_openresty)是一个基于Nginx的可伸缩的Web平台，由中国人章亦春发起，提供了很多高质量的第三方模块。</p><p>OpenResty是一个强大的Web应用服务器，Web开发人员可以使用lua脚本语言调动Nginx支持的各种C以及Lua模块，更主要的是在性能方面，OpenResty可以快速的构造出足以胜任10K以上的并发连接响应的超高性能的Web应用系统。</p><h2 id="二-OpenResty的运行原理"><a href="#二-OpenResty的运行原理" class="headerlink" title="二.OpenResty的运行原理"></a>二.OpenResty的运行原理</h2><p> Nginx 采用的是 master-worker 模型，一个 master 进程管理多个 worker 进程，基本的事件处理都是放在 woker 中，master 负责一些全局初始化，以及对 worker 的管理。</p><p> OpenResty中，每个worker进程使用一个LuaVM，当请求被分配到worker时，将在这个LuaVM中创建一个coroutine协程，协程之间数据隔离，每个协程都具有独立的全局变量。</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL3N6X21tYml6X3BuZy9IVjR5VEk2UGpiSUl3ZGhhT0JvVU1xaWJMcmVXUzhpYjZtQ1lNaWFpYnhLZUdOQ25YU0xLWE5FYmdIdGJrRWx5VmFKNkZpYVM4Vk1jMXhweGVtRHBvaWJNZTRpYmcvNjQw?x-oss-process=image/format,png" alt="img"></p><p>协程和多线程下的线程类似：有自己的堆栈，自己的局部变量，有自己的指令指针，但是和其他协程程序共享全局变量等信息。线程和协程的主要不同在于：多处理器的情况下，概念上来说多线程是同时运行多个线程，而协程是通过代码来完成协程的切换，任何时刻只有一个协程程序在运行。并且这个在运行的协程只有明确被要求挂起时才会被挂起<br>OpenResty处理请求流程<br>Nginx会把一个请求分成不同阶段，第三方模块可以根据自己的行为，挂在到不同阶段中以达到自身目的。OpenResty采用了同样的特性，不同阶段有着不同的处理行为。</p><h2 id="三-OpenResty的优势"><a href="#三-OpenResty的优势" class="headerlink" title="三.OpenResty的优势"></a>三.OpenResty的优势</h2><p>首先我们选择使用OpenResty，其是由Nginx核心加很多第三方模块组成，其最大的亮点是默认集成了Lua开发环境，使得Nginx可以作为一个Web Server使用。<br>借助于Nginx的事件驱动模型和非阻塞IO，可以实现高性能的Web应用程序。<br>OpenResty提供了大量组件如Mysql、Redis、Memcached等等，使在Nginx上开发Web应用更方便更简单。目前在京东如实时价格、秒杀、动态服务、单品页、列表页等都在使用Nginx+Lua架构，其他公司如淘宝、去哪儿网等。</p><h2 id="四-OpenResty的nginx架构的特点"><a href="#四-OpenResty的nginx架构的特点" class="headerlink" title="四.OpenResty的nginx架构的特点"></a>四.OpenResty的nginx架构的特点</h2><p> Nginx采用多进程模式，对于每个worker进程都是独立的，因此不需要加锁，所以节省了锁带来的性能开销。采用独立的进程的好处在于worker进程之间相互不会影响，当一个进程退出后，其他进程依然工作，以保证服务不会终端。<br> Nginx采用异步非堵塞的方式去处理请求，异步非堵塞就是当一个线程调用出现阻塞而等待时，其他线程可以去处理其他任务。</p><h3 id="多阶段处理"><a href="#多阶段处理" class="headerlink" title="多阶段处理"></a>多阶段处理</h3><p>基于 Nginx 使用的多模块设计思想，Nginx 将HTTP请求的处理过程划分为多个阶段。这样可以使一个HTTP请求的处理过程由很多模块参与处理，每个模块只专注于一个独立而简单的功能处理，可以使性能更好、更稳定，同时拥有更好的扩展性。</p><p>OpenResty在HTTP处理阶段基础上分别在Rewrite/Access阶段、Content阶段、Log阶段注册了自己的handler，加上系统初始阶段master的两个阶段，共11个阶段为Lua脚本提供处理介入的能力。下图描述了OpenResty可以使用的主要阶段：</p><p><img src="https://res.cloudinary.com/practicaldev/image/fetch/s--7aySW_GE--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn-images-1.medium.com/max/1005/1%2AgfAT3GULg89r8MivUVCjQg.jpeg" alt="https://res.cloudinary.com/practicaldev/image/fetch/s--7aySW_GE--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn-images-1.medium.com/max/1005/1%2AgfAT3GULg89r8MivUVCjQg.jpeg"></p><p>OpenResty将我们编写的Lua代码挂载到不同阶段进行处理，每个阶段分工明确，代码独立。</p><p>init_by_lua*：Master进程加载 Nginx 配置文件时运行，一般用来注册全局变量或者预加载Lua模块。</p><p>init_worker_by_lua*：每个worker进程启动时执行，通常用于定时拉取配置/数据或者进行后端服务的健康检查。</p><p>set_by_lua*：变量初始化。</p><p>rewrite_by_lua*:可以实现复杂的转发、重定向逻辑。</p><p>access_by_lua*:IP准入、接口权限等情况集中处理。</p><p>content_by_lua*:内容处理器，接收请求处理并输出响应。</p><p>header_filter_by_lua*:响应头部或者cookie处理。</p><p>body_filter_by_lua*:对响应数据进行过滤，如截断或者替换。</p><p>log_by_lua*:会话完成后，本地异步完成日志记录。</p><h2 id="五-Lua及其ngx-lua简介"><a href="#五-Lua及其ngx-lua简介" class="headerlink" title="五.Lua及其ngx_lua简介"></a>五.Lua及其ngx_lua简介</h2><h3 id="1-Lua"><a href="#1-Lua" class="headerlink" title="1.Lua"></a>1.Lua</h3><p> Lua 是一个小巧的脚本语言。作者是巴西人。该语言的设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能<br> Lua的特点：<br>Lua脚本可以很容易的被C/C++代码调用，也可以反过来调用C/C++的函数，这使得Lua在应用程序中可以被广泛应用。不仅仅作为扩展脚本，也可以作为普通的配置文件，代替XML,Ini等文件格式，并且更容易理解和维护。<br>Lua由标准C编写而成，代码简洁优美，几乎在所有操作系统和平台上都可以编译，运行。一个完整的Lua解释器不过200k，在目前所有脚本引擎中，Lua的速度是最快的。这一切都决定了Lua是作为嵌入式脚本的最佳选择。</p><h3 id="2-ngx-lua"><a href="#2-ngx-lua" class="headerlink" title="2.ngx_lua"></a>2.ngx_lua</h3><p> ngx_lua是将Lua嵌入Nginx，让Nginx执行Lua脚本，并且高并发、非阻塞的处理各种请求。Lua内建协程，可以很好的将异步回调转换成顺序调用的形式。ngx_lua在Lua中进行的IO操作都会委托给Nginx的事件模型，从而实现非阻塞调用。开发者可以采用串行的方式编写程序，ngx_lua会自动的在进行阻塞的IO操作中终端，保存上下文，然后将IO操作委托给Nginx事件处理机制，在IO操作完成后，ngx_lua会恢复上下文，程序继续执行，这些操作都是对用户程序透明的。<br> 每个Nginx的worker进程持有一个Lua解释器或LuaJIT实例，被这个worker处理的所有请求共享这个实例。每个请求的context上下文会被Lua轻量级的协程分隔，从而保证各个请求时独立的。</p><h3 id="3-ngx-lua模块的原理"><a href="#3-ngx-lua模块的原理" class="headerlink" title="3.ngx_lua模块的原理"></a>3.ngx_lua模块的原理</h3><p>每个工作进程worker创建一个Lua虚拟机（LuaVM）,工作进程worker内部协议共享VM。<br>每个Nginx I/O原语封装后注入Lua虚拟机，并允许Lua代码直接访问。<br>每个外部请求都由一个Lua协程处理，协程之间数据隔离<br>Lua代码调用I/O操作等异步时，会挂起当前协程，而不阻塞工作机进程。<br>I/O等异步操作完成时，还原相关协程相关协议的上下文，并继续运行。</p><p><a href="https://blog.csdn.net/even160941/article/details/97308725" target="_blank" rel="noopener">https://blog.csdn.net/even160941/article/details/97308725</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、<code>OpenResty</code> 作者章亦春访谈实录，链接：<a href="https://www.cnblogs.com/zampo/p/4269147.html" target="_blank" rel="noopener">https://www.cnblogs.com/zampo/p/4269147.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分 &lt;code&gt;su&lt;/code&gt;命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Java中restful请求实现总结</title>
    <link href="https://zjrongxiang.github.io/2021/06/14/2021-07-17-Java%E4%B8%ADrestful%E8%AF%B7%E6%B1%82%E5%AE%9E%E7%8E%B0%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/06/14/2021-07-17-Java中restful请求实现总结/</id>
    <published>2021-06-14T04:42:00.000Z</published>
    <updated>2021-07-17T02:18:28.953Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分 <code>su</code>命令</p></li><li><p>第二部分 <code>su</code> 和 <code>su -</code>区别</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="第一部分-restful请求"><a href="#第一部分-restful请求" class="headerlink" title="第一部分 restful请求"></a>第一部分 restful请求</h2><h2 id="第二部分-HttpURLConnection方式"><a href="#第二部分-HttpURLConnection方式" class="headerlink" title="第二部分 HttpURLConnection方式"></a>第二部分 <code>HttpURLConnection</code>方式</h2><h2 id="第三部分-HttpClient"><a href="#第三部分-HttpClient" class="headerlink" title="第三部分 HttpClient"></a>第三部分 <code>HttpClient</code></h2><h2 id="第四部分-Spring"><a href="#第四部分-Spring" class="headerlink" title="第四部分 Spring"></a>第四部分 <code>Spring</code></h2><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Su Command in Linux (Switch User)，链接：<a href="https://linuxize.com/post/su-command-in-linux/" target="_blank" rel="noopener">https://linuxize.com/post/su-command-in-linux/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分 &lt;code&gt;su&lt;/code&gt;命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux系统中的su命令</title>
    <link href="https://zjrongxiang.github.io/2021/06/14/2021-06-14-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84su%E5%91%BD%E4%BB%A4/"/>
    <id>https://zjrongxiang.github.io/2021/06/14/2021-06-14-Linux系统中的su命令/</id>
    <published>2021-06-14T04:42:00.000Z</published>
    <updated>2021-06-14T06:52:40.909Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分 <code>su</code>命令</p></li><li><p>第二部分 <code>su</code> 和 <code>su -</code>区别</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><code>Linux</code>操作系统是多用户多任务操作系统。多用户多任务就是可以在操作系统上建立多个用户，多个用户可以在同一时间内登录并执行各自不同的任务，互不影响。不同用户具有不同的权限，每个用户是在权限允许的范围内完成不同的任务，<code>Linux</code>正是通过这种权限的划分与管理，实现了多用户多任务的运行机制。</p><p>在日常运维中，<code>su</code>命令是最简单的用户切换命令，通过该命令可以实现任何用户身份的切换（包括从普通用户切换为 root 用户、从 root 用户切换为普通用户以及普通用户之间的切换）。普通用户之间切换以及普通用户切换至 root 用户，需要目标用户密钥，只有正确输入密钥，才能实现切换；从 root 用户切换至其他用户，无需知晓对方密钥，直接可切换成功。</p><h2 id="第一部分-su命令"><a href="#第一部分-su命令" class="headerlink" title="第一部分 su命令"></a>第一部分 <code>su</code>命令</h2><p><code>su</code> 命令的基本格式如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@VM-0-5-ubuntu:~# su [选项] 用户名</span><br></pre></td></tr></table></figure><p>参数选项：</p><ul><li><code>-</code>：当前用户不仅切换为指定用户的身份，同时所用的工作环境也切换为此用户的环境（包括 PATH 变量、MAIL 变量等）。使用 - 选项可省略用户名，默认会切换为 root 用户。</li><li><code>-l</code>：同<code>-</code>的使用类似，也就是在切换用户身份的同时，完整切换工作环境，但后面需要添加欲切换的使用者账号。</li><li><code>-p</code>：表示切换为指定用户的身份，但不改变当前的工作环境（不使用切换用户的配置文件）。</li><li><code>-m</code>：和 <code>-p</code> 一样；</li><li><code>-c</code> 命令：仅切换用户执行一次命令，执行后自动切换回来，该选项后通常会带有要执行的命令。</li></ul><h2 id="第二部分-su-和-su-区别"><a href="#第二部分-su-和-su-区别" class="headerlink" title="第二部分 su 和 su -区别"></a>第二部分 <code>su</code> 和 <code>su -</code>区别</h2><p>在实际运维使用中，经常踩的坑就是 <code>su</code> 和 <code>su -</code>的区别了。运维人员通常认为两者是相同，或者不知道 <code>su -</code>。</p><p>事实上，有<code>-</code>和没有 <code>-</code>是完全不同的，<code>-</code>选项表示在切换用户身份的同时，连当前使用的环境变量也切换成指定用户的。环境变量是用来定义操作系统环境的，因此如果系统环境没有随用户身份切换，很多命令无法正确执行。</p><p>初学者可以这样理解它们之间的区别，即有 - 选项，切换用户身份更彻底；反之，只切换了一部分。在不使用 <code>su -</code>的情况下，虽然用户身份成功切换，但环境变量依旧用的是原用户的，切换并不完整。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Su Command in Linux (Switch User)，链接：<a href="https://linuxize.com/post/su-command-in-linux/" target="_blank" rel="noopener">https://linuxize.com/post/su-command-in-linux/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分 &lt;code&gt;su&lt;/code&gt;命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>RestFul接口规范总结</title>
    <link href="https://zjrongxiang.github.io/2021/06/14/2021-06-24-RestFul%E6%8E%A5%E5%8F%A3%E8%A7%84%E8%8C%83%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/06/14/2021-06-24-RestFul接口规范总结/</id>
    <published>2021-06-14T04:42:00.000Z</published>
    <updated>2021-07-01T01:03:39.502Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分 <code>su</code>命令</p></li><li><p>第二部分 <code>su</code> 和 <code>su -</code>区别</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="一、协议"><a href="#一、协议" class="headerlink" title="一、协议"></a>一、协议</h3><p><code>API</code>与用户的通信协议，使用<code>HTTP</code>协议。有互联网交互需要使用<code>HTTPS</code>协议。</p><h2 id="二、域名"><a href="#二、域名" class="headerlink" title="二、域名"></a>二、域名</h2><p>应该尽量将<code>API</code>部署在专用域名之下。</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> http://api.example.com</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><p>或者放在放在主域名下。</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> http://example.org/api</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><p>域名背后建议接口高可用部署。</p><h2 id="三、版本"><a href="#三、版本" class="headerlink" title="三、版本"></a>三、版本</h2><p>将<code>API</code>的版本号放入URL，例如：</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; http:<span class="comment">//api.example.com/v1</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="四、路径"><a href="#四、路径" class="headerlink" title="四、路径"></a>四、路径</h2><p>路径又称”终点”（endpoint），表示API的具体网址。在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的”集合”（collection），所以API中的名词也应该使用复数。</p><p>举例来说，有一个<code>API</code>提供动物园（zoo）的信息，还包括各种动物和雇员的信息，则它的路径应该设计成下面这样。</p><blockquote><ul><li><a href="https://api.example.com/v1/zoos" target="_blank" rel="noopener">https://api.example.com/v1/zoos</a></li><li><a href="https://api.example.com/v1/animals" target="_blank" rel="noopener">https://api.example.com/v1/animals</a></li><li><a href="https://api.example.com/v1/employees" target="_blank" rel="noopener">https://api.example.com/v1/employees</a></li></ul></blockquote><h2 id="五、HTTP动词"><a href="#五、HTTP动词" class="headerlink" title="五、HTTP动词"></a>五、HTTP动词</h2><p>对于资源的具体操作类型，由HTTP动词表示。常用的HTTP动词有下面五个（括号里是对应的SQL命令）。</p><blockquote><ul><li>GET（SELECT）：从服务器取出资源（一项或多项）。</li><li>POST（CREATE）：在服务器新建一个资源。</li><li>PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。</li><li>PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。</li><li>DELETE（DELETE）：从服务器删除资源。</li></ul></blockquote><blockquote><ul><li>HEAD：获取资源的元数据。</li><li>OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。</li></ul></blockquote><p>下面是一些例子。</p><blockquote><ul><li>GET /zoos：列出所有动物园</li><li>POST /zoos：新建一个动物园</li><li>GET /zoos/ID：获取某个指定动物园的信息</li><li>PUT /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息）</li><li>PATCH /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息）</li><li>DELETE /zoos/ID：删除某个动物园</li><li>GET /zoos/ID/animals：列出某个指定动物园的所有动物</li><li>DELETE /zoos/ID/animals/ID：删除某个指定动物园的指定动物</li></ul></blockquote><h2 id="六、过滤信息"><a href="#六、过滤信息" class="headerlink" title="六、过滤信息"></a>六、过滤信息</h2><p>如果记录数量很多，服务器不可能都将它们返回给用户。<code>API</code>应该提供参数，过滤返回结果。</p><p>下面是一些常见的参数。</p><blockquote><ul><li>?limit=10：指定返回记录的数量</li><li>?offset=10：指定返回记录的开始位置。</li><li>?page=2&amp;per_page=100：指定第几页，以及每页的记录数。</li><li>?sortby=name&amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。</li><li>?animal_type_id=1：指定筛选条件</li></ul></blockquote><p>参数的设计允许存在冗余，即允许<code>API</code>路径和URL参数偶尔有重复。比如，GET /zoo/ID/animals 与 GET /animals?zoo_id=ID 的含义是相同的。</p><h2 id="七、状态码"><a href="#七、状态码" class="headerlink" title="七、状态码"></a>七、状态码</h2><p>服务器向用户返回的状态码和提示信息。</p><blockquote><ul><li>200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。</li><li>201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。</li><li>202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务）</li><li>204 NO CONTENT - [DELETE]：用户删除数据成功。</li><li>400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。</li><li>401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。</li><li>403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。</li><li>404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。</li><li>406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。</li><li>410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。</li><li>422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。</li><li>500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。</li></ul></blockquote><h2 id="八、错误处理（Error-handling）"><a href="#八、错误处理（Error-handling）" class="headerlink" title="八、错误处理（Error handling）"></a>八、错误处理（Error handling）</h2><p>如果状态码是4xx，就应该向用户返回出错信息。一般来说，返回的信息中将error作为键名，出错信息作为键值即可。</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; &#123;</span><br><span class="line">&gt;     error: <span class="string">"Invalid API key"</span></span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="九、返回结果"><a href="#九、返回结果" class="headerlink" title="九、返回结果"></a>九、返回结果</h2><p>针对不同操作，服务器向用户返回的结果应该符合以下规范。</p><blockquote><ul><li>GET /collection：返回资源对象的列表（数组）</li><li>GET /collection/resource：返回单个资源对象</li><li>POST /collection：返回新生成的资源对象</li><li>PUT /collection/resource：返回完整的资源对象</li><li>PATCH /collection/resource：返回完整的资源对象</li><li>DELETE /collection/resource：返回一个空文档</li></ul></blockquote><h2 id="十、Hypermedia-API"><a href="#十、Hypermedia-API" class="headerlink" title="十、Hypermedia API"></a>十、Hypermedia API</h2><p><code>RESTful API</code>最好做到<code>Hypermedia</code>，即返回结果中提供链接，连向其他API方法，使得用户不查文档，也知道下一步应该做什么。</p><p>比如，当用户向<code>api.example.com</code>的根目录发出请求，会得到这样一个文档。</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; &#123;<span class="string">"link"</span>: &#123;</span><br><span class="line">&gt;   <span class="string">"rel"</span>:   <span class="string">"collection https://www.example.com/zoos"</span>,</span><br><span class="line">&gt;   <span class="string">"href"</span>:  <span class="string">"https://api.example.com/zoos"</span>,</span><br><span class="line">&gt;   <span class="string">"title"</span>: <span class="string">"List of zoos"</span>,</span><br><span class="line">&gt;   <span class="string">"type"</span>:  <span class="string">"application/vnd.yourformat+json"</span></span><br><span class="line">&gt; &#125;&#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>上面代码表示，文档中有一个link属性，用户读取这个属性就知道下一步该调用什么API了。rel表示这个API与当前网址的关系（collection关系，并给出该collection的网址），href表示API的路径，title表示API的标题，type表示返回类型。</p><p>Hypermedia API的设计被称为<a href="https://en.wikipedia.org/wiki/HATEOAS" target="_blank" rel="noopener">HATEOAS</a>。例如Github的API就是这种设计，访问<a href="https://api.github.com/" target="_blank" rel="noopener">api.github.com</a>会得到一个所有可用API的网址列表。</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; &#123;</span><br><span class="line">&gt;   <span class="string">"current_user_url"</span>: <span class="string">"https://api.github.com/user"</span>,</span><br><span class="line">&gt;   <span class="string">"authorizations_url"</span>: <span class="string">"https://api.github.com/authorizations"</span>,</span><br><span class="line">&gt;   <span class="comment">// ...</span></span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>从上面可以看到，如果想获取当前用户的信息，应该去访问<a href="https://api.github.com/user" target="_blank" rel="noopener">api.github.com/user</a>，然后就得到了下面结果。</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; &#123;</span><br><span class="line">&gt;   <span class="string">"message"</span>: <span class="string">"Requires authentication"</span>,</span><br><span class="line">&gt;   <span class="string">"documentation_url"</span>: <span class="string">"https://developer.github.com/v3"</span></span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>上面代码表示，服务器给出了提示信息，以及文档的网址。</p><h2 id="十一、其他"><a href="#十一、其他" class="headerlink" title="十一、其他"></a>十一、其他</h2><ul><li><p>服务器返回的数据格式，应该使用<code>JSON</code>，避免使用XML</p></li><li><p>URI结尾不应包含（/）</p></li><li>正斜杠分隔符（/）必须用来指示层级关系</li><li>应使用连字符（ - ）来提高URI的可读性</li><li>不得在URI中使用下划线（_）</li><li>URI路径中全都使用小写字母</li></ul><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Su Command in Linux (Switch User)，链接：<a href="https://linuxize.com/post/su-command-in-linux/" target="_blank" rel="noopener">https://linuxize.com/post/su-command-in-linux/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分 &lt;code&gt;su&lt;/code&gt;命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Flink DataStream AP总结</title>
    <link href="https://zjrongxiang.github.io/2021/06/14/2021-07-11-Flink%20DataStream%20AP%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/06/14/2021-07-11-Flink DataStream AP总结/</id>
    <published>2021-06-14T04:42:00.000Z</published>
    <updated>2021-07-11T07:15:14.096Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分 <code>su</code>命令</p></li><li><p>第二部分 <code>su</code> 和 <code>su -</code>区别</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="第三部分-算子"><a href="#第三部分-算子" class="headerlink" title="第三部分 算子"></a>第三部分 算子</h2><p>用户通过算子能将一个或多个 DataStream 转换成新的 DataStream，在应用程序中可以将多个数据转换算子合并成一个复杂的数据流拓扑。</p><p>这部分内容将描述 Flink DataStream API 中基本的数据转换API，数据转换后各种数据分区方式，以及算子的链接策略。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Su Command in Linux (Switch User)，链接：<a href="https://linuxize.com/post/su-command-in-linux/" target="_blank" rel="noopener">https://linuxize.com/post/su-command-in-linux/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分 &lt;code&gt;su&lt;/code&gt;命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Flink链接Kafka总结</title>
    <link href="https://zjrongxiang.github.io/2021/06/14/2021-07-11-Flink%E9%93%BE%E6%8E%A5Kafka%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/06/14/2021-07-11-Flink链接Kafka总结/</id>
    <published>2021-06-14T04:42:00.000Z</published>
    <updated>2021-07-11T06:07:29.902Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分 <code>su</code>命令</p></li><li><p>第二部分 <code>su</code> 和 <code>su -</code>区别</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Flink提供了Kafka connector用于消费/生产Apache Kafka topic的数据。Flink的Kafka consumer集成了checkpoint机制以提供精确一次的处理语义。在具体的实现过程中，Flink不依赖于Kafka内置的消费组位移管理，而是在内部自行记录和维护consumer的位移。</p><h2 id="第一部分-依赖准备"><a href="#第一部分-依赖准备" class="headerlink" title="第一部分 依赖准备"></a>第一部分 依赖准备</h2><p>用户在使用时需要根据Kafka版本来选择相应的connector，如下表所示：</p><table><thead><tr><th>Maven依赖</th><th>支持的最低Flink版本</th><th>Kafka客户端类名</th><th>说明</th></tr></thead><tbody><tr><td>flink-connector-kafka-0.8_2.10</td><td>1.0.0</td><td>FlinkKafkaConsumer08、FlinkKafkaProducer08</td><td>使用的是Kafka老版本low-level consumer，即SimpleConsumer. Flink在内部会提交位移到Zookeeper</td></tr><tr><td>flink-connector-kafka-0.9_2.10</td><td>1.0.0</td><td>FlinkKafkaConsumer09、FlinkKafkaProducer09</td><td>使用Kafka新版本consumer</td></tr><tr><td>flink-connector-kafka-0.10_2.10</td><td>1.2.0</td><td>FlinkKafkaConsumer010、FlinkKafkaProducer010</td><td>支持使用Kafka 0.10.0.0版本新引入的内置时间戳信息</td></tr></tbody></table><p>maven依赖配置：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.10_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="第二部分-消费者"><a href="#第二部分-消费者" class="headerlink" title="第二部分 消费者"></a>第二部分 消费者</h2><h3 id="消费者的构造"><a href="#消费者的构造" class="headerlink" title="消费者的构造"></a>消费者的构造</h3><p>Flink kafka connector使用的consumer取决于用户使用的是老版本consumer还是新版本consumer，新旧两个版本对应的connector类名是不同的，分别是：FlinkKafkaConsumer09（或FlinkKafkaConsumer010）以及FlinkKafkaConsumer08。它们都支持同时消费多个topic。</p><p>该Connector的构造函数包含以下几个字段：</p><ol><li>待消费的topic列表</li><li>key/value解序列化器，用于将字节数组形式的Kafka消息解序列化回对象</li><li>Kafka consumer的属性对象，常用的consumer属性包括：bootstrap.servers（新版本consumer专用）、zookeeper.connect（旧版本consumer专用）和group.id</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">properties.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line"><span class="comment">// only required for Kafka 0.8</span></span><br><span class="line">properties.setProperty(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">properties.setProperty(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">DataStream&lt;String&gt; stream = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer08&lt;&gt;(<span class="string">"topic"</span>, <span class="keyword">new</span> SimpleStringSchema(), properties));</span><br></pre></td></tr></table></figure><h3 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h3><p>Flink Kafka Consumer 需要知道如何将 Kafka 中的二进制数据转换为 Java 或者 Scala 对象。<code>KafkaDeserializationSchema</code> 允许用户指定这样的 schema，每条 Kafka 中的消息会调用 <code>T deserialize(ConsumerRecord&lt;byte[], byte[]&gt; record)</code> 反序列化。</p><p>为了方便使用，Flink 提供了以下几种 schemas：</p><ol><li><p><code>TypeInformationSerializationSchema</code>（和 <code>TypeInformationKeyValueSerializationSchema</code>) 基于 Flink 的 <code>TypeInformation</code> 创建 <code>schema</code>。 如果该数据的读和写都发生在 Flink 中，那么这将是非常有用的。此 schema 是其他通用序列化方法的高性能 Flink 替代方案。</p></li><li><p><code>JsonDeserializationSchema</code>（和 <code>JSONKeyValueDeserializationSchema</code>）将序列化的 JSON 转化为 ObjectNode 对象，可以使用 <code>objectNode.get(&quot;field&quot;).as(Int/String/...)()</code> 来访问某个字段。 KeyValue objectNode 包含一个含所有字段的 key 和 values 字段，以及一个可选的”metadata”字段，可以访问到消息的 offset、partition、topic 等信息。</p></li><li><p><code>AvroDeserializationSchema</code> 使用静态提供的 schema 读取 Avro 格式的序列化数据。 它能够从 Avro 生成的类（<code>AvroDeserializationSchema.forSpecific(...)</code>）中推断出 schema，或者可以与 <code>GenericRecords</code> 一起使用手动提供的 schema（用 <code>AvroDeserializationSchema.forGeneric(...)</code>）。此反序列化 schema 要求序列化记录不能包含嵌入式架构！</p><ul><li>此模式还有一个版本，可以在 <a href="https://docs.confluent.io/current/schema-registry/docs/index.html" target="_blank" rel="noopener">Confluent Schema Registry</a> 中查找编写器的 schema（用于编写记录的 schema）。</li><li>使用这些反序列化 schema 记录将读取从 schema 注册表检索到的 schema 转换为静态提供的 schema（或者通过 <code>ConfluentRegistryAvroDeserializationSchema.forGeneric(...)</code> 或 <code>ConfluentRegistryAvroDeserializationSchema.forSpecific(...)</code>）。</li></ul></li></ol><p>   要使用此反序列化 schema 必须添加以下依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-avro<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>当遇到因一些原因而无法反序列化的损坏消息时，反序列化 schema 会返回 <code>null</code>，以允许 Flink Kafka 消费者悄悄地跳过损坏的消息。请注意，由于 consumer 的容错能力（请参阅下面的部分以获取更多详细信息），在损坏的消息上失败作业将使 consumer 尝试再次反序列化消息。因此，如果反序列化仍然失败，则 consumer 将在该损坏的消息上进入不间断重启和失败的循环。</p><h3 id="容错性"><a href="#容错性" class="headerlink" title="容错性"></a>容错性</h3><p>伴随着启用 Flink 的 checkpointing 后，Flink Kafka Consumer 将使用 topic 中的记录，并以一致的方式定期检查其所有 Kafka offset 和其他算子的状态。如果 Job 失败，Flink 会将流式程序恢复到最新 checkpoint 的状态，并从存储在 checkpoint 中的 offset 开始重新消费 Kafka 中的消息。</p><p>因此，设置 checkpoint 的间隔定义了程序在发生故障时最多需要返回多少。</p><p>为了使 Kafka Consumer 支持容错，需要在 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/zh/docs/deployment/config/#execution-checkpointing-interval" target="_blank" rel="noopener">执行环境</a> 中启用拓扑的 checkpointing。</p><p>如果未启用 checkpoint，那么 Kafka consumer 将定期向 Zookeeper 提交 offset。</p><h2 id="第二部分-生产者"><a href="#第二部分-生产者" class="headerlink" title="第二部分 生产者"></a>第二部分 生产者</h2><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/zh/docs/connectors/datastream/kafka/" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.13/zh/docs/connectors/datastream/kafka/</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Su Command in Linux (Switch User)，链接：<a href="https://linuxize.com/post/su-command-in-linux/" target="_blank" rel="noopener">https://linuxize.com/post/su-command-in-linux/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分 &lt;code&gt;su&lt;/code&gt;命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Kafka安全用户管理API总结</title>
    <link href="https://zjrongxiang.github.io/2021/06/14/2021-07-11-Kafka%E5%AE%89%E5%85%A8%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86API%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/06/14/2021-07-11-Kafka安全用户管理API总结/</id>
    <published>2021-06-14T04:42:00.000Z</published>
    <updated>2021-07-14T11:41:15.603Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分 <code>su</code>命令</p></li><li><p>第二部分 <code>su</code> 和 <code>su -</code>区别</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://github.com/strimzi/strimzi-kafka-oauth/blob/1a4bddca665f01d2728860c91cea64b7dd2c5785/oauth-keycloak-authorizer/src/main/java/io/strimzi/kafka/oauth/server/authorizer/KeycloakRBACAuthorizer.java" target="_blank" rel="noopener">https://github.com/strimzi/strimzi-kafka-oauth/blob/1a4bddca665f01d2728860c91cea64b7dd2c5785/oauth-keycloak-authorizer/src/main/java/io/strimzi/kafka/oauth/server/authorizer/KeycloakRBACAuthorizer.java</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Su Command in Linux (Switch User)，链接：<a href="https://linuxize.com/post/su-command-in-linux/" target="_blank" rel="noopener">https://linuxize.com/post/su-command-in-linux/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分 &lt;code&gt;su&lt;/code&gt;命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Flink Table API &amp; SQL编程总结</title>
    <link href="https://zjrongxiang.github.io/2021/06/14/2021-06-23-Flink%20Table%20API%20&amp;%20SQL%E7%BC%96%E7%A8%8B%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/06/14/2021-06-23-Flink Table API &amp; SQL编程总结/</id>
    <published>2021-06-14T04:42:00.000Z</published>
    <updated>2021-06-23T15:01:47.391Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分 <code>su</code>命令</p></li><li><p>第二部分 <code>su</code> 和 <code>su -</code>区别</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Apache Flink提供了两种顶层的关系型API，分别为Table API和SQL，Flink通过Table API&amp;SQL实现了批流统一。其中Table API是用于Scala和Java的语言集成查询API，它允许以非常直观的方式组合关系运算符（例如select，where和join）的查询。Flink SQL基于<a href="https://calcite.apache.org/" target="_blank" rel="noopener">Apache Calcite</a> 实现了标准的SQL，用户可以使用标准的SQL处理数据集。Table API和SQL与Flink的DataStream和DataSet API紧密集成在一起，用户可以实现相互转化，比如可以将DataStream或者DataSet注册为table进行操作数据。值得注意的是，<strong>Table API and SQL</strong>目前尚未完全完善，还在积极的开发中，所以并不是所有的算子操作都可以通过其实现。</p><h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>从Flink1.9开始，Flink为Table &amp; SQL API提供了两种planner,分别为Blink planner和old planner，其中old planner是在Flink1.9之前的版本使用。主要区别如下：</p><p><strong>尖叫提示</strong>：对于生产环境，目前推荐使用old planner.</p><ul><li><code>flink-table-common</code>: 通用模块，包含 Flink Planner 和 Blink Planner 一些共用的代码</li><li><code>flink-table-api-java</code>: java语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用)</li><li><code>flink-table-api-scala</code>: scala语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用)</li><li><code>flink-table-api-java-bridge</code>: java语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用)</li><li><code>flink-table-api-scala-bridge</code>: scala语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用)</li><li><code>flink-table-planner</code>:planner 和runtime. planner为Flink1,9之前的old planner(推荐使用)</li><li><code>flink-table-planner-blink</code>: 新的Blink planner.</li><li><code>flink-table-runtime-blink</code>: 新的Blink runtime.</li><li><code>flink-table-uber</code>: 将上述的API模块及old planner打成一个jar包，形如flink-table-*.jar，位与/lib目录下</li><li><code>flink-table-uber-blink</code>:将上述的API模块及Blink 模块打成一个jar包，形如fflink-table-blink-*.jar，位与/lib目录下</li></ul><h2 id="Blink-planner-amp-old-planner"><a href="#Blink-planner-amp-old-planner" class="headerlink" title="Blink planner &amp; old planner"></a>Blink planner &amp; old planner</h2><p>Blink planner和old planner有许多不同的特点，具体列举如下：</p><ul><li>Blink planner将批处理作业看做是流处理作业的特例。所以，不支持Table 与DataSet之间的转换，批处理的作业也不会被转成DataSet程序，而是被转为DataStream程序。</li><li>Blink planner不支持 <code>BatchTableSource</code>，使用的是有界的StreamTableSource。</li><li>Blink planner仅支持新的 <code>Catalog</code>，不支持<code>ExternalCatalog</code> (已过时)。</li><li>对于FilterableTableSource的实现，两种Planner是不同的。old planner会谓词下推到<code>PlannerExpression</code>(未来会被移除)，而Blink planner 会谓词下推到 <code>Expression</code>(表示一个产生计算结果的逻辑树)。</li><li>仅仅Blink planner支持key-value形式的配置，即通过Configuration进行参数设置。</li><li>关于PlannerConfig的实现，两种planner有所不同。</li><li>Blink planner 会将多个sink优化成一个DAG(仅支持TableEnvironment，StreamTableEnvironment不支持)，old planner总是将每一个sink优化成一个新的DAG，每一个DAG都是相互独立的。</li><li>old planner不支持catalog统计，Blink planner支持catalog统计。</li></ul><h2 id="第一部分-Flink-Table-amp-SQL程序的pom依赖"><a href="#第一部分-Flink-Table-amp-SQL程序的pom依赖" class="headerlink" title="第一部分 Flink Table &amp; SQL程序的pom依赖"></a>第一部分 Flink Table &amp; SQL程序的pom依赖</h2><p>根据使用的语言不同，可以选择下面的依赖，包括scala版和java版，如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- java版 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-java-bridge_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- scala版 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-scala-bridge_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>除此之外，如果需要在本地的IDE中运行Table API &amp; SQL的程序，则需要添加下面的pom依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Flink 1.9之前的old planner --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 新的Blink planner --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>另外，如果需要实现自定义的格式(比如和kafka交互)或者用户自定义函数，需要添加如下依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Table-API-amp-SQL的编程模板"><a href="#Table-API-amp-SQL的编程模板" class="headerlink" title="Table API &amp; SQL的编程模板"></a>Table API &amp; SQL的编程模板</h2><p>所有的Table API&amp;SQL的程序(无论是批处理还是流处理)都有着相同的形式，下面将给出通用的编程结构形式：</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个TableEnvironment对象，指定planner、处理模式(batch、streaming)</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 创建一个表</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"table1"</span>);</span><br><span class="line"><span class="comment">// 注册一个外部的表</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"outputTable"</span>);</span><br><span class="line"><span class="comment">// 通过Table API的查询创建一个Table 对象</span></span><br><span class="line"><span class="keyword">Table</span> tapiResult <span class="comment">= tableEnv.from(</span><span class="comment">"table1"</span><span class="comment">).select(...)</span>;</span><br><span class="line"><span class="comment">// 通过SQL查询的查询创建一个Table 对象</span></span><br><span class="line"><span class="keyword">Table</span> sqlResult  <span class="comment">= tableEnv.sqlQuery(</span><span class="comment">"SELECT ... FROM table1 ... "</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将结果写入TableSink</span></span><br><span class="line">tapiResult.insertInto(<span class="string">"outputTable"</span>);</span><br><span class="line"><span class="comment">// 执行</span></span><br><span class="line">tableEnv.execute(<span class="string">"java_job"</span>);</span><br></pre></td></tr></table></figure><p>注意：Table API &amp; SQL的查询可以相互集成，另外还可以在DataStream或者DataSet中使用Table API &amp; SQL的API，实现DataStreams、 DataSet与Table之间的相互转换。</p><h2 id="创建TableEnvironment"><a href="#创建TableEnvironment" class="headerlink" title="创建TableEnvironment"></a>创建TableEnvironment</h2><p>TableEnvironment是Table API &amp; SQL程序的一个入口，主要包括如下的功能：</p><ul><li>在内部的catalog中注册Table</li><li>注册catalog</li><li>加载可插拔模块</li><li>执行SQL查询</li><li>注册用户定义函数</li><li><code>DataStream</code> 、<code>DataSet</code>与Table之间的相互转换</li><li>持有对<code>ExecutionEnvironment</code> 、<code>StreamExecutionEnvironment</code>的引用</li></ul><p>一个Table必定属于一个具体的TableEnvironment，不可以将不同TableEnvironment的表放在一起使用(比如join，union等操作)。</p><p>TableEnvironment是通过调用 <code>BatchTableEnvironment.create()</code> 或者StreamTableEnvironment.create()的静态方法进行创建的。另外，默认两个planner的jar包都存在与classpath下，所有需要明确指定使用的planner。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="comment">// FLINK 流处理查询</span></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line">import org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings fsSettings = <span class="module-access"><span class="module"><span class="identifier">EnvironmentSettings</span>.</span></span><span class="keyword">new</span><span class="constructor">Instance()</span>.use<span class="constructor">OldPlanner()</span>.<span class="keyword">in</span><span class="constructor">StreamingMode()</span>.build<span class="literal">()</span>;</span><br><span class="line">StreamExecutionEnvironment fsEnv = <span class="module-access"><span class="module"><span class="identifier">StreamExecutionEnvironment</span>.</span></span>get<span class="constructor">ExecutionEnvironment()</span>;</span><br><span class="line">StreamTableEnvironment fsTableEnv = <span class="module-access"><span class="module"><span class="identifier">StreamTableEnvironment</span>.</span></span>create(fsEnv, fsSettings);</span><br><span class="line"><span class="comment">//或者TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="comment">// FLINK 批处理查询</span></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line">import org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line">import org.apache.flink.table.api.java.BatchTableEnvironment;</span><br><span class="line"></span><br><span class="line">ExecutionEnvironment fbEnv = <span class="module-access"><span class="module"><span class="identifier">ExecutionEnvironment</span>.</span></span>get<span class="constructor">ExecutionEnvironment()</span>;</span><br><span class="line">BatchTableEnvironment fbTableEnv = <span class="module-access"><span class="module"><span class="identifier">BatchTableEnvironment</span>.</span></span>create(fbEnv);</span><br><span class="line"></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="comment">// BLINK 流处理查询</span></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line">import org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">StreamExecutionEnvironment bsEnv = <span class="module-access"><span class="module"><span class="identifier">StreamExecutionEnvironment</span>.</span></span>get<span class="constructor">ExecutionEnvironment()</span>;</span><br><span class="line">EnvironmentSettings bsSettings = <span class="module-access"><span class="module"><span class="identifier">EnvironmentSettings</span>.</span></span><span class="keyword">new</span><span class="constructor">Instance()</span>.use<span class="constructor">BlinkPlanner()</span>.<span class="keyword">in</span><span class="constructor">StreamingMode()</span>.build<span class="literal">()</span>;</span><br><span class="line">StreamTableEnvironment bsTableEnv = <span class="module-access"><span class="module"><span class="identifier">StreamTableEnvironment</span>.</span></span>create(bsEnv, bsSettings);</span><br><span class="line"><span class="comment">// 或者 TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="comment">// BLINK 批处理查询</span></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line">import org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line">import org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings bbSettings = <span class="module-access"><span class="module"><span class="identifier">EnvironmentSettings</span>.</span></span><span class="keyword">new</span><span class="constructor">Instance()</span>.use<span class="constructor">BlinkPlanner()</span>.<span class="keyword">in</span><span class="constructor">BatchMode()</span>.build<span class="literal">()</span>;</span><br><span class="line">TableEnvironment bbTableEnv = <span class="module-access"><span class="module"><span class="identifier">TableEnvironment</span>.</span></span>create(bbSettings);</span><br></pre></td></tr></table></figure><h2 id="在catalog中创建表"><a href="#在catalog中创建表" class="headerlink" title="在catalog中创建表"></a>在catalog中创建表</h2><h3 id="临时表与永久表"><a href="#临时表与永久表" class="headerlink" title="临时表与永久表"></a>临时表与永久表</h3><p>表可以分为临时表和永久表两种，其中永久表需要一个catalog(比如Hive的Metastore)俩维护表的元数据信息，一旦永久表被创建，只要连接到该catalog就可以访问该表，只有显示删除永久表，该表才可以被删除。临时表的生命周期是Flink Session，这些表不能够被其他的Flink Session访问，这些表不属于任何的catalog或者数据库，如果与临时表相对应的数据库被删除了，该临时表也不会被删除。</p><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><h4 id="虚表-Virtual-Tables"><a href="#虚表-Virtual-Tables" class="headerlink" title="虚表(Virtual Tables)"></a>虚表(Virtual Tables)</h4><p>一个Table对象相当于SQL中的视图(虚表)，它封装了一个逻辑执行计划，可以通过一个catalog创建，具体如下：</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取一个TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// table对象，查询的结果集</span></span><br><span class="line"><span class="keyword">Table</span> projTable <span class="comment">= tableEnv.from(</span><span class="comment">"X"</span><span class="comment">).select(...)</span>;</span><br><span class="line"><span class="comment">// 注册一个表，名称为 "projectedTable"</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"projectedTable"</span>, projTable);</span><br></pre></td></tr></table></figure><h4 id="外部数据源表-Connector-Tables"><a href="#外部数据源表-Connector-Tables" class="headerlink" title="外部数据源表(Connector Tables)"></a>外部数据源表(Connector Tables)</h4><p>可以把外部的数据源注册成表，比如可以读取MySQL数据库数据、Kafka数据等</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tableEnvironment</span><br><span class="line">  .connect(...)</span><br><span class="line">  .<span class="keyword">with</span><span class="constructor">Format(<span class="operator">...</span>)</span></span><br><span class="line">  .<span class="keyword">with</span><span class="constructor">Schema(<span class="operator">...</span>)</span></span><br><span class="line">  .<span class="keyword">in</span><span class="constructor">AppendMode()</span></span><br><span class="line">  .create<span class="constructor">TemporaryTable(<span class="string">"MyTable"</span>)</span></span><br></pre></td></tr></table></figure><h3 id="扩展创建表的标识属性"><a href="#扩展创建表的标识属性" class="headerlink" title="扩展创建表的标识属性"></a>扩展创建表的标识属性</h3><p>表的注册总是包含三部分标识属性：catalog、数据库、表名。用户可以在内部设置一个catalog和一个数据库作为当前的catalog和数据库，所以对于catalog和数据库这两个标识属性是可选的，即如果不指定，默认使用的是“current catalog”和 “current database”。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tEnv = ...;</span><br><span class="line">tEnv.useCatalog(<span class="string">"custom_catalog"</span>);<span class="comment">//设置catalog</span></span><br><span class="line">tEnv.useDatabase(<span class="string">"custom_database"</span>);<span class="comment">//设置数据库</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= ...</span>;</span><br><span class="line"><span class="comment">// 注册一个名为exampleView的视图，catalog名为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为custom_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"exampleView"</span>, <span class="keyword">table</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为exampleView的视图，catalog的名为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为other_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"other_database.exampleView"</span>, <span class="keyword">table</span>);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 注册一个名为'View'的视图，catalog的名称为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为custom_database，'View'是保留关键字，需要使用``(反引号)</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"`View`"</span>, <span class="keyword">table</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为example.View的视图，catalog的名为custom_catalog，</span></span><br><span class="line"><span class="comment">// 数据库名为custom_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"`example.View`"</span>, <span class="keyword">table</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为'exampleView'的视图， catalog的名为'other_catalog'</span></span><br><span class="line"><span class="comment">// 数据库名为other_database' </span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"other_catalog.other_database.exampleView"</span>, <span class="keyword">table</span>);</span><br></pre></td></tr></table></figure><h2 id="查询表"><a href="#查询表" class="headerlink" title="查询表"></a>查询表</h2><h3 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h3><p>Table API是一个集成Scala与Java语言的查询API，与SQL相比，它的查询不是一个标准的SQL语句，而是由一步一步的操作组成的。如下展示了一个使用Table API实现一个简单的聚合查询。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"><span class="comment">//注册Orders表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询注册的表</span></span><br><span class="line"><span class="keyword">Table</span> orders <span class="comment">= tableEnv.from(</span><span class="comment">"Orders"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 计算操作</span></span><br><span class="line"><span class="keyword">Table</span> revenue <span class="comment">= orders</span></span><br><span class="line">  .filter(<span class="string">"cCountry === 'FRANCE'"</span>)</span><br><span class="line">  .groupBy(<span class="string">"cID, cName"</span>)</span><br><span class="line">  .select(<span class="string">"cID, cName, revenue.sum AS revSum"</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p>Flink SQL依赖于<a href="https://calcite.apache.org/" target="_blank" rel="noopener">Apache Calcite</a>，其实现了标准的SQL语法，如下案例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// 获取TableEnvironment</span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line">//注册Orders表</span><br><span class="line"></span><br><span class="line">// 计算逻辑同上面的Table API</span><br><span class="line">Table revenue = tableEnv.sqlQuery(</span><br><span class="line">    "<span class="keyword">SELECT</span> cID, cName, <span class="keyword">SUM</span>(revenue) <span class="keyword">AS</span> revSum <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">FROM</span> Orders <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">WHERE</span> cCountry = <span class="string">'FRANCE'</span> <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">GROUP</span> <span class="keyword">BY</span> cID, cName<span class="string">"</span></span><br><span class="line"><span class="string">  );</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 注册"</span>RevenueFrance<span class="string">"外部输出表</span></span><br><span class="line"><span class="string">// 计算结果插入"</span>RevenueFrance<span class="string">"表</span></span><br><span class="line"><span class="string">tableEnv.sqlUpdate(</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">INSERT</span> <span class="keyword">INTO</span> RevenueFrance <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">SELECT</span> cID, cName, <span class="keyword">SUM</span>(revenue) <span class="keyword">AS</span> revSum <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">FROM</span> Orders <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">WHERE</span> cCountry = <span class="string">'FRANCE'</span> <span class="string">" +</span></span><br><span class="line"><span class="string">    "</span><span class="keyword">GROUP</span> <span class="keyword">BY</span> cID, cName<span class="string">"</span></span><br><span class="line"><span class="string">  );</span></span><br></pre></td></tr></table></figure><h2 id="输出表"><a href="#输出表" class="headerlink" title="输出表"></a>输出表</h2><p>一个表通过将其写入到TableSink，然后进行输出。TableSink是一个通用的支持多种文件格式(CSV、Parquet, Avro)和多种外部存储系统(JDBC, Apache HBase, Apache Cassandra, Elasticsearch)以及多种消息对列(Apache Kafka, RabbitMQ)的接口。</p><p>批处理的表只能被写入到 <code>BatchTableSink</code>,流处理的表需要指明AppendStreamTableSink、RetractStreamTableSink或者 <code>UpsertStreamTableSink</code></p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建输出表</span></span><br><span class="line">final Schema schema = <span class="keyword">new</span> <span class="constructor">Schema()</span></span><br><span class="line">    .field(<span class="string">"a"</span>, DataTypes.<span class="constructor">INT()</span>)</span><br><span class="line">    .field(<span class="string">"b"</span>, DataTypes.<span class="constructor">STRING()</span>)</span><br><span class="line">    .field(<span class="string">"c"</span>, DataTypes.<span class="constructor">LONG()</span>);</span><br><span class="line"></span><br><span class="line">tableEnv.connect(<span class="keyword">new</span> <span class="constructor">FileSystem(<span class="string">"/path/to/file"</span>)</span>)</span><br><span class="line">    .<span class="keyword">with</span><span class="constructor">Format(<span class="params">new</span> Csv()</span>.field<span class="constructor">Delimiter('|')</span>.derive<span class="constructor">Schema()</span>)</span><br><span class="line">    .<span class="keyword">with</span><span class="constructor">Schema(<span class="params">schema</span>)</span></span><br><span class="line">    .create<span class="constructor">TemporaryTable(<span class="string">"CsvSinkTable"</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算结果表</span></span><br><span class="line">Table result = ...</span><br><span class="line"><span class="comment">// 输出结果表到注册的TableSink</span></span><br><span class="line">result.insert<span class="constructor">Into(<span class="string">"CsvSinkTable"</span>)</span>;</span><br></pre></td></tr></table></figure><h2 id="Table-API-amp-SQL底层的转换与执行"><a href="#Table-API-amp-SQL底层的转换与执行" class="headerlink" title="Table API &amp; SQL底层的转换与执行"></a>Table API &amp; SQL底层的转换与执行</h2><p>上文提到了Flink提供了两种planner，分别为old planner和Blink planner，对于不同的planner而言，Table API &amp; SQL底层的执行与转换是有所不同的。</p><h4 id="Old-planner"><a href="#Old-planner" class="headerlink" title="Old planner"></a>Old planner</h4><p>根据是流处理作业还是批处理作业，Table API &amp;SQL会被转换成DataStream或者DataSet程序。一个查询在内部表示为一个逻辑查询计划，会被转换为两个阶段:</p><ul><li>1.逻辑查询计划优化</li><li>2.转换成DataStream或者DataSet程序</li></ul><p>上面的两个阶段只有下面的操作被执行时才会被执行：</p><ul><li>当一个表被输出到TableSink时，比如调用了Table.insertInto()方法</li><li>当执行更新查询时，比如调用TableEnvironment.sqlUpdate()方法</li><li>当一个表被转换为DataStream或者DataSet时</li></ul><p>一旦执行上述两个阶段，Table API &amp; SQL的操作会被看做是普通的DataStream或者DataSet程序，所以当<code>StreamExecutionEnvironment.execute()</code>或者<code>ExecutionEnvironment.execute()</code> 被调用时，会执行转换后的程序。</p><h4 id="Blink-planner"><a href="#Blink-planner" class="headerlink" title="Blink planner"></a>Blink planner</h4><p>无论是批处理作业还是流处理作业，如果使用的是Blink planner，底层都会被转换为DataStream程序。在一个查询在内部表示为一个逻辑查询计划，会被转换成两个阶段：</p><ul><li>1.逻辑查询计划优化</li><li>2.转换成DataStream程序</li></ul><p>对于<code>TableEnvironment</code> and <code>StreamTableEnvironment</code>而言，一个查询的转换是不同的</p><p>首先对于TableEnvironment，当TableEnvironment.execute()方法执行时，Table API &amp; SQL的查询才会被转换，因为TableEnvironment会将多个sink优化为一个DAG。</p><p>对于StreamTableEnvironment，转换发生的时间与old planner相同。</p><h2 id="与DataStream-amp-DataSet-API集成"><a href="#与DataStream-amp-DataSet-API集成" class="headerlink" title="与DataStream &amp; DataSet API集成"></a>与DataStream &amp; DataSet API集成</h2><p>对于Old planner与Blink planner而言，只要是流处理的操作，都可以与DataStream API集成，<strong>仅仅只有Old planner才可以与DataSet API集成</strong>，由于Blink planner的批处理作业会被转换成DataStream程序，所以不能够与DataSet API集成。值得注意的是，下面提到的table与DataSet之间的转换仅适用于Old planner。</p><p>Table API &amp; SQL的查询很容易与DataStream或者DataSet程序集成，并可以将Table API &amp; SQL的查询嵌入DataStream或者DataSet程序中。DataStream或者DataSet可以转换成表，反之，表也可以被转换成DataStream或者DataSet。</p><h3 id="从DataStream或者DataSet中注册临时表-视图"><a href="#从DataStream或者DataSet中注册临时表-视图" class="headerlink" title="从DataStream或者DataSet中注册临时表(视图)"></a>从DataStream或者DataSet中注册临时表(视图)</h3><p><strong>尖叫提示：</strong>只能将DataStream或者DataSet转换为临时表(视图)</p><p>下面演示DataStream的转换，对于DataSet的转换类似。</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 获取<span class="keyword">StreamTableEnvironment</span></span><br><span class="line"><span class="keyword">StreamTableEnvironment </span>tableEnv = ...<span class="comment">; </span></span><br><span class="line"><span class="symbol">DataStream</span>&lt;Tuple2&lt;Long, <span class="keyword">String&gt;&gt; </span><span class="keyword">stream </span>= ...</span><br><span class="line">// 将DataStream注册为一个名为myTable的视图，其中字段分别为<span class="string">"f0"</span>, <span class="string">"f1"</span></span><br><span class="line"><span class="symbol">tableEnv.createTemporaryView</span>(<span class="string">"myTable"</span>, <span class="keyword">stream);</span></span><br><span class="line"><span class="keyword">// </span>将DataStream注册为一个名为myTable2的视图,其中字段分别为<span class="string">"myLong"</span>, <span class="string">"myString"</span></span><br><span class="line"><span class="symbol">tableEnv.createTemporaryView</span>(<span class="string">"myTable2"</span>, <span class="keyword">stream, </span><span class="string">"myLong, myString"</span>)<span class="comment">;</span></span><br></pre></td></tr></table></figure><h3 id="将DataStream或者DataSet转化为Table对象"><a href="#将DataStream或者DataSet转化为Table对象" class="headerlink" title="将DataStream或者DataSet转化为Table对象"></a>将DataStream或者DataSet转化为Table对象</h3><p>可以直接将DataStream或者DataSet转换为Table对象，之后可以使用Table API进行查询操作。下面演示DataStream的转换，对于DataSet的转换类似。</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 获取<span class="keyword">StreamTableEnvironment</span></span><br><span class="line"><span class="keyword">StreamTableEnvironment </span>tableEnv = ...<span class="comment">; </span></span><br><span class="line"><span class="symbol">DataStream</span>&lt;Tuple2&lt;Long, <span class="keyword">String&gt;&gt; </span><span class="keyword">stream </span>= ...</span><br><span class="line">// 将DataStream转换为Table对象，默认的字段为<span class="string">"f0"</span>, <span class="string">"f1"</span></span><br><span class="line"><span class="symbol">Table</span> table1 = tableEnv.fromDataStream(<span class="keyword">stream);</span></span><br><span class="line"><span class="keyword">// </span>将DataStream转换为Table对象，默认的字段为<span class="string">"myLong"</span>, <span class="string">"myString"</span></span><br><span class="line"><span class="symbol">Table</span> table2 = tableEnv.fromDataStream(<span class="keyword">stream, </span><span class="string">"myLong, myString"</span>)<span class="comment">;</span></span><br></pre></td></tr></table></figure><h3 id="将表转换为DataStream或者DataSet"><a href="#将表转换为DataStream或者DataSet" class="headerlink" title="将表转换为DataStream或者DataSet"></a>将表转换为DataStream或者DataSet</h3><p>当将Table转为DataStream或者DataSet时，需要指定DataStream或者DataSet的数据类型。通常最方便的数据类型是row类型，Flink提供了很多的数据类型供用户选择，具体包括Row、POJO、样例类、Tuple和原子类型。</p><h4 id="将表转换为DataStream"><a href="#将表转换为DataStream" class="headerlink" title="将表转换为DataStream"></a>将表转换为DataStream</h4><p>一个流处理查询的结果是动态变化的，所以将表转为DataStream时需要指定一个更新模式，共有两种模式：<strong>Append Mode</strong>和<strong>Retract Mode</strong>。</p><ul><li><strong>Append Mode</strong></li></ul><p>如果动态表仅只有Insert操作，即之前输出的结果不会被更新，则使用该模式。如果更新或删除操作使用追加模式会失败报错</p><ul><li><strong>Retract Mode</strong></li></ul><p>始终可以使用此模式。返回值是boolean类型。它用true或false来标记数据的插入和撤回，返回true代表数据插入，false代表数据的撤回。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment. </span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 包含两个字段的表(String name, Integer age)</span></span><br><span class="line">Table table = ...</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为Row</span></span><br><span class="line">DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为定义好的TypeInformation</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(</span><br><span class="line">  Types.STRING(),</span><br><span class="line">  Types.INT());</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = </span><br><span class="line">  tableEnv.toAppendStream(table, tupleType);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用的模式为Retract Mode撤回模式，类型为Row</span></span><br><span class="line"><span class="comment">// 对于转换后的DataStream&lt;Tuple2&lt;Boolean, X&gt;&gt;，X表示流的数据类型，</span></span><br><span class="line"><span class="comment">// boolean值表示数据改变的类型，其中INSERT返回true，DELETE返回的是false</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = </span><br><span class="line">  tableEnv.toRetractStream(table, Row.class);</span><br></pre></td></tr></table></figure><h4 id="将表转换为DataSet"><a href="#将表转换为DataSet" class="headerlink" title="将表转换为DataSet"></a>将表转换为DataSet</h4><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取BatchTableEnvironment</span></span><br><span class="line">BatchTableEnvironment tableEnv = <span class="module-access"><span class="module"><span class="identifier">BatchTableEnvironment</span>.</span></span>create(env);</span><br><span class="line"><span class="comment">// 包含两个字段的表(String name, Integer age)</span></span><br><span class="line">Table table = ...</span><br><span class="line"><span class="comment">// 将表转为DataSet数据类型为Row</span></span><br><span class="line">DataSet&lt;Row&gt; dsRow = tableEnv.<span class="keyword">to</span><span class="constructor">DataSet(<span class="params">table</span>, Row.<span class="params">class</span>)</span>;</span><br><span class="line"><span class="comment">// 将表转为DataSet，通过TypeInformation定义Tuple2&lt;String, Integer&gt;数据类型</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(</span><br><span class="line">  Types.<span class="constructor">STRING()</span>,</span><br><span class="line">  Types.<span class="constructor">INT()</span>);</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = </span><br><span class="line">  tableEnv.<span class="keyword">to</span><span class="constructor">DataSet(<span class="params">table</span>, <span class="params">tupleType</span>)</span>;</span><br></pre></td></tr></table></figure><h3 id="表的Schema与数据类型之间的映射"><a href="#表的Schema与数据类型之间的映射" class="headerlink" title="表的Schema与数据类型之间的映射"></a>表的Schema与数据类型之间的映射</h3><p>表的Schema与数据类型之间的映射有两种方式：分别是基于字段下标位置的映射和基于字段名称的映射。</p><h4 id="基于字段下标位置的映射"><a href="#基于字段下标位置的映射" class="headerlink" title="基于字段下标位置的映射"></a>基于字段下标位置的映射</h4><p>该方式是按照字段的顺序进行一一映射，使用方式如下：</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, <span class="keyword">Integer</span>&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"和"f1"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，选取tuple的第一个元素，指定一个名为"myLong"的字段名</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"myLong"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，为tuple的第一个元素指定名为"myLong"，为第二个元素指定myInt的字段名</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"myLong, myInt"</span><span class="comment">)</span>;</span><br></pre></td></tr></table></figure><h4 id="基于字段名称的映射"><a href="#基于字段名称的映射" class="headerlink" title="基于字段名称的映射"></a>基于字段名称的映射</h4><p>基于字段名称的映射方式支持任意的数据类型包括POJO类型，可以很灵活地定义表Schema映射，所有的字段被映射成一个具体的字段名称，同时也可以使用”as”为字段起一个别名。其中Tuple元素的第一个元素为f0,第二个元素为f1，以此类推。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, <span class="keyword">Integer</span>&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"和"f1"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，选择tuple的第二个元素，指定一个名为"f1"的字段名</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"f1"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，交换字段的顺序</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"f1, f0"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，交换字段的顺序，并为f1起别名为"myInt"，为f0起别名为"myLong</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"f1 as myInt, f0 as myLong"</span><span class="comment">)</span>;</span><br></pre></td></tr></table></figure><h4 id="原子类型"><a href="#原子类型" class="headerlink" title="原子类型"></a>原子类型</h4><p>Flink将<code>Integer</code>, <code>Double</code>, <code>String</code>或者普通的类型称之为原子类型，一个数据类型为原子类型的DataStream或者DataSet可以被转成单个字段属性的表，这个字段的类型与DataStream或者DataSet的数据类型一致，这个字段的名称可以进行指定。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 数据类型为原子类型Long</span></span><br><span class="line">DataStream&lt;Long&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为myLong"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"myLong"</span><span class="comment">)</span>;</span><br></pre></td></tr></table></figure><h4 id="Tuple类型"><a href="#Tuple类型" class="headerlink" title="Tuple类型"></a>Tuple类型</h4><p>Tuple类型的DataStream或者DataSet都可以转为表，可以重新设定表的字段名(即根据tuple元素的位置进行一一映射，转为表之后，每个元素都有一个别名)，如果不为字段指定名称，则使用默认的名称(java语言默认的是f0,f1,scala默认的是_1),用户也可以重新排列字段的顺序，并为每个字段起一个别名。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">//Tuple2&lt;Long, String&gt;类型的DataStream</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为 "f0", "f1"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为 "myLong", "myString"(按照Tuple元素的顺序位置)</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"myLong, myString"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为 "f0", "f1"，并且交换顺序</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"f1, f0"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，只选择Tuple的第二个元素，指定字段名为"f1"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"f1"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，为Tuple的第二个元素指定别名为myString，为第一个元素指定字段名为myLong</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"f1 as 'myString', f0 as 'myLong'"</span><span class="comment">)</span>;</span><br></pre></td></tr></table></figure><h4 id="POJO类型"><a href="#POJO类型" class="headerlink" title="POJO类型"></a>POJO类型</h4><p>当将POJO类型的DataStream或者DataSet转为表时，如果不指定表名，则默认使用的是POJO字段本身的名称，原始字段名称的映射需要指定原始字段的名称，可以为其起一个别名，也可以调换字段的顺序，也可以只选择部分的字段。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">//数据类型为Person的POJO类型，字段包括"name"和"age"</span></span><br><span class="line">DataStream&lt;Person&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名称为"age", "name"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream)</span>;</span><br><span class="line"><span class="comment">//  将DataStream转为表，为"age"字段指定别名myAge, 为"name"字段指定别名myName</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"age as myAge, name as myName"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">//  将DataStream转为表，只选择一个name字段</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"name"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">//  将DataStream转为表，只选择一个name字段，并起一个别名myName</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"name as myName"</span><span class="comment">)</span>;</span><br></pre></td></tr></table></figure><h4 id="Row类型"><a href="#Row类型" class="headerlink" title="Row类型"></a>Row类型</h4><p>Row类型的DataStream或者DataSet转为表的过程中，可以根据字段的位置或者字段名称进行映射，同时也可以为字段起一个别名，或者只选择部分字段。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// Row类型的DataStream，通过RowTypeInfo指定两个字段"name"和"age"</span></span><br><span class="line">DataStream&lt;Row&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为原始字段名"name"和"age"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据位置映射，为第一个字段指定myName别名，为第二个字段指定myAge别名</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"myName, myAge"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，为name字段起别名myName，为age字段起别名myAge</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"name as myName, age as myAge"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，只选择name字段</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"name"</span><span class="comment">)</span>;</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，只选择name字段，并起一个别名"myName"</span></span><br><span class="line"><span class="keyword">Table</span> <span class="keyword">table</span> <span class="comment">= tableEnv.fromDataStream(stream,</span> <span class="comment">"name as myName"</span><span class="comment">)</span>;</span><br></pre></td></tr></table></figure><h2 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h2><h3 id="Old-planner-1"><a href="#Old-planner-1" class="headerlink" title="Old planner"></a>Old planner</h3><p>Apache Flink利用Apache Calcite来优化和转换查询。当前执行的优化包括投影和过滤器下推，去相关子查询以及其他类型的查询重写。Old Planner目前不支持优化JOIN的顺序，而是按照查询中定义的顺序执行它们。</p><p>通过提供一个<code>CalciteConfig</code>对象，可以调整在不同阶段应用的优化规则集。这可通过调用<code>CalciteConfig.createBuilder()</code>方法来进行创建，并通过调用<code>tableEnv.getConfig.setPlannerConfig(calciteConfig)</code>方法将该对象传递给TableEnvironment。</p><h3 id="Blink-planner-1"><a href="#Blink-planner-1" class="headerlink" title="Blink planner"></a>Blink planner</h3><p>Apache Flink利用并扩展了Apache Calcite来执行复杂的查询优化。这包括一系列基于规则和基于成本的优化(cost_based)，例如：</p><ul><li>基于Apache Calcite的去相关子查询</li><li>投影裁剪</li><li>分区裁剪</li><li>过滤器谓词下推</li><li>过滤器下推</li><li>子计划重复数据删除以避免重复计算</li><li>特殊的子查询重写，包括两个部分：<ul><li>将IN和EXISTS转换为左半联接( left semi-join)</li><li>将NOT IN和NOT EXISTS转换为left anti-join</li></ul></li><li>调整join的顺序，需要启用 <code>table.optimizer.join-reorder-enabled</code></li></ul><p><strong>注意：</strong> IN / EXISTS / NOT IN / NOT EXISTS当前仅在子查询重写的结合条件下受支持。</p><p>查询优化器不仅基于计划，而且还可以基于数据源的统计信息以及每个操作的细粒度开销(例如io，cpu，网络和内存）,从而做出更加明智且合理的优化决策。</p><p>高级用户可以通过<code>CalciteConfig</code>对象提供自定义优化规则，通过调用tableEnv.getConfig.setPlannerConfig(calciteConfig)，将参数传递给TableEnvironment。</p><h3 id="查看执行计划"><a href="#查看执行计划" class="headerlink" title="查看执行计划"></a>查看执行计划</h3><p>SQL语言支持通过explain来查看某条SQL的执行计划，Flink Table API也可以通过调用explain()方法来查看具体的执行计划。该方法返回一个字符串用来描述三个部分计划，分别为：</p><ol><li>关系查询的抽象语法树，即未优化的逻辑查询计划，</li><li>优化的逻辑查询计划</li><li>实际执行计划</li></ol><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = <span class="module-access"><span class="module"><span class="identifier">StreamExecutionEnvironment</span>.</span></span>get<span class="constructor">ExecutionEnvironment()</span>;</span><br><span class="line">StreamTableEnvironment tEnv = <span class="module-access"><span class="module"><span class="identifier">StreamTableEnvironment</span>.</span></span>create(env);</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.from<span class="constructor">Elements(<span class="params">new</span> Tuple2&lt;&gt;(1, <span class="string">"hello"</span>)</span>);</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.from<span class="constructor">Elements(<span class="params">new</span> Tuple2&lt;&gt;(1, <span class="string">"hello"</span>)</span>);</span><br><span class="line">Table table1 = tEnv.from<span class="constructor">DataStream(<span class="params">stream1</span>, <span class="string">"count, word"</span>)</span>;</span><br><span class="line">Table table2 = tEnv.from<span class="constructor">DataStream(<span class="params">stream2</span>, <span class="string">"count, word"</span>)</span>;</span><br><span class="line">Table table = table1</span><br><span class="line">  .where(<span class="string">"LIKE(word, 'F%')"</span>)</span><br><span class="line">  .union<span class="constructor">All(<span class="params">table2</span>)</span>;</span><br><span class="line"><span class="comment">// 查看执行计划</span></span><br><span class="line">String explanation = tEnv.explain(table);</span><br><span class="line"><span class="module-access"><span class="module"><span class="identifier">System</span>.</span></span>out.println(explanation);</span><br></pre></td></tr></table></figure><p>执行计划的结果为：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">== 抽象语法树 ==</span><br><span class="line">LogicalUnion(<span class="keyword">all</span>=[<span class="keyword">true</span>])</span><br><span class="line">  LogicalFilter(condition=[<span class="keyword">LIKE</span>(<span class="meta">$1</span>, _UTF<span class="number">-16</span>L<span class="string">E'F%'</span>)])</span><br><span class="line">    FlinkLogicalDataStreamScan(id=[<span class="number">1</span>], fields=[count, word])</span><br><span class="line">  FlinkLogicalDataStreamScan(id=[<span class="number">2</span>], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== 优化的逻辑执行计划 ==</span><br><span class="line">DataStreamUnion(<span class="keyword">all</span>=[<span class="keyword">true</span>], <span class="keyword">union</span> <span class="keyword">all</span>=[count, word])</span><br><span class="line">  DataStreamCalc(<span class="keyword">select</span>=[count, word], <span class="keyword">where</span>=[<span class="keyword">LIKE</span>(word, _UTF<span class="number">-16</span>L<span class="string">E'F%'</span>)])</span><br><span class="line">    DataStreamScan(id=[<span class="number">1</span>], fields=[count, word])</span><br><span class="line">  DataStreamScan(id=[<span class="number">2</span>], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== 物理执行计划 ==</span><br><span class="line">Stage <span class="number">1</span> : Data Source</span><br><span class="line">content : collect elements <span class="keyword">with</span> CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage <span class="number">2</span> : Data Source</span><br><span class="line">content : collect elements <span class="keyword">with</span> CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage <span class="number">3</span> : <span class="keyword">Operator</span></span><br><span class="line">content : <span class="keyword">from</span>: (count, word)</span><br><span class="line">ship_strategy : REBALANCE</span><br><span class="line"></span><br><span class="line">Stage <span class="number">4</span> : <span class="keyword">Operator</span></span><br><span class="line">content : <span class="keyword">where</span>: (<span class="keyword">LIKE</span>(word, _UTF<span class="number">-16</span>L<span class="string">E'F%'</span>)), <span class="keyword">select</span>: (count, word)</span><br><span class="line">ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">Stage <span class="number">5</span> : <span class="keyword">Operator</span></span><br><span class="line">content : <span class="keyword">from</span>: (count, word)</span><br><span class="line">ship_strategy : REBALANCE</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了Flink TableAPI &amp;SQL，首先介绍了Flink Table API &amp;SQL的基本概念 ，然后介绍了构建Flink Table API &amp; SQL程序所需要的依赖，接着介绍了Flink的两种planner，还介绍了如何注册表以及DataStream、DataSet与表的相互转换，最后介绍了Flink的两种planner对应的查询优化并给出了一个查看执行计划的案例。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Su Command in Linux (Switch User)，链接：<a href="https://linuxize.com/post/su-command-in-linux/" target="_blank" rel="noopener">https://linuxize.com/post/su-command-in-linux/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分 &lt;code&gt;su&lt;/code&gt;命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Lua语言系列--语言基础</title>
    <link href="https://zjrongxiang.github.io/2021/05/23/2021-05-27-Lua%E8%AF%AD%E8%A8%80%E7%B3%BB%E5%88%97--%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80/"/>
    <id>https://zjrongxiang.github.io/2021/05/23/2021-05-27-Lua语言系列--语言基础/</id>
    <published>2021-05-23T05:30:00.000Z</published>
    <updated>2021-06-21T13:28:14.097Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   <code>Lua</code>语言入门</li><li>第二部分  数值</li><li>第三部分  字符串</li><li>第四部分 表</li><li>第五部分 函数</li><li>第六部分 输入和输出</li><li>第七部分 知识补充</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="第一部分-Lua语言入门"><a href="#第一部分-Lua语言入门" class="headerlink" title="第一部分   Lua语言入门"></a>第一部分   <code>Lua</code>语言入门</h2><p>Lua是解释型语言。</p><h3 id="1-1-程序段"><a href="#1-1-程序段" class="headerlink" title="1.1 程序段"></a>1.1 程序段</h3><h3 id="1-2-语法规范"><a href="#1-2-语法规范" class="headerlink" title="1.2 语法规范"></a>1.2 语法规范</h3><p>Lua语言中标识符（名称）由任意字母、数值和下划线组成的字符串，但是不能以数值开头。</p><p>Lua中关键字（保留字）：</p><ul><li><p>逻辑运算关键字：and、 or、not</p></li><li><p>基本类型：function、table、nil</p></li><li><p>控制类：for、 while、do 、break、in、return、until、goto、repeat</p></li><li><p>逻辑变量：true、false</p></li><li><p>if控制类：if、then 、else、elseif</p></li></ul><ul><li>变量作用域：local</li></ul><p>Lua语言对于大小写敏感。</p><p>Lua语言使用连字符<code>--</code>作为单行注释。多行注释为：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--[[</span></span><br><span class="line"><span class="comment">print("多行注释")</span></span><br><span class="line"><span class="comment">--]]</span></span><br></pre></td></tr></table></figure><h3 id="1-3-全局变量"><a href="#1-3-全局变量" class="headerlink" title="1.3 全局变量"></a>1.3 全局变量</h3><p>Lua语言中，全局变量无需声明，可以直接使用。没有初始化的全局变量初始值为nil。</p><h3 id="1-4-类型和值"><a href="#1-4-类型和值" class="headerlink" title="1.4 类型和值"></a>1.4 类型和值</h3><p>Lua语言属于动态语言。</p><p>lua语言中有8种基本类型。</p><ul><li>nil（空）</li><li>boolean（布尔）</li><li>number（数值）</li><li>string（字符串）</li><li>userdata（用户数据）</li><li>function（函数）</li><li>thread（线程）</li><li>table（表）</li></ul><p>可以使用函数type来返回变量数据类型。注意type函数返回的是一个字符串。</p><h3 id="1-6-练习"><a href="#1-6-练习" class="headerlink" title="1.6 练习"></a>1.6 练习</h3><h2 id="第二部分-数值"><a href="#第二部分-数值" class="headerlink" title="第二部分  数值"></a>第二部分  数值</h2><h2 id="第三部分-字符串"><a href="#第三部分-字符串" class="headerlink" title="第三部分  字符串"></a>第三部分  字符串</h2><h2 id="第四部分-表"><a href="#第四部分-表" class="headerlink" title="第四部分 表"></a>第四部分 表</h2><h2 id="第五部分-函数"><a href="#第五部分-函数" class="headerlink" title="第五部分 函数"></a>第五部分 函数</h2><h2 id="第六部分-输入和输出"><a href="#第六部分-输入和输出" class="headerlink" title="第六部分 输入和输出"></a>第六部分 输入和输出</h2><h2 id="第七部分-知识补充"><a href="#第七部分-知识补充" class="headerlink" title="第七部分 知识补充"></a>第七部分 知识补充</h2><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   &lt;code&gt;Lua&lt;/code&gt;语言入门&lt;/li&gt;
&lt;li&gt;第二部分  数值&lt;/li&gt;
&lt;
      
    
    </summary>
    
      <category term="Nginx" scheme="https://zjrongxiang.github.io/categories/Nginx/"/>
    
    
  </entry>
  
  <entry>
    <title>Nginx使用说明</title>
    <link href="https://zjrongxiang.github.io/2021/05/23/2021-05-23-Nginx%E5%8E%9F%E7%90%86/"/>
    <id>https://zjrongxiang.github.io/2021/05/23/2021-05-23-Nginx原理/</id>
    <published>2021-05-23T05:30:00.000Z</published>
    <updated>2021-07-04T04:45:22.241Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   回本溯源</li><li>第二部分  <code>HDFS</code>大量小文件的危害</li><li>第三部分  小文件治理方案总结</li><li>第四部分 总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>多进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root     55976     1  0 Jul03 ?        00:00:00 nginx: master process nginx -p /usr/local/orange -c /usr/local/orange/conf/nginx.conf</span><br><span class="line">nobody   55977 55976  0 Jul03 ?        00:00:01 nginx: worker process         </span><br><span class="line">nobody   55978 55976  0 Jul03 ?        00:00:00 nginx: worker process         </span><br><span class="line">nobody   55979 55976  0 Jul03 ?        00:00:00 nginx: worker process         </span><br><span class="line">nobody   55980 55976  0 Jul03 ?        00:00:00 nginx: worker process</span><br></pre></td></tr></table></figure><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] <code>HDFS NameNode</code>内存全景，链接：<a href="https://tech.meituan.com/2016/08/26/namenode.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/08/26/namenode.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   回本溯源&lt;/li&gt;
&lt;li&gt;第二部分  &lt;code&gt;HDFS&lt;/code&gt;大量小文件的危害
      
    
    </summary>
    
      <category term="Nginx" scheme="https://zjrongxiang.github.io/categories/Nginx/"/>
    
    
  </entry>
  
  <entry>
    <title>揭开HDFS存储的面纱</title>
    <link href="https://zjrongxiang.github.io/2021/05/15/2021-05-15-%E6%8F%AD%E5%BC%80HDFS%E5%AD%98%E5%82%A8%E7%9A%84%E9%9D%A2%E7%BA%B1/"/>
    <id>https://zjrongxiang.github.io/2021/05/15/2021-05-15-揭开HDFS存储的面纱/</id>
    <published>2021-05-15T05:30:00.000Z</published>
    <updated>2021-05-15T16:21:49.643Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   回本溯源</li><li>第二部分  <code>HDFS</code>大量小文件的危害</li><li>第三部分  小文件治理方案总结</li><li>第四部分 总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://blog.csdn.net/m0_37613244/article/details/109920466?utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control" target="_blank" rel="noopener">https://blog.csdn.net/m0_37613244/article/details/109920466?utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control</a></p><p>HDFS Namenode本地目录的存储结构和Datanode数据块存储目录结构，也就是hdfs-site.xml中配置的dfs.namenode.name.dir和dfs.namenode.data.dir</p><h2 id="第一部分-NameNode元数据"><a href="#第一部分-NameNode元数据" class="headerlink" title="第一部分 NameNode元数据"></a>第一部分 NameNode元数据</h2><h2 id="第二部分-DataNode数据"><a href="#第二部分-DataNode数据" class="headerlink" title="第二部分 DataNode数据"></a>第二部分 DataNode数据</h2><p>You need to look in your <strong>hdfs-default.xml</strong> configuration file for the <strong>dfs.data.dir</strong> setting. The default setting is: <strong>${hadoop.tmp.dir}/dfs/data</strong> and note that the ${hadoop.tmp.dir} is actually in core-default.xml described <a href="http://hadoop.apache.org/docs/r1.2.1/core-default.html" target="_blank" rel="noopener">here</a>.</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] <code>HDFS NameNode</code>内存全景，链接：<a href="https://tech.meituan.com/2016/08/26/namenode.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/08/26/namenode.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   回本溯源&lt;/li&gt;
&lt;li&gt;第二部分  &lt;code&gt;HDFS&lt;/code&gt;大量小文件的危害
      
    
    </summary>
    
      <category term="HDFS" scheme="https://zjrongxiang.github.io/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>Yarn资源调度的</title>
    <link href="https://zjrongxiang.github.io/2021/05/15/2021-05-15-%E6%8F%AD%E5%BC%80HDFS%E5%AD%98%E5%82%A8%E7%9A%84%E9%9D%A2%E7%BA%B1%20-%20%E5%89%AF%E6%9C%AC/"/>
    <id>https://zjrongxiang.github.io/2021/05/15/2021-05-15-揭开HDFS存储的面纱 - 副本/</id>
    <published>2021-05-15T05:30:00.000Z</published>
    <updated>2021-05-24T07:42:45.422Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   回本溯源</li><li>第二部分  <code>HDFS</code>大量小文件的危害</li><li>第三部分  小文件治理方案总结</li><li>第四部分 总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>yarn.resourcemanager.store.class : 有三种StateStore，分别是基于zookeeper, HDFS, leveldb, HA高可用集群必须用ZKRMStateStore</p><table><thead><tr><th style="text-align:left">存储</th><th style="text-align:left">yarn.resourcemanager.store.class</th></tr></thead><tbody><tr><td style="text-align:left">ZooKeeper</td><td style="text-align:left">org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</td></tr><tr><td style="text-align:left">FileSystem</td><td style="text-align:left">org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore</td></tr><tr><td style="text-align:left">LevelDB</td><td style="text-align:left">org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore</td></tr></tbody></table><p>By default the number of completed applications stored in state store is 10000. <a href="https://maprdocs.mapr.com/51/ReferenceGuide/Default-YARN-Parameters.html" target="_blank" rel="noopener">https://maprdocs.mapr.com/51/ReferenceGuide/Default-YARN-Parameters.html</a></p><p>Try to move/delete some completed applications</p><p>hadoop fs -mv /var/mapr/cluster/yarn/rm/system/FSRMStateRoot/RMAppRoot/* /path_to_local_dir</p><p>hadoop conf | grep yarn.resourcemanager.max-completed-applications</p><p><a href="https://www.programmersought.com/article/36321434084/" target="_blank" rel="noopener">https://www.programmersought.com/article/36321434084/</a></p><p><a href="https://issues.apache.org/jira/browse/YARN-7150" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/YARN-7150</a></p><p><a href="https://my.oschina.net/dabird/blog/3089265" target="_blank" rel="noopener">https://my.oschina.net/dabird/blog/3089265</a></p><p><a href="https://cloud.tencent.com/developer/article/1491079" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1491079</a></p><p><a href="https://my.oschina.net/dabird/blog/4273830" target="_blank" rel="noopener">https://my.oschina.net/dabird/blog/4273830</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] <code>HDFS NameNode</code>内存全景，链接：<a href="https://tech.meituan.com/2016/08/26/namenode.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/08/26/namenode.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   回本溯源&lt;/li&gt;
&lt;li&gt;第二部分  &lt;code&gt;HDFS&lt;/code&gt;大量小文件的危害
      
    
    </summary>
    
      <category term="HDFS" scheme="https://zjrongxiang.github.io/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>Pyspark实现原理和源码分析</title>
    <link href="https://zjrongxiang.github.io/2021/05/06/2020-10-06-Pyspark%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2021/05/06/2020-10-06-Pyspark实现原理总结/</id>
    <published>2021-05-06T05:30:00.000Z</published>
    <updated>2021-05-16T09:40:11.720Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  常用快捷键</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://blog.csdn.net/oTengYue/article/details/105379628" target="_blank" rel="noopener">https://blog.csdn.net/oTengYue/article/details/105379628</a></p><p><a href="https://www.readfog.com/a/1631040025628086272" target="_blank" rel="noopener">https://www.readfog.com/a/1631040025628086272</a></p><p>spark为了保证核心架构的统一性，在核心架构外围封装了一层python，spark的核心架构功能包括计算资源的申请，task的管理和分配， driver与executor之间的通信，executor之间的通信，rdd的载体等都是在基于JVM的</p><p>spark的这种设计可以说是非常方便的去进行多种开发语言的扩展。但是也可以明显看出与在jvm内部运行的udf相比，在python worker中执行udf时，额外增加了数据在executor jvm和pythin worker之间序列化、反序列化、及通信IO等损耗，并且在程序运行上python相比java的具有一定的性能劣势。在计算逻辑比重比较大的spark任务中，使用自定义udf的pyspark程序会明显有更多的性能损耗。当然在spark sql 中使用内置udf会降低或除去上述描述中产生的性能差异。</p><p>程序模型提交命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@quickstart pysparkExample]# cat run.sh </span><br><span class="line">/usr/lib/spark/bin/spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--archives hdfs:///user/admin/python/python3.5.2.zip \</span><br><span class="line">--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./python3.5.2.zip/conda/bin/python \</span><br><span class="line">test.py</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">[root@quickstart pysparkExample]# ./run.sh</span><br><span class="line">21/05/16 00:02:32 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/172.17.0.2:8032</span><br><span class="line">21/05/16 00:02:32 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers</span><br><span class="line">21/05/16 00:02:32 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (2816 MB per container)</span><br><span class="line">21/05/16 00:02:32 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead</span><br><span class="line">21/05/16 00:02:32 INFO yarn.Client: Setting up container launch context for our AM</span><br><span class="line">21/05/16 00:02:32 INFO yarn.Client: Setting up the launch environment for our AM container</span><br><span class="line">21/05/16 00:02:32 INFO yarn.Client: Preparing resources for our AM container</span><br><span class="line">21/05/16 00:02:33 INFO yarn.YarnSparkHadoopUtil: getting token for namenode: hdfs://quickstart.cloudera:8020/user/admin/.sparkStaging/application_1621088965108_0002</span><br><span class="line">21/05/16 00:02:33 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 9 for admin on 172.17.0.2:8020</span><br><span class="line">21/05/16 00:02:34 INFO hive.metastore: Trying to connect to metastore with URI thrift://quickstart.cloudera:9083</span><br><span class="line">21/05/16 00:02:34 INFO hive.metastore: Opened a connection to metastore, current connections: 1</span><br><span class="line">21/05/16 00:02:34 INFO hive.metastore: Connected to metastore.</span><br><span class="line">21/05/16 00:02:34 INFO hive.metastore: Closed a connection to metastore, current connections: 0</span><br><span class="line">21/05/16 00:02:34 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/admin/python/python3.5.2.zip</span><br><span class="line">21/05/16 00:02:34 INFO yarn.Client: Uploading resource file:/home/pyspark/pysparkExample/test.py -&gt; hdfs://quickstart.cloudera:8020/user/admin/.sparkStaging/application_1621088965108_0002/test.py</span><br><span class="line">21/05/16 00:02:34 INFO yarn.Client: Uploading resource file:/tmp/spark-f65e84d5-0438-473c-9dff-03aeb95d4f18/__spark_conf__7787627711692930444.zip -&gt; hdfs://quickstart.cloudera:8020/user/admin/.sparkStaging/application_1621088965108_0002/__spark_conf__7787627711692930444.zip</span><br><span class="line">21/05/16 00:02:35 INFO spark.SecurityManager: Changing view acls to: root,admin</span><br><span class="line">21/05/16 00:02:35 INFO spark.SecurityManager: Changing modify acls to: root,admin</span><br><span class="line">21/05/16 00:02:35 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, admin); users with modify permissions: Set(root, admin)</span><br><span class="line">21/05/16 00:02:35 INFO yarn.Client: Submitting application 2 to ResourceManager</span><br><span class="line">21/05/16 00:02:35 INFO impl.YarnClientImpl: Submitted application application_1621088965108_0002</span><br><span class="line">21/05/16 00:02:36 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)</span><br><span class="line">21/05/16 00:02:36 INFO yarn.Client: </span><br><span class="line"> client token: Token &#123; kind: YARN_CLIENT_TOKEN, service:  &#125;</span><br><span class="line"> diagnostics: N/A</span><br><span class="line"> ApplicationMaster host: N/A</span><br><span class="line"> ApplicationMaster RPC port: -1</span><br><span class="line"> queue: root.admin</span><br><span class="line"> start time: 1621094555087</span><br><span class="line"> final status: UNDEFINED</span><br><span class="line"> tracking URL: http://quickstart.cloudera:8088/proxy/application_1621088965108_0002/</span><br><span class="line"> user: admin</span><br><span class="line">21/05/16 00:02:37 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)</span><br><span class="line">21/05/16 00:02:38 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)</span><br><span class="line">21/05/16 00:02:39 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)</span><br><span class="line">21/05/16 00:02:40 INFO yarn.Client: Application report for application_1621088965108_0002 (state: FINISHED)</span><br><span class="line">21/05/16 00:02:40 INFO yarn.Client: </span><br><span class="line"> client token: Token &#123; kind: YARN_CLIENT_TOKEN, service:  &#125;</span><br><span class="line"> diagnostics: N/A</span><br><span class="line"> ApplicationMaster host: 172.17.0.2</span><br><span class="line"> ApplicationMaster RPC port: 0</span><br><span class="line"> queue: root.admin</span><br><span class="line"> start time: 1621094555087</span><br><span class="line"> final status: SUCCEEDED</span><br><span class="line"> tracking URL: http://quickstart.cloudera:8088/proxy/application_1621088965108_0002/history/application_1621088965108_0002/1</span><br><span class="line"> user: admin</span><br><span class="line">21/05/16 00:02:40 INFO yarn.Client: Deleting staging directory .sparkStaging/application_1621088965108_0002</span><br><span class="line">21/05/16 00:02:40 INFO util.ShutdownHookManager: Shutdown hook called</span><br><span class="line">21/05/16 00:02:40 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f65e84d5-0438-473c-9dff-03aeb95d4f18</span><br></pre></td></tr></table></figure><p>yarn日志：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">21/05/15 16:19:29 INFO yarn.ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]</span><br><span class="line">21/05/15 16:19:30 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1621095402395_0001_000001</span><br><span class="line">21/05/15 16:19:30 INFO spark.SecurityManager: Changing view acls to: admin</span><br><span class="line">21/05/15 16:19:30 INFO spark.SecurityManager: Changing modify acls to: admin</span><br><span class="line">21/05/15 16:19:30 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)</span><br><span class="line">21/05/15 16:19:30 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread</span><br><span class="line">21/05/15 16:19:30 INFO yarn.ApplicationMaster: Waiting for spark context initialization</span><br><span class="line">21/05/15 16:19:30 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... </span><br><span class="line">21/05/15 16:19:31 INFO spark.SparkContext: Running Spark version 1.6.0</span><br><span class="line">21/05/15 16:19:31 INFO spark.SecurityManager: Changing view acls to: admin</span><br><span class="line">21/05/15 16:19:31 INFO spark.SecurityManager: Changing modify acls to: admin</span><br><span class="line">21/05/15 16:19:31 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)</span><br><span class="line">21/05/15 16:19:31 INFO util.Utils: Successfully started service 'sparkDriver' on port 46545.</span><br><span class="line">21/05/15 16:19:31 INFO slf4j.Slf4jLogger: Slf4jLogger started</span><br><span class="line">21/05/15 16:19:31 INFO Remoting: Starting remoting</span><br><span class="line">21/05/15 16:19:31 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.17.0.2:35921]</span><br><span class="line">21/05/15 16:19:31 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@172.17.0.2:35921]</span><br><span class="line">21/05/15 16:19:31 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 35921.</span><br><span class="line">21/05/15 16:19:31 INFO spark.SparkEnv: Registering MapOutputTracker</span><br><span class="line">21/05/15 16:19:31 INFO spark.SparkEnv: Registering BlockManagerMaster</span><br><span class="line">21/05/15 16:19:31 INFO storage.DiskBlockManager: Created local directory at /yarn/nm/usercache/admin/appcache/application_1621095402395_0001/blockmgr-edbfee4f-522d-4c06-81a0-5b83b750e88a</span><br><span class="line">21/05/15 16:19:31 INFO storage.MemoryStore: MemoryStore started with capacity 491.7 MB</span><br><span class="line">21/05/15 16:19:31 INFO spark.SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">21/05/15 16:19:31 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter</span><br><span class="line">21/05/15 16:19:31 INFO util.Utils: Successfully started service 'SparkUI' on port 43419.</span><br><span class="line">21/05/15 16:19:31 INFO ui.SparkUI: Started SparkUI at http://172.17.0.2:43419</span><br><span class="line">21/05/15 16:19:31 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler</span><br><span class="line">21/05/15 16:19:31 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40989.</span><br><span class="line">21/05/15 16:19:31 INFO netty.NettyBlockTransferService: Server created on 40989</span><br><span class="line">21/05/15 16:19:31 INFO storage.BlockManager: external shuffle service port = 7337</span><br><span class="line">21/05/15 16:19:31 INFO storage.BlockManagerMaster: Trying to register BlockManager</span><br><span class="line">21/05/15 16:19:31 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.2:40989 with 491.7 MB RAM, BlockManagerId(driver, 172.17.0.2, 40989)</span><br><span class="line">21/05/15 16:19:31 INFO storage.BlockManagerMaster: Registered BlockManager</span><br><span class="line">21/05/15 16:19:32 INFO scheduler.EventLoggingListener: Logging events to hdfs://quickstart.cloudera:8020/user/spark/applicationHistory/application_1621095402395_0001_1</span><br><span class="line">21/05/15 16:19:32 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8</span><br><span class="line">21/05/15 16:19:32 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done</span><br><span class="line">21/05/15 16:19:32 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@172.17.0.2:46545)</span><br><span class="line">21/05/15 16:19:32 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/172.17.0.2:8030</span><br><span class="line">21/05/15 16:19:32 INFO yarn.YarnRMClient: Registering the ApplicationMaster</span><br><span class="line">21/05/15 16:19:32 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals</span><br><span class="line">21/05/15 16:19:32 INFO yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0</span><br><span class="line">21/05/15 16:19:32 INFO spark.SparkContext: Invoking stop() from shutdown hook</span><br><span class="line">21/05/15 16:19:32 INFO ui.SparkUI: Stopped Spark web UI at http://172.17.0.2:43419</span><br><span class="line">21/05/15 16:19:32 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors</span><br><span class="line">21/05/15 16:19:32 INFO cluster.YarnClusterSchedulerBackend: Asking each executor to shut down</span><br><span class="line">21/05/15 16:19:32 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line">21/05/15 16:19:32 INFO storage.MemoryStore: MemoryStore cleared</span><br><span class="line">21/05/15 16:19:32 INFO storage.BlockManager: BlockManager stopped</span><br><span class="line">21/05/15 16:19:32 INFO storage.BlockManagerMaster: BlockManagerMaster stopped</span><br><span class="line">21/05/15 16:19:32 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span><br><span class="line">21/05/15 16:19:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.</span><br><span class="line">21/05/15 16:19:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.</span><br><span class="line">21/05/15 16:19:32 INFO spark.SparkContext: Successfully stopped SparkContext</span><br><span class="line">21/05/15 16:19:32 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED</span><br><span class="line">21/05/15 16:19:32 INFO Remoting: Remoting shut down</span><br><span class="line">21/05/15 16:19:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.</span><br><span class="line">21/05/15 16:19:32 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.</span><br><span class="line">21/05/15 16:19:32 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1621095402395_0001</span><br><span class="line">21/05/15 16:19:32 INFO util.ShutdownHookManager: Shutdown hook called</span><br><span class="line">21/05/15 16:19:32 INFO util.ShutdownHookManager: Deleting directory /yarn/nm/usercache/admin/appcache/application_1621095402395_0001/spark-76c43a00-f562-4e4e-be1a-be3a2fcefc21/pyspark-b56e6390-71a2-4475-8ffe-4164798ab6c4</span><br><span class="line">21/05/15 16:19:32 INFO util.ShutdownHookManager: Deleting directory /yarn/nm/usercache/admin/appcache/application_1621095402395_0001/spark-76c43a00-f562-4e4e-be1a-be3a2fcefc21</span><br></pre></td></tr></table></figure><p><a href="http://sharkdtu.com/posts/pyspark-internal.html" target="_blank" rel="noopener">http://sharkdtu.com/posts/pyspark-internal.html</a></p><p><a href="https://cloud.tencent.com/developer/article/1589011" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1589011</a></p><p><a href="https://cloud.tencent.com/developer/article/1558621" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1558621</a></p><p><a href="https://www.nativex.com/cn/blog/2019-12-27-2/" target="_blank" rel="noopener">https://www.nativex.com/cn/blog/2019-12-27-2/</a></p><p>但是在大数据场景下，JVM和Python进程间频繁的数据通信导致其性能损耗较多，恶劣时还可能会直接卡死，所以建议对于大规模机器学习或者Streaming应用场景还是慎用PySpark，尽量使用原生的Scala/Java编写应用程序，对于中小规模数据量下的简单离线任务，可以使用PySpark快速部署提交。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Job Scheduling，链接：<a href="https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  常用快捷键&lt;/li&gt;
&lt;li&gt;参考文献及资料&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;背景&quot;&gt;
      
    
    </summary>
    
      <category term="spark" scheme="https://zjrongxiang.github.io/categories/spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark on Yarn任务动态伸缩机制介绍</title>
    <link href="https://zjrongxiang.github.io/2021/05/02/2021-05-02-Spark%20on%20Yarn%E4%BB%BB%E5%8A%A1%E5%8A%A8%E6%80%81%E4%BC%B8%E7%BC%A9%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D/"/>
    <id>https://zjrongxiang.github.io/2021/05/02/2021-05-02-Spark on Yarn任务动态伸缩机制介绍/</id>
    <published>2021-05-02T05:30:00.000Z</published>
    <updated>2021-05-03T17:39:04.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  配置实现</li><li>第二部分 动态配置原理和源码分析</li><li>第三部分 总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><code>Spark</code>默认使用的是资源预分配的模式。即在任务运行之前，需要提前指定任务运行需要的资源量。但是在实际线上生产环境使用过程就存在资源浪费和不足的问题，特别是<code>Spark Streaming</code>类型的任务。例如很多日志数据在一天中量并不是均匀分布的，而是一个“双驼峰”。对于预分配模式，就存在日志峰值期间，运算资源不足导致数据处理的延迟，而在日志低峰时期存在资源闲置却无法释放（特别是资源管理器粗粒度模式）。使得生产线上环境资源未能高效使用。</p><p><img src="D:\myblog\source\_posts\images\picture\spark DRA\dynamic-allocation-in-spark-19-638.jpg" alt="dynamic-allocation-in-spark-19-638"></p><p><code>Spark</code>在<code>Spark 1.2</code>版本后，对于<code>Spark On Yarn</code>模式，开始支持动态资源分配（<code>Dynamic Resource Allocation</code>，后文我们也简称<code>DRA</code>）。该机制下<code>Spark Core</code>和<code>Spark Streaming</code>任务就可以根据<code>Application</code>的负载情况，动态的增加和减少<code>Executors</code>。</p><p><img src="D:\myblog\source\_posts\images\picture\spark DRA\dynamic-allocation-in-spark-20-638.jpg" alt="dynamic-allocation-in-spark-20-638"></p><h2 id="第一部分-配置实现"><a href="#第一部分-配置实现" class="headerlink" title="第一部分 配置实现"></a>第一部分 配置实现</h2><p>对于<code>Spark on Yarn</code>模式需要提前配置Yarn服务，主要是配置<code>External shuffle service</code>（<code>Spark 1.2</code>开始引入）。<code>Spark</code>计算需要<code>shuffle</code>时候，每个<code>Executor</code> 需要把上一个 <code>stage</code> 的 <code>mapper</code> 输出写入磁盘，然后作为 <code>server</code> 等待下一个<code>stage</code> 的<code>reducer</code> 来获取 map 的输出。因此如果 <code>Executor</code> 在 <code>map</code> 阶段完成后被回收，<code>reducer</code> 将无法找到 <code>block</code>的位置。所以开启 <code>Dynamic Resource Allocation</code> 时，必须开启 <code>External shuffle service</code>。这样，mapper 的输出位置（元数据信息）将会由 <code>External shuffle service</code>（长期运行的守护进程） 来登记保存，<code>Executor</code> 不需要再保留状态信息，可以安全回收。</p><h3 id="1-1-Yarn服务配置"><a href="#1-1-Yarn服务配置" class="headerlink" title="1.1 Yarn服务配置"></a>1.1 Yarn服务配置</h3><p>首先需要对<code>Yarn</code>的<code>NodeManager</code>服务进行配置，使其支持<code>Spark</code>的<code>Shuffle Service</code>。</p><ul><li><p>修改每台<code>NodeManager</code>上的配置文件<code>yarn-site.xml</code>：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--修改和增加--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle,spark_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">　<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">　<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">　<span class="tag">&lt;<span class="name">name</span>&gt;</span>spark.shuffle.service.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">　<span class="tag">&lt;<span class="name">value</span>&gt;</span>7337<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置服务依赖包。将<code>$SPARK_HOME/lib/spark-1.6.0-yarn-shuffle.jar</code>（注意实际版本号）复制到每台<code>NodeManager</code>的<code>${HADOOP_HOME}/share/hadoop/yarn/lib/</code>下。</p></li><li>重启所有<code>NodeManager</code>生效配置调整。</li></ul><h3 id="1-2-Spark-core-任务配置"><a href="#1-2-Spark-core-任务配置" class="headerlink" title="1.2 Spark core 任务配置"></a>1.2 Spark core 任务配置</h3><h4 id="1-2-1-配置方法"><a href="#1-2-1-配置方法" class="headerlink" title="1.2.1 配置方法"></a>1.2.1 配置方法</h4><p>通常配置<code>Saprk</code>应用任务的参数有三种方式：</p><ul><li><p>修改配置文件<code>spark-defaults.conf</code>，全局生效；</p><p>配置文件位置：<code>$SPARK_HOME/conf/spark-defaults.conf</code>，具体参数如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//启用External shuffle Service服务</span><br><span class="line">spark.shuffle.service.enabled true</span><br><span class="line">//Shuffle Service服务端口，必须和yarn-site中的一致</span><br><span class="line">spark.shuffle.service.port 7337</span><br><span class="line">//开启动态资源分配</span><br><span class="line">spark.dynamicAllocation.enabled true</span><br><span class="line">//每个Application最小分配的executor数</span><br><span class="line">spark.dynamicAllocation.minExecutors 1</span><br><span class="line">//每个Application最大并发分配的executor数</span><br><span class="line">spark.dynamicAllocation.maxExecutors 30</span><br><span class="line">spark.dynamicAllocation.schedulerBacklogTimeout 1s</span><br><span class="line">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s</span><br></pre></td></tr></table></figure></li><li><p>spark-submit 命令配置，个性化生效；</p><p>参考下面的案例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master yarn-cluster \</span><br><span class="line">    --driver-cores 2 \</span><br><span class="line">    --driver-memory 2G \</span><br><span class="line">    --num-executors 10 \</span><br><span class="line">    --executor-cores 5 \</span><br><span class="line">    --executor-memory 2G \</span><br><span class="line">    --conf spark.dynamicAllocation.enabled=true \</span><br><span class="line">    --conf spark.shuffle.service.enabled=true \</span><br><span class="line">    --conf spark.dynamicAllocation.minExecutors=5 \</span><br><span class="line">    --conf spark.dynamicAllocation.maxExecutors=30 \</span><br><span class="line">    --conf spark.dynamicAllocation.initialExecutors=10 </span><br><span class="line">    --class com.spark.sql.jdbc.SparkDFtoOracle2 \</span><br><span class="line">    Spark-hive-sql-Dataframe-0.0.1-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure></li><li><p>代码中配置，个性化生效；</p><p>参考下面的<code>scala</code>代码案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.set(<span class="string">"spark.dynamicAllocation.enabled"</span>, <span class="literal">true</span>);</span><br><span class="line">conf.set(<span class="string">"spark.shuffle.service.enabled"</span>, <span class="literal">true</span>);</span><br><span class="line">conf.set(<span class="string">"spark.dynamicAllocation.minExecutors"</span>, <span class="string">"5"</span>);</span><br><span class="line">conf.set(<span class="string">"spark.dynamicAllocation.maxExecutors"</span>, <span class="string">"30"</span>);</span><br><span class="line">conf.set(<span class="string">"spark.dynamicAllocation.initialExecutors"</span>, <span class="string">"10"</span>);</span><br></pre></td></tr></table></figure></li></ul><p>接下来我们介绍详细的参数含义。</p><h4 id="1-2-2-配置说明"><a href="#1-2-2-配置说明" class="headerlink" title="1.2.2 配置说明"></a>1.2.2 配置说明</h4><table><thead><tr><th style="text-align:left">Property Name</th><th style="text-align:left">Default</th><th style="text-align:left">Meaning</th><th style="text-align:left">Since Version</th></tr></thead><tbody><tr><td style="text-align:left"><code>spark.dynamicAllocation.enabled</code></td><td style="text-align:left">false</td><td style="text-align:left">Whether to use dynamic resource allocation, which scales the number of executors registered with this application up and down based on the workload. For more detail, see the description <a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation" target="_blank" rel="noopener">here</a>.  This requires <code>spark.shuffle.service.enabled</code> or <code>spark.dynamicAllocation.shuffleTracking.enabled</code> to be set. The following configurations are also relevant: <code>spark.dynamicAllocation.minExecutors</code>, <code>spark.dynamicAllocation.maxExecutors</code>, and <code>spark.dynamicAllocation.initialExecutors</code> <code>spark.dynamicAllocation.executorAllocationRatio</code></td><td style="text-align:left">1.2.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.executorIdleTimeout</code></td><td style="text-align:left">60s</td><td style="text-align:left">If dynamic allocation is enabled and an executor has been idle for more than this duration, the executor will be removed. For more detail, see this <a href="https://spark.apache.org/docs/latest/job-scheduling.html#resource-allocation-policy" target="_blank" rel="noopener">description</a>.</td><td style="text-align:left">1.2.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.cachedExecutorIdleTimeout</code></td><td style="text-align:left">infinity</td><td style="text-align:left">If dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration, the executor will be removed. For more details, see this <a href="https://spark.apache.org/docs/latest/job-scheduling.html#resource-allocation-policy" target="_blank" rel="noopener">description</a>.</td><td style="text-align:left">1.4.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.initialExecutors</code></td><td style="text-align:left"><code>spark.dynamicAllocation.minExecutors</code></td><td style="text-align:left">Initial number of executors to run if dynamic allocation is enabled.  If <code>--num-executors</code> (or <code>spark.executor.instances</code>) is set and larger than this value, it will be used as the initial number of executors.</td><td style="text-align:left">1.3.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.maxExecutors</code></td><td style="text-align:left">infinity</td><td style="text-align:left">Upper bound for the number of executors if dynamic allocation is enabled.</td><td style="text-align:left">1.2.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.minExecutors</code></td><td style="text-align:left">0</td><td style="text-align:left">Lower bound for the number of executors if dynamic allocation is enabled.</td><td style="text-align:left">1.2.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.executorAllocationRatio</code></td><td style="text-align:left">1</td><td style="text-align:left">By default, the dynamic allocation will request enough executors to maximize the parallelism according to the number of tasks to process. While this minimizes the latency of the job, with small tasks this setting can waste a lot of resources due to executor allocation overhead, as some executor might not even do any work. This setting allows to set a ratio that will be used to reduce the number of executors w.r.t. full parallelism. Defaults to 1.0 to give maximum parallelism. 0.5 will divide the target number of executors by 2 The target number of executors computed by the dynamicAllocation can still be overridden by the <code>spark.dynamicAllocation.minExecutors</code> and <code>spark.dynamicAllocation.maxExecutors</code> settings</td><td style="text-align:left">2.4.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.schedulerBacklogTimeout</code></td><td style="text-align:left">1s</td><td style="text-align:left">If dynamic allocation is enabled and there have been pending tasks backlogged for more than this duration, new executors will be requested. For more detail, see this <a href="https://spark.apache.org/docs/latest/job-scheduling.html#resource-allocation-policy" target="_blank" rel="noopener">description</a>.</td><td style="text-align:left">1.2.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.sustainedSchedulerBacklogTimeout</code></td><td style="text-align:left"><code>schedulerBacklogTimeout</code></td><td style="text-align:left">Same as <code>spark.dynamicAllocation.schedulerBacklogTimeout</code>, but used only for subsequent executor requests. For more detail, see this <a href="https://spark.apache.org/docs/latest/job-scheduling.html#resource-allocation-policy" target="_blank" rel="noopener">description</a>.</td><td style="text-align:left">1.2.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.shuffleTracking.enabled</code></td><td style="text-align:left"><code>false</code></td><td style="text-align:left">Experimental. Enables shuffle file tracking for executors, which allows dynamic allocation without the need for an external shuffle service. This option will try to keep alive executors that are storing shuffle data for active jobs.</td><td style="text-align:left">3.0.0</td></tr><tr><td style="text-align:left"><code>spark.dynamicAllocation.shuffleTracking.timeout</code></td><td style="text-align:left"><code>infinity</code></td><td style="text-align:left">When shuffle tracking is enabled, controls the timeout for executors that are holding shuffle data. The default value means that Spark will rely on the shuffles being garbage collected to be able to release executors. If for some reason garbage collection is not cleaning up shuffles quickly enough, this option can be used to control when to time out executors even when they are storing shuffle data.</td><td style="text-align:left">3.0.0</td></tr></tbody></table><h3 id="1-3-Spark-Streaming-任务配置"><a href="#1-3-Spark-Streaming-任务配置" class="headerlink" title="1.3 Spark Streaming 任务配置"></a>1.3 Spark Streaming 任务配置</h3><p>对于Spark Streaming 流处理任务，Spark官方并未在文档中给出介绍。<code>Dynamic Resource Allocation</code>配置指引如下：</p><ul><li><p>必要配置（Spark 3.0.0）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启Spark Streaming流处理动态资源分配参数开关（默认关闭）</span></span><br><span class="line">spark.streaming.dynamicAllocation.enabled=true</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置最大和最小的Executor数量</span></span><br><span class="line">spark.streaming.dynamicAllocation.minExecutors=1（必须正整数）</span><br><span class="line">spark.streaming.dynamicAllocation.maxExecutors=50（必须正整数，默认Int.MaxValue，即无限大）</span><br></pre></td></tr></table></figure></li><li><p>可选配置（Spark 3.0.0）</p><p>这些参数可以不用配置，都已经提供了一个较为合理的默认值。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.streaming.dynamicAllocation.scalingUpRatio（必须正数，默认0.9）</span><br><span class="line">spark.streaming.dynamicAllocation.scalingInterval（单位秒，默认60）</span><br><span class="line">spark.streaming.dynamicAllocation.scalingDownRatio（必须正数，默认0.3）</span><br></pre></td></tr></table></figure></li></ul><h2 id="第二部分-动态配置原理和源码分析"><a href="#第二部分-动态配置原理和源码分析" class="headerlink" title="第二部分 动态配置原理和源码分析"></a>第二部分 动态配置原理和源码分析</h2><p>介绍完使用配置后，接下来将详细介绍实现原理。以便理解各参数的含义和参数调优。</p><h3 id="2-1-Spark-Core任务"><a href="#2-1-Spark-Core任务" class="headerlink" title="2.1 Spark Core任务"></a>2.1 Spark Core任务</h3><p>为了动态伸缩Spark任务的计算资源（Executor为基本分配单位），首先需要确定的度量是任务的繁忙程度。<code>DRA</code>机制将Spark任务是否有挂起任务(pending task)作为判断标准，一旦有挂起任务表示当前的Executor数量不够支撑所有的task并行运行，所以会申请增加资源。</p><h4 id="2-1-1-资源请求（Request）策略"><a href="#2-1-1-资源请求（Request）策略" class="headerlink" title="2.1.1 资源请求（Request）策略"></a>2.1.1 资源请求（Request）策略</h4><p>当Spark任务开启<code>DRA</code>机制，<code>SparkContext</code>会启动后台<code>ExecutorAllocationManager</code>，用来管理集群的Executors。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//package org.apache.spark SparkContext.scala</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dynamicAllocationEnabled = <span class="type">Utils</span>.isDynamicAllocationEnabled(_conf)</span><br><span class="line">    _executorAllocationManager =</span><br><span class="line">      <span class="keyword">if</span> (dynamicAllocationEnabled) &#123;</span><br><span class="line">        schedulerBackend <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> b: <span class="type">ExecutorAllocationClient</span> =&gt;</span><br><span class="line">            <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ExecutorAllocationManager</span>(</span><br><span class="line">              schedulerBackend.asInstanceOf[<span class="type">ExecutorAllocationClient</span>], listenerBus, _conf,</span><br><span class="line">              cleaner = cleaner, resourceProfileManager = resourceProfileManager))</span><br><span class="line">          <span class="keyword">case</span> _ =&gt;</span><br><span class="line">            <span class="type">None</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line">    _executorAllocationManager.foreach(_.start())</span><br></pre></td></tr></table></figure><p>Start()方法将<code>ExecutorAllocationListener</code>加入到<code>listenerBus</code>中，<code>ExecutorAllocationListener</code>通过监听<code>listenerBus</code>里的事件，动态添加，删除<code>Executor</code>。并且通过<code>Thread</code>不断添加<code>Executor</code>，遍历<code>Executor</code>，将超时的<code>Executor</code>杀掉并移除。</p><p>Spark会周期性（<code>intervalMillis</code>=100毫秒）计算实际需要的Executor的最大数量<code>maxNeeded</code>。公式如下。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> maxNeeded = math.ceil(numRunningOrPendingTasks * executorAllocationRatio /</span><br><span class="line">      tasksPerExecutor).toInt</span><br></pre></td></tr></table></figure><p>逻辑代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateAndSyncNumExecutorsTarget</span></span>(now: <span class="type">Long</span>): <span class="type">Int</span> = synchronized &#123;</span><br><span class="line">  <span class="keyword">if</span> (initializing) &#123;</span><br><span class="line">    <span class="number">0</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> updatesNeeded = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">ExecutorAllocationManager</span>.<span class="type">TargetNumUpdates</span>]</span><br><span class="line">    numExecutorsTargetPerResourceProfileId.foreach &#123; <span class="keyword">case</span> (rpId, targetExecs) =&gt;</span><br><span class="line">      <span class="keyword">val</span> maxNeeded = maxNumExecutorsNeededPerResourceProfile(rpId)</span><br><span class="line">      <span class="keyword">if</span> (maxNeeded &lt; targetExecs) &#123;</span><br><span class="line">        decrementExecutorsFromTarget(maxNeeded, rpId, updatesNeeded)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (addTime != <span class="type">NOT_SET</span> &amp;&amp; now &gt;= addTime) &#123;</span><br><span class="line">        addExecutorsToTarget(maxNeeded, rpId, updatesNeeded)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    doUpdateRequest(updatesNeeded.toMap, now)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>当集群中有<code>Executor</code>出现<code>pending task</code>，计算判断条件<code>maxNeeded &gt; targetExecs</code>，并且等待时间超过<code>schedulerBacklogTimeout</code>(默认<code>1s</code>)，则会触发方法<code>addExecutorsToTarget(maxNeeded, rpId, updatesNeeded)</code>。对于首次增加Executor。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.dynamicAllocation.schedulerBacklogTimeout = 1s（秒）</span><br></pre></td></tr></table></figure><ul><li>后续按照周期性时间<code>sustainedSchedulerBacklogTimeout</code>来检测pending task，一旦出现pending task，即触发增加Executor。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = 1s(秒)</span><br></pre></td></tr></table></figure><p>每次（轮）触发增加<code>Executor</code>资源请求，增加的数量翻倍，即是一个指数数列（2的n次方），例如：<code>1、2、4、8</code>。</p><h4 id="2-1-2-资源释放（Remove）策略"><a href="#2-1-2-资源释放（Remove）策略" class="headerlink" title="2.1.2 资源释放（Remove）策略"></a>2.1.2 资源释放（Remove）策略</h4><p>对于移除策略如下：</p><ul><li>如果Executor闲置（<code>maxNeeded &lt; targetExecs</code>）时间超过以下参数，并且executor中没有cache（数据缓存在内存），则spark应用将会释放该Executor。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.dynamicAllocation.executorIdleTimeout（单位为秒） 默认60s</span><br></pre></td></tr></table></figure><ul><li>如果空闲Executor中有cache，那么这个超时参数为：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.dynamicAllocation.cachedExecutorIdleTimeout 默认值：Integer.MAX_VALUE（即永不超时）</span><br></pre></td></tr></table></figure><p>对于Executor的退出，设计上需要考虑状态的问题，主要：</p><ul><li><p>需要移除的<code>Executor</code>存在<code>cache</code>。</p><p>如果需要移除的<code>Executor</code>含有<code>RDD cache</code>。这时候超时时间为整型最大值（相当于无限）。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">val</span> <span class="type">DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT</span> =</span><br><span class="line">  <span class="type">ConfigBuilder</span>(<span class="string">"spark.dynamicAllocation.cachedExecutorIdleTimeout"</span>)</span><br><span class="line">    .version(<span class="string">"1.4.0"</span>)</span><br><span class="line">    .timeConf(<span class="type">TimeUnit</span>.<span class="type">SECONDS</span>)</span><br><span class="line">    .checkValue(_ &gt;= <span class="number">0</span>L, <span class="string">"Timeout must be &gt;= 0."</span>)</span><br><span class="line">    .createWithDefault(<span class="type">Integer</span>.<span class="type">MAX_VALUE</span>)</span><br></pre></td></tr></table></figure></li><li><p>Shuffle状态的保存问题。如果需要移除的Executor包含了Shuffle状态数据（在shuffle期间，Spark executor先要将map的输出写入到磁盘，然后该executor充当一个文件服务器，将这些文件共享给其他的executor访问）。需要提前启动<code>External shuffle service</code>，由专门外置服务提供存储，Executor中不再负责保存，架构上功能解耦。</p></li></ul><p>另外添加和移除Executor之后，需要告知<code>DAGSchedule</code>进行相关信息更新。</p><h4 id="2-1-3-配置建议"><a href="#2-1-3-配置建议" class="headerlink" title="2.1.3 配置建议"></a>2.1.3 配置建议</h4><p>Spark的动态伸缩机制的几点建议：</p><ul><li>给Executor数量设置一个合理的伸缩区间，即<code>[minExecutors-maxExecutors]</code>区间值。</li><li>配置资源粒度较小的Executor，例如CPU数量为3-4个。动态伸缩的最小伸缩单位是单个Executor，如果出现资源伸缩，特别是Executor数目下降后业务量突增，新申请资源未就绪，已有的Executor就可能由于任务过载而导致集群崩溃。</li><li>如果程序中有shuffle,例如(reduce<em>,groupBy</em>),建议设置一个合理的并行数，避免杀掉过多的Executors。</li><li>对于每个Stage持续时间很短的应用，不适合动态伸缩机制。这样会频繁增加和移除Executors，造成系统颠簸。特别是在 Spark on Yarn模式下资源的申请处理速度并不快。</li></ul><h3 id="2-2-Spark-Streaming-任务"><a href="#2-2-Spark-Streaming-任务" class="headerlink" title="2.2 Spark Streaming 任务"></a>2.2 Spark Streaming 任务</h3><p>Spark Streaming任务可以看成连续运行的微（micro-batch）批任务，如果直接套用Spark Core的动态伸缩机制就水土不服了。一般一个微批任务较短（默认60秒），实际线上任务可能更小，动态伸缩的反应时间较长（特别是on Yarn模式），一个微批任务结束，动态伸缩策略还没生效。所以针对Spark Streaming任务，项目组设计新的动态机制（Spark 2.0.0 版本引入）。</p><p>提案：<a href="https://issues.apache.org/jira/browse/SPARK-12133" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-12133</a></p><h4 id="2-2-1-源码分析"><a href="#2-2-1-源码分析" class="headerlink" title="2.2.1 源码分析"></a>2.2.1 源码分析</h4><p>Spark Streaming任务会统计微批任务运行时间的延迟时间，最朴素的想法就是按照这个度量指标来作为动态伸缩的触发指标。这部分源码在<code>org.apache.spark.streaming.scheduler</code>中：</p><ul><li><p>周期性计算微批运行完成的平均时间，然后和<code>batch interval</code>进行比较；</p><p>这里的周期大小由参数<code>spark.streaming.dynamicAllocation.scalingInterval</code>决定，大小为<code>scalingIntervalSecs * 1000</code>。例如默认值为：60*1000毫秒，即60秒。</p><p>通过<code>streamingListener</code>计算微批平均处理时间（<code>averageBatchProcTime</code>），然后计算微批处理率（ratio，微批平均处理时间/微批处理周期）。</p><p>然后和参数值上限（<code>scalingUpRatio</code>）和下限（<code>scalingDownRatio</code>）进行比较。详细控制函数如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">manageAllocation</span></span>(): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">   logInfo(<span class="string">s"Managing executor allocation with ratios = [<span class="subst">$scalingUpRatio</span>, <span class="subst">$scalingDownRatio</span>]"</span>)</span><br><span class="line">   <span class="keyword">if</span> (batchProcTimeCount &gt; <span class="number">0</span>) &#123;</span><br><span class="line">     <span class="keyword">val</span> averageBatchProcTime = batchProcTimeSum / batchProcTimeCount</span><br><span class="line">     <span class="keyword">val</span> ratio = averageBatchProcTime.toDouble / batchDurationMs</span><br><span class="line">     logInfo(<span class="string">s"Average: <span class="subst">$averageBatchProcTime</span>, ratio = <span class="subst">$ratio</span>"</span> )</span><br><span class="line">     <span class="keyword">if</span> (ratio &gt;= scalingUpRatio) &#123;</span><br><span class="line">       logDebug(<span class="string">"Requesting executors"</span>)</span><br><span class="line">       <span class="keyword">val</span> numNewExecutors = math.max(math.round(ratio).toInt, <span class="number">1</span>)</span><br><span class="line">       requestExecutors(numNewExecutors)</span><br><span class="line">     &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ratio &lt;= scalingDownRatio) &#123;</span><br><span class="line">       logDebug(<span class="string">"Killing executors"</span>)</span><br><span class="line">       killExecutor()</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   batchProcTimeSum = <span class="number">0</span></span><br><span class="line">   batchProcTimeCount = <span class="number">0</span></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li><li><p>增加Executor数量；如果<code>ratio &gt;= scalingUpRatio</code>，然后按照下面的公司增加数量：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> numNewExecutors = math.max(math.round(ratio).toInt, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>例如<code>ratio=1.6&gt;0.9(scalingUpRatio)</code>，这时候说明有大量微批任务出现了延迟，按照公式计算<code>numNewExecutors=2</code>。接下来会调用<code>requestExecutors(numNewExecutors)</code>方法去申请2个Executor。</p></li><li><p>减少Executor数量；如果<code>ratio &lt;= scalingDownRatio</code>，这直接调用<code>killExecutor()</code>方法（方法中判断没有receiver运行的Executor）去kill Executor。</p></li></ul><h4 id="2-2-2-配置建议"><a href="#2-2-2-配置建议" class="headerlink" title="2.2.2 配置建议"></a>2.2.2 配置建议</h4><p>Spark Streaming动态资源分配起作用前，需要至少完成一个Batch处理(<code>batchProcTimeCount &gt; 0</code>)。</p><ul><li><p>Spark Core和Spark Streaming的动态配置开关配置是分别设置的。</p><p>如果两个配置开关同时配置为true，会抛出错误。建议如下配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.dynamicAllocation.enabled=false （默认是false，可以不配置）</span><br><span class="line">spark.streaming.dynamicAllocation.enabled=true</span><br></pre></td></tr></table></figure></li></ul><h2 id="第三部分-总结"><a href="#第三部分-总结" class="headerlink" title="第三部分 总结"></a>第三部分 总结</h2><h3 id="3-1-对比"><a href="#3-1-对比" class="headerlink" title="3.1 对比"></a>3.1 对比</h3><p>Spark Core中动态伸缩机制是基于空闲时间来控制回收Executor。而在Spark Streaming中，一个Executor每隔很短的时间都会有一批作业被调度，所以在streaming里面是基于平均每批作业处理的时间。</p><h3 id="3-2-Structed-Streaming任务动态伸缩"><a href="#3-2-Structed-Streaming任务动态伸缩" class="headerlink" title="3.2 Structed Streaming任务动态伸缩"></a>3.2 <code>Structed Streaming</code>任务动态伸缩</h3><p>在spark Streaming中，最小的可能延迟受限于每批的调度间隔以及任务启动时间。所以这不能满足更低延迟的需求。如果能够连续的处理，尤其是简单的处理而没有任何的阻塞操作。这种连续处理的架构可以使得端到端延迟最低降低到<code>1ms</code>级别，而不是目前的<code>10-100ms</code>级别，这就是Spark 2.2.0版本引入新的Spark流处理框架：<code>Structed Streaming</code>。</p><blockquote><p><a href="https://issues.apache.org/jira/browse/SPARK-20928" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-20928</a></p></blockquote><p>当然项目组自然也会考虑该框架的资源伸缩机制（未完成）</p><blockquote><p><a href="https://issues.apache.org/jira/browse/SPARK-24815" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-24815</a></p></blockquote><p>后续趋势上看，Spark项目会将更多精力放在<code>Structed Streaming</code>。</p><h3 id="3-3-Spark-Streaming-背压机制"><a href="#3-3-Spark-Streaming-背压机制" class="headerlink" title="3.3 Spark Streaming 背压机制"></a>3.3 Spark <strong>Streaming</strong> 背压机制</h3><p>为了应对Spark Streaming处理数据波动，除了资源动态伸缩机制，在Spark 1.5版本项目在Spark Streaming 中引入了的背压（<code>Backpressure</code>）机制。</p><p>Spark Streaming任务中，当batch的处理时间大于batch interval时，意味着数据处理速度跟不上数据接收速度。这时候在数据接收端(Receiver)Executor就会开始积压数据。如果数据存储采用MEMORY_ONLY模式（内存）就会导致OOM，采用MEMORY_AND_DISK多余的数据保存到磁盘上，增加数据IO时间。</p><p>背压（<code>Backpressure</code>）机制，通过动态控制数据接收速率来适配集群数据处理能力。这是被动防守型的应对，将数据缓存在Kafka消息层。如果数据持续保持高量级，就需要主动启停任务来增加计算资源。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Job Scheduling，链接：<a href="https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup</a></p><p>2、About Spark Streaming，链接：<a href="https://www.turbofei.wang/spark/2019/05/26/about-spark-streaming" target="_blank" rel="noopener">https://www.turbofei.wang/spark/2019/05/26/about-spark-streaming</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  配置实现&lt;/li&gt;
&lt;li&gt;第二部分 动态配置原理和源码分析&lt;/li&gt;
&lt;li&gt;第三部分 总
      
    
    </summary>
    
      <category term="spark" scheme="https://zjrongxiang.github.io/categories/spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark中的动态伸缩和反压机制</title>
    <link href="https://zjrongxiang.github.io/2021/05/02/2021-05-02-Spark%E4%B8%AD%E7%9A%84%E5%8A%A8%E6%80%81%E4%BC%B8%E7%BC%A9%E5%92%8C%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6/"/>
    <id>https://zjrongxiang.github.io/2021/05/02/2021-05-02-Spark中的动态伸缩和反压机制/</id>
    <published>2021-05-02T05:30:00.000Z</published>
    <updated>2021-05-02T14:11:27.463Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  常用快捷键</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://fares.codes/posts/dynamic-scaling-and-backpressure/" target="_blank" rel="noopener">https://fares.codes/posts/dynamic-scaling-and-backpressure/</a></p><h2 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h2><ul><li>建议采用以下做法以实现更好的自动缩放比例：<ul><li>最好从相当大的集群和数量的执行程序开始，并在必要时进行缩减。（执行程序映射到<a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">YARN容器</a>。）</li><li>执行者的数量应至少等于接收者的数量。</li><li>设置每个执行器的核心数，以使执行器具有一些多余的容量，这些容量超出了运行接收器所需的容量。</li><li>内核总数必须大于接收器数量；否则，应用程序将无法处理收到的数据。</li></ul></li><li>设置<code>spark.streaming.backpressure.enabled</code>为<code>true</code>，则Spark Streaming可以控制接收速率（基于当前的批处理调度延迟和处理时间），以便系统仅以其可以处理的速度接收数据。</li><li>为了获得最佳性能，请考虑使用Kryo序列化程序在Spark数据的序列化表示和反序列化表示之间进行转换。这不是Spark的默认设置，但是您可以显式更改它：将<code>spark.serializer</code>属性设置 为<code>org.apache.spark.serializer.KryoSerializer</code>。</li><li>开发人员可以通过在不再需要DStream时取消缓存它们来减少内存消耗。</li></ul><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Job Scheduling，链接：<a href="https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  常用快捷键&lt;/li&gt;
&lt;li&gt;参考文献及资料&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;背景&quot;&gt;
      
    
    </summary>
    
      <category term="spark" scheme="https://zjrongxiang.github.io/categories/spark/"/>
    
    
  </entry>
  
  <entry>
    <title>suse操作系统rpm命令</title>
    <link href="https://zjrongxiang.github.io/2021/04/15/2021-05-30-suse%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9Frpm%E5%91%BD%E4%BB%A4/"/>
    <id>https://zjrongxiang.github.io/2021/04/15/2021-05-30-suse操作系统rpm命令/</id>
    <published>2021-04-15T13:30:00.000Z</published>
    <updated>2021-06-03T00:50:27.051Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>RPM 安装操作</p><p>命令：</p><p>rpm -i 需要安装的包文件名</p><p>举例如下：</p><p>rpm -i example.rpm 安装 example.rpm 包；</p><p>rpm -iv example.rpm 安装 example.rpm 包并在安装过程中显示正在安装的文件信息；</p><p>rpm -ivh example.rpm 安装 example.rpm 包并在安装过程中显示正在安装的文件信息及安装进度；</p><p>RPM 查询操作</p><p>命令：</p><p>rpm -q …</p><p>附加查询命令：</p><p>a 查询所有已经安装的包以下两个附加命令用于查询安装包的信息；</p><p>i 显示安装包的信息；</p><p>l 显示安装包中的所有文件被安装到哪些目录下；</p><p>s 显示安装版中的所有文件状态及被安装到哪些目录下；以下两个附加命令用于指定需要查询的是安装包还是已安装后的文件；</p><p>p 查询的是安装包的信息；</p><p>f 查询的是已安装的某文件信息；</p><p>举例如下：</p><p>rpm -qa | grep tomcat4 查看 tomcat4 是否被安装；</p><p>rpm -qip example.rpm 查看 example.rpm 安装包的信息；</p><p>rpm -qif /bin/df 查看/bin/df 文件所在安装包的信息；</p><p>rpm -qlf /bin/df 查看/bin/df 文件所在安装包中的各个文件分别被安装到哪个目录下；</p><p>RPM 卸载操作</p><p>命令：</p><p>rpm -e 需要卸载的安装包</p><p>在卸载之前，通常需要使用rpm -q …命令查出需要卸载的安装包名称。</p><p>举例如下：</p><p>rpm -e tomcat4 卸载 tomcat4 软件包</p><p>RPM 升级操作</p><p>命令：</p><p>rpm -U 需要升级的包</p><p>举例如下：</p><p>rpm -Uvh example.rpm 升级 example.rpm 软件包</p><p>RPM 验证操作</p><p>命令：</p><p>rpm -V 需要验证的包</p><p>举例如下：</p><p>rpm -Vf /etc/tomcat4/tomcat4.conf</p><p>输出信息类似如下：</p><p>S.5….T c /etc/tomcat4/tomcat4.conf</p><p>其中，S 表示文件大小修改过，T 表示文件日期修改过。限于篇幅，更多的验证信息请您参考rpm 帮助文件：man rpm</p><p>RPM 的其他附加命令</p><p>–force 强制操作 如强制安装删除等；</p><p>–requires 显示该包的依赖关系；</p><p>–nodeps 忽略依赖关系并继续操作；</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] Orange官网，链接：<a href="http://orange.sumory.com/" target="_blank" rel="noopener">http://orange.sumory.com/</a></p><p>[2] Orange网关官网docker，链接：<a href="https://hub.docker.com/r/syhily/orange" target="_blank" rel="noopener">https://hub.docker.com/r/syhily/orange</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;RPM 安装操作&lt;/p&gt;
&lt;p&gt;命令：&lt;/p&gt;
&lt;p&gt;rpm -i 需要安装的包文件名&lt;/p&gt;
&lt;p&gt;举例如下：&lt;/p&gt;
&lt;p&gt;rpm -i
      
    
    </summary>
    
      <category term="orange" scheme="https://zjrongxiang.github.io/categories/orange/"/>
    
    
      <category term="orange" scheme="https://zjrongxiang.github.io/tags/orange/"/>
    
  </entry>
  
  <entry>
    <title>orange网关原理的源码分析</title>
    <link href="https://zjrongxiang.github.io/2021/04/15/2021-05-30-orange%E7%BD%91%E5%85%B3%E7%94%9F%E4%BA%A7%E7%BB%B4%E6%8A%A4%E6%89%8B%E5%86%8C/"/>
    <id>https://zjrongxiang.github.io/2021/04/15/2021-05-30-orange网关生产维护手册/</id>
    <published>2021-04-15T13:30:00.000Z</published>
    <updated>2021-07-11T13:18:24.348Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Orange v0.6.0版本是一个重构版本， 着重为了解决之前版本在有大量规则配置时性能损耗的问题。 基本的设计思路是将原来的规则细分成两层， 第一层叫做<a href="http://orange.sumory.com/docs/concept/selector/" target="_blank" rel="noopener">selector</a>, 用于将流量进行第一步划分， 在进入某个selector后才按照之前的设计进行<a href="http://orange.sumory.com/docs/concept/rule/" target="_blank" rel="noopener">规则</a>匹配， 匹配到后进行相关处理。</p><p><a href="https://lengrongfu.github.io/2019/05/21/orange-%E5%8E%9F%E7%90%86/" target="_blank" rel="noopener">https://lengrongfu.github.io/2019/05/21/orange-%E5%8E%9F%E7%90%86/</a></p><p><a href="https://book.aikaiyuan.com/openresty/understanding-orange.html" target="_blank" rel="noopener">https://book.aikaiyuan.com/openresty/understanding-orange.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/67481992" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/67481992</a></p><h3 id="生产单机部署"><a href="#生产单机部署" class="headerlink" title="生产单机部署"></a>生产单机部署</h3><h3 id="生产集群部署"><a href="#生产集群部署" class="headerlink" title="生产集群部署"></a>生产集群部署</h3><h3 id="配置更新"><a href="#配置更新" class="headerlink" title="配置更新"></a>配置更新</h3><h2 id="日志切割"><a href="#日志切割" class="headerlink" title="日志切割"></a>日志切割</h2><p>Nginx日志对于统计、系统服务排错很有用。Nginx日志主要分为两种：access_log(访问日志)和error_log(错误日志)。通过访问日志我们可以得到用户的IP地址、浏览器的信息，请求的处理时间等信息。错误日志记录了访问出错的信息，可以帮助我们定位错误的原因。本文将详细描述一下如何配置Nginx日志。</p><p>访问日志主要记录客户端的请求。客户端向Nginx服务器发起的每一次请求都记录在这里。客户端IP，浏览器信息，referer，请求处理时间，请求URL等都可以在访问日志中得到。当然具体要记录哪些信息，你可以通过log_format指令定义。</p><p>错误日志在Nginx中是通过error_log指令实现的。该指令记录服务器和请求处理过程中的错误信息。</p><p>Nginx中通过access_log和error_log指令配置访问日志和错误日志，通过log_format我们可以自定义日志格式。如果日志文件路径中使用了变量，我们可以通过open_log_file_cache指令来设置缓存，提升性能。</p><p>access_log，level表示日志等级，日志等级分为[ debug | info | notice | warn | error | crit ]，从左至右，日志详细程度逐级递减，即debug最详细，crit最少</p><p>error_log level可以是debug, info, notice, warn, error, crit, alert,emerg中的任意值。可以看到其取值范围是按紧急程度从低到高排列的。只有日志的错误级别等于或高于level指定的值才会写入错误日志中。默认值是error。</p><h3 id="orange中日志和默认配置"><a href="#orange中日志和默认配置" class="headerlink" title="orange中日志和默认配置"></a>orange中日志和默认配置</h3><p>orange项目<code>logs/</code>目录下面有日志文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 主日志</span></span><br><span class="line">access.log</span><br><span class="line">error.log</span><br><span class="line"><span class="meta">#</span><span class="bash"> orange管理界面日志</span></span><br><span class="line">dashboard_access.log</span><br><span class="line">dashboard_error.log</span><br><span class="line"><span class="meta">#</span><span class="bash"> 管理api接口日志</span></span><br><span class="line">api_access.log</span><br><span class="line">api_error.log</span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line">default_upstream_access.log</span><br><span class="line">default_upstream_error.log</span><br></pre></td></tr></table></figure><p>在配置文件<code>conf/nginx.conf</code>中这样定义：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">http</span> &#123;</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="attribute">access_log</span>  ./logs/access.log  main;</span><br><span class="line"><span class="attribute">error_log</span> ./logs/error.log <span class="literal">info</span>;</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">8001</span>;    </span><br><span class="line">    <span class="attribute">access_log</span> ./logs/default_upstream_access.log main;</span><br><span class="line">    <span class="attribute">error_log</span> ./logs/default_upstream_error.log;    </span><br><span class="line">    &#125;</span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">9999</span>;</span><br><span class="line">    <span class="attribute">access_log</span> ./logs/dashboard_access.log main;    </span><br><span class="line">    <span class="attribute">error_log</span> ./logs/dashboard_error.log <span class="literal">info</span>;    </span><br><span class="line">    &#125;</span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">7777</span>;</span><br><span class="line">    <span class="attribute">access_log</span> ./logs/api_access.log main;</span><br><span class="line">    <span class="attribute">error_log</span> ./logs/api_error.log <span class="literal">info</span>;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="日志切割-1"><a href="#日志切割-1" class="headerlink" title="日志切割"></a>日志切割</h3><p>日常运维中orange的日志文件随着时间逐渐增多，日志单文件较大不方便日常文件生命周期清理。所以需要对原日志进行切割，每日一个日志文件。</p><p>常见方法有：</p><ul><li>使用shell脚本重命名日志文件，使用<code>crontab</code>定时调用该脚本。</li><li>使用<code>logrotate</code>工具。</li></ul><p>从<code>nginx 0.7.6</code>版本开始<code>access_log</code>的路径配置可以包含变量。</p><p>建议的配置：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">map</span> <span class="variable">$time_iso8601</span> <span class="variable">$logdate</span> &#123;</span><br><span class="line">  '~^(?&lt;ymd&gt;\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;)' $ymd;</span><br><span class="line">  <span class="attribute">default</span>                       <span class="string">'date-not-found'</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="attribute">access_log</span> logs/access-<span class="variable">$logdate</span>.log main;</span><br><span class="line"><span class="attribute">error_log</span> ./logs/error-<span class="variable">$logdate</span>.log <span class="literal">info</span>;</span><br><span class="line"><span class="attribute">open_log_file_cache</span> max=<span class="number">10</span>;</span><br></pre></td></tr></table></figure><p><a href="https://jingsam.github.io/2019/01/15/nginx-access-log.html" target="_blank" rel="noopener">https://jingsam.github.io/2019/01/15/nginx-access-log.html</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>[1] Orange官网，链接：<a href="http://orange.sumory.com/" target="_blank" rel="noopener">http://orange.sumory.com/</a></p><p>[2] Orange网关官网docker，链接：<a href="https://hub.docker.com/r/syhily/orange" target="_blank" rel="noopener">https://hub.docker.com/r/syhily/orange</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;Orange v0.6.0版本是一个重构版本， 着重为了解决之前版本在有大量规则配置时性能损耗的问题。 基本的设计思路是将原来的规则细分成两
      
    
    </summary>
    
      <category term="orange" scheme="https://zjrongxiang.github.io/categories/orange/"/>
    
    
      <category term="orange" scheme="https://zjrongxiang.github.io/tags/orange/"/>
    
  </entry>
  
</feed>
