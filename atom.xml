<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RongXiang</title>
  
  <subtitle>我的烂笔头</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zjrongxiang.github.io/"/>
  <updated>2020-01-12T06:24:49.724Z</updated>
  <id>https://zjrongxiang.github.io/</id>
  
  <author>
    <name>rong xiang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>第一个Spring Boot练习项目</title>
    <link href="https://zjrongxiang.github.io/2020/01/11/2020-01-01-SpringBoot%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88%E7%AC%AC%E4%B8%80%E4%B8%AASpringBoot%E5%AE%9E%E8%B7%B5%E7%BB%83%E4%B9%A0%E9%A1%B9%E7%9B%AE%EF%BC%89/"/>
    <id>https://zjrongxiang.github.io/2020/01/11/2020-01-01-SpringBoot系列文章（第一个SpringBoot实践练习项目）/</id>
    <published>2020-01-11T05:30:00.000Z</published>
    <updated>2020-01-12T06:24:49.724Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分 开发环境准备</li><li>第二部分 使用Maven构建项目</li><li>第三部分 项目目录结构</li><li>第四部分  编写HelloWorld项目</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>第一次使用Spring Boot构建测试项目，实现一个简单的Http请求处理，通过这个例子对Spring Boot有一个初步的了解。</p><h2 id="第一部分-开发环境准备"><a href="#第一部分-开发环境准备" class="headerlink" title="第一部分 开发环境准备"></a>第一部分 开发环境准备</h2><ul><li><p>Maven版本：Maven 3，version 3.3.9</p></li><li><p>Java版本：Java 1.8（Java8）</p></li></ul><h2 id="第二部分-使用Maven构建项目"><a href="#第二部分-使用Maven构建项目" class="headerlink" title="第二部分 使用Maven构建项目"></a>第二部分 使用Maven构建项目</h2><p>Spring官网提供Spring Initializr工具生成项目</p><ul><li>第一步：登录Spring Initializr网站： <a href="https://start.spring.io/" target="_blank" rel="noopener">https://start.spring.io/</a></li><li>第二步：配置项目的参数：</li></ul><p><img src="\images\picture\Spring\initializr.jpg" alt=""></p><blockquote><p>Group ID是项目组织唯一的标识符，实际对应项目中的package包。</p><p>Artifact ID是项目的唯一的标识符，实际对应项目的project name名称，Artifact不可包含大写字母。</p></blockquote><p>然后点击生成项目压缩文件，并下载到本地。</p><ul><li>第三步：使用IDE加载项目（使用IntelliJ IDEA）<ol><li>菜单中选择<code>File</code>–&gt;<code>New</code>–&gt;<code>Project from Existing Sources...</code></li><li>选择解压后的项目文件夹，点击<code>OK</code></li><li>点击<code>Import project from external model</code>并选择<code>Maven</code>，点击<code>Next</code>到底为止。</li></ol></li></ul><h2 id="第三部分-项目目录结构"><a href="#第三部分-项目目录结构" class="headerlink" title="第三部分 项目目录结构"></a>第三部分 项目目录结构</h2><p>开发环境是Win7环境，导入后，使用<code>tree /f</code>命令查看项目结构目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> tree /f</span></span><br><span class="line">│  .gitignore</span><br><span class="line">│  HELP.md</span><br><span class="line">│  mvnw</span><br><span class="line">│  mvnw.cmd</span><br><span class="line">│  pom.xml</span><br><span class="line">├─.idea</span><br><span class="line">│  │  azureSettings.xml</span><br><span class="line">│  │  compiler.xml</span><br><span class="line">│  │  misc.xml</span><br><span class="line">│  │  workspace.xml</span><br><span class="line">│  └─inspectionProfiles</span><br><span class="line">│          Project_Default.xml</span><br><span class="line">├─.mvn</span><br><span class="line">│  └─wrapper</span><br><span class="line">│          maven-wrapper.jar</span><br><span class="line">│          maven-wrapper.properties</span><br><span class="line">│          MavenWrapperDownloader.java</span><br><span class="line">└─src</span><br><span class="line">    ├─main</span><br><span class="line">    │  ├─java</span><br><span class="line">    │  │  └─com</span><br><span class="line">    │  │      └─example</span><br><span class="line">    │  │          └─demo</span><br><span class="line">    │  │                  DemoApplication.java</span><br><span class="line">    │  └─resources</span><br><span class="line">    │          application.properties</span><br><span class="line">    └─test</span><br><span class="line">        └─java</span><br><span class="line">            └─com</span><br><span class="line">                └─example</span><br><span class="line">                    └─demo</span><br><span class="line">                            DemoApplicationTests.java</span><br></pre></td></tr></table></figure><ul><li><code>.gitignore</code>文件是git控制文件。</li><li><code>mvnw</code>(linux shell)和<code>mvnw.cmd</code>(windows),还有<code>.mvn</code>文件夹(包含Maven Wrapper Java库及其属性文件)。mvnw全名是Maven Wrapper,它的原理是在maven-wrapper.properties文件中记录你要使用的Maven版本，当用户执行mvnw clean 命令时，发现当前用户的Maven版本和期望的版本不一致，那么就下载期望的版本，然后用期望的版本来执行mvn命令，比如刚才的mvn clean。带有mvnw文件项目，只要有java环境，仅仅通过使用本项目的mvnw脚本就可以完成编译，打包，发布等一系列操作。</li><li><code>HELP.md</code>Maven的帮助文件。</li><li><code>pom.xml</code>Project Object Model 的缩写，即项目对象模型。maven 的配置文件，用以描述项目的各种信息。</li><li><p><code>.idea/</code>文件夹来存放项目的配置信息。其中包括版本控制信息、历史记录等等。</p></li><li><p><code>src/main/java</code>下的程序入口：<code>DemoApplication.java</code>。</p></li><li><code>src/main/resources</code>下的配置文件：<code>application.properties</code>。</li><li><code>src/test/</code>下的测试入口：<code>DemoApplicationTests.java</code>。</li></ul><h2 id="第四部分-编写HelloWorld项目"><a href="#第四部分-编写HelloWorld项目" class="headerlink" title="第四部分 编写HelloWorld项目"></a>第四部分 编写HelloWorld项目</h2><p>引入Web模块，在pom.xml文件添加下面的依赖包：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>新建controller程序(HelloSpringBootController)：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.demo.controller;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloSpringBootController</span> </span>&#123;</span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">hello</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Hello, SpringBoot!"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>完成后项目的结构如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">├─src</span><br><span class="line">│  ├─main</span><br><span class="line">│  │  ├─java</span><br><span class="line">│  │  │  └─com</span><br><span class="line">│  │  │      └─example</span><br><span class="line">│  │  │          └─demo</span><br><span class="line">│  │  │              │  DemoApplication.java</span><br><span class="line">│  │  │              │</span><br><span class="line">│  │  │              └─controller</span><br><span class="line">│  │  │                      HelloSpringBootController.java</span><br><span class="line">│  │  │</span><br><span class="line">│  │  └─resources</span><br><span class="line">│  │          application.properties</span><br><span class="line">│  │</span><br><span class="line">│  └─test</span><br><span class="line">│      └─java</span><br><span class="line">│          └─com</span><br><span class="line">             └─example</span><br><span class="line">                 └─demo</span><br><span class="line">                          DemoApplicationTests.java</span><br></pre></td></tr></table></figure><p>最后使用maven编译：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mvn clean  </span><br><span class="line">mvn package  #编译项目</span><br></pre></td></tr></table></figure><p>会生成编译结果，在项目根目录生成target文件目录。其中<code>demo-0.0.1-SNAPSHOT.jar</code>为编译后的入口程序。在IDEA中，或者使用<code>java -jar demo-0.0.1-SNAPSHOT.jar</code>命令在win CMD命令窗口中运行。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">"C:\Program Files\Java\jdk-10.0.2\bin\java.exe" -Dfile.encoding=GBK -jar C:\Users\rongxiang\Desktop\SpringBoot\demo\target\demo-0.0.1-SNAPSHOT.jar</span><br><span class="line"></span><br><span class="line">  .   ____          _            __ _ _</span><br><span class="line"> /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \</span><br><span class="line">( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \</span><br><span class="line"> \\/  ___)| |_)| | | | | || (_| |  ) ) ) )</span><br><span class="line">  '  |____| .__|_| |_|_| |_\__, | / / / /</span><br><span class="line"> =========|_|==============|___/=/_/_/_/</span><br><span class="line"> :: Spring Boot ::        (v2.2.2.RELEASE)</span><br><span class="line"></span><br><span class="line">2020-01-12 13:56:37.333  INFO 5060 --- [           main] com.example.demo.DemoApplication         : Starting DemoApplication v0.0.1-SNAPSHOT on rongxiang-PC with PID 5060 (C:\Users\rongxiang\Desktop\SpringBoot\demo\target\demo-0.0.1-SNAPSHOT.jar started by rongxiang in C:\Users\rongxiang\Desktop\SpringBoot\demo)</span><br><span class="line">2020-01-12 13:56:37.338  INFO 5060 --- [           main] com.example.demo.DemoApplication         : No active profile set, falling back to default profiles: default</span><br><span class="line">2020-01-12 13:56:40.479  INFO 5060 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)</span><br><span class="line">2020-01-12 13:56:40.497  INFO 5060 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]</span><br><span class="line">2020-01-12 13:56:40.497  INFO 5060 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.29]</span><br><span class="line">2020-01-12 13:56:40.598  INFO 5060 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext</span><br><span class="line">2020-01-12 13:56:40.598  INFO 5060 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 3142 ms</span><br><span class="line">2020-01-12 13:56:40.945  INFO 5060 --- [           main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'</span><br><span class="line">2020-01-12 13:56:41.227  INFO 5060 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''</span><br><span class="line">2020-01-12 13:56:41.233  INFO 5060 --- [           main] com.example.demo.DemoApplication         : Started DemoApplication in 4.773 seconds (JVM running for 5.475)</span><br></pre></td></tr></table></figure><p>在本地启了一个web服务（Tomcat started on port(s): 8080 (http)），对外服务端口为8080。</p><p>浏览器中输入url地址（<a href="http://localhost:8080/），网页显示“Hello" target="_blank" rel="noopener">http://localhost:8080/），网页显示“Hello</a>, SpringBoot!”，说明服务服务正常。</p><p>注意：</p><blockquote><p>如果运行工程，出现这个报错信息：Failed to clean project: Failed to delete</p><p>由于之前编译的工程还在运行，无法clean，导致maven生命周期无法继续进行。即由于已启动了另一个tomcat 进程，导致报错,关闭tomcat进程即可。可以在程序控制台中终止该进程即可。</p></blockquote><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Spring官网，链接：<a href="https://spring.io/" target="_blank" rel="noopener">https://spring.io/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分 开发环境准备&lt;/li&gt;
&lt;li&gt;第二部分 使用Maven构建项目&lt;/li&gt;
&lt;li&gt;第三部分 
      
    
    </summary>
    
      <category term="Spring" scheme="https://zjrongxiang.github.io/categories/Spring/"/>
    
    
  </entry>
  
  <entry>
    <title>外网环境访问内网（NAT）Kafka集群介绍</title>
    <link href="https://zjrongxiang.github.io/2019/09/07/2019-09-07-%E5%A4%96%E7%BD%91%E7%8E%AF%E5%A2%83%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%EF%BC%88NAT%EF%BC%89Kafka%E9%9B%86%E7%BE%A4%E4%BB%8B%E7%BB%8D/"/>
    <id>https://zjrongxiang.github.io/2019/09/07/2019-09-07-外网环境访问内网（NAT）Kafka集群介绍/</id>
    <published>2019-09-07T05:30:00.000Z</published>
    <updated>2019-12-28T14:48:17.399Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   Kafka几个配置参数介绍</li><li>第二部分   外网环境访问内网（NAT）Kafka集群配置</li><li>第三部分  总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>其实这是一个踩坑笔记。首先介绍踩坑背景。生产环境我们有两个网络区域，记为网络区域A（内网）、网络区域B（外网），其中为了外网环境能访问内网环境，内网对内部IP实施了IP映射（NAT），将内网IP映射为外部IP。Kafka版本为：kafka_2.11-0.10.2.0。IP清单及网络数据流图如下：</p><table><thead><tr><th style="text-align:center">hostname</th><th style="text-align:center">内网ip</th><th style="text-align:center">外网Ip</th></tr></thead><tbody><tr><td style="text-align:center">kafka.node1</td><td style="text-align:center">192.168.1.1</td><td style="text-align:center">10.0.0.1</td></tr><tr><td style="text-align:center">kafka.node2</td><td style="text-align:center">192.168.1.2</td><td style="text-align:center">10.0.0.2</td></tr><tr><td style="text-align:center">kafka.node3</td><td style="text-align:center">192.168.1.3</td><td style="text-align:center">10.0.0.3</td></tr></tbody></table><p>整个架构图为：</p><p><img src="\images\picture\kafka\kafkaNat.png" alt=""></p><p>外网网络区域的客户端开始使用NAT地址（10.0.0.1-3）地址访问内部kafka，发现无法生产和消费kafka数据（telnet netip 9092是通的），会报解析服务器hostname失败的错误。而内部网络的客户端使用内网地址（192.168.1.1-3）是可以正常生产和消费kafka数据。</p><p>原因：advertised.listeners配置的是内网实地址，这个地址注册到Zookeeper中，当消费者和生产者访问时，Zookeeper将该地址提供给消费者和生产者。由于是内网地址，外网根本看不到这个地址（路由寻址）。所以无法获取元数据信息，通信异常。</p><h2 id="第一部分-Kafka几个配置参数介绍"><a href="#第一部分-Kafka几个配置参数介绍" class="headerlink" title="第一部分 Kafka几个配置参数介绍"></a>第一部分 Kafka几个配置参数介绍</h2><p>首先要了解一下几个配置：</p><ul><li><p>host.name<br><strong>已弃用。</strong> 仅当listeners属性未配置时被使用，已用listeners属性代替。表示broker的hostname。</p></li><li><p>advertised.host.name<br><strong>已弃用</strong>。仅当advertised.listeners或者listeners属性未配置时被使用。官网建议使用advertised.listeners。该配置的意思是注册到zookeeper上的broker的hostname或ip。是提供给客户端与kafka通信使用的。如果没有设置则使用host.name。</p></li><li><p>listeners<br>监听列表，broker对外提供服务时绑定的IP和端口。多个以逗号隔开，如果监听器名称是一个安全的协议， listener.security.protocol.map也必须设置。主机名称设置0.0.0.0绑定所有的接口，主机名称为空则绑定默认的接口。如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">listeners = PLAINTEXT://myhost:9092,SSL://:9091 CLIENT://0.0.0.0:9092,REPLICATION://localhost:9093</span><br></pre></td></tr></table></figure><p>如果未指定该配置，则使用java.net.InetAddress.getCanonicalHostName()函数的的返回值。</p></li><li><p>advertised.listeners<br>客户端使用。发布至zookeeper的监听，broker会上送此地址到zookeeper，zookeeper会将此地址提供给消费者和生产者，消费者和生产者根据此地址获取消息。如果和上面的listeners不同则以此为准，在IaaS环境，此配置项可能和 broker绑定的接口主机名称不同，如果此配置项没有配置则以上面的listeners为准。</p></li></ul><h2 id="第二部分-外网环境访问内网（NAT）Kafka集群配置"><a href="#第二部分-外网环境访问内网（NAT）Kafka集群配置" class="headerlink" title="第二部分 外网环境访问内网（NAT）Kafka集群配置"></a>第二部分 外网环境访问内网（NAT）Kafka集群配置</h2><h3 id="2-1-配置hosts方式"><a href="#2-1-配置hosts方式" class="headerlink" title="2.1 配置hosts方式"></a>2.1 配置hosts方式</h3><ul><li><p>Kafka集群节点配置</p><p>每一台Kafka节点的hosts节点配置内部地址映射：</p><p>192.168.1.1 kafka.node1<br>192.168.1.2 kafka.node2<br>192.168.1.3 kafka.node3</p><p>Kafka中的配置文件（config/server.properties配置文件）</p><p>advertised.listeners=PLAINTEXT://kafka.node1:9092</p><p>advertised.listeners=PLAINTEXT://kafka.node2:9092</p><p>advertised.listeners=PLAINTEXT://kafka.node3:9092</p></li><li><p>客户端节点配置</p><p>客户端的hosts文件也需要配置外部地址映射：</p><p>10.0.0.1 kafka.node1<br>10.0.0.2 kafka.node2<br>10.0.0.3 kafka.node3</p><p>应用程序使用</p><p>bootstrap.servers:  [‘kafka.node1:9092’,’kafka.node2:9092’,’kafka.node3:9092’]</p></li></ul><p>配置完成后，重启Kafka集群，重新使用客户端链接，测试客户端可以正常向Topic生产和消费数据。</p><h3 id="2-2-内外部流量分离"><a href="#2-2-内外部流量分离" class="headerlink" title="2.2 内外部流量分离"></a>2.2 内外部流量分离</h3><p>通常对于外部网络访问内网安全区域，架构使用安全套接字层 (SSL) 来保护外部客户端与 Kafka 之间的流量。而使用明文进行内部网络的broker间的通信。当 Kafka 侦听器绑定到用于内部和外部通信的网络接口时，配置侦听器就非常简单了。但在许多情况下，例如在云原生环境上部署时，集群中 Kafka broker 的外部通告地址将与 Kafka 使用的内部网络接口不同。在此情况下，可以 <code>server.properties</code> 中的参数 <code>advertised.listeners</code> 进行如下配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Configure protocol map</span></span><br><span class="line">listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:SSL</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use plaintext <span class="keyword">for</span> inter-broker communication</span></span><br><span class="line">inter.broker.listener.name=INTERNAL</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Specify that Kafka listeners should <span class="built_in">bind</span> to all <span class="built_in">local</span> interfaces</span></span><br><span class="line">listeners=INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Separately, specify externally visible address</span></span><br><span class="line">advertised.listeners=INTERNAL://kafkabroker-n.mydomain.com:9092,EXTERNAL://kafkabroker-n.mydomain.com:9093</span><br></pre></td></tr></table></figure><p>内部使用9092端口，而外部网络使用9093端口。</p><h2 id="第三部分-总结"><a href="#第三部分-总结" class="headerlink" title="第三部分 总结"></a>第三部分 总结</h2><p>Kafka集群在内外网网络环境下，需要关注地址映射。使用hosts 本地DNS进行主机名和内外地址的映射。至此爬出该坑。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、kafka - advertised.listeners和listeners，链接：<a href="https://www.cnblogs.com/fxjwind/p/6225909.html" target="_blank" rel="noopener">https://www.cnblogs.com/fxjwind/p/6225909.html</a></p><p>2、Kafka从上手到实践-Kafka集群：Kafka Listeners，链接：<a href="http://www.devtalking.com/articles/kafka-practice-16/" target="_blank" rel="noopener">http://www.devtalking.com/articles/kafka-practice-16/</a></p><p>3、使用 Cloud Dataflow 处理来自 Kafka 的外部托管消息 链接：<a href="https://cloud.google.com/solutions/processing-messages-from-kafka-hosted-outside-gcp?hl=zh-cn" target="_blank" rel="noopener">https://cloud.google.com/solutions/processing-messages-from-kafka-hosted-outside-gcp?hl=zh-cn</a></p><p>4、Kafka Listeners - Explained 链接：<a href="https://rmoff.net/2018/08/02/kafka-listeners-explained/" target="_blank" rel="noopener">https://rmoff.net/2018/08/02/kafka-listeners-explained/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   Kafka几个配置参数介绍&lt;/li&gt;
&lt;li&gt;第二部分   外网环境访问内网（NAT）Ka
      
    
    </summary>
    
      <category term="Kafka" scheme="https://zjrongxiang.github.io/categories/Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>Helm简介和安装部署</title>
    <link href="https://zjrongxiang.github.io/2019/08/08/2019-08-08-Helm%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    <id>https://zjrongxiang.github.io/2019/08/08/2019-08-08-Helm简介和安装部署/</id>
    <published>2019-08-08T13:30:00.000Z</published>
    <updated>2019-11-18T13:02:59.451Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   Minikube集群启动</li><li>第一部分   Kubernetes中StatefulSet介绍</li><li>第三部分   部署Zookeeper集群</li><li>第四部分   部署Kafka集群</li><li>第五部分   总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Application deployment management for Kubernetes。</p><h2 id="第一部分-Helm的部署"><a href="#第一部分-Helm的部署" class="headerlink" title="第一部分 Helm的部署"></a>第一部分 Helm的部署</h2><p>Helm是由helm CLI和Tiller组成，是典型的C/S应用。helm运行与客户端，提供命令行界面，而Tiller应用运行在Kubernetes内部。</p><h3 id="1-1-部署Helm-CLI客户端"><a href="#1-1-部署Helm-CLI客户端" class="headerlink" title="1.1 部署Helm CLI客户端"></a>1.1 部署Helm CLI客户端</h3><p>helm客户端是一个单纯的可执行文件，我们从github上直接下载压缩包（由于墙的原因可能很慢）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm# wget https://get.helm.sh/helm-v2.14.3-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>解压缩介质文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm# tar -zxvf helm-v2.14.3-linux-amd64.tar.gz </span><br><span class="line">linux-amd64/</span><br><span class="line">linux-amd64/helm</span><br><span class="line">linux-amd64/README.md</span><br><span class="line">linux-amd64/LICENSE</span><br><span class="line">linux-amd64/tiller</span><br></pre></td></tr></table></figure><p>部署：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm# mv linux-amd64/helm /usr/local/bin</span><br></pre></td></tr></table></figure><p>检查：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm# helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:"v2.14.3", GitCommit:"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085", GitTreeState:"clean"&#125;</span><br><span class="line">Error: could not find tiller</span><br></pre></td></tr></table></figure><p>提示未连接到服务端tiller。下面我们部署服务端。</p><h3 id="1-2-部署服务端"><a href="#1-2-部署服务端" class="headerlink" title="1.2 部署服务端"></a>1.2 部署服务端</h3><p>Helm 的服务器端部分 Tiller 通常运行在 Kubernetes 集群内部。但是对于开发，它也可以在本地运行，并配置为与远程 Kubernetes 群集通信。</p><p>安装 <code>tiller</code> 到群集中最简单的方法就是运行 <code>helm init</code>。这将验证 <code>helm</code> 本地环境设置是否正确（并在必要时进行设置）。然后它会连接到 <code>kubectl</code> 默认连接的任何集群（<code>kubectl config view</code>）。一旦连接，它将安装 <code>tiller</code> 到 <code>kube-system</code> 命名空间中。</p><p><code>helm init</code> 以后，可以运行 <code>kubectl get pods --namespace kube-system</code> 并看到 Tiller 正在运行。</p><p>你可以通过参数运行 <code>helm init</code>:</p><ul><li><code>--canary-image</code> 参数安装金丝雀版本</li><li><code>--tiller-image</code> 安装特定的镜像（版本）</li><li><code>--kube-context</code> 使用安装到特定群集</li><li><code>--tiller-namespace</code> 用一个特定的命名空间 (namespace) 安装</li><li><code>--service-account</code> 使用 Service Account 安装 <a href="https://whmzsu.github.io/helm-doc-zh-cn/quickstart/securing_installation-zh_cn.html#rbac" target="_blank" rel="noopener">RBAC enabled clusters</a></li><li><code>--automount-service-account false</code> 不适用 service account 安装</li></ul><p>一旦安装了 Tiller，运行 <code>helm version</code> 会显示客户端和服务器版本。（如果它仅显示客户端版本， helm 则无法连接到服务器, 使用 <code>kubectl</code> 查看是否有任何 tiller Pod 正在运行。）</p><p>除非设置 <code>--tiller-namespace</code> 或 <code>TILLER_NAMESPACE</code> 参数，否则 Helm 将在命名空间 <code>kube-system</code> 中查找 Tiller 。</p><blockquote><p>在缺省配置下， Helm 会利用 “<a href="http://gcr.io/kubernetes-helm/tiller" target="_blank" rel="noopener">gcr.io/kubernetes-helm/tiller</a>“ 镜像在Kubernetes集群上安装配置 Tiller；并且利用 “<a href="https://kubernetes-charts.storage.googleapis.com" target="_blank" rel="noopener">https://kubernetes-charts.storage.googleapis.com</a>“ 作为缺省的 stable repository 的地址。由于在国内可能无法访问 “<a href="http://gcr.io" target="_blank" rel="noopener">gcr.io</a>“, “<a href="http://storage.googleapis.com" target="_blank" rel="noopener">storage.googleapis.com</a>“ 等域名，阿里云容器服务为此提供了镜像站点。</p></blockquote><p>首先创建服务。创建rbac-config.yaml文件，文件内容为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">cluster-admin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">  - kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">    namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure><p>通过yaml文件创建服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm# kubectl create -f rbac-config.yaml</span><br><span class="line">serviceaccount/tiller created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/tiller created</span><br></pre></td></tr></table></figure><p>启Helm pod，即安装tiller：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm#helm init -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.10.0 \</span><br><span class="line">    --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts \</span><br><span class="line">    --service-account tiller </span><br><span class="line"><span class="meta">#</span><span class="bash"> 回显</span></span><br><span class="line">Creating /root/.helm </span><br><span class="line">Creating /root/.helm/repository </span><br><span class="line">Creating /root/.helm/repository/cache </span><br><span class="line">Creating /root/.helm/repository/local </span><br><span class="line">Creating /root/.helm/plugins </span><br><span class="line">Creating /root/.helm/starters </span><br><span class="line">Creating /root/.helm/cache/archive </span><br><span class="line">Creating /root/.helm/repository/repositories.yaml </span><br><span class="line">Adding stable repo with URL: https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts </span><br><span class="line">Adding local repo with URL: http://127.0.0.1:8879/charts </span><br><span class="line"><span class="meta">$</span><span class="bash">HELM_HOME has been configured at /root/.helm.</span></span><br><span class="line">Warning: Tiller is already installed in the cluster.</span><br><span class="line">(Use --client-only to suppress this message, or --upgrade to upgrade Tiller to the current version.)</span><br></pre></td></tr></table></figure><p>这时候我们再次执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/helm# helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:"v2.14.3", GitCommit:"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085", GitTreeState:"clean"&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:"v2.10.0", GitCommit:"9ad53aac42165a5fadc6c87be0dea6b115f93090", GitTreeState:"clean"&#125;</span><br></pre></td></tr></table></figure><p>这样Helm的服务端tiller就部署完毕。</p><h3 id="1-3-Helm-CLI-命令简要汇总"><a href="#1-3-Helm-CLI-命令简要汇总" class="headerlink" title="1.3 Helm CLI 命令简要汇总"></a>1.3 Helm CLI 命令简要汇总</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// 安装一个 Chart</span><br><span class="line">helm install stable/mysql</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 列出 Kubernetes 中已部署的 Chart</span><br><span class="line">helm list --all</span><br><span class="line"></span><br><span class="line">// helm repo 的操作</span><br><span class="line">helm repo update</span><br><span class="line">helm repo list</span><br><span class="line">helm repo add dev https://example.com/dev-charts</span><br><span class="line"></span><br><span class="line">// 创建一个 Chart，会产生一个 Chart 所需的目录结构</span><br><span class="line">helm create deis-workflow</span><br><span class="line"></span><br><span class="line">// 安装自定义 chart</span><br><span class="line">helm inspect values stable/mysql  # 列出一个 chart 的可配置项</span><br><span class="line"></span><br><span class="line">helm install -f config.yaml stable/mysql # 可以将修改的配置项写到文件中通过 -f 指定并替换</span><br><span class="line">helm install --set name: value stable/mysql # 也可以通过 --set 方式替换</span><br><span class="line"></span><br><span class="line">// 当新版本 chart 发布时，或者当你需要更改 release 配置时，helm 必须根据现在已有的 release 进行升级</span><br><span class="line">helm upgrade -f panda.yaml happy-panda stable/mariadb</span><br><span class="line"></span><br><span class="line">// 删除 release</span><br><span class="line">helm delete happy-panda</span><br></pre></td></tr></table></figure><h2 id="参考文献及材料"><a href="#参考文献及材料" class="headerlink" title="参考文献及材料"></a>参考文献及材料</h2><p>1、Helm User Guide - Helm 用户指南 <a href="https://whmzsu.github.io/helm-doc-zh-cn/" target="_blank" rel="noopener">https://whmzsu.github.io/helm-doc-zh-cn/</a></p><p>2、Kubernetes 包管理工具 Helm 简介 <a href="https://www.jianshu.com/p/d55e91e28f94" target="_blank" rel="noopener">https://www.jianshu.com/p/d55e91e28f94</a></p><p>3、Helm介绍 <a href="https://zhaohuabing.com/2018/04/16/using-helm-to-deploy-to-kubernetes/" target="_blank" rel="noopener">https://zhaohuabing.com/2018/04/16/using-helm-to-deploy-to-kubernetes/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   Minikube集群启动&lt;/li&gt;
&lt;li&gt;第一部分   Kubernetes中State
      
    
    </summary>
    
      <category term="Helm" scheme="https://zjrongxiang.github.io/categories/Helm/"/>
    
    
  </entry>
  
  <entry>
    <title>从Spark on Yarn到Python on Yarn</title>
    <link href="https://zjrongxiang.github.io/2019/08/01/2019-01-28-%E4%BB%8ESpark%20on%20Yarn%E5%88%B0Python%20on%20Yarn/"/>
    <id>https://zjrongxiang.github.io/2019/08/01/2019-01-28-从Spark on Yarn到Python on Yarn/</id>
    <published>2019-08-01T11:30:00.000Z</published>
    <updated>2019-08-01T15:13:51.655Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>术语说明</li><li>背景</li><li>第一部分   Apache Spark运行模式介绍</li><li>第二部分   Spark on Yarn</li><li>第三部分   Pyspark Application原理</li><li>第四部分   Python on Yarn配置及运行</li><li>第五部分   总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Apache Spark属于重要的大数据计算框架，另外spark还提供了Python的原生API和机器学习组件Spark Ml，使的可以通过Python编写机器学习任务由Spark运行。本篇文件从Spark运行模式开始讲起，重点介绍Spark on Yarn运行模式，最后重点介绍Python on Yarn（即Pyspark on Yarn）上运行原理和案例。</p><h2 id="第一部分-Apache-Spark运行模式"><a href="#第一部分-Apache-Spark运行模式" class="headerlink" title="第一部分 Apache Spark运行模式"></a>第一部分 Apache Spark运行模式</h2><p>目前 Apache Spark已知支持5种运行模式。按照节点资源数量可以分为单节点模式（2种）和集群模式（3种）。</p><ul><li>单节点模式：本地模式、本地伪集群模式</li><li>集群模式：Standalone模式、Spark on Yarn模式、Spark on Mesos模式</li><li>原生云模式：在Kubernetes上运行。随着Docker容器和原生云技术的兴起，Spark开始支持在Kubernetes上运行。</li></ul><blockquote><p>对于Spark on Kubernetes可以参考官方文档：<a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html。另外可以参考我的另外一篇技术总结：《在Minikube上运行Spark集群》。" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/running-on-kubernetes.html。另外可以参考我的另外一篇技术总结：《在Minikube上运行Spark集群》。</a></p></blockquote><h3 id="1-1-本地模式（单节点模式）"><a href="#1-1-本地模式（单节点模式）" class="headerlink" title="1.1 本地模式（单节点模式）"></a>1.1 本地模式（单节点模式）</h3><p>本地模式又称为Loacl[N]模式。该模式只需要在单节点上解压spark包即可运行，使用多个线程模拟Spark分布式计算，Master和Worker运行在同一个JVM虚拟机中。这里参数N代表可以使用（预申请）N个线程资源，每个线程拥有一个Core（默认值N=1）。</p><blockquote><p>如果参数为：Loacl[*]，表明：Run Spark locally with as many worker threads as logical cores on your machine。即线程数和物理核数相同。</p></blockquote><p>例如下面的启动命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ./spark-submit –class org.apache.spark.examples.JavaWordCount –master <span class="built_in">local</span>[*] spark-examples_2.11-2.3.1.jar file:///opt/README.md</span></span><br></pre></td></tr></table></figure><blockquote><p>该模式不依懒于HDFS分布式文件系统。例如上面的命令使用的本地文件系统。</p></blockquote><h3 id="1-2-本地伪集群模式（单节点模式）"><a href="#1-2-本地伪集群模式（单节点模式）" class="headerlink" title="1.2 本地伪集群模式（单节点模式）"></a>1.2 本地伪集群模式（单节点模式）</h3><p>该模式和Local[N]类似，不同的是，它会在单机启动多个进程来模拟集群下的分布式场景，而不像Local[N]这种多个线程在一个进程下共享资源。通常用来测试和验证应用程序逻辑上有没有问题，或者想使用Spark的计算框架而而受限于没有太多资源。</p><p>作业提交命令中使用local-cluster[x,y,z]参数模式：x代表要生成的executor数，y和z分别代表每个executor所拥有的core和memory数值。例如下面的命令作业申请了2个executor 进程，每个进程分配3个core和1G的内存，来运行应用程序。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ./spark-submit –master <span class="built_in">local</span>-cluster[2, 3, 1024]</span></span><br></pre></td></tr></table></figure><h3 id="1-3-Standalone模式（集群模式）"><a href="#1-3-Standalone模式（集群模式）" class="headerlink" title="1.3 Standalone模式（集群模式）"></a>1.3 Standalone模式（集群模式）</h3><h4 id="1-3-1-构架部署"><a href="#1-3-1-构架部署" class="headerlink" title="1.3.1 构架部署"></a>1.3.1 构架部署</h4><p>Standalone为spark自带资源管理系统（即经典的Master/Slaves架构模式）。该模式下集群由Master和Worker节点组成，程序通过与Master节点交互申请资源，Worker节点启动Executor运行。具体数据流图如下：</p><p><img src="\images\picture\python-on-yarn\spark.bmp" alt=""></p><p>另外考虑到Master节点存在单点故障。Spark支持使用Zookeeper实现HA高可用（high avalible）。Zookeeper提供一种领导选举的机制，通过该机制可以保证集群中只有一个Master节点处于RecoveryState.Active状态，其他Master节点处于RecoveryState.Standby状态。</p><p><img src="\images\picture\python-on-yarn\zk-spark.bmp" alt=""></p><h4 id="1-3-2-作业运行模式"><a href="#1-3-2-作业运行模式" class="headerlink" title="1.3.2 作业运行模式"></a>1.3.2 作业运行模式</h4><p>在该模式下，用户提交任务有两种方式：Standalone-client和Standalone-cluster。</p><h4 id="1-5-1-Client模式"><a href="#1-5-1-Client模式" class="headerlink" title="1.5.1 Client模式"></a>1.5.1 Client模式</h4><p>执行流程：</p><p>(1)客户端启动Driver进程。</p><p>(2)Driver向Master申请启动Application启动需要的资源。</p><p>(3)资源申请成功后，Driver将task发送到相应的Worker节点执行，并负责监控task运行情况。</p><p>(4)Worker将task执行结果返回到客户端的Driver进程。</p><blockquote><p>Client模式适用于调试程序。Driver进程在客户端侧启动，如果生产采用这种模式，当业务量较大时，客户端需要启动大量Driver进程，会消耗大量系统资源，导致资源枯竭。</p></blockquote><h4 id="1-5-2-Cluster模式"><a href="#1-5-2-Cluster模式" class="headerlink" title="1.5.2 Cluster模式"></a>1.5.2 Cluster模式</h4><p>执行流程：</p><p>(1)客户端会想Master节点申请启动Driver。</p><p>(2)Master受理客户端的请求，分配一个Work节点，启动Driver进程。</p><p>(3)Driver启动后，重新想Master节点申请运行资源，Master分配资源，并在相应的Worker节点上启动Executor进程。</p><p>(4)Driver发送task到相应的Worker节点运行，并负责监控task。</p><p>(5)Worker将task执行结果返回到Driver进程。</p><blockquote><p>Driver运行有Master在集群Worker节点上随机分配，相当于在集群上负载资源。</p></blockquote><p>两种方式最大的区别就是Driver进程运行的位置。Cluster模式相对于Client模式更适合于生成环境的部署。</p><h3 id="1-4-Spark-on-Yarn（集群模式）"><a href="#1-4-Spark-on-Yarn（集群模式）" class="headerlink" title="1.4 Spark on Yarn（集群模式）"></a>1.4 Spark on Yarn（集群模式）</h3><p>目前大部分企业级Spark都是跑在已有的Hadoop集群（hadoop 2.0系统）中，均使用Yarn来作为Spark的Cluster Manager，为Spark提供资源管理服务，Spark自身完成任务调度和计算。这部分内容会在后文中细致介绍。</p><h3 id="1-5-Spark-on-Mesos（集群模式）"><a href="#1-5-Spark-on-Mesos（集群模式）" class="headerlink" title="1.5 Spark on Mesos（集群模式）"></a>1.5 Spark on Mesos（集群模式）</h3><p>参考官方文档介绍：<a href="https://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/running-on-mesos.html</a></p><h2 id="第二部分-Spark-on-Yarn"><a href="#第二部分-Spark-on-Yarn" class="headerlink" title="第二部分 Spark on Yarn"></a>第二部分 Spark on Yarn</h2><p>我们知道MapReduce任务是运行在Yarn上的，同样Spark Application也可以运行在Yarn上。这种模式下，资源的管理、协调、执行和监控交给Yarn集群完成。</p><blockquote><p>Yarn集群上可以运行：MapReduce任务、Spark Application、Hbase集群、Storm集群、Flink集群等等，还有我们后续重点介绍的Python on Yarn。</p></blockquote><p>从节点功能上看，Yarn也采用类似Standalone模式的Master/Slave结构。资源框架中RM（ResourceManager）对应Master，NM（NodeManager）对应Slave。RM负责各个NM资源的统一管理和调度，NM节点负责启动和执行任务以及各任务间的资源隔离。</p><p>当集群中存在多种计算框架时，架构上选用Yarn统一管理资源要比Standalone更合适。类似Standalone模式，Spark on Yarn也有两种运行方式：Yarn-Client模式和Yarn-Cluster模式。从适用场景上看，Yarn-Cluster模式适用于生产环境，而Yarn-Client模式更适用于开发（交互式调试）。</p><h3 id="2-1-Client模式"><a href="#2-1-Client模式" class="headerlink" title="2.1 Client模式"></a>2.1 Client模式</h3><p>在Yarn-client模式下，Driver运行在本地Client上，通过AM（ApplicationMaster）向RM申请资源。本地Driver负责与所有的executor container进行交互，并将最后的结果汇总。结束掉Client，相当于kill掉这个spark应用。</p><p><img src="\images\picture\python-on-yarn\yarn-client-mode.png" alt=""></p><ul><li>Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend。</li><li>ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派。</li><li>Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）。</li><li>一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task。</li><li>client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。</li><li>应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。</li></ul><h3 id="2-2-Cluster模式"><a href="#2-2-Cluster模式" class="headerlink" title="2.2 Cluster模式"></a>2.2 Cluster模式</h3><p>在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：</p><ol><li>第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动。</li><li>第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成。</li></ol><p>应用的运行结果不能在客户端显示（可以在history server中查看），所以最好将结果保存在HDFS而非stdout输出，客户端的终端显示的是作为YARN的job的简单运行状况，下图是yarn-cluster模式：</p><p><img src="\images\picture\python-on-yarn\yarn-cluster-mode.png" alt=""></p><p>执行过程： </p><ul><li>Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等。</li><li>ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化。</li><li>ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束。</li><li>一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，而Executor对象的创建及维护是由。CoarseGrainedExecutorBackend负责的，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等。</li><li>ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。</li><li>应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。</li></ul><h3 id="2-3-两种模式的比较"><a href="#2-3-两种模式的比较" class="headerlink" title="2.3 两种模式的比较"></a>2.3 两种模式的比较</h3><p>在client模式下，Spark Application运行的Driver会在提交程序的节点上，而该节点可以是YARN集群内部节点，也可以不是。一般来说提交Spark Application的客户端节点不是YARN集群内部的节点，那么在客户端节点上可以根据自己的需要安装各种需要的软件和环境，以支撑Spark Application正常运行。在cluster模式下，Spark Application运行时的所有进程都在YARN集群的NodeManager节点上，而且具体在哪些NodeManager上运行是由YARN的调度策略所决定的。</p><p>对比这两种模式，最关键的是Spark Application运行时Driver所在的节点不同，而且，如果想要对Driver所在节点的运行环境进行配置，区别很大，但这对于PySpark Application运行来说是非常关键的。</p><h2 id="第三部分-Pyspark-Application原理"><a href="#第三部分-Pyspark-Application原理" class="headerlink" title="第三部分 Pyspark Application原理"></a>第三部分 Pyspark Application原理</h2><p>PySpark是Spark为使用Python程序编写Spark Application而实现的客户端库，通过PySpark也可以编写Spark Application并在Spark集群上运行。Python具有非常丰富的科学计算、机器学习处理库，如numpy、pandas、scipy等等。为了能够充分利用这些高效的Python模块，很多机器学习程序都会使用Python实现，同时也希望能够在Spark集群上运行。</p><p>理解PySpark Application的运行原理，有助于我们使用Python编写Spark Application，并能够对PySpark Application进行各种调优。PySpark构建于Spark的Java API之上，数据在Python脚本里面进行处理，而在JVM中缓存和Shuffle数据，数据处理流程如下图所示:</p><p><img src="http://www.uml.org.cn/bigdata/images/2017111321.png" alt="img"></p><p>Spark Application会在Driver中创建pyspark.SparkContext对象，后续通过pyspark.SparkContext对象来构建Job DAG并提交DAG运行。使用Python编写PySpark Application，在Python编写的Driver中也有一个pyspark.SparkContext对象，该pyspark.SparkContext对象会通过Py4J模块启动一个JVM实例，创建一个JavaSparkContext对象。PY4J只用在Driver上，后续在Python程序与JavaSparkContext对象之间的通信，都会通过PY4J模块来实现，而且都是本地通信。</p><p>PySpark Application中也有RDD，对Python RDD的Transformation操作，都会被映射到Java中的PythonRDD对象上。对于远程节点上的Python RDD操作，Java PythonRDD对象会创建一个Python子进程，并基于Pipe的方式与该Python子进程通信，将用户编写Python处理代码和数据发送到Python子进程中进行处理。</p><h2 id="第四部分-Python-on-Yarn配置及运行"><a href="#第四部分-Python-on-Yarn配置及运行" class="headerlink" title="第四部分 Python on Yarn配置及运行"></a>第四部分 Python on Yarn配置及运行</h2><h3 id="4-1-Yarn节点配置Python环境"><a href="#4-1-Yarn节点配置Python环境" class="headerlink" title="4.1 Yarn节点配置Python环境"></a>4.1 Yarn节点配置Python环境</h3><p>该模式需要在Yarn集群上每个NM节点（Node Manager）上部署Python编译环境，即安装Python安装包、依赖模块。用户编写的Pyspark Application由集群中Yarn调度执行。</p><blockquote><p>通常使用Anaconda安装包进行统一部署，简化环境的部署。</p></blockquote><p>该模式存在下面缺点：</p><ul><li>新增依赖包部署安装代价大。如果后续用户编写的Spark Application需要依赖新的Python模块或包，那么就需要依次在集群Node Manager上部署更新依赖包。</li><li>用户对于Python环境的依赖差异化无法满足。通常不同用户编写Spark Application会依赖不同的Python环境，比如Python2、Python3环境等等。该模式下只能支持一种环境，无法满足Python多环境的需求。</li><li>各节点的Python环境需要统一。由于用户提交的Spark Application具体在哪些Node Manager上执行，由YARN调度决定，所以必须保证每个节点的Python环境（基础环境+依赖环境）都是相同的，环境维护成本高。</li></ul><h3 id="4-2-Yarn节点不配置Python环境"><a href="#4-2-Yarn节点不配置Python环境" class="headerlink" title="4.2 Yarn节点不配置Python环境"></a>4.2 Yarn节点不配置Python环境</h3><p>该模式不需要提前在集群Node Manager上预安装Python环境。</p><blockquote><p>参考文章：<a href="http://quasiben.github.io/blog/2016/4/15/conda-spark/" target="_blank" rel="noopener">http://quasiben.github.io/blog/2016/4/15/conda-spark/</a></p></blockquote><p>我们基于华为C60集群（开源集群相同）以及Anaconda环境对该模式进行了测试验证。具体实现思路如下所示：</p><ol><li>在一台SUSE节点上部署Anaconda，并创建虚拟Python环境（如果需要可以部署安装部分依赖包）。</li><li>创建conda虚拟环境，并整体打包为zip文件。</li><li>用户提交PySpark Application时，使用<code>--archives</code>参数指定该zip文件路径。</li></ol><p>详细操作步骤如下：</p><ul><li><strong>第一步</strong></li></ul><p>下载Anaconda3-4.2.0-Linux-x86_64.sh安装软件（基于python3.5），在SUSE服务器上部署安装。Anaconda的安装路径为/usr/anaconda3。查看客户端服务器的python环境清单：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/usr/anaconda3 # conda env list</span><br><span class="line"><span class="meta">#</span><span class="bash"> conda environments:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line">root                  *  /usr/anaconda3</span><br></pre></td></tr></table></figure><p>其中root环境为目前的主环境。为了便于环境版本管理我们新建一个专用环境（mlpy_env）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/usr/anaconda3/envs # conda create -n mlpy --clone root</span><br></pre></td></tr></table></figure><p>上述命令创建了一个名称为mlpy_env的Python环境，clone选项将对应的软件包都安装到该环境中，包括一些C的动态链接库文件。</p><p>接着，将该Python环境打包，执行如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/usr/anaconda3/envs # cd /root/anaconda2/envs</span><br><span class="line">dkfzxwma07app08:/usr/anaconda3/envs # zip -r mlpy_env.zip mlpy_env</span><br></pre></td></tr></table></figure><p>将该zip压缩包拷贝到指定目录中（或者后续引用使用绝对路径），方便后续提交PySpark Application：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/usr/anaconda3/envs # cp mlpy_env.zip /tmp/</span><br></pre></td></tr></table></figure><p>最后，我们可以提交我们的PySpark Application，执行如下命令（或打包成shell脚本）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python </span><br><span class="line">spark-submit \</span><br><span class="line">--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python \</span><br><span class="line">--master yarn-cluster \</span><br><span class="line">--archives /tmp/mlpy_env.zip#ANACONDA \</span><br><span class="line">/var/lib/hadoop-hdfs/pyspark/test_pyspark_dependencies.py</span><br></pre></td></tr></table></figure><blockquote><p>注意：下面命令指的是zip包将在ANACONDA的目录中展开，需要注意路径。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> --archives /tmp/mlpy_env.zip<span class="comment">#ANACONDA</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>环境打包需要注意压缩路径。</p></blockquote><p>上面的依赖zip压缩包将整个Python的运行环境都包含在里面，在提交PySpark Application时会将该环境zip包上传到运行Application的所在的每个节点上。解压缩后为Python代码提供运行时环境。如果不想每次都从客户端将该环境文件上传到集群中运行节点上，也可以提前将zip包上传到HDFS文件系统中，并修改–archives参数的值为hdfs:///tmp/mlpy_env.zip #ANACONDA（注意环境差异），也是可以的。</p><p>另外，需要说明的是，如果我们开发的/var/lib/hadoop-hdfs/pyspark /test_pyspark_dependencies.py文件中，依赖多个其他Python文件，想要通过上面的方式运行，必须将这些依赖的Python文件拷贝到我们创建的环境中，对应的目录为mlpy_env/lib/python2.7/site-packages/下面。</p><blockquote><p>注意：pyspark不支持python3.6版本，所以python环境使用python3.5</p><p>否则程序执行回显会有这样的报错信息：</p><p>TypeError: namedtuple() missing 3 required keyword-only arguments: ‘verbose’, ‘rename’, and ‘module’</p><p><a href="https://issues.apache.org/jira/browse/SPARK-19019?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-19019?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel</a></p></blockquote><h3 id="4-3-一个机器学习任务栗子"><a href="#4-3-一个机器学习任务栗子" class="headerlink" title="4.3 一个机器学习任务栗子"></a>4.3 一个机器学习任务栗子</h3><p>举一个Kmeans无监督算法的Python案例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.clustering <span class="keyword">import</span> KMeans, KMeansModel</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> array</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建spark context</span></span><br><span class="line">sc = SparkContext(appName=<span class="string">"kmeans"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载和解析数据文件</span></span><br><span class="line">stg_path = <span class="string">"hdfs://hacluster"</span> + <span class="string">"/user/"</span> + str(os.environ[<span class="string">'USER'</span>]) + <span class="string">"/.sparkStaging/"</span> + str(sc.applicationId) + <span class="string">"/"</span> </span><br><span class="line">data = sc.textFile(os.path.join(stg_path,<span class="string">'kmeans_data.txt'</span>)) </span><br><span class="line">parsedData = data.map(<span class="keyword">lambda</span> line: array([float(x) <span class="keyword">for</span> x <span class="keyword">in</span> line.split(<span class="string">' '</span>)]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">clusters = KMeans.train(parsedData, <span class="number">2</span>, maxIterations=<span class="number">10</span>,runs=<span class="number">10</span>,initializationMode=<span class="string">"random"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">error</span><span class="params">(point)</span>:</span></span><br><span class="line">    i = clusters.predict(point)</span><br><span class="line">    center = clusters.centers[i]</span><br><span class="line">    print(<span class="string">"("</span> + str(point[<span class="number">0</span>]) + <span class="string">","</span> + str(point[<span class="number">1</span>]) + <span class="string">","</span> + str(point[<span class="number">2</span>]) + <span class="string">")"</span> + <span class="string">"blongs to cluster "</span> + str(i+<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># print("Cluster Number:" + str(len(clusters.centers)))</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum([x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> (point - center)]))</span><br><span class="line"></span><br><span class="line">WSSSE = parsedData.map(<span class="keyword">lambda</span> point: error(point)).reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">print(<span class="string">"Within Set Sum of Squared Error = "</span> + str(WSSSE))</span><br><span class="line"><span class="comment"># 打印类心</span></span><br><span class="line"><span class="keyword">for</span> mCenter <span class="keyword">in</span> clusters.centers:</span><br><span class="line">    print(mCenter)</span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">myModelPath = <span class="string">"hdfs://hacluster"</span>+<span class="string">"/user/model/"</span>+<span class="string">"KMeansModel.ml"</span></span><br><span class="line">clusters.save(sc, myModelPath)</span><br><span class="line"><span class="comment"># 加载模型并测试</span></span><br><span class="line">loadModel = KMeansModel.load(sc, myModelPath)</span><br><span class="line">print(loadModel.predict(array([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])))</span><br></pre></td></tr></table></figure><p>整理成下面的提交命令，将作业提交到Yarn集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/tmp/pyspark # cat run.sh</span><br><span class="line">PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python \</span><br><span class="line">/approot1/utility/hadoopclient/Spark/spark/bin/spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--archives /tmp/pyspark/mlpy_env.zip#ANACONDA \</span><br><span class="line">--files /tmp/pyspark/kmeans_data.txt \</span><br><span class="line">--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python \</span><br><span class="line">/tmp/pyspark/kmeanTest.py</span><br></pre></td></tr></table></figure><p>模型训练结果会写到路径下面：/user/model：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/tmp/pyspark/pythonpkg # hdfs dfs -ls /user/model</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x+  - itdw hadoop          0 2019-07-30 11:30 /user/model/KMeansModel.ml</span><br></pre></td></tr></table></figure><p>模型加载的预测结果可以在Yarn日志中查询：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dkfzxwma07app08:/tmp/pyspark # yarn logs -applicationId application_1562162322775_150207</span><br><span class="line"><span class="meta">#</span><span class="bash"> 提取部分回显</span></span><br><span class="line">LogType:stdout</span><br><span class="line">Log Upload Time:星期二 七月 30 13:47:23 +0800 2019</span><br><span class="line">LogLength:120414</span><br><span class="line">Log Contents:</span><br><span class="line">Within Set Sum of Squared Error = 0.6928203230275529</span><br><span class="line">[ 9.1  9.1  9.1]</span><br><span class="line">[ 0.1  0.1  0.1]</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>当然对于输出可以选择其他输出源(表或者文件)。</p><h2 id="第五部分-总结"><a href="#第五部分-总结" class="headerlink" title="第五部分 总结"></a>第五部分 总结</h2><h3 id="5-1-混合多语言数据流"><a href="#5-1-混合多语言数据流" class="headerlink" title="5.1 混合多语言数据流"></a>5.1 混合多语言数据流</h3><p>通常一个完整的机器学习应用的数据流设计中，可以将数据ETL准备阶段和算法计算分离出来。使用Java/scala/sql进行数据的预处理，输出算法计算要求的数据格式。这会极大降低算法计算的数据输入规模，降低算法计算的节点的IO。</p><p>机器学习的算法计算部分具有高迭代计算特性，对于非分布式的机器学习算法，我们通常部署在高性能的节点上，基于丰富、高性能的Python科学计算模块，使用Python语言实现。而对于数据准备阶段，更适合使用原生的Scala/java编程语言实现Spark Application来处理数据，包括转换、统计、压缩等等，将满足算法输入格式的数据输出到HDFS文件系统中。特别对于数据规模较大的情况，在Spark集群上处理数据，Scala/Java实现的Spark Application运行（多机并行分布式处理）性能要好一些。然后输出数据交给Python进行迭代计算训练。</p><p>当然对于分布式机器学习框架，将数据迭代部分分解到多个节点并行处理，由参数服务器管理迭代参数的汇总和更新。在这种计算框架下可以利用数据集群天然的计算资源，实现分布式部署。这就形成了一个高效的混合的多语言的数据处理流。</p><h3 id="5-2-架构建议和总结"><a href="#5-2-架构建议和总结" class="headerlink" title="5.2 架构建议和总结"></a>5.2 架构建议和总结</h3><p>1、对于Python on Yarn架构下，采用“Yarn节点不配置Python环境”模式，便于Python环境的管理。这时候可以将Python环境zip文件上传至集群HDFS文件系统，避免每次提交任务都需要上传zip文件，但是不可避免集群内部HDFS文件系统分发到运行节点产生的网络IO。但比集群外部的上传效率高。</p><p>2、对于机器学习任务数据流建议采用混合多语言数据流方式，发挥各计算组件的优势。</p><p>3、对于分布式机器学习框架，建议结合集群的计算资源，直接在集群上展开分布式计算（例如Tensorflow计算框架）。而不是单独新建新的机器学习分布式集群。减少两个集群的数据搬运，并且使得数据和计算更加贴近，最重要的提高机器学习任务端到端的效率。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Running Spark Python Applications，链接：<a href="https://www.cloudera.com/documentation/enterprise/5-9-x/topics/spark_python.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-9-x/topics/spark_python.html</a></p><p>2、基于YARN集群构建运行PySpark Application，链接： <a href="http://shiyanjun.cn/archives/1738.html" target="_blank" rel="noopener">http://shiyanjun.cn/archives/1738.html</a></p><p>3、Running Spark on YARN，链接： <a href="https://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/running-on-yarn.html</a></p><p>4、Running Spark on Kubernetes，链接：<a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p><p>5、Apache Spark Resource Management and YARN App Models，链接：<a href="https://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/" target="_blank" rel="noopener">https://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/</a></p><p>6、Spark On Yarn的两种模式yarn-cluster和yarn-client深度剖析，链接：<a href="https://www.cnblogs.com/ITtangtang/p/7967386.html" target="_blank" rel="noopener">https://www.cnblogs.com/ITtangtang/p/7967386.html</a></p><p>7、Introducing Skein: Deploy Python on Apache YARN the Easy Way，链接：<a href="https://jcrist.github.io/introducing-skein.html" target="_blank" rel="noopener">https://jcrist.github.io/introducing-skein.html</a></p><p>8、当Spark遇上TensorFlow分布式深度学习框架原理和实践，链接：<a href="https://juejin.im/post/5ad4b620f265da23a04a0ad0" target="_blank" rel="noopener">https://juejin.im/post/5ad4b620f265da23a04a0ad0</a></p><p>9、Spark On Yarn的优势，链接：<a href="https://www.cnblogs.com/ITtangtang/p/7967386.html" target="_blank" rel="noopener">https://www.cnblogs.com/ITtangtang/p/7967386.html</a></p><p>10、基于YARN集群构建运行PySpark Application，链接：<a href="http://www.uml.org.cn/bigdata/201711132.asp" target="_blank" rel="noopener">http://www.uml.org.cn/bigdata/201711132.asp</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;术语说明&lt;/li&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   Apache Spark运行模式介绍&lt;/li&gt;
&lt;li&gt;第二部
      
    
    </summary>
    
      <category term="python spark yarn" scheme="https://zjrongxiang.github.io/categories/python-spark-yarn/"/>
    
    
  </entry>
  
  <entry>
    <title>TDengine时间序列数据库压力测试</title>
    <link href="https://zjrongxiang.github.io/2019/07/01/2019-07-01-TDengine%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95/"/>
    <id>https://zjrongxiang.github.io/2019/07/01/2019-07-01-TDengine时间序列数据库压力测试/</id>
    <published>2019-07-01T15:30:00.000Z</published>
    <updated>2019-07-30T07:38:47.228Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分   部署压测工具vegeta</p></li><li><p>第二部分   部署TDengine数据库</p></li><li><p>第三部分   TDengine数据库RESTful接口介绍</p></li><li><p>第四部分   压力测试</p></li><li><p>第五部分   压力测试结果</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近涛思数据团队开源了自己的时序数据库，关注度较高。按照官方介绍，性能较为强悍，所以使用压测工具对该数据库进行了性能压测。压测使用的工具Vegeta 是一个用 Go 语言编写的多功能的 HTTP 负载测试工具。Vegeta 提供了命令行工具和一个开发库。</p><h2 id="第一部分-部署压测工具vegeta"><a href="#第一部分-部署压测工具vegeta" class="headerlink" title="第一部分 部署压测工具vegeta"></a>第一部分 部署压测工具vegeta</h2><h3 id="1-1-部署"><a href="#1-1-部署" class="headerlink" title="1.1 部署"></a>1.1 部署</h3><p>从github上下载vegeta安装介质：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wget https://github.com/tsenart/vegeta/releases/download/cli%2Fv12.5.1/vegeta-12.5.1-linux-amd64.tar.gz</span></span><br></pre></td></tr></table></figure><p>该工具开箱即用，解压tar包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tar -zxvf vegeta-12.5.1-linux-amd64.tar.gz</span></span><br></pre></td></tr></table></figure><h3 id="1-2-简单测试使用"><a href="#1-2-简单测试使用" class="headerlink" title="1.2 简单测试使用"></a>1.2 简单测试使用</h3><p>我们使用vegeta测试一下下面的压力场景（对百度主页发起每秒100次（-rate=100）的请求，持续10秒（-duration=10s）），测试结果重定向到文件（result/results.bin）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"GET https://www.baidu.com"</span> |./vegeta attack -duration=10s -rate=100 &gt;result/results.bin</span></span><br></pre></td></tr></table></figure><p>查看一下测试结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/vegeta# ./vegeta report result/results.bin</span><br><span class="line">Requests      [total, rate]            1000, 100.10</span><br><span class="line">Duration      [total, attack, wait]    10.002241136s, 9.990128173s, 12.112963ms</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  2.315958984s, 1.889487082s, 6.064678156s, 6.583899297s, 6.961230585s</span><br><span class="line">Bytes In      [total, mean]            227000, 227.00</span><br><span class="line">Bytes Out     [total, mean]            0, 0.00</span><br><span class="line">Success       [ratio]                  100.00%</span><br><span class="line">Status Codes  [code:count]             200:1000  </span><br><span class="line">Error Set:</span><br></pre></td></tr></table></figure><p>另外可以生成html文件报告（可视化）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/vegeta# ./vegeta plot result/results.bin &gt; result/plot.html</span><br></pre></td></tr></table></figure><h2 id="第二部分-部署TDengine数据库"><a href="#第二部分-部署TDengine数据库" class="headerlink" title="第二部分 部署TDengine数据库"></a>第二部分 部署TDengine数据库</h2><h3 id="2-1-制作docker镜像"><a href="#2-1-制作docker镜像" class="headerlink" title="2.1 制作docker镜像"></a>2.1 制作docker镜像</h3><blockquote><p>由于墙的原因我们在VPS上打包镜像，然后本机拉取部署。</p></blockquote><p>为了保证测试环境的隔离性，我们制作docker镜像，使用docker环境进行测试。首先拉取ubuntu的基础镜像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run -t -i ubuntu:16.04 /bin/bash</span></span><br></pre></td></tr></table></figure><p>通过TDengine源码安装。在这过程有大量操作系统工具未安装，需要使用atp-get安装部署。</p><h4 id="2-1-1-第一步-clone项目"><a href="#2-1-1-第一步-clone项目" class="headerlink" title="2.1.1 第一步 clone项目"></a>2.1.1 第一步 clone项目</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/taosdata/TDengine.git</span></span><br></pre></td></tr></table></figure><h4 id="2-1-2-第二步-编译"><a href="#2-1-2-第二步-编译" class="headerlink" title="2.1.2 第二步 编译"></a>2.1.2 第二步 编译</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir build &amp;&amp; <span class="built_in">cd</span> build</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cmake .. &amp;&amp; cmake --build .</span></span><br></pre></td></tr></table></figure><h4 id="2-1-3-第三步-安装"><a href="#2-1-3-第三步-安装" class="headerlink" title="2.1.3 第三步 安装"></a>2.1.3 第三步 安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> make install</span></span><br></pre></td></tr></table></figure><h3 id="2-2-生成镜像"><a href="#2-2-生成镜像" class="headerlink" title="2.2 生成镜像"></a>2.2 生成镜像</h3><p>打包成镜像，推送到Docker Hub：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@vultr:~# docker commit a35c43242a8e rongxiang1986/tdengine</span><br><span class="line">root@vultr:~# docker tag rongxiang1986/tdengine rongxiang1986/tdengine:1.0</span><br><span class="line">root@vultr:~# docker push rongxiang1986/tdengine:1.0</span><br></pre></td></tr></table></figure><h3 id="2-3-拉取镜像部署"><a href="#2-3-拉取镜像部署" class="headerlink" title="2.3 拉取镜像部署"></a>2.3 拉取镜像部署</h3><p>拉取镜像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/TDengine/TDengine# docker pull rongxiang1986/tdengine:1.0</span><br></pre></td></tr></table></figure><p>启动一个docker容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/TDengine/TDengine# docker run -t -i --name tdengine -d -p 6020:6020 rongxiang1986/tdengine:1.0 /bin/bash</span><br></pre></td></tr></table></figure><p>查看正在运行的容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/TDengine/TDengine# docker ps</span><br><span class="line"><span class="meta">#</span><span class="bash"> bec2e166c29f</span></span><br></pre></td></tr></table></figure><p>进入容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/data/TDengine/TDengine# docker attach bec2e166c29f</span><br></pre></td></tr></table></figure><p>最后启动数据库服务，下面是启动回显信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/TDengine/build# ./build/bin/taosd -c test/cfg &amp;</span><br><span class="line">[1] 23</span><br><span class="line">root@bec2e166c29f:~/TDengine/TDengine/build# TDengine:[23]: Starting TDengine service...</span><br><span class="line">07/17 14:48:21.615248 7ff90254e700 UTL timezone not configured, set to system default:Etc/UTC (Etc, +0000)</span><br><span class="line">07/17 14:48:21.615261 7ff90254e700 UTL locale not configured, set to system default:C</span><br><span class="line">07/17 14:48:21.616266 7ff90254e700 UTL    taos config &amp; system info:</span><br><span class="line">07/17 14:48:21.616274 7ff90254e700 UTL ==================================</span><br><span class="line">07/17 14:48:21.616278 7ff90254e700 UTL  internalIp:             172.17.0.3 </span><br><span class="line">07/17 14:48:21.616283 7ff90254e700 UTL  localIp:                172.17.0.3 </span><br><span class="line">07/17 14:48:21.616288 7ff90254e700 UTL  httpIp:                 0.0.0.0 </span><br><span class="line">07/17 14:48:21.616294 7ff90254e700 UTL  httpPort:               6020 </span><br><span class="line">07/17 14:48:21.616299 7ff90254e700 UTL  mgmtShellPort:          6030 </span><br><span class="line">07/17 14:48:21.616304 7ff90254e700 UTL  vnodeShellPort:         6035 </span><br><span class="line">07/17 14:48:21.616308 7ff90254e700 UTL  configDir:              test/cfg </span><br><span class="line">07/17 14:48:21.616312 7ff90254e700 UTL  dataDir:                /root/TDengine/TDengine/build/test/data </span><br><span class="line">07/17 14:48:21.616318 7ff90254e700 UTL  logDir:                 /root/TDengine/TDengine/build/test/log </span><br><span class="line">07/17 14:48:21.616325 7ff90254e700 UTL  scriptDir:              /etc/taos </span><br><span class="line">07/17 14:48:21.616330 7ff90254e700 UTL  numOfThreadsPerCore:    1.000000 </span><br><span class="line">07/17 14:48:21.616338 7ff90254e700 UTL  ratioOfQueryThreads:    0.500000 </span><br><span class="line">07/17 14:48:21.616344 7ff90254e700 UTL  numOfVnodesPerCore:     8 </span><br><span class="line">07/17 14:48:21.616349 7ff90254e700 UTL  numOfTotalVnodes:       0 </span><br><span class="line">07/17 14:48:21.616355 7ff90254e700 UTL  tables:                 1000 </span><br><span class="line">07/17 14:48:21.616360 7ff90254e700 UTL  cache:                  16384(byte)</span><br><span class="line">07/17 14:48:21.616366 7ff90254e700 UTL  rows:                   4096 </span><br><span class="line">07/17 14:48:21.616371 7ff90254e700 UTL  fileBlockMinPercent:    0.250000 </span><br><span class="line">07/17 14:48:21.616376 7ff90254e700 UTL  ablocks:                4 </span><br><span class="line">07/17 14:48:21.616382 7ff90254e700 UTL  tblocks:                100 </span><br><span class="line">07/17 14:48:21.616386 7ff90254e700 UTL  monitorInterval:        30(s)</span><br><span class="line">07/17 14:48:21.616391 7ff90254e700 UTL  rpcTimer:               300(ms)</span><br><span class="line">07/17 14:48:21.616396 7ff90254e700 UTL  rpcMaxTime:             600(s)</span><br><span class="line">07/17 14:48:21.616402 7ff90254e700 UTL  ctime:                  3600(s)</span><br><span class="line">07/17 14:48:21.616407 7ff90254e700 UTL  statusInterval:         1(s)</span><br><span class="line">07/17 14:48:21.616414 7ff90254e700 UTL  shellActivityTimer:     3(s)</span><br><span class="line">07/17 14:48:21.616420 7ff90254e700 UTL  meterMetaKeepTimer:     7200(s)</span><br><span class="line">07/17 14:48:21.616423 7ff90254e700 UTL  metricMetaKeepTimer:    600(s)</span><br><span class="line">07/17 14:48:21.616428 7ff90254e700 UTL  maxUsers:               1000 </span><br><span class="line">07/17 14:48:21.616432 7ff90254e700 UTL  maxDbs:                 1000 </span><br><span class="line">07/17 14:48:21.616439 7ff90254e700 UTL  maxTables:              650000 </span><br><span class="line">07/17 14:48:21.616442 7ff90254e700 UTL  maxVGroups:             1000 </span><br><span class="line">07/17 14:48:21.616445 7ff90254e700 UTL  minSlidingTime:         10(ms)</span><br><span class="line">07/17 14:48:21.616454 7ff90254e700 UTL  minIntervalTime:        10(ms)</span><br><span class="line">07/17 14:48:21.616460 7ff90254e700 UTL  maxStreamCompDelay:     20000(ms)</span><br><span class="line">07/17 14:48:21.616463 7ff90254e700 UTL  maxFirstStreamCompDelay:10000(ms)</span><br><span class="line">07/17 14:48:21.616467 7ff90254e700 UTL  retryStreamCompDelay:   10(ms)</span><br><span class="line">07/17 14:48:21.616470 7ff90254e700 UTL  clog:                   1 </span><br><span class="line">07/17 14:48:21.616474 7ff90254e700 UTL  comp:                   2 </span><br><span class="line">07/17 14:48:21.616480 7ff90254e700 UTL  days:                   10 </span><br><span class="line">07/17 14:48:21.616483 7ff90254e700 UTL  keep:                   3650 </span><br><span class="line">07/17 14:48:21.616488 7ff90254e700 UTL  defaultDB:               </span><br><span class="line">07/17 14:48:21.616494 7ff90254e700 UTL  defaultUser:            root </span><br><span class="line">07/17 14:48:21.616503 7ff90254e700 UTL  defaultPass:            taosdata </span><br><span class="line">07/17 14:48:21.616508 7ff90254e700 UTL  timezone:               Etc/UTC (Etc, +0000) </span><br><span class="line">07/17 14:48:21.616515 7ff90254e700 UTL  locale:                 C </span><br><span class="line">07/17 14:48:21.616520 7ff90254e700 UTL  charset:                UTF-8 </span><br><span class="line">07/17 14:48:21.616525 7ff90254e700 UTL  maxShellConns:          2000 </span><br><span class="line">07/17 14:48:21.616531 7ff90254e700 UTL  maxMeterConnections:    10000 </span><br><span class="line">07/17 14:48:21.616535 7ff90254e700 UTL  maxMgmtConnections:     2000 </span><br><span class="line">07/17 14:48:21.616540 7ff90254e700 UTL  maxVnodeConnections:    10000 </span><br><span class="line">07/17 14:48:21.616543 7ff90254e700 UTL  enableHttp:             1 </span><br><span class="line">07/17 14:48:21.616549 7ff90254e700 UTL  enableMonitor:          1 </span><br><span class="line">07/17 14:48:21.616553 7ff90254e700 UTL  httpCacheSessions:      2000 </span><br><span class="line">07/17 14:48:21.616556 7ff90254e700 UTL  httpMaxThreads:         2 </span><br><span class="line">07/17 14:48:21.616561 7ff90254e700 UTL  numOfLogLines:          10000000 </span><br><span class="line">07/17 14:48:21.616565 7ff90254e700 UTL  asyncLog:               1 </span><br><span class="line">07/17 14:48:21.616570 7ff90254e700 UTL  debugFlag:              131 </span><br><span class="line">07/17 14:48:21.616574 7ff90254e700 UTL  mDebugFlag:             135 </span><br><span class="line">07/17 14:48:21.616579 7ff90254e700 UTL  dDebugFlag:             131 </span><br><span class="line">07/17 14:48:21.616584 7ff90254e700 UTL  sdbDebugFlag:           135 </span><br><span class="line">07/17 14:48:21.616590 7ff90254e700 UTL  taosDebugFlag:          131 </span><br><span class="line">07/17 14:48:21.616595 7ff90254e700 UTL  tmrDebugFlag:           131 </span><br><span class="line">07/17 14:48:21.616601 7ff90254e700 UTL  cDebugFlag:             131 </span><br><span class="line">07/17 14:48:21.616605 7ff90254e700 UTL  jniDebugFlag:           131 </span><br><span class="line">07/17 14:48:21.616612 7ff90254e700 UTL  odbcDebugFlag:          131 </span><br><span class="line">07/17 14:48:21.616617 7ff90254e700 UTL  uDebugFlag:             131 </span><br><span class="line">07/17 14:48:21.616621 7ff90254e700 UTL  httpDebugFlag:          131 </span><br><span class="line">07/17 14:48:21.616629 7ff90254e700 UTL  monitorDebugFlag:       131 </span><br><span class="line">07/17 14:48:21.616634 7ff90254e700 UTL  qDebugFlag:             131 </span><br><span class="line">07/17 14:48:21.616637 7ff90254e700 UTL  gitinfo:                82cbce3261d06ab37c3bd4786c7b2e3d2316c42a </span><br><span class="line">07/17 14:48:21.616643 7ff90254e700 UTL  buildinfo:              Built by ubuntu at 2019-07-05 18:42 </span><br><span class="line">07/17 14:48:21.616648 7ff90254e700 UTL  version:                1.6.0.0 </span><br><span class="line">07/17 14:48:21.616653 7ff90254e700 UTL  os pageSize:            4096(KB)</span><br><span class="line">07/17 14:48:21.616658 7ff90254e700 UTL  os openMax:             1048576</span><br><span class="line">07/17 14:48:21.616662 7ff90254e700 UTL  os streamMax:           16</span><br><span class="line">07/17 14:48:21.616666 7ff90254e700 UTL  os numOfCores:          8</span><br><span class="line">07/17 14:48:21.616670 7ff90254e700 UTL  os totalDisk:           426(GB)</span><br><span class="line">07/17 14:48:21.616676 7ff90254e700 UTL  os totalMemory:         32028(MB)</span><br><span class="line">07/17 14:48:21.616682 7ff90254e700 UTL  os sysname:             Linux</span><br><span class="line">07/17 14:48:21.616686 7ff90254e700 UTL  os nodename:            bec2e166c29f</span><br><span class="line">07/17 14:48:21.616690 7ff90254e700 UTL  os release:             4.15.0-51-generic</span><br><span class="line">07/17 14:48:21.616694 7ff90254e700 UTL  os version:             #55~16.04.1-Ubuntu SMP Thu May 16 09:24:37 UTC 2019</span><br><span class="line">07/17 14:48:21.616700 7ff90254e700 UTL  os machine:             x86_64</span><br><span class="line">07/17 14:48:21.616707 7ff90254e700 UTL ==================================</span><br><span class="line">07/17 14:48:21.616712 7ff90254e700 DND Server IP address is:172.17.0.3</span><br><span class="line">07/17 14:48:21.616717 7ff90254e700 DND starting to initialize TDengine engine ...</span><br><span class="line">07/17 14:48:21.619440 7ff90254e700 HTP failed to open telegraf schema config file:test/cfg/taos.telegraf.cfg, use default schema</span><br><span class="line">07/17 14:48:21.869736 7ff90254e700 DND vnode is initialized successfully</span><br><span class="line">07/17 14:48:21.869780 7ff90254e700 MND starting to initialize TDengine mgmt ...</span><br><span class="line">07/17 14:48:21.872494 7ff90254e700 MND first access, set total vnodes:64</span><br><span class="line">07/17 14:48:21.917758 7ff90254e700 MND TDengine mgmt is initialized successfully</span><br><span class="line">07/17 14:48:21.917781 7ff90254e700 HTP starting to initialize http service ...</span><br><span class="line">07/17 14:48:21.918417 7ff90254e700 DND TDengine is initialized successfully</span><br><span class="line">07/17 14:48:21.918533 7ff8e4f41700 HTP http service init success at ip:0.0.0.0:6020</span><br><span class="line">TDengine:[23]: Started TDengine service successfully.</span><br><span class="line">07/17 14:48:22.022278 7ff8ff835700 MON starting to initialize monitor service ..</span><br><span class="line">07/17 14:48:22.022747 7ff8eb109700 MND user:monitor login from 172.17.0.3, code:0</span><br><span class="line">07/17 14:48:22.024412 7ff901038700 MON dnode:172.17.0.3 is started</span><br><span class="line">07/17 14:48:22.026780 7ff901038700 MON monitor service init success</span><br></pre></td></tr></table></figure><p>上面回显service init success说明服务service启动成功。</p><h3 id="2-4-镜像使用说明"><a href="#2-4-镜像使用说明" class="headerlink" title="2.4 镜像使用说明"></a>2.4 镜像使用说明</h3><p>TDengine项目地址：<a href="https://github.com/taosdata/TDengine。使用ubuntu16.04作为基础镜像，部署安装TDengine。拉取镜像后，启动容器后：" target="_blank" rel="noopener">https://github.com/taosdata/TDengine。使用ubuntu16.04作为基础镜像，部署安装TDengine。拉取镜像后，启动容器后：</a></p><ul><li><p>启动服务：To start the TDengine server, run the command below in terminal:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /root/TDengine/TDengine/build<span class="comment"># ./build/bin/taosd -c test/cfg</span></span></span><br></pre></td></tr></table></figure></li><li><p>启动客户端：In another terminal, use the TDengine shell to connect the server:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /root/TDengine/TDengine/build<span class="comment">#./build/bin/taos -c test/cfg</span></span></span><br></pre></td></tr></table></figure></li></ul><h2 id="第三部分-TDengine数据库RESTful接口介绍"><a href="#第三部分-TDengine数据库RESTful接口介绍" class="headerlink" title="第三部分 TDengine数据库RESTful接口介绍"></a>第三部分 TDengine数据库RESTful接口介绍</h2><p>按照官方给的例子我们新建案例数据库和表，并且新增数据记录。首先进入shell交互：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@a35c43242a8e:~/TDengine/TDengine/build# ./build/bin/taos -c test/cfg</span><br><span class="line">07/17 04:30:19.818000 7fa154b0e700 MND user:root login from 172.17.0.2, code:0</span><br><span class="line"></span><br><span class="line">Welcome to the TDengine shell, server version:1.6.0.0  client version:1.6.0.0</span><br><span class="line">Copyright (c) 2017 by TAOS Data, Inc. All rights reserved.</span><br><span class="line"></span><br><span class="line"><span class="meta">taos&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure><p>创建案例数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">taos&gt;</span><span class="bash"> create database db;</span></span><br><span class="line">07/17 04:31:46.970580 7fa14ffff700 MND DB:0.db is created by root</span><br><span class="line">Query OK, 1 row(s) affected (0.001848s)</span><br><span class="line"></span><br><span class="line"><span class="meta">taos&gt;</span><span class="bash"> use db;</span></span><br><span class="line">Database changed.</span><br><span class="line"></span><br><span class="line"><span class="meta">taos&gt;</span><span class="bash"> create table t (ts timestamp, cdata int);</span></span><br><span class="line">Query OK, 1 row(s) affected (0.334998s)</span><br><span class="line"></span><br><span class="line"><span class="meta">taos&gt;</span><span class="bash"> insert into t values (<span class="string">'2019-07-15 10:00:00'</span>, 10);</span></span><br><span class="line">Query OK, 1 row(s) affected (0.001639s)</span><br><span class="line"></span><br><span class="line"><span class="meta">taos&gt;</span><span class="bash"> insert into t values (<span class="string">'2019-07-15 10:01:05'</span>, 20);</span></span><br><span class="line">Query OK, 1 row(s) affected (0.000245s)</span><br><span class="line"></span><br><span class="line"><span class="meta">taos&gt;</span><span class="bash"> select * from t;</span></span><br><span class="line">          ts          |   cdata   |</span><br><span class="line">===================================</span><br><span class="line"> 19-07-15 10:00:00.000|         10|</span><br><span class="line"> 19-07-15 10:01:05.000|         20|</span><br><span class="line">Query OK, 2 row(s) in set (0.001408s)</span><br></pre></td></tr></table></figure><p>退出shell交互后，我们使用Restfull接口与数据库交互。</p><blockquote><p>注意：目前RESTfull接口认证方式使用Http Basic Authorization请求格式，token使用base64(username:password)，即base64(root:taosdata)=cm9vdDp0YW9zZGF0YQ==</p><p>可以在在线网站上：<a href="https://www.base64encode.org/" target="_blank" rel="noopener">https://www.base64encode.org/</a> encode一下。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@a35c43242a8e:~/TDengine/TDengine/build# curl -H 'Authorization: Basic cm9vdDp0YW9zZGF0YQ==' -d 'select * from db.t' localhost:6020/rest/sql</span><br><span class="line"><span class="meta">#</span><span class="bash">下面是回显：</span></span><br><span class="line">07/17 04:33:34.834283 7fa154b0e700 MND user:root login from 172.17.0.2, code:0</span><br><span class="line">&#123;"status":"succ","head":["ts","cdata"],"data":[["2019-07-15 10:00:00.000",10],["2019-07-15 10:01:05.000",20]],"rows":2&#125;</span><br></pre></td></tr></table></figure><p>返回结果是一个JSON格式串，规范化一下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;  </span><br><span class="line">   <span class="attr">"status"</span>:<span class="string">"succ"</span>,</span><br><span class="line">   <span class="attr">"head"</span>:[  </span><br><span class="line">      <span class="string">"ts"</span>,</span><br><span class="line">      <span class="string">"cdata"</span></span><br><span class="line">   ],</span><br><span class="line">   <span class="attr">"data"</span>:[  </span><br><span class="line">      [  </span><br><span class="line">         <span class="string">"2019-07-15 10:00:00.000"</span>,</span><br><span class="line">         <span class="number">10</span></span><br><span class="line">      ],</span><br><span class="line">      [  </span><br><span class="line">         <span class="string">"2019-07-15 10:01:05.000"</span>,</span><br><span class="line">         <span class="number">20</span></span><br><span class="line">      ]</span><br><span class="line">   ],</span><br><span class="line">   <span class="attr">"rows"</span>:<span class="number">2</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="第四部分-对RESTful接口压力测试"><a href="#第四部分-对RESTful接口压力测试" class="headerlink" title="第四部分 对RESTful接口压力测试"></a>第四部分 对RESTful接口压力测试</h2><p>最后我们使用vegeta对restful接口进行压力测试：</p><h3 id="4-1-查询压测"><a href="#4-1-查询压测" class="headerlink" title="4.1 查询压测"></a>4.1 查询压测</h3><p>使用目标文件的内容进行压力测试。</p><p>首先创建target.txt文件以及数据文件data.json，内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat target.txt</span><br><span class="line">POST http://localhost:6020/rest/sql</span><br><span class="line">Authorization: Basic cm9vdDp0YW9zZGF0YQ==</span><br><span class="line">@data.json</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat data.json</span><br><span class="line">select * from db.t</span><br></pre></td></tr></table></figure><p>最后使用下面的命令开始压测（每秒15000次请求，持续5分钟）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest#../vegeta attack -rate 15000 -targets target.txt -duration 5m &gt; out.dat</span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成报告</span></span><br><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report out.dat</span><br><span class="line"><span class="meta">#</span><span class="bash"> 回显：</span></span><br><span class="line">Requests      [total, rate]            4500044, 15000.15</span><br><span class="line">Duration      [total, attack, wait]    5m0.000202809s, 4m59.999911126s, 291.683µs</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  314.712µs, 230.113µs, 869.502µs, 1.501018ms, 205.870168ms</span><br><span class="line">Bytes In      [total, mean]            954009328, 212.00</span><br><span class="line">Bytes Out     [total, mean]            90000880, 20.00</span><br><span class="line">Success       [ratio]                  100.00%</span><br><span class="line">Status Codes  [code:count]             200:4500044  </span><br><span class="line">Error Set:</span><br></pre></td></tr></table></figure><h3 id="4-2-写入压测"><a href="#4-2-写入压测" class="headerlink" title="4.2 写入压测"></a>4.2 写入压测</h3><p>首先创建writetarget.txt和数据文件：writedata.json，内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat writetarget.txt</span><br><span class="line">POST http://localhost:6020/rest/sql</span><br><span class="line">Authorization: Basic cm9vdDp0YW9zZGF0YQ==</span><br><span class="line">@writedata.json</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat writedata.json</span><br><span class="line">insert into db.cpu values (NOW,20,12);</span><br></pre></td></tr></table></figure><blockquote><p>这里写入语句使用时间函数NOW，保证写入时间无重复。</p></blockquote><p>最后使用命令开始压测（每秒30000次请求，持续1分钟）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta attack -rate 30000 -targets writetarget.txt -duration 1m &gt; writeout.dat</span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成报告</span></span><br><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report writeout30000.dat</span><br><span class="line">Requests      [total, rate]            1800018, 30000.31</span><br><span class="line">Duration      [total, attack, wait]    1m0.000154842s, 59.99998565s, 169.192µs</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  254.824µs, 137.651µs, 904.696µs, 1.687601ms, 12.528831ms</span><br><span class="line">Bytes In      [total, mean]            115201152, 64.00</span><br><span class="line">Bytes Out     [total, mean]            70200702, 39.00</span><br><span class="line">Success       [ratio]                  100.00%</span><br><span class="line">Status Codes  [code:count]             200:1800018  </span><br><span class="line">Error Set:</span><br></pre></td></tr></table></figure><h2 id="第五部分-压力测试结果"><a href="#第五部分-压力测试结果" class="headerlink" title="第五部分 压力测试结果"></a>第五部分 压力测试结果</h2><p>由于机器环境的差异，只是做了尝试性测试，不代表产品的实际性能。</p><blockquote><p>官方测试报告参考：<a href="https://www.taosdata.com/downloads/TDengine_Testing_Report_cn.pdf" target="_blank" rel="noopener">https://www.taosdata.com/downloads/TDengine_Testing_Report_cn.pdf</a></p></blockquote><h3 id="5-1-查询"><a href="#5-1-查询" class="headerlink" title="5.1 查询"></a>5.1 查询</h3><p>对于查询性能我们每秒15000的请求结果如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report out15000.dat</span><br><span class="line">Requests      [total, rate]            4500044, 15000.15</span><br><span class="line">Duration      [total, attack, wait]    5m0.000202809s, 4m59.999911126s, 291.683µs</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  314.712µs, 230.113µs, 869.502µs, 1.501018ms, 205.870168ms</span><br><span class="line">Bytes In      [total, mean]            954009328, 212.00</span><br><span class="line">Bytes Out     [total, mean]            90000880, 20.00</span><br><span class="line">Success       [ratio]                  100.00%</span><br><span class="line">Status Codes  [code:count]             200:4500044  </span><br><span class="line">Error Set:</span><br></pre></td></tr></table></figure><p>当提高到每秒20000次时，数据库出现响应失败，成功率只有38.78%：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report out20000.dat     </span><br><span class="line">Requests      [total, rate]            904794, 2824.86</span><br><span class="line">Duration      [total, attack, wait]    6m19.120695351s, 5m20.296894938s, 58.823800413s</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  38.726984614s, 43.82040722s, 1m13.377381823s, 1m25.843611324s, 1m52.393219406s</span><br><span class="line">Bytes In      [total, mean]            58181450, 64.30</span><br><span class="line">Bytes Out     [total, mean]            8016960, 8.86</span><br><span class="line">Success       [ratio]                  38.78%</span><br><span class="line">Status Codes  [code:count]             0:503946  200:350896  400:49952  </span><br><span class="line">Error Set:</span><br><span class="line">400 Bad Request</span><br><span class="line">Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in use</span><br><span class="line">Post http://localhost:6020/rest/sql: net/http: request canceled (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">Post http://localhost:6020/rest/sql: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in use (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:34037-&gt;127.0.0.1:6020: read: connection reset by peer</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:53020-&gt;127.0.0.1:6020: read: connection reset by peer</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:47081-&gt;127.0.0.1:6020: read: connection reset by peer</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:35442-&gt;127.0.0.1:6020: read: connection reset by peer</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:58175-&gt;127.0.0.1:6020: read: connection reset by peer</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:41857-&gt;127.0.0.1:6020: read: connection reset by peer</span><br><span class="line">Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:58943-&gt;127.0.0.1:6020: read: connection reset by peer</span><br></pre></td></tr></table></figure><h3 id="5-2-写入"><a href="#5-2-写入" class="headerlink" title="5.2 写入"></a>5.2 写入</h3><p>对于写入测试。对于查询性能我们每秒30000的请求结果如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report writeout30000.dat</span><br><span class="line">Requests      [total, rate]            1800018, 30000.31</span><br><span class="line">Duration      [total, attack, wait]    1m0.000154842s, 59.99998565s, 169.192µs</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  254.824µs, 137.651µs, 904.696µs, 1.687601ms, 12.528831ms</span><br><span class="line">Bytes In      [total, mean]            115201152, 64.00</span><br><span class="line">Bytes Out     [total, mean]            70200702, 39.00</span><br><span class="line">Success       [ratio]                  100.00%</span><br><span class="line">Status Codes  [code:count]             200:1800018  </span><br><span class="line">Error Set:</span><br></pre></td></tr></table></figure><p>当提高到每秒50000次时，数据库性能开始恶化，成功率只有55.73%：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report writeout50000.dat</span><br><span class="line">Requests      [total, rate]            160712, 2676.70</span><br><span class="line">Duration      [total, attack, wait]    1m47.789030336s, 1m0.041100866s, 47.74792947s</span><br><span class="line">Latencies     [mean, 50, 95, 99, max]  36.064568863s, 39.103737526s, 50.372069878s, 54.866214748s, 1m12.470342472s</span><br><span class="line">Bytes In      [total, mean]            5731840, 35.67</span><br><span class="line">Bytes Out     [total, mean]            3492840, 21.73</span><br><span class="line">Success       [ratio]                  55.73%</span><br><span class="line">Status Codes  [code:count]             0:71152  200:89560  </span><br><span class="line">Error Set:</span><br><span class="line">Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in use</span><br><span class="line">Post http://localhost:6020/rest/sql: net/http: request canceled (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">Post http://localhost:6020/rest/sql: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in use (Client.Timeout exceeded while awaiting headers)</span><br></pre></td></tr></table></figure><p>结论：单机环境下同等场景（都是压测RESTfull接口）下，TDengine可以抗住每秒15000次的读请求和每秒30000次写请求。influxdb只能抗住每秒6000次持续读、写。按照官网介绍如果写入是批量形式会更快。</p><h2 id="参考文献和材料"><a href="#参考文献和材料" class="headerlink" title="参考文献和材料"></a>参考文献和材料</h2><p>1、推荐一款高性能 HTTP 负载测试工具 Vegeta 链接：<a href="https://www.hi-linux.com/posts/4650.html" target="_blank" rel="noopener">https://www.hi-linux.com/posts/4650.html</a></p><p>2、TDengine官网 链接：<a href="https://www.taosdata.com/cn/" target="_blank" rel="noopener">https://www.taosdata.com/cn/</a></p><p>3、比Hadoop快至少10倍的物联网大数据平台，我把它开源了 链接：<a href="https://weibo.com/ttarticle/p/show?id=2309404394278649462890" target="_blank" rel="noopener">https://weibo.com/ttarticle/p/show?id=2309404394278649462890</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分   部署压测工具vegeta&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第二部分
      
    
    </summary>
    
      <category term="TDengine Vegeta" scheme="https://zjrongxiang.github.io/categories/TDengine-Vegeta/"/>
    
    
  </entry>
  
  <entry>
    <title>sqlflow初体验</title>
    <link href="https://zjrongxiang.github.io/2019/05/06/2019-05-06-Sqlflow%E5%88%9D%E4%BD%93%E9%AA%8C/"/>
    <id>https://zjrongxiang.github.io/2019/05/06/2019-05-06-Sqlflow初体验/</id>
    <published>2019-05-06T14:42:00.000Z</published>
    <updated>2019-05-06T16:33:09.526Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分   Sqlflow安装部署</p></li><li><p>第二部分   机器学习例子</p></li><li><p>第三部分   系统架构</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>2019年5 月 6 日，在QCon 全球软件开发大会（北京站）上，蚂蚁金服副 CTO 胡喜正式宣布开源机器学习工具 SQLFlow。实际上3个月前sqkflow项目已经在github上开源了。</p><p>本篇文件主要参考sqlflow官网的案例和说明对sqlflow进行了体验，并记录下来。</p><p>sqlflow按照官网的定义，将SQL引擎（例如MySQL，Hive，SparkSQL或SQL Server）和tensorflow和其他机器学习的桥梁。扩展了SQL语言，支持对机器学习模型训练、预测和推理。</p><blockquote><p>目前开源版本仅支持MySQL和TensorFlow</p></blockquote><p>介绍文档中也提到，在sqlflow之前也有SQL引擎提供了支持机器学习功能的扩展。</p><ul><li><a href="https://docs.microsoft.com/en-us/sql/advanced-analytics/tutorials/rtsql-create-a-predictive-model-r?view=sql-server-2017" target="_blank" rel="noopener">Microsoft SQL Server</a>：Microsoft SQL Server具有机器学习服务，可以将R或Python中的机器学习程序作为外部脚本运行。</li><li><a href="https://www.linkedin.com/pulse/sql-deep-learning-sql-dl-omri-shiv" target="_blank" rel="noopener">Teradata SQL for DL</a>：Teradata还提供RESTful服务，可以从扩展的SQL SELECT语法中调用。</li><li><a href="https://cloud.google.com/bigquery/docs/bigqueryml-intro" target="_blank" rel="noopener">Google BigQuery</a>：Google BigQuery通过引入<code>CREATE MODEL</code>语句在SQL中实现机器学习。</li></ul><h2 id="第一部分-Sqlflow安装部署"><a href="#第一部分-Sqlflow安装部署" class="headerlink" title="第一部分 Sqlflow安装部署"></a>第一部分 Sqlflow安装部署</h2><h3 id="1-1-部署mysql做为数据源"><a href="#1-1-部署mysql做为数据源" class="headerlink" title="1.1 部署mysql做为数据源"></a>1.1 部署mysql做为数据源</h3><h4 id="（1）构建镜像"><a href="#（1）构建镜像" class="headerlink" title="（1）构建镜像"></a>（1）构建镜像</h4><p>官网提供了一个dockerfile，可以git clone整个项目。</p><p><a href="https://github.com/sql-machine-learning/sqlflow/tree/7c873780bd8a3a9ea4d39ed7d0fcf154b2f8821f/example/datasets" target="_blank" rel="noopener">https://github.com/sql-machine-learning/sqlflow/tree/7c873780bd8a3a9ea4d39ed7d0fcf154b2f8821f/example/datasets</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入Dockerfile文件所在目录</span></span><br><span class="line">cd example/datasets</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用Dockerfile构建镜像</span></span><br><span class="line">docker build -t sqlflow:data .</span><br></pre></td></tr></table></figure><p>可以查看创建了一个docker images：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker images</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建了REPOSITORY：sqlflow镜像，TAG为：data</span></span><br></pre></td></tr></table></figure><h4 id="（2）启动mysql容器"><a href="#（2）启动mysql容器" class="headerlink" title="（2）启动mysql容器"></a>（2）启动mysql容器</h4><p>用镜像启mysql容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -d --name sqlflowdata \</span><br><span class="line">   -p 3306:3306 \</span><br><span class="line">   -e MYSQL_ROOT_PASSWORD=root \</span><br><span class="line">   -e MYSQL_ROOT_HOST=% \</span><br><span class="line">   sqlflow:data</span><br></pre></td></tr></table></figure><p>使用镜像：sqlflow:data，启动一个名为：sqlflowdata的容器，并且把3306端口映射到宿主机。mysql的root用户的密码为root。</p><h4 id="（3）生成测试数据"><a href="#（3）生成测试数据" class="headerlink" title="（3）生成测试数据"></a>（3）生成测试数据</h4><p>进入容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it sqlflowdata bash</span><br></pre></td></tr></table></figure><p>执行SQL语句：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 建库建表，注意宿主机目录：datasets</span></span><br><span class="line">cat /popularize_churn.sql | mysql -uroot -proot</span><br><span class="line">cat /popularize_iris.sql | mysql -uroot -proot</span><br><span class="line"><span class="meta">#</span><span class="bash"> 建库</span></span><br><span class="line">echo "CREATE DATABASE IF NOT EXISTS sqlflow_models;" | mysql -uroot -proot</span><br></pre></td></tr></table></figure><p>至此完成mysql容器的启动和测试数据的生成。按Ctrl+P+Q，正常退出不关闭容器。</p><h3 id="1-2-使用docker部署slqflow"><a href="#1-2-使用docker部署slqflow" class="headerlink" title="1.2 使用docker部署slqflow"></a>1.2 使用docker部署slqflow</h3><h4 id="（1）拉取镜像并启动容器"><a href="#（1）拉取镜像并启动容器" class="headerlink" title="（1）拉取镜像并启动容器"></a>（1）拉取镜像并启动容器</h4><p>首先从docker Hub上拉取镜像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker pull sqlflow/sqlflow:latest</span></span><br></pre></td></tr></table></figure><p>启动容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker run --rm -it --name sqlflowServer -p 8888:8888 sqlflow/sqlflow:latest \</span></span><br><span class="line">bash -c "sqlflowserver --datasource='mysql://root:root@tcp(192.168.31.3:3306)/?maxAllowedPacket=0' &amp;</span><br><span class="line">SQLFLOW_SERVER=localhost:50051 jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root"</span><br></pre></td></tr></table></figure><p>命令使用镜像：sqlflow/sqlflow:lates，启动了名为：sqlflowServer的容器。将8888端口映射到宿主机上。这里需要配置datasource，指向mysql使用套接字：192.168.31.3:3306。这里使用之前构建的mysql容器的连接信息，可以根据实际情况配置。</p><blockquote><p>如果mysql套接字配置错误，报错信息：connect: connection refused</p></blockquote><p>如果没有报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2019/05/06 14:47:30 Server Started at :50051</span><br><span class="line">[I 14:47:30.261 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret</span><br><span class="line">[I 14:47:30.874 NotebookApp] Serving notebooks from local directory: /</span><br><span class="line">[I 14:47:30.874 NotebookApp] The Jupyter Notebook is running at:</span><br><span class="line">[I 14:47:30.874 NotebookApp] http://(fd2b9b3f994b or 127.0.0.1):8888/?token=265b3fc832b5b48689fa9f88483125dc9335188dd7c1d863</span><br><span class="line">[I 14:47:30.874 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</span><br><span class="line">[W 14:47:30.877 NotebookApp] No web browser found: could not locate runnable browser.</span><br></pre></td></tr></table></figure><p>这里启动了Jupyter Notebook服务，对外服务端口为8888，并且映射到宿主机。例如这里可以使用下面的url范围web界面：<a href="http://192.168.31.3:8888/?token=265b3fc832b5b48689fa9f88483125dc9335188dd7c1d863" target="_blank" rel="noopener">http://192.168.31.3:8888/?token=265b3fc832b5b48689fa9f88483125dc9335188dd7c1d863</a></p><h4 id="（2）简单测试"><a href="#（2）简单测试" class="headerlink" title="（2）简单测试"></a>（2）简单测试</h4><p>Jupyter Notebook 新建一个python3交互环境。测试一下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">%</span><span class="bash">%sqlflow</span></span><br><span class="line">select * from iris.train limit 5;</span><br></pre></td></tr></table></figure><p><img src="\images\picture\jupyter book.png" alt=""></p><h3 id="第二部分-机器学习例子"><a href="#第二部分-机器学习例子" class="headerlink" title="第二部分 机器学习例子"></a>第二部分 机器学习例子</h3><p>使用iris数据集体验机器学习的例子，使用Jupyter Notebook 完成：</p><h4 id="（1）训练模型："><a href="#（1）训练模型：" class="headerlink" title="（1）训练模型："></a>（1）训练模型：</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%sqlflow</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> iris.train TRAIN DNNClassifier <span class="keyword">WITH</span> n_classes = <span class="number">3</span>, hidden_units = [<span class="number">10</span>, <span class="number">20</span>] <span class="keyword">COLUMN</span> sepal_length, sepal_width, petal_length, petal_width LABEL <span class="keyword">class</span> <span class="keyword">INTO</span> sqlflow_models.my_dnn_model;</span><br></pre></td></tr></table></figure><p>使用iris.train表中的数据训练神经网络。</p><p>模型训练结果输入到sqlflow_models.my_dnn_model，回显训练正确率为：0.97273</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Training <span class="keyword">set</span> accuracy: 0.97273</span><br><span class="line">Done <span class="comment">training</span></span><br></pre></td></tr></table></figure><h4 id="（2）模型应用"><a href="#（2）模型应用" class="headerlink" title="（2）模型应用"></a>（2）模型应用</h4><p>使用训练结果对数据进行预测应用：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%sqlflow</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> iris.test PREDICT iris.predict.class <span class="keyword">USING</span> sqlflow_models.my_dnn_model;</span><br></pre></td></tr></table></figure><p>使用iris.test中的数据喂给训练好的模型，预测结果输出到表：iris.predict。</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Done predicting. <span class="keyword">Predict</span> <span class="keyword">table</span> : iris.<span class="keyword">predict</span></span><br></pre></td></tr></table></figure><p>查看结果表中的数据案例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%sqlflow</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> iris.predict <span class="keyword">limit</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">+--------------+</span>-------------<span class="code">+--------------+</span>-------------<span class="code">+-------+</span></span><br><span class="line">| sepal<span class="emphasis">_length | sepal_</span>width | petal<span class="emphasis">_length | petal_</span>width | class |</span><br><span class="line"><span class="code">+--------------+</span>-------------<span class="code">+--------------+</span>-------------<span class="code">+-------+</span></span><br><span class="line">|     6.3      |     2.7     |     4.9      |     1.8     |   2   |</span><br><span class="line">|     5.7      |     2.8     |     4.1      |     1.3     |   1   |</span><br><span class="line"><span class="code">+--------------+</span>-------------<span class="code">+--------------+</span>-------------<span class="code">+-------+</span></span><br></pre></td></tr></table></figure><h3 id="第三部分-系统架构"><a href="#第三部分-系统架构" class="headerlink" title="第三部分 系统架构"></a>第三部分 系统架构</h3><p>系统原型使用下面的架构：</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SQL <span class="function"><span class="title">statement</span> -&gt;</span> <span class="function"><span class="title">our</span> SQL parser --standard SQL-&gt;</span> MySQL</span><br><span class="line">                                \-<span class="function"><span class="title">extended</span> SQL-&gt;</span> <span class="function"><span class="title">code</span> generator -&gt;</span> execution engine</span><br></pre></td></tr></table></figure><p>原型运行的数据流为：</p><ol><li>它通过<a href="https://dev.mysql.com/downloads/connector/python/" target="_blank" rel="noopener">MySQL Connector Python API</a>从MySQL检索数据</li><li>从MySQL检索模型</li><li>通过调用用户指定的TensorFlow估算器训练模型或使用训练模型进行预测</li><li>并将训练过的模型或预测结果写入表格</li></ol><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、sqlflow项目官网 链接：<a href="https://github.com/sql-machine-learning/sqlflow" target="_blank" rel="noopener">https://github.com/sql-machine-learning/sqlflow</a></p><p>2、会 SQL 就能搞定 AI！蚂蚁金服重磅开源机器学习工具 SQLFlow 链接：<a href="https://www.infoq.cn/article/vlVqC68h2MT-028lh68C" target="_blank" rel="noopener">https://www.infoq.cn/article/vlVqC68h2MT-028lh68C</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分   Sqlflow安装部署&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第二部分 
      
    
    </summary>
    
      <category term="sqlflow" scheme="https://zjrongxiang.github.io/categories/sqlflow/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive介绍及部署</title>
    <link href="https://zjrongxiang.github.io/2018/08/14/2018-08-13-Hive-install/"/>
    <id>https://zjrongxiang.github.io/2018/08/14/2018-08-13-Hive-install/</id>
    <published>2018-08-14T11:30:00.000Z</published>
    <updated>2018-08-21T05:07:53.600Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Hive（蜂巢）是Hadoop的组件，官方介绍为：</p><blockquote><p><a href="http://hive.apache.org/" target="_blank" rel="noopener"><strong>Hive™</strong></a>: A data warehouse infrastructure that provides data summarization and ad hoc querying.</p></blockquote><p>Hive有三种部署方式（本质是Hive Metastore的三种部署方式）：</p><ol><li><p>Embedded Metastore Database (Derby) 内嵌模式</p><p>内嵌模式使用的是内嵌的Derby数据库来存储元数据，也不需要额外起Metastore服务。这个是默认的，配置简单，但是一次只能一个客户端连接（Derby只提供单进程存储），适用于用来实验，不适用于生产环境。 </p></li><li><p>Local Metastore Server 本地元存储</p><p>采用外部数据库来存储元数据 。本地元存储不需要单独起metastore服务，用的是跟hive在同一个进程里的metastore服务 。</p><p>目前支持：Derby，Mysql，微软SQLServer，Oracle和Postgres </p></li><li><p>Remote Metastore Server 远程元存储</p><p>采用外部数据库来存储元数据 。远程元存储需要单独起metastore服务，然后每个客户端都在配置文件里配置连接到该metastore服务。远程元存储的metastore服务和hive运行在不同的进程里。</p><p>远程元存储是生产环境部署方式。</p></li></ol><h2 id="本地部署过程"><a href="#本地部署过程" class="headerlink" title="本地部署过程"></a>本地部署过程</h2><blockquote><p>由于设备资源限制，没有太多机器配置类似生产环境的集群环境。所以通过docker搭建大集群环境。</p></blockquote><h4 id="搭建目标："><a href="#搭建目标：" class="headerlink" title="搭建目标："></a>搭建目标：</h4><ul><li>集群中hadoop集群由3台构成（1台master，2台slaves）</li><li>Hive的元数据库使用Mysql，并且单独包裹在一个docker环境中。</li></ul><h4 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h4><p>准备hadoop集群环境。启docker集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CONTAINER ID   IMAGE    COMMAND      CREATED    STATUS   PORTS      NAMES</span><br><span class="line">c27312e13270  kiwenlau/hadoop:1.0  "sh -c 'service ssh …"  2 hours ago  Up 2 hours hadoop-slave2</span><br><span class="line">f8b69885f3ef  kiwenlau/hadoop:1.0  "sh -c 'service ssh …" 2 hours ago  Up 2 hours hadoop-slave1</span><br><span class="line">439b359d230e  kiwenlau/hadoop:1.0  "sh -c 'service ssh …"   2 hours ago  Up 2 hours  0.0.0.0:8088-&gt;8088/tcp, 0.0.0.0:50070-&gt;50070/tcp   hadoop-master</span><br></pre></td></tr></table></figure><h4 id="Hive部署"><a href="#Hive部署" class="headerlink" title="Hive部署"></a>Hive部署</h4><h5 id="下载安装包："><a href="#下载安装包：" class="headerlink" title="下载安装包："></a>下载安装包：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入hadoop-master主机，进入hadoop目录：/use/<span class="built_in">local</span>/hadoop</span></span><br><span class="line">wget http://apache.claz.org/hive/hive-2.3.3/apache-hive-2.3.3-bin.tar.gz</span><br><span class="line">tar -zxvf apache-hive-2.3.3-bin.tar.gz</span><br><span class="line">mv apache-hive-2.3.3-bin hive</span><br></pre></td></tr></table></figure><h5 id="配置Hive环境变量："><a href="#配置Hive环境变量：" class="headerlink" title="配置Hive环境变量："></a>配置Hive环境变量：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">hive</span></span><br><span class="line">export HIVE_HOME=/usr/local/hadoop/hive</span><br><span class="line">PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$PATH</span><br><span class="line"><span class="meta">#</span><span class="bash"> 生效环境变量</span></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h5 id="调整Hive的配置文件："><a href="#调整Hive的配置文件：" class="headerlink" title="调整Hive的配置文件："></a>调整Hive的配置文件：</h5><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 进入hive 配置文件目录：</span></span><br><span class="line">cd conf</span><br><span class="line">cp hive-<span class="keyword">default</span>.xml.template hive-site.xml</span><br><span class="line"><span class="meta"># 修改配置文件</span></span><br><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure><h5 id="新建HDFS分布式文件目录："><a href="#新建HDFS分布式文件目录：" class="headerlink" title="新建HDFS分布式文件目录："></a>新建HDFS分布式文件目录：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hadoop已经设置好环境变量，新建下面目录</span></span><br><span class="line">hadoop fs -mkdir -p /user/hive/warehouse  </span><br><span class="line">hadoop fs -mkdir -p /user/hive/tmp  </span><br><span class="line">hadoop fs -mkdir -p /user/hive/log </span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置目录权限</span></span><br><span class="line">hadoop fs -chmod -R 777 /user/hive/warehouse  </span><br><span class="line">hadoop fs -chmod -R 777 /user/hive/tmp  </span><br><span class="line">hadoop fs -chmod -R 777 /user/hive/log</span><br></pre></td></tr></table></figure><p>可以用下面命令进行检查：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@hadoop-master:/usr/local/hadoop/hive/conf# hadoop fs -ls /user/hive</span><br><span class="line">Found 3 items</span><br><span class="line">drwxrwxrwx   - root supergroup          0 2018-08-14 07:34 /user/hive/log</span><br><span class="line">drwxrwxrwx   - root supergroup          0 2018-08-14 07:34 /user/hive/tmp</span><br><span class="line">drwxrwxrwx   - root supergroup          0 2018-08-14 07:34 /user/hive/warehouse</span><br></pre></td></tr></table></figure><h5 id="修改配置文件（hive-site-xml）："><a href="#修改配置文件（hive-site-xml）：" class="headerlink" title="修改配置文件（hive-site.xml）："></a>修改配置文件（hive-site.xml）：</h5><p>hive数据仓库数据路径：/user/hive/warehouse</p><p>需要使用hdfs新建文件目录。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置查询日志存放目录：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.querylog.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/log/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Location of Hive run time structured log file<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>数据库JDBC连接配置（172.18.0.5为mysql的ip地址，暴露3306端口）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://172.18.0.5:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>数据库驱动：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>数据库用户名：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>数据库密码：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置Hive临时目录：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="regexp">/usr/</span>local<span class="regexp">/hadoop/</span>hive<span class="regexp">/tmp</span></span><br></pre></td></tr></table></figure><p>并在 <code>hive-site.xml</code> 中修改:</p><p>把<code>${system:java.io.tmpdir}</code> 改成真实物理绝对路径  /usr/local/hadoop/hive/tmp</p><p>把 <code>${system:user.name}</code> 改成 <code>${user.name}</code></p><blockquote><p>可以在外面编辑好配置文件，拷贝进docke：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; docker cp hive-site.<span class="keyword">xml</span> <span class="title">439b359d230e</span>:/usr/local/hadoop/hive/conf/hive-site.xml</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h4 id="配置hive-env-sh文件："><a href="#配置hive-env-sh文件：" class="headerlink" title="配置hive-env.sh文件："></a>配置hive-env.sh文件：</h4><p>尾部加上下面的配置（或者修改注释部分的配置亦可）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/usr/local/hadoop</span><br><span class="line">export HIVE_CONF_DIR=/usr/local/hadoop/hive/conf</span><br><span class="line">export HIVE_AUX_JARS_PATH=/usr/local/hadoop/hive/lib</span><br></pre></td></tr></table></figure><h4 id="配置Mysql"><a href="#配置Mysql" class="headerlink" title="配置Mysql"></a>配置Mysql</h4><p>启mysql容器，容器名：first-mysql，使用和hadoop一个桥接网络hadoop，密码为123456</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name first-mysql --net=hadoop -p 3306:3306 -e MYSQL\_ROOT\_PASSWORD=123456 -d mysql:5.7</span><br></pre></td></tr></table></figure><p>回显：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                                              NAMES</span><br><span class="line">84ae224cee53        mysql:5.7             "docker-entrypoint.s…"   32 minutes ago      Up 32 minutes       0.0.0.0:3306-&gt;3306/tcp                             first-mysql</span><br></pre></td></tr></table></figure><p>在Hadoop-master中配置mysql客户端（用来访问mysql服务器）：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-<span class="builtin-name">get</span> install mysql-client-core-5.6</span><br></pre></td></tr></table></figure><p>测试远程连接：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">mysql</span> <span class="selector-tag">-h172</span><span class="selector-class">.18</span><span class="selector-class">.0</span><span class="selector-class">.5</span> <span class="selector-tag">-P3306</span> <span class="selector-tag">-uroot</span> <span class="selector-tag">-p123456</span></span><br></pre></td></tr></table></figure><p>新建数据库，数据库名为hive：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; <span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> hive;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br></pre></td></tr></table></figure><p>初始化（Hive主机上）：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> <span class="string">/usr/local/hadoop/hive/bin</span></span><br><span class="line"><span class="string">./schematool</span> -initSchema -dbType mysql</span><br></pre></td></tr></table></figure><p>回显：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@hadoop-master:/usr/local/hadoop/hive/bin# ./schematool -initSchema -dbType mysql</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">。。。（略）</span><br><span class="line">schemaTool completed</span><br><span class="line"><span class="meta">#</span><span class="bash"> 初始化成功</span></span><br></pre></td></tr></table></figure><p>下载配置mysql驱动包，放在Hive的lib路径下面：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/hive/lib</span><br><span class="line">wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.38/mysql-connector-java-5.1.38.jar</span><br></pre></td></tr></table></figure><h3 id="启动Hive"><a href="#启动Hive" class="headerlink" title="启动Hive"></a>启动Hive</h3><p>做完上面准备工作后，开始启动hive：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root@hadoop-master:/usr/local/hadoop/hive/bin# ./hive</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/usr/local/hadoop/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/usr/local/hadoop/hive/lib/hive-common-2.3.3.jar!/hive-log4j2.properties Async: true</span><br><span class="line">Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.</span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure><p>最后进入hive的命令界面。</p><h3 id="踩坑备注"><a href="#踩坑备注" class="headerlink" title="踩坑备注"></a>踩坑备注</h3><p>1、Hive提示SSL连接警告</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tue Aug 14 10:53:12 UTC 2018 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br></pre></td></tr></table></figure><p>虽然Hive SQL执行成功，但是报上面的错误。产生的原因是使用JDBC连接MySQL服务器时为设置<code>useSSL</code>参数 。</p><p>解决办法：javax.jdo.option.ConnectionURL 配置的value值进行调整，设置<code>useSSL=false</code> ，注意xml中的语法。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://172.18.0.5:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    JDBC connect string for a JDBC metastore.</span><br><span class="line">    To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.</span><br><span class="line">    For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br></pre></td></tr></table></figure><p>重启Hive，不再有警告。</p><h2 id="远程部署"><a href="#远程部署" class="headerlink" title="远程部署"></a>远程部署</h2><p>对于远程部署需要单独启metastore服务，具体需要调整下面的配置文件（hive-site.xml）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hadoop-master:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>启动metastore服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service metastore &amp;</span><br></pre></td></tr></table></figure><p>当然这属于简单方式将Hive都扎堆部署在一个容器中。可以在集群其他几点启metastore服务，提升架构的高可用性，避免单点问题。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>1、Apache Hive-2.3.0 快速搭建与使用，<a href="https://segmentfault.com/a/1190000011303459" target="_blank" rel="noopener">https://segmentfault.com/a/1190000011303459</a></p><p>2、Hive提示警告SSL，<a href="https://blog.csdn.net/u012922838/article/details/73291524" target="_blank" rel="noopener">https://blog.csdn.net/u012922838/article/details/73291524</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;Hive（蜂巢）是Hadoop的组件，官方介绍为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://hive.apa
      
    
    </summary>
    
      <category term="hadoop" scheme="https://zjrongxiang.github.io/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>在Ubuntu上部署Minikube</title>
    <link href="https://zjrongxiang.github.io/2018/06/25/2018-06-25-%E5%9C%A8Ubuntu%E4%B8%8A%E9%83%A8%E7%BD%B2Minikube/"/>
    <id>https://zjrongxiang.github.io/2018/06/25/2018-06-25-在Ubuntu上部署Minikube/</id>
    <published>2018-06-25T11:30:00.000Z</published>
    <updated>2019-08-08T12:01:50.010Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>Kubernetes是Google推出的容器编排工具，这是Google保密十几年的强大武器Borg的开源版本。Kubernetes这个名字源于古希腊，意思是舵手。既然docker被比喻成大海上驮着集装箱的鲸鱼，那么Kubernetes就是舵手，掌握鲸鱼的游弋方向，寓意深刻。</p><p>Kubernetes第一个正式版本于2015年7月发布。从Kubernetes 1.3开始提供了一个叫<code>Minikube</code>的强大测试工具，可以在主流操作系统平台（win、os、linux）上运行单节点的小型集群，这个工具默认安装和配置了一个Linux VM，Docker和Kubernetes的相关组件，并且提供Dashboard。</p><p>本篇主要介绍Ubuntu平台上部署Minikube。Minikube利用本地虚拟机环境部署Kubernetes，其基本架构如下图所示。 Minitube项目地址：<a href="https://github.com/kubernetes/minikube" target="_blank" rel="noopener">https://github.com/kubernetes/minikube</a> </p><p><img src="\images\picture\minikube.jpeg" alt=""></p><h3 id="第一部分-准备"><a href="#第一部分-准备" class="headerlink" title="第一部分 准备"></a>第一部分 准备</h3><p>Minikube在OS X和Windows上部署需要安装虚拟机实现（用虚拟机来初始化Kubernetes环境），但是Linux例外可以使用自己的环境。参见：<a href="https://github.com/kubernetes/minikube#quickstart" target="_blank" rel="noopener">https://github.com/kubernetes/minikube#quickstart</a> </p><h4 id="1-1-准备工作"><a href="#1-1-准备工作" class="headerlink" title="1.1 准备工作"></a>1.1 准备工作</h4><p>检查CPU是否支持虚拟化，即BIOS中参数（<code>VT-x</code>/<code>AMD-v</code> ）设置为enable。</p><h4 id="1-2-安装虚拟机"><a href="#1-2-安装虚拟机" class="headerlink" title="1.2 安装虚拟机"></a>1.2 安装虚拟机</h4><p>Minikube在不同操作系统上支持不同的虚拟驱动：</p><ul><li><p>macOS</p><ul><li><a href="https://github.com/kubernetes/minikube/blob/master/docs/drivers.md?spm=a2c4e.11153940.blogcont221687.20.7dd57733jMa8yH#xhyve-driver" target="_blank" rel="noopener">xhyve driver</a>, <a href="https://www.virtualbox.org/wiki/Downloads?spm=a2c4e.11153940.blogcont221687.21.7dd57733jMa8yH" target="_blank" rel="noopener">VirtualBox</a> 或 <a href="https://www.vmware.com/products/fusion?spm=a2c4e.11153940.blogcont221687.22.7dd57733jMa8yH" target="_blank" rel="noopener">VMware Fusion</a></li></ul></li><li><p>Linux</p><ul><li><p><a href="https://www.virtualbox.org/wiki/Downloads?spm=a2c4e.11153940.blogcont221687.23.7dd57733jMa8yH" target="_blank" rel="noopener">VirtualBox</a> 或 <a href="https://github.com/kubernetes/minikube/blob/master/docs/drivers.md?spm=a2c4e.11153940.blogcont221687.24.7dd57733jMa8yH#kvm-driver" target="_blank" rel="noopener">KVM</a></p></li><li><blockquote><p><strong>注意:</strong> Minikube 也支持 <code>--vm-driver=none</code> 选项来在本机运行 Kubernetes 组件，这时候需要本机安装了 Docker。</p></blockquote></li></ul></li><li><p>Windows</p><ul><li><a href="https://www.virtualbox.org/wiki/Downloads?spm=a2c4e.11153940.blogcont221687.26.7dd57733jMa8yH" target="_blank" rel="noopener">VirtualBox</a> 或 <a href="https://github.com/kubernetes/minikube/blob/master/docs/drivers.md?spm=a2c4e.11153940.blogcont221687.27.7dd57733jMa8yH#hyperV-driver" target="_blank" rel="noopener">Hyper-V</a> </li></ul></li></ul><p>本篇在Ubuntu部署VirtualBox虚拟驱动。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> wget https://download.virtualbox.org/virtualbox/5.1.38/virtualbox-5.1_5.1.38-122592~Ubuntu~xenial_i386.deb</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">dpkg -i virtualbox-5.1_5.1.38-122592~Ubuntu~xenial_i386.deb</span></span><br></pre></td></tr></table></figure><h3 id="第二部分-安装minikube"><a href="#第二部分-安装minikube" class="headerlink" title="第二部分 安装minikube"></a>第二部分 安装minikube</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v0.28.0/minikube-linux-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/<span class="built_in">local</span>/bin/</span></span><br><span class="line"><span class="meta">  %</span><span class="bash"> Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span></span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">100 40.8M  100 40.8M    0     0  4671k      0  0:00:08  0:00:08 --:--:-- 7574k</span><br></pre></td></tr></table></figure><h3 id="第三部分-安装Kubectl"><a href="#第三部分-安装Kubectl" class="headerlink" title="第三部分 安装Kubectl"></a>第三部分 安装Kubectl</h3><p>kubectl即kubernetes的客户端，通过他可以进行类似docker run等容器管理操作 。</p><p>下载：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl</span></span><br><span class="line"><span class="meta">  %</span><span class="bash"> Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span></span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">100 52.5M  100 52.5M    0     0  6654k      0  0:00:08  0:00:08 --:--:-- 10.3M</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> chmod +x kubectl</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mv kubectl /usr/<span class="built_in">local</span>/bin/</span></span><br></pre></td></tr></table></figure><h3 id="第四部分-启集群"><a href="#第四部分-启集群" class="headerlink" title="第四部分 启集群"></a>第四部分 启集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:~# minikube start --registry-mirror=https://registry.docker-cn.com</span><br><span class="line">Starting local Kubernetes v1.10.0 cluster...</span><br><span class="line">Starting VM...</span><br><span class="line">Downloading Minikube ISO</span><br><span class="line"> 153.08 MB / 153.08 MB [============================================] 100.00% 0s</span><br><span class="line">Getting VM IP address...</span><br><span class="line">Moving files into cluster...</span><br><span class="line">Downloading kubeadm v1.10.0</span><br><span class="line">Downloading kubelet v1.10.0</span><br><span class="line">Finished Downloading kubelet v1.10.0</span><br><span class="line">Finished Downloading kubeadm v1.10.0</span><br><span class="line">Setting up certs...</span><br><span class="line">Connecting to cluster...</span><br><span class="line">Setting up kubeconfig...</span><br><span class="line">Starting cluster components...</span><br><span class="line">Kubectl is now configured to use the cluster.</span><br><span class="line">Loading cached images from config file.</span><br></pre></td></tr></table></figure><p>进入minikube虚拟机：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:~# minikube ssh</span><br><span class="line">                         _             _            </span><br><span class="line">            _         _ ( )           ( )           </span><br><span class="line">  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __  </span><br><span class="line">/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\</span><br><span class="line">| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/</span><br><span class="line">(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash">  通过<span class="built_in">exit</span>退出集群</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">exit</span></span></span><br><span class="line">logout</span><br></pre></td></tr></table></figure><p>虚拟机地址：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube ip</span></span><br><span class="line">192.168.99.100</span><br></pre></td></tr></table></figure><p>启停集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube start/stop</span></span><br></pre></td></tr></table></figure><p>获取集群信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/home/rongxiang# kubectl get node </span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION</span><br><span class="line">minikube   Ready     master    7h        v1.10.0</span><br></pre></td></tr></table></figure><p>删除集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube delete  </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> rm -rf ~/.minikube  </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubeadm reset</span></span><br></pre></td></tr></table></figure><h3 id="第五部分-心酸踩坑"><a href="#第五部分-心酸踩坑" class="headerlink" title="第五部分 心酸踩坑"></a>第五部分 心酸踩坑</h3><p>如果在启集群时遇到下面类似的错误，不要慌。国内环境99%的原因是GFW的原因，集群在抓取Google站点docker镜像时候被墙咔嚓了，然后time out。</p><p>WTF！我开始不知道呀。网上的部署指引都那么轻松！！然后重复删除集群，重新装，配参数，给docker配代理。尼玛，最后代理都被咔嚓了。终于撞了南墙，去阿里云拉取镜像，几秒钟搞定。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube start</span></span><br><span class="line">Starting local Kubernetes v1.10.0 cluster...</span><br><span class="line">Starting VM...</span><br><span class="line">Getting VM IP address...</span><br><span class="line">Moving files into cluster...</span><br><span class="line">Setting up certs...</span><br><span class="line">Connecting to cluster...</span><br><span class="line">Setting up kubeconfig...</span><br><span class="line">Starting cluster components...</span><br><span class="line">E0626 12:57:46.868961   26526 start.go:299] Error restarting cluster:  restarting kube-proxy: waiting for kube-proxy to be up for configmap update: timed out waiting for the condition</span><br><span class="line">================================================================================</span><br><span class="line">An error has occurred. Would you like to opt in to sending anonymized crash</span><br><span class="line">information to minikube to help prevent future errors?</span><br><span class="line">To opt out of these messages, run the command:</span><br><span class="line">minikube config set WantReportErrorPrompt false</span><br><span class="line">================================================================================</span><br><span class="line">Please enter your response [Y/n]:</span><br></pre></td></tr></table></figure><h3 id="第六部分-远程访问-minikube-dashboard"><a href="#第六部分-远程访问-minikube-dashboard" class="headerlink" title="第六部分 远程访问 minikube dashboard"></a>第六部分 远程访问 minikube dashboard</h3><p>在虚拟机启动前，设置端口转发。注意这里使用tcp而不是http。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage modifyvm <span class="string">"minikube"</span> --natpf1 <span class="string">"kubedashboard,tcp,,30000,,30000"</span></span></span><br></pre></td></tr></table></figure><p>然后启动虚拟机，这时候局域网上的其他服务器就可以通过宿主机的IP:30000访问web UI。</p><p>或者：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl proxy --address=<span class="string">'0.0.0.0'</span> --<span class="built_in">disable</span>-filter=<span class="literal">true</span></span></span><br></pre></td></tr></table></figure><h3 id="附录-补充VBoxManage管理"><a href="#附录-补充VBoxManage管理" class="headerlink" title="附录  补充VBoxManage管理"></a>附录  补充VBoxManage管理</h3><p>查询虚拟机：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage list vms</span></span><br><span class="line">"&lt;inaccessible&gt;" &#123;4a3cefe1-11d1-45d2-91c5-1e39fccb6a8d&#125;</span><br><span class="line">"minikube" &#123;dfcd1bdf-afc1-49e6-a270-9d8ff14bf167&#125;</span><br></pre></td></tr></table></figure><p>删除虚拟机：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage unregistervm --delete 4a3cefe1-11d1-45d2-91c5-1e39fccb6a8d</span></span><br></pre></td></tr></table></figure><h3 id="参考文献及材料"><a href="#参考文献及材料" class="headerlink" title="参考文献及材料"></a>参考文献及材料</h3><p>1、Minitube项目地址：<a href="https://github.com/kubernetes/minikube" target="_blank" rel="noopener">https://github.com/kubernetes/minikube</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;Kubernetes是Google推出的容器编排工具，这是Google保密十几年的强大武器Borg的开源版本。Ku
      
    
    </summary>
    
      <category term="Minikube Ubuntu" scheme="https://zjrongxiang.github.io/categories/Minikube-Ubuntu/"/>
    
    
  </entry>
  
  <entry>
    <title>在Minikube上运行Flink集群</title>
    <link href="https://zjrongxiang.github.io/2018/06/25/2019-04-23-%E5%9C%A8Minikube%E4%B8%8A%E8%BF%90%E8%A1%8CFlink%E9%9B%86%E7%BE%A4%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://zjrongxiang.github.io/2018/06/25/2019-04-23-在Minikube上运行Flink集群（一）/</id>
    <published>2018-06-25T11:30:00.000Z</published>
    <updated>2019-05-10T14:51:25.370Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   部署准备</li><li>第二部分  验证</li><li>总结</li><li>参考文献及资料 </li></ul><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><h3 id="第一部分-部署准备"><a href="#第一部分-部署准备" class="headerlink" title="第一部分 部署准备"></a>第一部分 部署准备</h3><p>首先当然需要部署minikube集群。启动minikube集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube start</span></span><br><span class="line">Starting local Kubernetes v1.10.0 cluster...</span><br><span class="line">Starting VM...</span><br><span class="line">Getting VM IP address...</span><br><span class="line">Moving files into cluster...</span><br><span class="line">Setting up certs...</span><br><span class="line">Connecting to cluster...</span><br><span class="line">Setting up kubeconfig...</span><br><span class="line">Starting cluster components...</span><br><span class="line">Kubectl is now configured to use the cluster.</span><br><span class="line">Loading cached images from config file.</span><br></pre></td></tr></table></figure><p> 上面的回显表明minikube已经启动成功。执行下面网络配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube ssh <span class="string">'sudo ip link set docker0 promisc on'</span></span></span><br></pre></td></tr></table></figure><h3 id="第二部分-部署Flink集群"><a href="#第二部分-部署Flink集群" class="headerlink" title="第二部分 部署Flink集群"></a>第二部分 部署Flink集群</h3><p>一个基本的Flink集群运行在minikube需要三个组件：</p><ul><li>Deployment/Job：运行 JobManager</li><li>Deployment for a pool of TaskManagers</li><li>Service exposing the JobManager’s REST and UI ports</li></ul><h4 id="1-1-创建命名空间"><a href="#1-1-创建命名空间" class="headerlink" title="1.1 创建命名空间"></a>1.1 创建命名空间</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl create -f namespace.yaml</span></span><br><span class="line">namespace/flink created</span><br></pre></td></tr></table></figure><p>其中namespace.yaml文件为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">flink</span></span><br></pre></td></tr></table></figure><p>查询minikube集群的的命名空间：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get namespaces</span></span><br><span class="line">NAME          STATUS    AGE</span><br><span class="line">flink         Active    1m</span><br><span class="line">kube-public   Active    254d</span><br><span class="line">kube-system   Active    254d</span><br></pre></td></tr></table></figure><h4 id="1-2-集群组件资源定义"><a href="#1-2-集群组件资源定义" class="headerlink" title="1.2 集群组件资源定义"></a>1.2 集群组件资源定义</h4><h5 id="1-2-1-启动flink-jobmanager组件"><a href="#1-2-1-启动flink-jobmanager组件" class="headerlink" title="1.2.1 启动flink-jobmanager组件"></a>1.2.1 启动flink-jobmanager组件</h5><p>Job Manager 服务是Flink集群的主服务，使用jobmanager-deployment.yaml创建。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">flink-jobmanager</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">        component:</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">flink:latest</span></span><br><span class="line"><span class="attr">        args:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6124</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">blob</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">8081</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">ui</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">JOB_MANAGER_RPC_ADDRESS</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">flink-jobmanager</span></span><br></pre></td></tr></table></figure><h5 id="1-2-2-启动flink-taskmanager组件"><a href="#1-2-2-启动flink-taskmanager组件" class="headerlink" title="1.2.2 启动flink-taskmanager组件"></a>1.2.2 启动flink-taskmanager组件</h5><p>使用taskmanager-deployment.yaml创建。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">flink-taskmanager</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">        component:</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">flink:latest</span></span><br><span class="line"><span class="attr">        args:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6121</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6122</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">JOB_MANAGER_RPC_ADDRESS</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">flink-jobmanager</span></span><br></pre></td></tr></table></figure><h5 id="1-2-3-启用flink服务"><a href="#1-2-3-启用flink服务" class="headerlink" title="1.2.3 启用flink服务"></a>1.2.3 启用flink服务</h5><p>使用jobmanager-service.yaml创建服务，并且将端口映射到minikube主机响应端口。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">flink-jobmanager</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">30123</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">blob</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6124</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">30124</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">30125</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ui</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">8081</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">30081</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">    component:</span> <span class="string">jobmanager</span></span><br></pre></td></tr></table></figure><h4 id="1-3-端口映射到虚拟机主机"><a href="#1-3-端口映射到虚拟机主机" class="headerlink" title="1.3 端口映射到虚拟机主机"></a>1.3 端口映射到虚拟机主机</h4><p>minikube虚拟机停止的情况下的端口转发命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage modifyvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30123_6123,tcp,,6123,,30123"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage modifyvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30124_6123,tcp,,6124,,30124"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage modifyvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30125_6125,tcp,,6125,,30125"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage modifyvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30081_8081,tcp,,8081,,30081"</span></span></span><br></pre></td></tr></table></figure><blockquote><p>格式说明：vboxmanage modifyvm 宿主机名称 natpf<1-n> “映射别名,tcp,,本机端口,,虚拟机端口” </1-n></p></blockquote><p>minikube虚拟机运行的情况下的端口转发命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage controlvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30123_6123,tcp,,6123,,30123"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage controlvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30124_6123,tcp,,6124,,30124"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage controlvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30125_6125,tcp,,6125,,30125"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage controlvm <span class="string">"minikube"</span> --natpf1 <span class="string">"30081_8081,tcp,,8081,,30081"</span></span></span><br></pre></td></tr></table></figure><blockquote><p>格式说明：vboxmanage controlvm 宿主机名称 natpf<1-n> “映射别名,tcp,,本机端口,,宿主机端口” </1-n></p></blockquote><p>另外如果要删除上面转发规则：</p><blockquote><p>vboxmanage controlvm 宿主机名称 natpf<1-n> delete 映射别名</1-n></p><p>vboxmanage modifyvm 宿主机名称 natpf<1-n> delete 映射别名</1-n></p></blockquote><h3 id="第三部分-验证"><a href="#第三部分-验证" class="headerlink" title="第三部分 验证"></a>第三部分 验证</h3><h4 id="3-1-minikube控制台界面"><a href="#3-1-minikube控制台界面" class="headerlink" title="3.1 minikube控制台界面"></a>3.1 minikube控制台界面</h4><p>为了是主机局域网类服务器都能访问minikube控制台，需要将端口映射出去。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> VBoxManage modifyvm <span class="string">"minikube"</span> --natpf1 <span class="string">"kubedashboard,tcp,,30000,,30000"</span></span></span><br></pre></td></tr></table></figure><p><img src="\images\picture\flink\minikube_dashboard.png" alt=""></p><h4 id="3-2-Flink控制台"><a href="#3-2-Flink控制台" class="headerlink" title="3.2 Flink控制台"></a>3.2 Flink控制台</h4><p><img src="\images\picture\flink\minikube_flink.png" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>（1）部署前提前拉取镜像到本地镜像库。</p><p>（2）需要将服务端口映射到本地机器端口，供局域网服务访问，为后续访问Flink提供方便。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>1、Kubernetes Setup ：<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/kubernetes.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/kubernetes.html</a></p><p>2、How to Deploy Flink Cluster &amp; Flink-exporter in Kubernetes Cluster：<a href="https://medium.com/pharos-production/how-to-deploy-flink-cluster-flink-exporter-in-kubernetes-cluster-48e24b440446" target="_blank" rel="noopener">https://medium.com/pharos-production/how-to-deploy-flink-cluster-flink-exporter-in-kubernetes-cluster-48e24b440446</a></p><p>3、<a href="https://github.com/melentye" target="_blank" rel="noopener">melentye</a>/<strong>flink-kubernetes</strong> <a href="https://github.com/melentye/flink-kubernetes" target="_blank" rel="noopener">https://github.com/melentye/flink-kubernetes</a></p><p>4、Set up Ingress on Minikube with the NGINX Ingress Controller <a href="https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   部署准备&lt;/li&gt;
&lt;li&gt;第二部分  验证&lt;/li&gt;
&lt;li&gt;总结&lt;/li&gt;
&lt;li&gt;参
      
    
    </summary>
    
      <category term="Minikube spark Kubernetes" scheme="https://zjrongxiang.github.io/categories/Minikube-spark-Kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>在Minikube上运行Kafka集群</title>
    <link href="https://zjrongxiang.github.io/2018/06/25/2019-07-27-%E5%9C%A8Minikube%E4%B8%8A%E8%BF%90%E8%A1%8CKafka%E9%9B%86%E7%BE%A4/"/>
    <id>https://zjrongxiang.github.io/2018/06/25/2019-07-27-在Minikube上运行Kafka集群/</id>
    <published>2018-06-25T11:30:00.000Z</published>
    <updated>2019-07-30T00:30:55.180Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   Minikube集群启动</li><li>第一部分   Kubernetes中StatefulSet介绍</li><li>第三部分   部署Zookeeper集群</li><li>第四部分   部署Kafka集群</li><li>第五部分   总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Kafka和zookeeper是在两种典型的有状态的集群服务。首先kafka和zookeeper都需要存储盘来保存有状态信息，其次kafka和zookeeper每一个实例都需要有对应的实例Id(Kafka需要broker.id,zookeeper需要my.id)来作为集群内部每个成员的标识，集群内节点之间进行内部通信时需要用到这些标识。</p><p>对于这类服务的部署，需要解决两个大的问题，一个是状态保存，另一个是集群管理(多服务实例管理)。kubernetes中提的StatefulSet(1.5版本之前称为Petset)方便了有状态集群服务在上的部署和管理。具体来说是通过Init Container来做集群的初始化工 作，用 Headless Service来维持集群成员的稳定关系，用Persistent Volume和Persistent Volume Claim提供网络存储来持久化数据，从而支持有状态集群服务的部署。</p><p>StatefulSet 是Kubernetes1.9版本中稳定的特性，本文使用的环境为 Kubernetes 1.10.0。</p><h2 id="第一部分-Minikube集群启动"><a href="#第一部分-Minikube集群启动" class="headerlink" title="第一部分 Minikube集群启动"></a>第一部分 Minikube集群启动</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:~# minikube start</span><br><span class="line">There is a newer version of minikube available (v1.2.0).  Download it here:</span><br><span class="line">https://github.com/kubernetes/minikube/releases/tag/v1.2.0</span><br><span class="line"></span><br><span class="line">To disable this notification, run the following:</span><br><span class="line">minikube config set WantUpdateNotification false</span><br><span class="line">Starting local Kubernetes v1.10.0 cluster...</span><br><span class="line">Starting VM...</span><br><span class="line">Downloading Minikube ISO</span><br><span class="line"> 153.08 MB / 153.08 MB [============================================] 100.00% 0s</span><br><span class="line">Getting VM IP address...</span><br><span class="line">Moving files into cluster...</span><br><span class="line">Downloading kubeadm v1.10.0</span><br><span class="line">Downloading kubelet v1.10.0</span><br><span class="line">Finished Downloading kubeadm v1.10.0</span><br><span class="line">Finished Downloading kubelet v1.10.0</span><br><span class="line">Setting up certs...</span><br><span class="line">Connecting to cluster...</span><br><span class="line">Setting up kubeconfig...</span><br><span class="line">Starting cluster components...</span><br><span class="line">Kubectl is now configured to use the cluster.</span><br><span class="line">Loading cached images from config file.</span><br></pre></td></tr></table></figure><h2 id="第二部分-Kubernetes中StatefulSet介绍"><a href="#第二部分-Kubernetes中StatefulSet介绍" class="headerlink" title="第二部分 Kubernetes中StatefulSet介绍"></a>第二部分 Kubernetes中StatefulSet介绍</h2><p>使用Kubernetes来调度无状态的应用较为简单</p><p>StatefulSet 这个对象是专门用来部署用状态应用的，可以为Pod提供稳定的身份标识，包括hostname、启动顺序、DNS名称等。</p><p>在最新发布的 <a href="https://www.kubernetes.org.cn/tags/kubernetes1-5" target="_blank" rel="noopener">Kubernetes 1.5</a> 我们将过去的 PetSet 功能升级到了 Beta 版本，并重新命名为StatefulSet</p><h2 id="第三部分-部署Zookeeper集群"><a href="#第三部分-部署Zookeeper集群" class="headerlink" title="第三部分 部署Zookeeper集群"></a>第三部分 部署Zookeeper集群</h2><h2 id="第四部分-部署Kafka集群"><a href="#第四部分-部署Kafka集群" class="headerlink" title="第四部分 部署Kafka集群"></a>第四部分 部署Kafka集群</h2><h3 id="参考文献及材料"><a href="#参考文献及材料" class="headerlink" title="参考文献及材料"></a>参考文献及材料</h3><p>1、kubernetes 中 kafka 和 zookeeper 有状态集群服务部署实践 (一) <a href="https://cloud.tencent.com/developer/article/1005492" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1005492</a></p><p>2、<a href="https://cloud.tencent.com/developer/article/1005491" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1005491</a></p><p>3、<a href="https://www.bogotobogo.com/DevOps/Docker/Docker_Kubernetes_StatefulSet.php" target="_blank" rel="noopener">https://www.bogotobogo.com/DevOps/Docker/Docker_Kubernetes_StatefulSet.php</a></p><p><a href="https://technology.amis.nl/2018/04/19/15-minutes-to-get-a-kafka-cluster-running-on-kubernetes-and-start-producing-and-consuming-from-a-node-application/" target="_blank" rel="noopener">https://technology.amis.nl/2018/04/19/15-minutes-to-get-a-kafka-cluster-running-on-kubernetes-and-start-producing-and-consuming-from-a-node-application/</a></p><p>4、<a href="https://kubernetes.io/zh/docs/tutorials/stateful-application/basic-stateful-set/" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/tutorials/stateful-application/basic-stateful-set/</a></p><p>5、<a href="https://jimmysong.io/kubernetes-handbook/guide/using-statefulset.html" target="_blank" rel="noopener">https://jimmysong.io/kubernetes-handbook/guide/using-statefulset.html</a></p><p>6、<a href="https://www.cnblogs.com/00986014w/p/9561901.html" target="_blank" rel="noopener">Kubernetes部署Kafka集群</a></p><p><a href="https://blog.usejournal.com/kafka-on-kubernetes-a-good-fit-95251da55837" target="_blank" rel="noopener">https://blog.usejournal.com/kafka-on-kubernetes-a-good-fit-95251da55837</a></p><p><a href="https://www.cnblogs.com/cocowool/p/kubernetes_statefulset.html" target="_blank" rel="noopener">https://www.cnblogs.com/cocowool/p/kubernetes_statefulset.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   Minikube集群启动&lt;/li&gt;
&lt;li&gt;第一部分   Kubernetes中State
      
    
    </summary>
    
      <category term="Minikube Kafka" scheme="https://zjrongxiang.github.io/categories/Minikube-Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>使用Cloudera Quickstart Docker镜像快速部署hadoop集群</title>
    <link href="https://zjrongxiang.github.io/2018/06/25/2019-04-30-%E4%BD%BF%E7%94%A8Cloudera%20Quickstart%20Docker%E9%95%9C%E5%83%8F%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2hadoop%E9%9B%86%E7%BE%A4/"/>
    <id>https://zjrongxiang.github.io/2018/06/25/2019-04-30-使用Cloudera Quickstart Docker镜像快速部署hadoop集群/</id>
    <published>2018-06-25T11:30:00.000Z</published>
    <updated>2019-07-28T01:46:58.830Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分   Docker镜像准备</li><li>第二部分   运行容器</li><li>第三部分   cloudera-manager管理</li><li>第四部分   组件使用测试</li><li>第五部分   总结</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>通常在个人笔记本上部署Hadoop测试集群（含生态圈各组件）是个很耗时的工作。Cloudera公司提供一个快速部署的Docker镜像，可以快速启动一个测试集群。</p><blockquote><p>测试环境为Ubuntu服务器</p></blockquote><h2 id="第一部分-Docker镜像准备"><a href="#第一部分-Docker镜像准备" class="headerlink" title="第一部分 Docker镜像准备"></a>第一部分 Docker镜像准备</h2><p>首先本机需要部署有docker环境，如果没有需要提前部署。</p><h3 id="1-1-拉取Docker镜像"><a href="#1-1-拉取Docker镜像" class="headerlink" title="1.1 拉取Docker镜像"></a>1.1 拉取Docker镜像</h3><p>可以从DockerHub上拉取cloudera/quickstart镜像。</p><blockquote><p>镜像项目地址为：<a href="https://hub.docker.com/r/cloudera/quickstart" target="_blank" rel="noopener">https://hub.docker.com/r/cloudera/quickstart</a></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker pull cloudera/quickstart:latest</span></span><br></pre></td></tr></table></figure><p>如果不具备联网环境，可以通过镜像介质包安装。介质可以在官网（需要注册用户）下载：<a href="https://www.cloudera.com/downloads/quickstart_vms/5-13.html" target="_blank" rel="noopener">https://www.cloudera.com/downloads/quickstart_vms/5-13.html</a></p><blockquote><p>由于墙的原因下载会很慢</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> wget https://downloads.cloudera.com/demo_vm/docker/cloudera-quickstart-vm-5.13.0-0-beta-docker.tar.gz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tar xzf cloudera-quickstart-vm-5.13.0-0-beta-docker.tar.gz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker import - cloudera/quickstart:latest &lt; cloudera-quickstart-vm-5.13.0-0-beta-docker/*.tar</span></span><br></pre></td></tr></table></figure><h3 id="1-2-检查镜像库"><a href="#1-2-检查镜像库" class="headerlink" title="1.2 检查镜像库"></a>1.2 检查镜像库</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker images|grep cloudera</span></span><br><span class="line">cloudera/quickstart   latest   4239cd2958c6   3 years ago         6.34GB</span><br></pre></td></tr></table></figure><p>说明镜像准备好了，下面基于镜像启动容器。</p><h2 id="第二部分-运行容器"><a href="#第二部分-运行容器" class="headerlink" title="第二部分 运行容器"></a>第二部分 运行容器</h2><h3 id="2-1-使用镜像启动容器"><a href="#2-1-使用镜像启动容器" class="headerlink" title="2.1 使用镜像启动容器"></a>2.1 使用镜像启动容器</h3><p>启动CDH集群的命令格式为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker run --hostname=quickstart.cloudera --privileged=<span class="literal">true</span> -t -i [OPTIONS] [IMAGE] /usr/bin/docker-quickstart</span></span><br></pre></td></tr></table></figure><p>官方提示的参数介绍如下：</p><table><thead><tr><th style="text-align:left">Option</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left">–hostname=quickstart.cloudera</td><td style="text-align:left">Required: Pseudo-distributed configuration assumes this hostname.容器主机名（/etc/hosts中指定hostname）。</td></tr><tr><td style="text-align:left">–privileged=true</td><td style="text-align:left">Required: For HBase, MySQL-backed Hive metastore, Hue, Oozie, Sentry, and Cloudera Manager.这是Hbase组件需要的模式。</td></tr><tr><td style="text-align:left">-t</td><td style="text-align:left">Required: Allocate a pseudoterminal. Once services are started, a Bash shell takes over. This switch starts a terminal emulator to run the services.</td></tr><tr><td style="text-align:left">-i</td><td style="text-align:left">Required: If you want to use the terminal, either immediately or connect to the terminal later.</td></tr><tr><td style="text-align:left">-p 8888</td><td style="text-align:left">Recommended: Map the Hue port in the guest to another port on the host.端口映射参数。</td></tr><tr><td style="text-align:left">-p [PORT]</td><td style="text-align:left">Optional: Map any other ports (for example, 7180 for Cloudera Manager, 80 for a guided tutorial).</td></tr><tr><td style="text-align:left">-d</td><td style="text-align:left">Optional: Run the container in the background.容器后台启动。</td></tr><tr><td style="text-align:left">–name</td><td style="text-align:left">容器的名字</td></tr><tr><td style="text-align:left">-v host_path:container_path</td><td style="text-align:left">主机上目录挂载到容器中目录上，主机上该放入任何东西，Docker容器中对于目录可以直接访问。</td></tr></tbody></table><p>当然还可以自定义其他docker启动参数。最后启动命令整理为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run -t -i -d \</span><br><span class="line">--name cdh \</span><br><span class="line">--hostname=quickstart.cloudera \</span><br><span class="line">--privileged=true \</span><br><span class="line">-v /data/CDH:/src \</span><br><span class="line">-p 8020:8020 -p 8022:8022 -p 7180:7180 -p 21050:21050 -p 50070:50070 -p 50075:50075 -p 50010:50010 -p 50020:50020 -p 8890:8890 -p 60010:60010 -p 10002:10002 -p 25010:25010 -p 25020:25020 -p 18088:18088 -p 8088:8088 -p 19888:19888 -p 7187:7187 -p 11000:11000 -p 8888:8888 cloudera/quickstart \</span><br><span class="line">/bin/bash -c '/usr/bin/docker-quickstart'</span><br></pre></td></tr></table></figure><blockquote><p>Cloudera 本身的 manager 是 7180 端口，提前配置端口映射。</p></blockquote><p>启动容器我们使用了-d后台启动参数，如果没有指定后台启动，终端将自动连接到容器，退出shell后容器会中止运行（可以通过使用Ctrl + P + Q命令退出，这样容器会继续保持运行）。</p><p>对于已经后台运行的容器，我们使用下面的命令进入容器shell：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker attach [CONTAINER HASH]</span></span><br></pre></td></tr></table></figure><h3 id="2-2-时钟同步问题"><a href="#2-2-时钟同步问题" class="headerlink" title="2.2 时钟同步问题"></a>2.2 时钟同步问题</h3><p>容器内部使用的时间时区为UTC，和主机（宿主机通常为CST（东八区））不同时区，会提示时钟同步问题。解决的办法是：</p><p>在环境变量中添加时区变量,并source生效:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vi /etc/profile</span></span><br><span class="line">文件末尾添加一行：TZ='Asia/Shanghai'; export TZ</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure><p>最后启动时钟同步服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@quickstart home]# service ntpd start</span><br><span class="line">Starting ntpd:                                             [  OK  ]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查服务状态</span></span><br><span class="line">[root@quickstart home]# service ntpd status</span><br><span class="line">ntpd (pid  13536) is running...</span><br></pre></td></tr></table></figure><p>这样完成时钟同步。</p><h3 id="2-3-使用集群服务"><a href="#2-3-使用集群服务" class="headerlink" title="2.3 使用集群服务"></a>2.3 使用集群服务</h3><p>这样集群的大部分服务组件均可使用。</p><h2 id="第三部分-cloudera-manager管理"><a href="#第三部分-cloudera-manager管理" class="headerlink" title="第三部分 cloudera-manager管理"></a>第三部分 cloudera-manager管理</h2><h3 id="3-1-启动管理服务"><a href="#3-1-启动管理服务" class="headerlink" title="3.1 启动管理服务"></a>3.1 启动管理服务</h3><p>CDH在该镜像中提供cloudera-manager组件，用户集群web管理界面，可以通过下面的命令启动。</p><blockquote><p>需要注意的是，启动后CDH会停止其他组件服务。</p></blockquote><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#./home/cloudera/cloudera-manager --express</span></span><br><span class="line">[QuickStart] Shutting down CDH services via init scripts...</span><br><span class="line">kafka-server: unrecognized service</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: /etc/zookeeper/conf/zoo.cfg</span><br><span class="line">[QuickStart] Disabling CDH services on boot...</span><br><span class="line">error reading information on service kafka-server: No such file or directory</span><br><span class="line">[QuickStart] Starting Cloudera Manager server...</span><br><span class="line">[QuickStart] Waiting <span class="keyword">for</span> Cloudera Manager API...</span><br><span class="line">[QuickStart] Starting Cloudera Manager agent...</span><br><span class="line">[QuickStart] Configuring deployment...</span><br><span class="line">Submitted <span class="built_in">jobs</span>: 14</span><br><span class="line">[QuickStart] Deploying client configuration...</span><br><span class="line">Submitted <span class="built_in">jobs</span>: 16</span><br><span class="line">[QuickStart] Starting Cloudera Management Service...</span><br><span class="line">Submitted <span class="built_in">jobs</span>: 24</span><br><span class="line">[QuickStart] Enabling Cloudera Manager daemons on boot...</span><br><span class="line">________________________________________________________________________________</span><br><span class="line"></span><br><span class="line">Success! You can now <span class="built_in">log</span> into Cloudera Manager from the QuickStart VM<span class="string">'s browser:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    http://quickstart.cloudera:7180</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Username: cloudera</span></span><br><span class="line"><span class="string">    Password: cloudera</span></span><br></pre></td></tr></table></figure><p>集群控制台的地址为：<a href="http://quickstart.cloudera:7180，需要注意的是这里quickstart.cloudera是主机名，需要客户端hosts中配置，否则使用实IP或者容器端口映射后使用宿主机IP（例如：192.168.31.3）。" target="_blank" rel="noopener">http://quickstart.cloudera:7180，需要注意的是这里quickstart.cloudera是主机名，需要客户端hosts中配置，否则使用实IP或者容器端口映射后使用宿主机IP（例如：192.168.31.3）。</a></p><p>用户名和密码为：cloudera/cloudera,登录界面如下：</p><p><img src="\images\picture\cloudera-manager1.jpg" alt=""></p><p>登录后，下图是集群控制台：</p><p><img src="\images\picture\cloudera-manager2.jpg" alt=""></p><p>从管理界面上可以看到除了主机和 manager ，其他服务组件均未启动。</p><h3 id="3-2-启动集群组件服务"><a href="#3-2-启动集群组件服务" class="headerlink" title="3.2 启动集群组件服务"></a>3.2 启动集群组件服务</h3><p>在控制台上，我们按照顺序启动HDFS、Hive、Hue、Yarn服务。</p><blockquote><p>如果服务启动异常，可以尝试重启服务组件。注意需要先启动HDFS后启动Hive，否则需要重启Hive。</p></blockquote><h2 id="第四部分-组件使用测试"><a href="#第四部分-组件使用测试" class="headerlink" title="第四部分 组件使用测试"></a>第四部分 组件使用测试</h2><h3 id="4-1-HDFS组件使用"><a href="#4-1-HDFS组件使用" class="headerlink" title="4.1 HDFS组件使用"></a>4.1 HDFS组件使用</h3><p>我们使用HDFS的Python API与集群hdfs文件系统进行交互测试：</p><h4 id="4-1-1-查看文件系统"><a href="#4-1-1-查看文件系统" class="headerlink" title="4.1.1 查看文件系统"></a>4.1.1 查看文件系统</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hdfs.client <span class="keyword">import</span> Client</span><br><span class="line">client = Client(<span class="string">"http://192.168.31.3:50070"</span>, root=<span class="string">"/"</span>, timeout=<span class="number">100</span>)</span><br><span class="line">print(client.list(<span class="string">"/"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ['benchmarks', 'hbase', 'tmp', 'user', 'var']</span></span><br><span class="line"><span class="comment"># 返回一个list记录主目录</span></span><br></pre></td></tr></table></figure><h4 id="4-1-2-上传新增文件"><a href="#4-1-2-上传新增文件" class="headerlink" title="4.1.2 上传新增文件"></a>4.1.2 上传新增文件</h4><p>注意这里需要在宿主机（客户端机器）配置hosts文件：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192<span class="selector-class">.168</span><span class="selector-class">.31</span><span class="selector-class">.3</span>       <span class="selector-tag">quickstart</span><span class="selector-class">.cloudera</span></span><br></pre></td></tr></table></figure><p>然后执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">client.upload(<span class="string">"/tmp"</span>, <span class="string">"/root/jupyter/nohup.out"</span>)</span><br><span class="line"><span class="comment"># '/tmp/nohup.out'</span></span><br><span class="line"><span class="comment"># 返回路径信息</span></span><br></pre></td></tr></table></figure><h4 id="4-1-3-下载hdfs文件"><a href="#4-1-3-下载hdfs文件" class="headerlink" title="4.1.3 下载hdfs文件"></a>4.1.3 下载hdfs文件</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client.download(<span class="string">"/tmp/nohup.out"</span>, <span class="string">"/tmp"</span>)</span><br><span class="line"><span class="comment"># 返回路径'/tmp/nohup.out'</span></span><br></pre></td></tr></table></figure><h2 id="第五部分-总结"><a href="#第五部分-总结" class="headerlink" title="第五部分 总结"></a>第五部分 总结</h2><p>1、Cloudera 的 docker 版本分成两部分启动。(1)启动各组件启动,使用命令为： /usr/bin/docker-quickstart，(2) 启动Cloudera manager 管理服务，启动命令为：/home/cloudera/cloudera-manager。docker启动时选择启动一项。</p><h2 id="参考文献及材料"><a href="#参考文献及材料" class="headerlink" title="参考文献及材料"></a>参考文献及材料</h2><p>1、cloudera/quickstart镜像地址：<a href="https://hub.docker.com/r/cloudera/quickstart" target="_blank" rel="noopener">https://hub.docker.com/r/cloudera/quickstart</a></p><p>2、cloudera/quickstart镜像部署指引：<a href="https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html</a></p><p>3、<a href="https://www.cnblogs.com/piperck/p/9917118.html" target="_blank" rel="noopener">利用 Docker 搭建单机的 Cloudera CDH 以及使用实践</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分   Docker镜像准备&lt;/li&gt;
&lt;li&gt;第二部分   运行容器&lt;/li&gt;
&lt;li&gt;第三部分
      
    
    </summary>
    
      <category term="Docker hadoop CDH" scheme="https://zjrongxiang.github.io/categories/Docker-hadoop-CDH/"/>
    
    
  </entry>
  
  <entry>
    <title>在Minikube上运行Spark集群</title>
    <link href="https://zjrongxiang.github.io/2018/06/25/2018-08-06-%E5%9C%A8Minikube%E4%B8%8A%E8%BF%90%E8%A1%8CSpark%E9%9B%86%E7%BE%A4/"/>
    <id>https://zjrongxiang.github.io/2018/06/25/2018-08-06-在Minikube上运行Spark集群/</id>
    <published>2018-06-25T11:30:00.000Z</published>
    <updated>2019-04-23T13:48:55.484Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>​     Spark2.3版本开始支持使用spark-submit直接提交任务给Kubernetes集群。执行机制原理：</p><ul><li>Spark创建一个在<a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/" target="_blank" rel="noopener">Kubernetes pod中</a>运行的Spark驱动程序。</li><li>驱动程序创建执行程序，这些执行程序也在Kubernetes pod中运行并连接到它们，并执行应用程序代码。</li><li>当应用程序完成时，执行程序窗格会终止并清理，但驱动程序窗格会保留日志并在Kubernetes API中保持“已完成”状态，直到它最终被垃圾收集或手动清理。</li></ul><p><img src="https://spark.apache.org/docs/latest/img/k8s-cluster-mode.png" alt="Spark集群组件"> </p><h3 id="第一部分-环境准备"><a href="#第一部分-环境准备" class="headerlink" title="第一部分 环境准备"></a>第一部分 环境准备</h3><h4 id="1-1-minikube虚拟机准备"><a href="#1-1-minikube虚拟机准备" class="headerlink" title="1.1 minikube虚拟机准备"></a>1.1 minikube虚拟机准备</h4><p>由于spark集群对内存和cpu资源要求较高，在minikube启动前，提前配置较多的资源给虚拟机。</p><blockquote><p>当minikube启动时，它以单节点配置开始，默认情况下占用<code>1Gb</code>内存和<code>2</code>CPU内核，但是，为了运行spark集群，这个资源配置是不够的，而且作业会失败。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube config <span class="built_in">set</span> memory 8192</span></span><br><span class="line">These changes will take effect upon a minikube delete and then a minikube start</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> minikube config <span class="built_in">set</span> cpus 2</span></span><br><span class="line">These changes will take effect upon a minikube delete and then a minikube start</span><br></pre></td></tr></table></figure><p>或者用下面的命令启集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube start --cpus 2 --memory 8192</span></span><br></pre></td></tr></table></figure><h4 id="1-2-Spark环境准备"><a href="#1-2-Spark环境准备" class="headerlink" title="1.2 Spark环境准备"></a>1.2 Spark环境准备</h4><p>第一步   下载saprk2.3</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> wget http://apache.mirrors.hoobly.com/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz</span></span><br></pre></td></tr></table></figure><p>解压缩：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> tar xvf spark-2.3.0-bin-hadoop2.7.tgz</span></span><br></pre></td></tr></table></figure><p>制作docker镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> spark-2.3.0-bin-hadoop2.7</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker build -t rongxiang/spark:2.3.0 -f kubernetes/dockerfiles/spark/Dockerfile .</span></span><br></pre></td></tr></table></figure><p>查看镜像情况：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker images</span></span><br><span class="line">REPOSITORY                                                              TAG                            IMAGE ID            CREATED             SIZE</span><br><span class="line">rongxiang1986/spark                                                     2.3.0                          c5c806314f25        5 days ago          346MB</span><br></pre></td></tr></table></figure><p>登录docker 账户：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker login</span></span><br><span class="line">Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.</span><br><span class="line">Username: </span><br><span class="line">Password: </span><br><span class="line">Login Succeeded</span><br></pre></td></tr></table></figure><p>将之前build好的镜像pull到docker hub上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker push rongxiang1986/spark:2.3.0</span></span><br></pre></td></tr></table></figure><blockquote><p>注意这里的格式要求（我踩坑了）：docker push 注册用户名/镜像名 </p></blockquote><p>在<a href="https://hub.docker.com/上查看，镜像确实push上去了。" target="_blank" rel="noopener">https://hub.docker.com/上查看，镜像确实push上去了。</a></p><h3 id="第二部分-提交Spark作业"><a href="#第二部分-提交Spark作业" class="headerlink" title="第二部分 提交Spark作业"></a>第二部分 提交Spark作业</h3><h4 id="2-1-作业提交"><a href="#2-1-作业提交" class="headerlink" title="2.1 作业提交"></a>2.1 作业提交</h4><p>提前配置serviceaccount信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl create serviceaccount spark</span></span><br><span class="line">serviceaccount/spark created</span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=default:spark --namespace=default</span></span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/spark-role created</span><br></pre></td></tr></table></figure><p> 提交作业：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ./spark-submit \</span></span><br><span class="line">--master k8s://https://192.168.99.100:8443 \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--name spark-pi \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \</span><br><span class="line">--conf spark.kubernetes.authenticate.executor.serviceAccountName=spark \</span><br><span class="line">--conf spark.executor.instances=2 \</span><br><span class="line">--conf spark.kubernetes.container.image=rongxiang1986/spark:2.3.0 \</span><br><span class="line">local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar</span><br></pre></td></tr></table></figure><p>提交命令的参数含义分别是：</p><ul><li><code>--class</code>：应用程序的入口点（命令中使用：org.apache.spark.examples.SparkPi）；</li><li><code>--master</code>：Kubernetes集群的URL（k8s://<a href="https://192.168.99.100:8443）；" target="_blank" rel="noopener">https://192.168.99.100:8443）；</a></li><li><code>--deploy-mode</code>：驱动程序部署位置（默认值：客户端），这里部署在集群中；</li><li><code>--conf spark.executor.instances=2</code>：运行作业启动的executor个数；</li><li><code>--conf spark.kubernetes.container.image=rongxiang1986/spark:2.3.0</code>：使用的docker镜像名称；</li><li><code>local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar</code>：应用程序依赖jar包路径；</li></ul><blockquote><p>注意：目前deploy-mode只支持cluster模式，不支持client模式。</p><p>Error: Client mode is currently not supported for Kubernetes.</p></blockquote><p>作业运行回显如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">2018-08-12 15:51:17 WARN  Utils:66 - Your hostname, deeplearning resolves to a loopback address: 127.0.1.1; using 192.168.31.3 instead (on interface enp0s31f6)</span><br><span class="line">2018-08-12 15:51:17 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address</span><br><span class="line">2018-08-12 15:51:18 INFO  LoggingPodStatusWatcherImpl:54 - State changed, new state: </span><br><span class="line"> pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver</span><br><span class="line"> namespace: default</span><br><span class="line"> labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver</span><br><span class="line"> pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0</span><br><span class="line"> creation time: 2018-08-12T07:51:18Z</span><br><span class="line"> service account name: spark</span><br><span class="line"> volumes: spark-token-rzrgk</span><br><span class="line"> node name: N/A</span><br><span class="line"> start time: N/A</span><br><span class="line"> container images: N/A</span><br><span class="line"> phase: Pending</span><br><span class="line"> status: []</span><br><span class="line">2018-08-12 15:51:18 INFO  LoggingPodStatusWatcherImpl:54 - State changed, new state: </span><br><span class="line"> pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver</span><br><span class="line"> namespace: default</span><br><span class="line"> labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver</span><br><span class="line"> pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0</span><br><span class="line"> creation time: 2018-08-12T07:51:18Z</span><br><span class="line"> service account name: spark</span><br><span class="line"> volumes: spark-token-rzrgk</span><br><span class="line"> node name: minikube</span><br><span class="line"> start time: N/A</span><br><span class="line"> container images: N/A</span><br><span class="line"> phase: Pending</span><br><span class="line"> status: []</span><br><span class="line">2018-08-12 15:51:18 INFO  LoggingPodStatusWatcherImpl:54 - State changed, new state: </span><br><span class="line"> pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver</span><br><span class="line"> namespace: default</span><br><span class="line"> labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver</span><br><span class="line"> pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0</span><br><span class="line"> creation time: 2018-08-12T07:51:18Z</span><br><span class="line"> service account name: spark</span><br><span class="line"> volumes: spark-token-rzrgk</span><br><span class="line"> node name: minikube</span><br><span class="line"> start time: 2018-08-12T07:51:18Z</span><br><span class="line"> container images: rongxiang1986/spark:2.3.0</span><br><span class="line"> phase: Pending</span><br><span class="line"> status: [ContainerStatus(containerID=null, image=rongxiang1986/spark:2.3.0, imageID=, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties=&#123;&#125;), name=spark-kubernetes-driver, ready=false, restartCount=0, state=ContainerState(running=null, terminated=null, waiting=ContainerStateWaiting(message=null, reason=ContainerCreating, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;)]</span><br><span class="line">2018-08-12 15:51:18 INFO  Client:54 - Waiting for application spark-pi to finish...</span><br><span class="line">2018-08-12 15:51:51 INFO  LoggingPodStatusWatcherImpl:54 - State changed, new state: </span><br><span class="line"> pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver</span><br><span class="line"> namespace: default</span><br><span class="line"> labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver</span><br><span class="line"> pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0</span><br><span class="line"> creation time: 2018-08-12T07:51:18Z</span><br><span class="line"> service account name: spark</span><br><span class="line"> volumes: spark-token-rzrgk</span><br><span class="line"> node name: minikube</span><br><span class="line"> start time: 2018-08-12T07:51:18Z</span><br><span class="line"> container images: rongxiang1986/spark:2.3.0</span><br><span class="line"> phase: Running</span><br><span class="line"> status: [ContainerStatus(containerID=docker://d43089c8340affc4534f796b94a90ae080670c36c095176575fbeebacaab648e, image=rongxiang1986/spark:2.3.0, imageID=docker-pullable://rongxiang1986/spark@sha256:3e93a2d462679015a9fb7d723f53ab1d62c5e3619e3f1564d182c3d297ddf75d, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties=&#123;&#125;), name=spark-kubernetes-driver, ready=true, restartCount=0, state=ContainerState(running=ContainerStateRunning(startedAt=Time(time=2018-08-12T07:51:51Z, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;), terminated=null, waiting=null, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;)]</span><br><span class="line">2018-08-12 15:51:57 INFO  LoggingPodStatusWatcherImpl:54 - State changed, new state: </span><br><span class="line"> pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver</span><br><span class="line"> namespace: default</span><br><span class="line"> labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver</span><br><span class="line"> pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0</span><br><span class="line"> creation time: 2018-08-12T07:51:18Z</span><br><span class="line"> service account name: spark</span><br><span class="line"> volumes: spark-token-rzrgk</span><br><span class="line"> node name: minikube</span><br><span class="line"> start time: 2018-08-12T07:51:18Z</span><br><span class="line"> container images: rongxiang1986/spark:2.3.0</span><br><span class="line"> phase: Succeeded</span><br><span class="line"> status: [ContainerStatus(containerID=docker://d43089c8340affc4534f796b94a90ae080670c36c095176575fbeebacaab648e, image=rongxiang1986/spark:2.3.0, imageID=docker-pullable://rongxiang1986/spark@sha256:3e93a2d462679015a9fb7d723f53ab1d62c5e3619e3f1564d182c3d297ddf75d, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties=&#123;&#125;), name=spark-kubernetes-driver, ready=false, restartCount=0, state=ContainerState(running=null, terminated=ContainerStateTerminated(containerID=docker://d43089c8340affc4534f796b94a90ae080670c36c095176575fbeebacaab648e, exitCode=0, finishedAt=Time(time=2018-08-12T07:51:57Z, additionalProperties=&#123;&#125;), message=null, reason=Completed, signal=null, startedAt=Time(time=2018-08-12T07:51:51Z, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;), waiting=null, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;)]</span><br><span class="line">2018-08-12 15:51:57 INFO  LoggingPodStatusWatcherImpl:54 - Container final statuses:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> Container name: spark-kubernetes-driver</span><br><span class="line"> Container image: rongxiang1986/spark:2.3.0</span><br><span class="line"> Container state: Terminated</span><br><span class="line"> Exit code: 0</span><br><span class="line">2018-08-12 15:51:57 INFO  Client:54 - Application spark-pi finished.</span><br><span class="line">2018-08-12 15:51:57 INFO  ShutdownHookManager:54 - Shutdown hook called</span><br><span class="line">2018-08-12 15:51:57 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-6dd1c204-4ad7-40c4-b47f-a34f18e1995d</span><br></pre></td></tr></table></figure><h4 id="2-2-日志查询"><a href="#2-2-日志查询" class="headerlink" title="2.2 日志查询"></a>2.2 日志查询</h4><p>可以通过命令查看容器执行日志，或者通过kubernetes-dashboard提供web界面查看。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl logs spark-pi-709e1c1b19813e7cbc1aeff45200c64e-driver</span></span><br></pre></td></tr></table></figure><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  DAGScheduler:54 - Job 0 finished: reduce at SparkPi.scala:38, took 0.576528 s</span><br><span class="line">Pi is roughly 3.1336756683783418</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  AbstractConnector:318 - Stopped Spark@9635fa&#123;HTTP/1.1,[http/1.1]&#125;&#123;0.0.0.0:4040&#125;</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  SparkUI:54 - Stopped Spark web UI at http://spark-pi<span class="string">-7314</span>d819cd3730b4bf7d02bfedd21373-driver-svc.default.svc:4040</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  KubernetesClusterSchedulerBackend:54 - Shutting down all executors</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint:54 - Asking each executor to shut down</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  KubernetesClusterSchedulerBackend:54 - Closing kubernetes client</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  MemoryStore:54 - MemoryStore cleared</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  BlockManager:54 - BlockManager stopped</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  SparkContext:54 - Successfully stopped SparkContext</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  ShutdownHookManager:54 - Shutdown hook called</span><br><span class="line">2018<span class="string">-08</span><span class="string">-12</span> 07:51:57 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark<span class="string">-435</span>d5ab2-f7b4<span class="string">-45</span>d0-a00f<span class="string">-0</span>bd9f162f9db</span><br></pre></td></tr></table></figure><p>执行结束后executor pod被自动清除。计算得到pi的值为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pi is roughly 3.1336756683783418</span><br></pre></td></tr></table></figure><p>如果作业通过cluster提交，driver容器会被保留，可以查看：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> minikube service list</span></span><br><span class="line">|-------------|------------------------------------------------------|-----------------------------|</span><br><span class="line">|  NAMESPACE  |                         NAME                         |             URL             |</span><br><span class="line">|-------------|------------------------------------------------------|-----------------------------|</span><br><span class="line">| default     | kubernetes                                           | No node port                |</span><br><span class="line">| default     | spark-pi-27fcc168740e372292b27185d124ad7b-driver-svc | No node port                |</span><br><span class="line">| kube-system | kube-dns                                             | No node port                |</span><br><span class="line">| kube-system | kubernetes-dashboard                                 | http://192.168.99.100:30000 |</span><br><span class="line">|-------------|------------------------------------------------------|-----------------------------|</span><br></pre></td></tr></table></figure><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>1、Running Spark on Kubernetes ：<a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p><p>2、在Minikube Kubernetes集群上运行Spark工作：<a href="https://iamninad.com/running-spark-job-on-kubernetes-minikube/" target="_blank" rel="noopener">https://iamninad.com/running-spark-job-on-kubernetes-minikube/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;​     Spark2.3版本开始支持使用spark-submit直接提交任务给Kubernetes集群。执行机
      
    
    </summary>
    
      <category term="Minikube spark Kubernetes" scheme="https://zjrongxiang.github.io/categories/Minikube-spark-Kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>5分钟介绍深度学习（科普）</title>
    <link href="https://zjrongxiang.github.io/2018/04/16/2018-04-15-network_intro/"/>
    <id>https://zjrongxiang.github.io/2018/04/16/2018-04-15-network_intro/</id>
    <published>2018-04-15T16:00:00.000Z</published>
    <updated>2019-01-13T09:47:48.188Z</updated>
    
    <content type="html"><![CDATA[<h3 id="历史背景"><a href="#历史背景" class="headerlink" title="历史背景"></a>历史背景</h3><p>最近几年Deep Learning、AI人工智能、机器学习等名词称为新闻热点，特别是Google Deep mind的Alpha Go战胜韩国棋手李世石，让深度学习妇孺皆知。</p><p>首先从概念范畴上讲，deep learning属于机器学习的一个分支，追根溯源其实是人工神经网络。顾名思义，人工神经网络是借鉴人类神经网路的结构原型，算生物仿生学（虽然人类到现在也没弄明白大脑原理）。</p><p><img src="http://images.gitbook.cn/30cf7530-b6ce-11e7-8181-ddae1c526ae3" style="zoom:100%"></p><p>例如下面就是一个3层结构的人工神经网络（1个输入层、1个隐藏层、1个输出层）。</p><p><img src="http://images0.cnblogs.com/blog2015/680781/201508/021735264703915.png" style="zoom:70%"></p><p>1989年Yann LeCun使用反向传播算法（Back Propagation）应用于多层神经网络训练。但一旦层数较大，网络的参数和训练计算量成倍增加，通常需要几周时间才能完成参数训练，另外反向传播算法容易梯度爆炸。研究人员获得好的结果，时间成本太大。</p><p>所以当时机器学习研究方向中，支持向量机（SVM）算法比多层神经网络更为热门，神经网络研究则相当冷门。</p><p>直到2012 年的ImageNet 图像分类竞赛中，Alex Krizhevsky使用CNN（卷积）多层网络（共8层、6千万个参数）赢得当年的比赛，领先第二名10.8个百分点。并且模型使用GPU芯片训练、引入正则技术（Dropout）。</p><p>从此多层神经网络成为机器学习中的研究热点。而为了“洗白”以前暗淡历史，被赋予了新的名称：Deep Learning。</p><h3 id="深度学习背后的数学"><a href="#深度学习背后的数学" class="headerlink" title="深度学习背后的数学"></a>深度学习背后的数学</h3><p>目前的人工智能均属于弱人工智能（不具备心智和意识）。事实上，深度学习目前主要在图像识别和声音识别场景中获得较好的效果。深度学习的成为热点，依赖于两方面条件的成熟:</p><ul><li>算力的提升，训练中大量使用GPU。</li><li>大量数据的获得和沉淀。</li></ul><p>弱人工智能背后的理论基础依赖于数学和统计理论，其实更应该算数据科学的范畴。</p><p>比如输入数据具有$m$维特征，而输出特征为$k$维（例如如果是个二分类问题，$k=2$）。我们使用3层神经网络（输入层为$m$维，即含有$m$个神经元；隐藏层为$n$维，输出层为$k$维）用来训练。</p><p>通常我们将输入数据看成$R^m$（$m$维欧几里得空间），输出数据看成$R^k$（$k$维欧式空间）,如下图：</p><p><img src="\images\picture\bpnnfigure.png" alt=""></p><p>从数学上看，神经网络的结构定义了一个函数空间：${R^{n}} \xrightarrow{\text{f}} {R^{h}}  \xrightarrow{\text{g}} {R^{m}}$ 。这个函数空间中元素是非线性的（隐藏层和输出层有非线性的激活函数）。空间中每个函数由网络中的参数（w，b）唯一决定。</p><p>神经网络训练的过程可以形象的理解为寻找最佳函数的过程：输入层“吃进”大量训练数据，通过非线性函数的作用，观察输出层输出结果和实际值的差异。这是一个监督学习的过程。</p><ul><li>如果差异（误差）在容忍范围内，停止训练，认为该函数是目标函数。</li><li>如果差异较大，反向传播算法根据梯度下降的反向更新网络中的参数（w，b），即挑选新的函数。</li><li>重新喂进数据，计算新挑选函数的误差。如此循环，直到找到目标函数（也可设置提前结束训练）。</li></ul><blockquote><p>为什么神经网络的发展最后偏向的是“深度”呢？即增加层数来提高网络的认知能力。为什么没有“宽度学习”？即增加网络隐藏层的维度（宽度）。</p><p>其实从数学上可以证明深度网络和“宽度网络”的等价性。证明提示：考虑网络定义的函数空间出发。</p></blockquote><h3 id="写在最后（畅想未来）"><a href="#写在最后（畅想未来）" class="headerlink" title="写在最后（畅想未来）"></a>写在最后（畅想未来）</h3><h4 id="深度学习的局限性思考"><a href="#深度学习的局限性思考" class="headerlink" title="深度学习的局限性思考"></a>深度学习的局限性思考</h4><p>目前深度学习被各行各业应用于各种场景，而且有些特定场景取得了良好结果。但是传统的深度学习仍属于监督学习，更像一个被动的执行者，按照人类既定的规则，吃进海量数据，然后训练。</p><p>那么深度网络是否真的理解和学到了模式？还是只学会对有限数据的模式识别？甚至就是一个庞大的记忆网络？这都是值得我们深度思考的。</p><h4 id="GAN对抗网络"><a href="#GAN对抗网络" class="headerlink" title="GAN对抗网络"></a>GAN对抗网络</h4><p>那么怎么能说明模型真的学习并理解了。我们提出了一个原则：如果你理解了一个事物，那么你就可以创造它。这样就发明了GAN对抗网络。让网络自己去创造事物，然后用现实数据去监督，当网络的创造能力和现实接近时，我们认为网络学会了。</p><blockquote><p>其实思想类似传统的遗传算法。</p></blockquote><h4 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h4><p>另外传统的深度学习，输入的环境（数据）是固定的。然而现实中我们学习过程其实是：环境（数据）与学习个体互相作用的交互过程。这个学习过程人类由于时间有限，是个漫长的过程。但是计算机有个天然优势，可以同时启用成千上万的学习个体完成与环境数据的交互学习过程。例如Alpha Go启用上万个体，两两互搏，配上强大算力，短时间完成学习，这是人类不可企及的。</p><h4 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h4><p>人类学习中还有个方法叫：触类旁通。其实就是不同场景训练模型的借鉴。例如A场景得到训练好的模型（网络参数），对于新的场景B，可以尝试直接用A场景的网络（或部分使用，拼接），以此来减少训练成本。</p><p>那么新的问题来了：是否具有统一的迁移标准，即什么模型是适合迁移的？如果这些问题没有理论基础支持，迁移学习也摆脱不了“炼丹术”的非议。</p><p><strong>深度学习从过去的暗淡无色到现在的光耀夺目。</strong></p><p><strong>然而任何方法都是有边际效应的。</strong></p><p><strong>人工智能的终点还很遥远，谁是下一颗耀眼的明星，需要学界和工业界共同探索。</strong></p><p>​                                                                                                                          </p><p>​                                                                                                                                  2018年4月15日 夜</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;历史背景&quot;&gt;&lt;a href=&quot;#历史背景&quot; class=&quot;headerlink&quot; title=&quot;历史背景&quot;&gt;&lt;/a&gt;历史背景&lt;/h3&gt;&lt;p&gt;最近几年Deep Learning、AI人工智能、机器学习等名词称为新闻热点，特别是Google Deep mind的Alp
      
    
    </summary>
    
      <category term="network" scheme="https://zjrongxiang.github.io/categories/network/"/>
    
    
      <category term="BP， network" scheme="https://zjrongxiang.github.io/tags/BP%EF%BC%8C-network/"/>
    
  </entry>
  
  <entry>
    <title>计算机语言中编译和解释的总结</title>
    <link href="https://zjrongxiang.github.io/2018/04/14/2018-04-14-interprete_and_compile/"/>
    <id>https://zjrongxiang.github.io/2018/04/14/2018-04-14-interprete_and_compile/</id>
    <published>2018-04-14T08:30:00.000Z</published>
    <updated>2018-04-15T03:25:43.418Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>非计算机科班，主要是总结给自己看的，如果有表达错误，请大家指正。</p></blockquote><h3 id="几个概念"><a href="#几个概念" class="headerlink" title="几个概念"></a>几个概念</h3><h4 id="高级语言与低级语言"><a href="#高级语言与低级语言" class="headerlink" title="高级语言与低级语言"></a>高级语言与低级语言</h4><p>高级语言（High Level Programming Language）和低级语言（Low Level Programming Language）是一对相对的共生概念（没有一个严格的量化区分标准）。</p><ul><li>低级语言更接近计算机底层资源（直接与硬件资源进行交互）。例如汇编语言。</li><li>高级语言进行了封装和抽象，语言设计更容易被人类思维逻辑所理解（和低级语言比较，学习曲线较缓）。例如C、C++、java、python等。</li></ul><p>随着计算机语言的蓬勃发展（计算机语言的文艺复兴），过去一些高级语言，也有人重新定位成低级语言，例如C语言。</p><h4 id="字节码与机器码"><a href="#字节码与机器码" class="headerlink" title="字节码与机器码"></a>字节码与机器码</h4><p>字节码（Byte Code）不是一种计算机语言。属于高级语言预编译生成的中间码。高级语言源码在预编译的过程中，就完成这部分工作，生成字节码。</p><p>机器码（Machine Code）是一组可以直接被CPU执行的指令集。所有语言（低级和高级）最后都需要编译或解释成机器码（CPU指令集），才能执行。</p><h3 id="编译器和解释器"><a href="#编译器和解释器" class="headerlink" title="编译器和解释器"></a>编译器和解释器</h3><h4 id="编译器（Interpreter）"><a href="#编译器（Interpreter）" class="headerlink" title="编译器（Interpreter）"></a>编译器（Interpreter）</h4><blockquote><p>A compiler is a computer program (or a set of programs) that transforms source code written in a programming language (the source language) into another computer language (the target language), with the latter often having a binary form known as object code. The most common reason for converting source code is to create an executable program.</p></blockquote><ul><li>编译器是一种计算机程序。</li><li>编译器是一个计算机语言的翻译工具，直接将源代码文件预编译（形象的说：翻译）成更低级的代码语言（字节码码、机器码）。</li><li>编译器不会去执行编译的结果，只生成编译的结果文件。</li></ul><h4 id="解释器（Compiler）"><a href="#解释器（Compiler）" class="headerlink" title="解释器（Compiler）"></a>解释器（Compiler）</h4><blockquote><p>In computer science, an interpreter is a computer program that directly executes, i.e. performs, instructions written in a programming or scripting language, without previously compiling them into a machine language program. An interpreter generally uses one of the following strategies for program execution:</p><p>1、parse the source code and perform its behavior directly.</p><p>2、translate source code into some efficient intermediate representation and immediately execute this.</p><p>3、explicitly execute stored precompiled code made by a compiler which is part of the interpreter system.</p></blockquote><ul><li>解释器是一种计算机程序。</li><li>解释器读取源代码或者中间码文件，转换成机器码并与计算机硬件交互。即逐行执行源码。</li><li>解释器会将源代码转换成一种中间代码不会输出更低级的编译结果文件。输出执行结果。</li></ul><p><img src="\images\picture\compiler.jpg" alt="网上有一张很形象的图"></p><h3 id="解释型语言和编译型语言"><a href="#解释型语言和编译型语言" class="headerlink" title="解释型语言和编译型语言"></a>解释型语言和编译型语言</h3><p>两者的区别主要是源码编译时间的差异。相同点都要翻译成机器码后由计算机执行。</p><h4 id="编译型语言"><a href="#编译型语言" class="headerlink" title="编译型语言"></a>编译型语言</h4><ul><li>编译语言的源码文件需要提前通过编译器编译成机器码文件（比如win中的exe可执行文件）。</li><li>执行时，只需执行编译结果文件。不需要重复翻译。</li><li>这类语言有：C、C++、Fortran、Pascal等。</li></ul><h4 id="解释型语言"><a href="#解释型语言" class="headerlink" title="解释型语言"></a>解释型语言</h4><ul><li>解释型语言在运行时进行翻译。比如VB语言，在执行的时候，解释器将语言翻译成机器码，然后执行。</li><li>这类语言有：Ruby、Perl、JavaScript、PHP等。</li></ul><h4 id="混合型语言"><a href="#混合型语言" class="headerlink" title="混合型语言"></a>混合型语言</h4><p>但是随着计算机语言的发展，有些语言兼具两者的特点。</p><ul><li><strong>JAVA语言</strong></li></ul><p>JAVA编译过程只是将<code>.java</code>文件翻译成字节码（Byte Code）（<code>.class</code>文件）。字节码文件交由java虚拟机（JVM）解释运行。也就是说Java源码文件既要编译也要JVM虚拟机进行解释后运行。所以有种说法认为Java是半解释型语言（semi-interpreted” language）。</p><ul><li><strong>Python语言</strong></li></ul><p>python其实类似Java。例如一个python文件<code>test.py</code> ，解释器首先尝试读取该文件历史编译结果（pyc文件）即<code>test.pyc</code>文件或者<code>test.pyo</code> 。如果没有历史文件或者编译文件的日期较旧（即py文件可能有更新），解释器会重新编译生成字节码文件（pyc文件），然后Python虚拟机对字节码解释执行。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>【1】 <a href="http://huang-jerryc.com/2016/11/20/do-you-konw-the-different-between-compiler-and-interpreter/" target="_blank" rel="noopener">你知道「编译」与「解释」的区别吗？</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;非计算机科班，主要是总结给自己看的，如果有表达错误，请大家指正。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;几个概念&quot;&gt;&lt;a href=&quot;#几个概念&quot; class=&quot;headerlink&quot; title=&quot;几个概念&quot;&gt;&lt;/a&gt;几个概念&lt;/h
      
    
    </summary>
    
      <category term="python" scheme="https://zjrongxiang.github.io/categories/python/"/>
    
    
      <category term="interpreter， compiler" scheme="https://zjrongxiang.github.io/tags/interpreter%EF%BC%8C-compiler/"/>
    
  </entry>
  
  <entry>
    <title>深入理解Python语言中import机制</title>
    <link href="https://zjrongxiang.github.io/2018/04/14/2018-04-14-python_import/"/>
    <id>https://zjrongxiang.github.io/2018/04/14/2018-04-14-python_import/</id>
    <published>2018-04-14T02:30:00.000Z</published>
    <updated>2018-04-15T07:58:12.909Z</updated>
    
    <content type="html"><![CDATA[<h3 id="包和模块"><a href="#包和模块" class="headerlink" title="包和模块"></a>包和模块</h3><p>首先要介绍Python中两个概念：包和模块。简单的理解（从文件系统角度），包（package）是一个文件夹，而模块（module）是一个python源码文件（扩展名为<code>.py</code>）。</p><ul><li><p><strong>包（package）</strong>：文件夹（文件夹中含有文件<code>__init__.py</code>），包里面含有很多模块组成。</p><blockquote><p><code>__init__.py</code>文件，在里面自定义初始化操作，或为空。</p></blockquote></li><li><p><strong>模块（module）</strong>：即python文件，文件中定义了函数、变量、常量、类等。</p></li></ul><h3 id="Import-方法"><a href="#Import-方法" class="headerlink" title="Import 方法"></a>Import 方法</h3><h4 id="Import-模块方法"><a href="#Import-模块方法" class="headerlink" title="Import 模块方法"></a>Import 模块方法</h4><p>先看一个例子。我们经常使用的模块<code>math</code> ，背后对应其实是一个python文件：<code>math.py</code> 。该文件在<code>C:\Anaconda3\Lib\site-packages\pymc3</code>目录里面（具体环境会有差异）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line">math.sqrt(<span class="number">2</span>)</span><br><span class="line"><span class="comment">#1.4142135623730951</span></span><br></pre></td></tr></table></figure><p>如果只要import <code>math.py</code>中具体的函数：</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from math <span class="keyword">import</span> <span class="built_in">sqrt</span>，<span class="built_in">sin</span></span><br><span class="line"><span class="built_in">sqrt</span>(<span class="number">2</span>)</span><br><span class="line"><span class="built_in">sin</span>（<span class="number">1</span>）</span><br></pre></td></tr></table></figure><p>另外可以将模块中所有内容导入：</p><figure class="highlight moonscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> <span class="built_in">math</span> <span class="keyword">import</span> *</span><br><span class="line">sqrt(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h4 id="Import-包方法"><a href="#Import-包方法" class="headerlink" title="Import 包方法"></a>Import 包方法</h4><p>包（package）可以简单理解为文件夹。该文件夹下须存在 <code>__init__.py</code> 文件, 内容可以为空。另外该主文件夹下面可以有子文件夹，如果也有 <code>__init__.py</code> 文件，这是子包。类似依次嵌套。</p><p>例如<code>Tensorflow</code>的包（文件树）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root@vultr:~/anaconda3/lib/python3.6/site-packages/tensorflow# tree -L 1</span><br><span class="line">.</span><br><span class="line">├── aux-bin</span><br><span class="line">├── contrib</span><br><span class="line">├── core</span><br><span class="line">├── examples</span><br><span class="line">├── include</span><br><span class="line">├── __init__.py</span><br><span class="line">├── libtensorflow_framework.so</span><br><span class="line">├── __pycache__</span><br><span class="line">├── python</span><br><span class="line">└── tools</span><br></pre></td></tr></table></figure><blockquote><p> <code>__init__.py</code> 文件在import包时，优先导入，作为import包的初始化。</p></blockquote><p>我们以<code>Tensorflow</code>为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入包</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#导入子包：contrib</span></span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib <span class="keyword">as</span> contrib</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> contrib</span><br><span class="line"><span class="comment">#导入具体的模块：mnist</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">import</span> tensorflow.examples.tutorials.mnist</span><br></pre></td></tr></table></figure><h3 id="命名空间（namespace）"><a href="#命名空间（namespace）" class="headerlink" title="命名空间（namespace）"></a>命名空间（namespace）</h3><p>Namespace是字典数据，供编译器、解释器对源代码中函数名、变量名、模块名等信息进行关联检索。</p><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>Python语言使用namespace（命名空间）来存储变量，namespace是一个mapping（映射）。namespace可以理解是一个字典（dict）数据类型，其中键名（key）为变量名，而键值（value）为变量的值。</p><blockquote><p>A <em>namespace</em> is a mapping from names to objects. Most namespaces are currently implemented as Python dictionaries。</p></blockquote><ul><li><strong>每一个函数拥有自己的namespace。</strong>称为local namespace（局部命名空间），记录函数的变量。</li><li><strong>每一个模块（module）拥有自己的namespace</strong>。称为global namespace（全局命名空间），记录模块的变量，包括包括模块中的函数、类，其他import（导入）的模块，还有模块级别的变量和常量。</li><li><strong>每一个包（package）拥有自己的namespace。</strong> 也是global namespace ，记录包中所有子包、模块的变量信息。</li><li><strong>Python的built-in names（内置函数、内置常量、内置类型）。</strong> 即内置命名空间。在Python解释器启动时创建，任何模块都可以访问。当退出解释器后删除。</li></ul><h4 id="命名空间的检索顺序"><a href="#命名空间的检索顺序" class="headerlink" title="命名空间的检索顺序"></a>命名空间的检索顺序</h4><p>当代码中需要访问或获取变量时（还有模块名、函数名），Python解释器会对命名空间进行顺序检索，直到根据键名（变量名）找到键值（变量值）。查找的顺序为（LEGB）：</p><ol><li>local namespace，即当前函数或者当前类。如找到，停止检索。</li><li>enclosing function namespace，嵌套函数中外部函数的namespace。</li><li>global namespace，即当前模块。如找到，停止检索。</li><li>build-in namespace，即内置命名空间。如果前面两次检索均为找到，解释器才会最后检索内置命名空间。如果仍然未找到就会报NameRrror（类似：<code>NameError: name &#39;a&#39; is not defined</code>）。</li></ol><h4 id="举栗子"><a href="#举栗子" class="headerlink" title="举栗子"></a>举栗子</h4><p>讲完了理论介绍，我们来举栗子，直观感受一下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">进入python环境</span></span><br><span class="line">Python 3.5.3 |Anaconda custom (64-bit)| (default, May 11 2017, 13:52:01) [MSC v.</span><br><span class="line">1900 64 bit (AMD64)] on win32</span><br><span class="line">Type "help", "copyright", "credits" or "license" for more information.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">print</span>(globals())</span></span><br><span class="line">&#123;'__name__': '__main__', '__doc__': None, '__spec__': None, '__package__': None,</span><br><span class="line"> '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__builtins__': &lt;mod</span><br><span class="line">ule 'builtins' (built-in)&gt;&#125;</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; x=1</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">print</span>(globals())</span></span><br><span class="line">&#123;'__name__': '__main__', '__doc__': None, '__spec__': None, '__package__': None,</span><br><span class="line"> '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__builtins__': &lt;mod</span><br><span class="line">ule 'builtins' (built-in)&gt;, 'x': 1&#125;</span><br></pre></td></tr></table></figure><p>上面的例子我们查看了global namespace的字典（dict），其中<code>&#39;__builtins__&#39;</code>就是内置命名空间。新建变量<code>x=1</code>后，全局命名空间会新增这个K-V对。</p><p>还可以通过下面的方法查看import模块、包的namespace。</p><p><strong>当我们import一个module（模块）或者package（包）时，伴随着新建一个global namespace（全局命名空间）。</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line">math.__dict__</span><br><span class="line">&#123;'__name__': 'math', 'tanh': &lt;built-in function tanh&gt;, 'nan': nan, 'atanh': &lt;bui</span><br><span class="line">lt-in function atanh&gt;,'acosh': &lt;built-in function acosh&gt;, </span><br><span class="line"><span class="meta">#</span><span class="bash">中间略</span></span><br><span class="line">'trunc': &lt;built-in function trunc&gt;, 'acos': &lt;built-in function acos&gt;, 'sqrt': &lt;built-in</span><br><span class="line"> function sqrt&gt;, 'floor': &lt;built-in function floor&gt;, 'gamma': &lt;built-in function</span><br><span class="line"><span class="meta"> gamma&gt;</span><span class="bash">, <span class="string">'cosh'</span>: &lt;built-in <span class="keyword">function</span> cosh&gt;&#125;</span></span><br><span class="line">import tensorflow</span><br><span class="line">tensorflow.__dict__</span><br><span class="line"><span class="meta">#</span><span class="bash">包的所有模块、函数等命名空间信息。大家可以试一下。</span></span><br></pre></td></tr></table></figure><p>大家可以动手试试其他的场景，比如函数内部查看locals() 。函数内部的变量global声明后，查看globals()字典会有怎样变化。这里就不再一一验证举栗了。</p><p>对于包，我们以tensorflow为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow</span><br><span class="line">tensorflow.__dict__</span><br><span class="line"><span class="comment">##中间略，只摘取部分信息。命名空间中包含module和function的信息。</span></span><br><span class="line"><span class="string">'angle'</span>: &lt;function tensorflow.python.ops.math_ops.angle&gt;,</span><br><span class="line"> <span class="string">'app'</span>: &lt;module <span class="string">'tensorflow.python.platform.app'</span> <span class="keyword">from</span> <span class="string">'/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py'</span>&gt;,</span><br><span class="line"> <span class="string">'arg_max'</span>: &lt;function tensorflow.python.ops.gen_math_ops.arg_max&gt;,</span><br></pre></td></tr></table></figure><h3 id="Import的过程"><a href="#Import的过程" class="headerlink" title="Import的过程"></a>Import的过程</h3><p>当我们执行import 模块、包时，主要有三个过程：检索、加载、名字绑定。</p><h4 id="第一步：检索（Finder）"><a href="#第一步：检索（Finder）" class="headerlink" title="第一步：检索（Finder）"></a>第一步：检索（Finder）</h4><p>Python解释器会对模块所属位置进行搜索：</p><h5 id="（1）检索：内置模块（已经加载到缓存中的模块）"><a href="#（1）检索：内置模块（已经加载到缓存中的模块）" class="headerlink" title="（1）检索：内置模块（已经加载到缓存中的模块）"></a>（1）检索：内置模块（已经加载到缓存中的模块）</h5><p>内置模块（已经加载到缓存中的模块），即在 <code>sys.modules</code> 中检索。Python已经加载到内存中的模块均会在这个字典中进行登记。如果已经登记，不再重复加载。直接将模块的名字加入正在import的模块的namespace。可以通过下面方法查看：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import sys</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">print</span>(sys.modules)</span></span><br><span class="line">&#123;'_signal': &lt;module '_signal' (built-in)&gt;, 'os.path': &lt;module 'ntpath' from 'C:</span><br><span class="line">\Anaconda3\\lib\\ntpath.py'&gt;,pickle': &lt;module 'pickle' from 'C:\\Anaconda3\\lib\\pickle.py'&gt;, </span><br><span class="line"><span class="meta">#</span><span class="bash">中间略</span></span><br><span class="line">'subprocess':module 'subprocess' from 'C:\\Anaconda3\\lib\\subprocess.py'&gt;, 'sys': &lt;module '</span><br><span class="line">ys' (built-in)&gt;, 'ctypes.util': &lt;module 'ctypes.util' from 'C:\\Anaconda3\\lib\</span><br><span class="line">ctypes\\util.py'&gt;, '_weakref': &lt;module '_weakref' (built-in)&gt;, '_imp': &lt;module</span><br><span class="line">_imp' (built-in)&gt;&#125;</span><br></pre></td></tr></table></figure><blockquote><p>如果不是built-in，value中会有模块的绝对路径信息。</p><p>通过key查找模块位置，如果value为None，就会抛出错误信息：ModuleNotFoundError。</p><p>如果key不存在，就会进入下一步检索。</p></blockquote><p>如果我们导入过包，例如tensorflow。</p><p>注意如果要使用其中模块，需要该模块的全名（即全路径信息），例如：<code>tensorflow.examples.tutorials.mnist.input_data</code> 。因为sys.modules中只有全路径的key。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow</span><br><span class="line">print(sys.modules)</span><br><span class="line"><span class="comment">##这个字典中会有tensorflow所有子包、模块的信息和具体的路径。</span></span><br><span class="line"><span class="comment">#'tensorflow.examples.tutorials.mnist.input_data': &lt;module 'tensorflow.examples.tutorials.mnist.input_data' from '/root/anaconda3/lib/python3.6/site-packages/tensorflow/examples/tutorials/mnist/input_data.py'&gt;</span></span><br></pre></td></tr></table></figure><h5 id="（2）检索-sys-meta-path"><a href="#（2）检索-sys-meta-path" class="headerlink" title="（2）检索 sys.meta_path"></a>（2）检索 <a href="http://docs.python.org/2/library/sys.html#sys.meta_path" target="_blank" rel="noopener">sys.meta_path</a></h5><p>逐个遍历其中的 <a href="http://docs.python.org/2/glossary.html#term-finder" target="_blank" rel="noopener">finder</a> 来查找模块。否则进入下一步检索。</p><h5 id="（3）检索模块所属包目录"><a href="#（3）检索模块所属包目录" class="headerlink" title="（3）检索模块所属包目录"></a>（3）检索模块所属包目录</h5><p>如果模块Module在包（Package）中（如<code>import Package.Module</code>），则以<code>Package.__path__</code>为搜索路径进行查找。</p><p><strong>（4）检索环境变量</strong></p><p>如果模块不在一个包中（如<code>import Module</code>），则以 <a href="http://docs.python.org/2/library/sys.html#sys.path" target="_blank" rel="noopener">sys.path</a> 为搜索路径进行查找。</p><blockquote><p>如果上面检索均为找到，抛出错误信息：ModuleNotFoundError。</p></blockquote><h4 id="第二步：加载（Loader）"><a href="#第二步：加载（Loader）" class="headerlink" title="第二步：加载（Loader）"></a>第二步：加载（Loader）</h4><p>加载完成对模块的初始化处理：</p><ul><li>设置属性。包括<code>__name__</code>、<code>__file__</code>、<code>__package__</code>和<code>__loader__</code> 。</li></ul><ul><li>编译源码。编译生成字节码文件（<code>.pyc</code>文件），如果是包，则是其对应的<code>__init__.py</code>文件编译为字节码（*.pyc）。如果字节码文件已存在且仍然是最新的（时间戳和py文件一致），则不会重新编译。</li></ul><ul><li>加载到内存。模块在第一次被加载时被编译，载入内存，并将信息加入到sys.modules中。</li></ul><blockquote><p>也可以强制用<code>reload()</code>函数重新加载模块（包）。</p></blockquote><h4 id="第三步：名字绑定"><a href="#第三步：名字绑定" class="headerlink" title="第三步：名字绑定"></a>第三步：名字绑定</h4><p>将模块和包的命名空间信息导入到当前执行Python文件的namespace（命名空间）。</p><h3 id="将模块、包的路径加入检索路径"><a href="#将模块、包的路径加入检索路径" class="headerlink" title="将模块、包的路径加入检索路径"></a>将模块、包的路径加入检索路径</h3><p>讲完了枯燥的理论背景，下面我们来介绍实际应用。当你写好一个模块文件，如何正确完成import模块？主要有下面两类方法：</p><h4 id="动态方法（sys-path中添加）"><a href="#动态方法（sys-path中添加）" class="headerlink" title="动态方法（sys.path中添加）"></a>动态方法（sys.path中添加）</h4><p>我们知道检索路径中sys.path，所以可以在import模块之前将模块的绝对路径添加到sys.path中。同样导入包需要加入包的文件夹绝对路径。具体方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="comment">##sys.path.append(dir)</span></span><br><span class="line">sys.path.append(<span class="string">'your\module（package）\file\path'</span>)</span><br><span class="line"><span class="comment">##sys.path.insert(pos,dir)</span></span><br><span class="line">sys.path.insert(<span class="number">0</span>,<span class="string">'your\module（package）\file\path'</span>)</span><br></pre></td></tr></table></figure><blockquote><p>注意：</p><p>1、这里pos参数是插入sys.path这个list数据的位置，pos=0，即list第一位，优先级高。</p><p>2、python程序向sys.path添加的目录只在此程序的生命周期之内有效。程序结束，失效。所以这是一种动态方法。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#win7</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">print(sys.path)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[<span class="string">''</span>, <span class="string">'C:\\Python27\\lib\\site-packages\\pip-8.1.1-py2.7.egg'</span>, <span class="string">'C:\\windows\\syst</span></span><br><span class="line"><span class="string">em32\\python27.zip'</span>, <span class="string">'C:\\Python27\\DLLs'</span>, <span class="string">'C:\\Python27\\lib'</span>, <span class="string">'C:\\Python27\\l</span></span><br><span class="line"><span class="string">ib\\plat-win'</span>, <span class="string">'C:\\Python27\\lib\\lib-tk'</span>, <span class="string">'C:\\Python27'</span>, <span class="string">'C:\\Users\\rongxian</span></span><br><span class="line"><span class="string">g\\AppData\\Roaming\\Python\\Python27\\site-packages'</span>, <span class="string">'C:\\Python27\\lib\\site-</span></span><br><span class="line"><span class="string">packages'</span>]</span><br></pre></td></tr></table></figure><h4 id="静态方法"><a href="#静态方法" class="headerlink" title="静态方法"></a>静态方法</h4><p>（1）另外检索路径还有系统环境变量，所以可以将模块（包）路径添加在系统环境变量中。</p><p>（2）粗暴一点直接将模块（包）拷贝到sys.path的其中一个路径下面。但是这种管理比较乱。</p><p>（3）Python在遍历sys.path的目录过程中，会解析 <code>.pth</code> 文件，将文件中所记录的路径加入到 sys.path ，这样 .pth 文件中的路径也可以找到了。例如我们在<code>C:\Python27\lib\site-packages</code> 中新建一个<code>.pth</code>文件。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># .pth file for the your module or package</span></span><br><span class="line"><span class="string">'your\module（package）\file\path'</span></span><br></pre></td></tr></table></figure><p>这样在模块（包）上线时，我们只需要将模块（包）的目录或者文件绝对路径放在新建的.path文件中即可。</p><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p>【1】<a href="http://www.cnblogs.com/russellluo/p/3328683.html#_3" target="_blank" rel="noopener">http://www.cnblogs.com/russellluo/p/3328683.html#_3</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;包和模块&quot;&gt;&lt;a href=&quot;#包和模块&quot; class=&quot;headerlink&quot; title=&quot;包和模块&quot;&gt;&lt;/a&gt;包和模块&lt;/h3&gt;&lt;p&gt;首先要介绍Python中两个概念：包和模块。简单的理解（从文件系统角度），包（package）是一个文件夹，而模块（modu
      
    
    </summary>
    
      <category term="python" scheme="https://zjrongxiang.github.io/categories/python/"/>
    
    
      <category term="python" scheme="https://zjrongxiang.github.io/tags/python/"/>
    
      <category term="import" scheme="https://zjrongxiang.github.io/tags/import/"/>
    
  </entry>
  
  <entry>
    <title>数据科学实践中常用开放数据集介绍</title>
    <link href="https://zjrongxiang.github.io/2018/04/05/2018-04-01-datasets_example/"/>
    <id>https://zjrongxiang.github.io/2018/04/05/2018-04-01-datasets_example/</id>
    <published>2018-04-05T11:30:00.000Z</published>
    <updated>2018-04-05T15:07:58.609Z</updated>
    
    <content type="html"><![CDATA[<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>数据科学研究的对象是数据，学习过程中需要相关数据集辅助大家练习、做实验。从而体会数据科学中算法方法论。中国古语云：巧妇难有无米之炊，说的就是数据对于数据科学学习的重要性。</p><p>这篇文章收集介绍了各种常用的开放数据集，供大家学习参考。会持续更新。</p><h3 id="开放数据集"><a href="#开放数据集" class="headerlink" title="开放数据集"></a>开放数据集</h3><p>这里主要将开放数据分为三类：图像类、自然语言（NLP）类、音频类。</p><h4 id="图像类"><a href="#图像类" class="headerlink" title="图像类"></a>图像类</h4><h5 id="MNIST手写数据集"><a href="#MNIST手写数据集" class="headerlink" title="MNIST手写数据集"></a>MNIST手写数据集</h5><ul><li><p>介绍：</p><p>MNIST（全称：Modified National Institute of Standards and Technology database）数据集是常见的深度学习开放数据集（基本属于深度学习的hello world数据集）。这是一个手写阿拉伯数据集（0-9数字），数据主要采集于美国高中学生。数据集总量为7W个手写数字图像（训练集6w个、测试机1w个）。</p><p><img src="\images\picture\digits.jpeg" alt=""></p></li></ul><table><thead><tr><th style="text-align:center">文件</th><th style="text-align:center">内容</th></tr></thead><tbody><tr><td style="text-align:center"><a href="http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz" target="_blank" rel="noopener">train-images-idx3-ubyte.gz</a></td><td style="text-align:center">训练集图片 - 60000张训练图片</td></tr><tr><td style="text-align:center"><a href="http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz" target="_blank" rel="noopener">train-labels-idx1-ubyte.gz</a></td><td style="text-align:center">训练集图片对应的数字标签（0-9）</td></tr><tr><td style="text-align:center"><a href="http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz" target="_blank" rel="noopener">t10k-images-idx3-ubyte.gz</a></td><td style="text-align:center">测试集图片 - 10000 张 图片</td></tr><tr><td style="text-align:center"><a href="http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz" target="_blank" rel="noopener">t10k-labels-idx1-ubyte.gz</a></td><td style="text-align:center">测试集图片对应的数字标签</td></tr></tbody></table><ul><li><p>数据存储大小：二进制文件，50M，压缩形式约10M。每张图像被归一化成28*28的像素矩阵。</p></li><li><p>图像数据格式：像素值为0到255. 0表示背景（白色），255表示前景（黑色）。例如下面手写数字1的数据矩阵表示：</p><p><img src="\images\picture\MNIST-Matrix.png" alt="图像数据矩阵"></p></li><li><p>官方网页连接：<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a></p></li><li><p>读取数据案例（Python）：</p><p>Tensorflow中已经有对MNIST数据集解析的脚本，我们可以直接调用：</p></li></ul><table><thead><tr><th style="text-align:center">文件</th><th style="text-align:center">目的</th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/mnist/input_data.py" target="_blank" rel="noopener">input_data.py</a>、<a href="https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/learn/python/learn/datasets/mnist.py" target="_blank" rel="noopener">mnist.py</a></td><td style="text-align:center">用于读取MNIST数据集</td></tr></tbody></table>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#tf为1.7版本</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">data_dir = <span class="string">'/root/tftest/mnistdata/'</span></span><br><span class="line"><span class="comment">#data_dir为数据集文件存放目录</span></span><br><span class="line">mnist = input_data.read_data_sets(data_dir, one_hot=<span class="keyword">True</span>，validation_size=<span class="number">5000</span>)</span><br><span class="line"><span class="comment">#mnist = input_data.read_data_sets(data_dir, one_hot=False)</span></span><br><span class="line"><span class="comment">#one_hot参数True，表示标签进行one-hot编码处理。</span></span><br><span class="line"><span class="comment">#validation_size参数可以从训练集中划出一部分数据作为验证集。默认是5w个，可以自己调节。</span></span><br><span class="line"></span><br><span class="line">x_train,y_train,x_test,y_test,x_vali,y_vali = \</span><br><span class="line">mnist.train.images,mnist.train.labels,mnist.test.images,mnist.test.labels,\</span><br><span class="line">mnist.validation.images,mnist.validation.labels</span><br><span class="line"><span class="comment">#x_train的数据类型为：&lt;class 'numpy.ndarray'&gt;</span></span><br></pre></td></tr></table></figure><p>​       上面的例子划分好数据就可以喂给各种算法模型进行训练。</p><ul><li><p>扩展：EMNIST数据集：<a href="https://arxiv.org/abs/1702.05373。" target="_blank" rel="noopener">https://arxiv.org/abs/1702.05373。</a></p><p>按照MNIST规范，数据集更大：包含240,000个训练图像和40,000个手写数字测试图像。</p></li></ul><h5 id="MS-COCO图像分割数据集"><a href="#MS-COCO图像分割数据集" class="headerlink" title="MS-COCO图像分割数据集"></a>MS-COCO图像分割数据集</h5><ul><li><p>介绍：</p><p>MS-COCO（全称是Common Objects in Context）是微软团队提供的一个可以用来进行图像识别的数据集。数据集中的图像分为训练、验证和测试集。COCO数据集现在有3种标注类型：<strong>object instances（目标实例）, object keypoints（目标上的关键点）, 和image captions（看图说话）</strong>，使用JSON文件存储。</p><p>一共有33w张图像，80个对象类别，每幅图5个字母、25w个关键点。</p></li><li><p>数据存储大小：约25G（压缩形式）</p></li><li><p>数据格式：中文介绍可以参考知乎这篇文章：<a href="https://zhuanlan.zhihu.com/p/29393415" target="_blank" rel="noopener">COCO数据集的标注格式</a> 。</p></li><li><p>官方网站：<a href="http://mscoco.org/" target="_blank" rel="noopener">http://mscoco.org/</a></p></li></ul><h5 id="ImageNet图像数据集"><a href="#ImageNet图像数据集" class="headerlink" title="ImageNet图像数据集"></a>ImageNet图像数据集</h5><ul><li><p>介绍：</p><p>Imagenet是深度学习中大名鼎鼎的数据集。数据集有1400多万幅图片，涵盖2万多个类别；其中有超过百万的图片有明确的类别标注和图像中物体位置的标注。深度学习中关于图像分类、定位、检测等研究工作大多基于此数据集展开。Imagenet数据集文档详细，有专门的团队维护，使用非常方便，在计算机视觉领域研究论文中应用非常广，几乎成为了目前深度学习图像领域算法性能检验的“标准”数据集。</p></li><li><p>数据存储大小：约150G</p></li><li><p>官方网站：<a href="http://www.image-net.org/" target="_blank" rel="noopener">http://www.image-net.org/</a></p></li></ul><p>Open Image图像数据集</p><ul><li><p>介绍：</p><p>Open Image为Google提供。数据集包含近900万个图像URL。这些图像已经用数千个类的图像级标签边框进行了注释。该数据集包含9,011,219张图像的训练集，41,260张图像的验证集以及125,436张图像的测试集。</p></li><li><p>数据大小：500G</p></li><li><p>官方网站：<a href="https://github.com/openimages/dataset" target="_blank" rel="noopener">https://github.com/openimages/dataset</a></p></li></ul><h5 id="VisualQA图像数据库"><a href="#VisualQA图像数据库" class="headerlink" title="VisualQA图像数据库"></a>VisualQA图像数据库</h5><ul><li><p>介绍：</p><p>VQA是一个包含有关图像的开放式问题的数据集。这些问题需要理解视野和语言。数据集有265,016张图片。</p></li><li><p>数据大小：25G</p></li><li><p>官方网站：<a href="http://www.visualqa.org/" target="_blank" rel="noopener">http://www.visualqa.org/</a></p></li></ul><h5 id="The-Street-View-House-Numbers-SVHN-Dataset街边号码牌数据集"><a href="#The-Street-View-House-Numbers-SVHN-Dataset街边号码牌数据集" class="headerlink" title="The Street View House Numbers (SVHN) Dataset街边号码牌数据集"></a>The Street View House Numbers (SVHN) Dataset街边号码牌数据集</h5><ul><li><p>介绍：</p><p>SVHN图像数据集用于开发机器学习和对象识别算法，对数据预处理和格式化的要求最低。它可以被看作与<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST</a>相似，但是将更多标记数据（超过600,000个数字图像）并入一个数量级并且来自显着更难以解决的真实世界问题（识别自然场景图像中的数字和数字）。SVHN数据从谷歌街景图片中的房屋号码中获得的。书记含有用于训练的73257个数字，用于测试的26032个数字以及用作额外训练数据的531131个附加数字。</p></li><li><p>数据集大小： [train.tar.gz]， [test.tar.gz]， [extra.tar.gz ] 共三个文件。</p></li><li><p>官方网站：<a href="http://ufldl.stanford.edu/housenumbers/" target="_blank" rel="noopener">http://ufldl.stanford.edu/housenumbers/</a></p></li></ul><h5 id="CIFAR-10图像数据集"><a href="#CIFAR-10图像数据集" class="headerlink" title="CIFAR-10图像数据集"></a>CIFAR-10图像数据集</h5><ul><li><p>介绍：</p><p>CIFAR-10数据集由10个类的60,000个图像组成（每个类在上图中表示为一行）。总共有50,000个训练图像和10,000个测试图像。数据集分为6个部分 - 5个培训批次和1个测试批次。每批有10,000个图像。</p></li><li><p>数据大小：170M</p></li><li><p>官方网站：<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">http://www.cs.toronto.edu/~kriz/cifar.html</a></p></li></ul><h5 id="Fashion-MNIST"><a href="#Fashion-MNIST" class="headerlink" title="Fashion-MNIST"></a>Fashion-MNIST</h5><ul><li><p>介绍</p><p>Fashion-MNIST包含60,000个训练图像和10,000个测试图像。它是一个类似MNIST的时尚产品数据库。开发人员认为MNIST已被过度使用，因此他们将其作为该数据集的直接替代品。每张图片都以灰度显示，并与10个类别的标签相关联。</p></li><li><p>数据集大小：30M</p></li><li><p>官方网站：<a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" rel="noopener">https://github.com/zalandoresearch/fashion-mnist</a></p></li></ul><h4 id="自然语言类数据库"><a href="#自然语言类数据库" class="headerlink" title="自然语言类数据库"></a>自然语言类数据库</h4><h5 id="IMDB电影评论数据集"><a href="#IMDB电影评论数据集" class="headerlink" title="IMDB电影评论数据集"></a>IMDB电影评论数据集</h5><ul><li><p>介绍：</p><p>这是电影爱好者的梦幻数据集。它具有比此领域以前的任何数据集更多的数据。除了训练和测试评估示例之外，还有更多未标记的数据供您使用。原始文本和预处理的单词格式包也包括在内。</p></li><li><p>数据集大小：80 M</p></li><li><p>官方网站：<a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="noopener">http://ai.stanford.edu/~amaas/data/sentiment/</a></p></li><li><p>模型案例：<a href="https://arxiv.org/abs/1705.09207" target="_blank" rel="noopener">https://arxiv.org/abs/1705.09207</a></p></li></ul><h5 id="Twenty-Newsgroups-Data-Set"><a href="#Twenty-Newsgroups-Data-Set" class="headerlink" title="Twenty Newsgroups Data Set"></a>Twenty Newsgroups Data Set</h5><ul><li><p>介绍：</p><p>该数据集包含有关新闻组的信息。为了管理这个数据集，从20个不同的新闻组中获取了1000篇Usenet文章。这些文章具有典型特征，如主题行，签名和引号。</p></li><li><p>数据集大小：20 M</p></li><li><p>官方网站：<a href="https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups</a></p></li><li><p>模型案例：<a href="https://arxiv.org/abs/1606.01781" target="_blank" rel="noopener">https://arxiv.org/abs/1606.01781</a></p></li></ul><h5 id="Sentiment140情感分析数据集"><a href="#Sentiment140情感分析数据集" class="headerlink" title="Sentiment140情感分析数据集"></a>Sentiment140情感分析数据集</h5><ul><li><p>介绍：</p><p>Sentiment140是一个可用于情感分析的数据集。</p></li><li><p>数据集大小：80 M</p></li><li><p>官方网站：<a href="http://help.sentiment140.com/for-students/" target="_blank" rel="noopener">http://help.sentiment140.com/for-students/</a></p></li><li><p>模型案例：<a href="http://www.aclweb.org/anthology/W17-5202" target="_blank" rel="noopener">http://www.aclweb.org/anthology/W17-5202</a></p></li></ul><h5 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a>WordNet</h5><ul><li><p>介绍：</p><p>WordNet是英语synsets的大型数据库。Synsets是同义词组，每个描述不同的概念。WordNet的结构使其成为NLP非常有用的工具。</p></li><li><p>数据集大小：10 M</p></li><li><p>官方网站：<a href="https://wordnet.princeton.edu/" target="_blank" rel="noopener">https://wordnet.princeton.edu/</a></p></li><li><p>模型案例：<a href="https://aclanthology.info/pdf/R/R11/R11-1097.pdf" target="_blank" rel="noopener">https://aclanthology.info/pdf/R/R11/R11-1097.pdf</a></p></li></ul><h5 id="Yelp评论"><a href="#Yelp评论" class="headerlink" title="Yelp评论"></a>Yelp评论</h5><ul><li><p>介绍：</p><p>这是Yelp为了学习目的而发布的一个开放数据集。它由数百万用户评论，商业属性和来自多个大都市地区的超过20万张照片组成。这是一个非常常用的全球NLP挑战数据集。</p></li><li><p>数据集大小：2.66 GB JSON，2.9 GB SQL和7.5 GB照片（全部压缩）</p></li><li><p>官方网站：<a href="https://www.yelp.com/dataset" target="_blank" rel="noopener">https://www.yelp.com/dataset</a></p></li><li><p>模型案例：<a href="https://arxiv.org/pdf/1710.00519.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1710.00519.pdf</a></p></li></ul><h5 id="维基百科语料库"><a href="#维基百科语料库" class="headerlink" title="维基百科语料库"></a>维基百科语料库</h5><ul><li><p>介绍：</p><p>该数据集是维基百科全文的集合。它包含来自400多万篇文章的将近19亿字。什么使得这个强大的NLP数据集是你可以通过单词，短语或段落本身的一部分进行搜索。</p></li><li><p>数据集大小： 20 MB</p></li><li><p>官方网站：<a href="https://corpus.byu.edu/wiki/" target="_blank" rel="noopener">https://corpus.byu.edu/wiki/</a></p></li><li><p>模型案例：<a href="https://arxiv.org/pdf/1711.03953.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1711.03953.pdf</a></p></li></ul><h5 id="博客作者身份语料库"><a href="#博客作者身份语料库" class="headerlink" title="博客作者身份语料库"></a>博客作者身份语料库</h5><ul><li><p>介绍：</p><p>此数据集包含从数千名博主收集的博客帖子，从blogger.com收集。每个博客都作为一个单独的文件提供。每个博客至少包含200次常用英语单词。</p></li><li><p>数据集大小： 300 MB</p></li><li><p>官方网站：<a href="http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm" target="_blank" rel="noopener">http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm</a></p></li><li><p>模型案例：<a href="https://arxiv.org/pdf/1609.06686.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.06686.pdf</a></p></li></ul><h5 id="欧洲语言的机器翻译"><a href="#欧洲语言的机器翻译" class="headerlink" title="欧洲语言的机器翻译"></a>欧洲语言的机器翻译</h5><ul><li><p>介绍：</p><p>数据集包含四种欧洲语言。</p></li><li><p>数据集大小： 约15 G</p></li><li><p>官方网站：<a href="http://statmt.org/wmt11/translation-task.html" target="_blank" rel="noopener">http://statmt.org/wmt11/translation-task.html</a></p></li><li><p>模型案例：<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">https://arxiv.org/abs/1706.03762</a></p></li></ul><h4 id="音频-语音数据集"><a href="#音频-语音数据集" class="headerlink" title="音频/语音数据集"></a>音频/语音数据集</h4><h5 id="口语数字数据集"><a href="#口语数字数据集" class="headerlink" title="口语数字数据集"></a>口语数字数据集</h5><ul><li><p>介绍：</p><p>为了解决识别音频样本中的口头数字的任务而创建。这是一个开放的数据集，所以希望随着人们继续贡献更多样本，它会不断增长。</p></li><li><p>数据集大小： 约10 G=M</p></li><li><p>记录数量：1500个音频样本</p></li><li><p>官方网站：<a href="https://github.com/Jakobovski/free-spoken-digit-dataset" target="_blank" rel="noopener">https://github.com/Jakobovski/free-spoken-digit-dataset</a></p></li><li><p>模型案例：<a href="https://arxiv.org/pdf/1712.00866" target="_blank" rel="noopener">https://arxiv.org/pdf/1712.00866</a></p></li></ul><h5 id="免费音乐档案（FMA）"><a href="#免费音乐档案（FMA）" class="headerlink" title="免费音乐档案（FMA）"></a>免费音乐档案（FMA）</h5><ul><li><p>介绍：</p><p>FMA是音乐分析的数据集。数据集由全长和HQ音频，预先计算的特征以及音轨和用户级元数据组成。它是一个开放数据集，用于评估MIR中的几个任务。以下是数据集连同其包含的csv文件列表：</p><ul><li><code>tracks.csv</code>：所有106,574首曲目的每首曲目元数据，如ID，标题，艺术家，流派，标签和播放次数。</li><li><code>genres.csv</code>：所有163种风格的ID与他们的名字和父母（用于推断流派层次和顶级流派）。</li><li><code>features.csv</code>：用<a href="https://librosa.github.io/librosa/" target="_blank" rel="noopener">librosa</a>提取的共同特征  。</li><li><code>echonest.csv</code>：由<a href="http://the.echonest.com/" target="_blank" rel="noopener">Echonest</a>  （现在的  <a href="https://www.spotify.com/" target="_blank" rel="noopener">Spotify</a>）为13,129首音轨的子集提供的音频功能  。</li></ul></li><li><p>数据集大小： 约1T</p></li><li><p>记录数量：1500个音频样本</p></li><li><p>官方网站：<a href="https://github.com/mdeff/fma" target="_blank" rel="noopener">https://github.com/mdeff/fma</a></p></li><li><p>模型案例：<a href="https://arxiv.org/pdf/1803.05337.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1803.05337.pdf</a></p></li></ul><h5 id="舞厅"><a href="#舞厅" class="headerlink" title="舞厅"></a>舞厅</h5><ul><li><p>介绍：</p><p>该数据集包含舞厅跳舞音频文件。以真实音频格式提供了许多舞蹈风格的一些特征摘录。 以下是数据集的一些特征：</p></li><li><p>数据集大小： 约14 G</p></li><li><p>记录数量：约700个音频样本</p></li><li><p>官方网站：<a href="http://mtg.upf.edu/ismir2004/contest/tempoContest/node5.html" target="_blank" rel="noopener">http://mtg.upf.edu/ismir2004/contest/tempoContest/node5.html</a></p></li><li><p>模型案例：<a href="https://pdfs.semanticscholar.org/0cc2/952bf70c84e0199fcf8e58a8680a7903521e.pdf" target="_blank" rel="noopener">https://pdfs.semanticscholar.org/0cc2/952bf70c84e0199fcf8e58a8680a7903521e.pdf</a></p></li></ul><h5 id="百万歌曲数据集"><a href="#百万歌曲数据集" class="headerlink" title="百万歌曲数据集"></a>百万歌曲数据集</h5><ul><li><p>介绍：</p><p>百万歌曲数据集是音频功能和元数据的一百万当代流行音乐曲目可自由可用的集合。 其目的是：</p><ul><li>鼓励对扩大到商业规模的算法进行研究</li><li>为评估研究提供参考数据集</li><li>作为使用API创建大型数据集的捷径（例如Echo Nest的）</li><li>帮助新研究人员在MIR领域开始工作</li></ul><p>数据集的核心是一百万首歌曲的特征分析和元数据。该数据集不包含任何音频，只包含派生的功能。示例音频可以通过使用哥伦比亚大学提供的<a href="https://github.com/tb2332/MSongsDB/tree/master/Tasks_Demos/Preview7digital" target="_blank" rel="noopener">代码</a>从<a href="http://www.7digital.com/" target="_blank" rel="noopener">7digital等</a>服务中获取。</p></li><li><p>数据集大小： 约280 G</p></li><li><p>记录数量：它的一百万首歌曲！</p></li><li><p>官方网站：<a href="https://labrosa.ee.columbia.edu/millionsong/" target="_blank" rel="noopener">https://labrosa.ee.columbia.edu/millionsong/</a></p></li><li><p>模型案例：<a href="http://www.ke.tu-darmstadt.de/events/PL-12/papers/08-aiolli.pdf" target="_blank" rel="noopener">http://www.ke.tu-darmstadt.de/events/PL-12/papers/08-aiolli.pdf</a></p></li></ul><h5 id="LibriSpeech"><a href="#LibriSpeech" class="headerlink" title="LibriSpeech"></a>LibriSpeech</h5><ul><li><p>介绍：</p><p>该数据集是大约1000小时的英语语音的大型语料库。这些数据来自LibriVox项目的有声读物。它已被分割并正确对齐。如果您正在寻找一个起点，请查看已准备好的声学模型，这些模型在<a href="http://www.kaldi-asr.org/downloads/build/6/trunk/egs/" target="_blank" rel="noopener">kaldi-asr.org</a>和语言模型上进行了训练，适合评估，<a href="http://www.openslr.org/11/" target="_blank" rel="noopener">网址</a>为<a href="http://www.openslr.org/11/" target="_blank" rel="noopener">http://www.openslr.org/11/</a>。</p></li><li><p>数据集大小： 约60 G</p></li><li><p>记录数量：1000小时的演讲</p></li><li><p>官方网站：<a href="http://www.openslr.org/12/" target="_blank" rel="noopener">http://www.openslr.org/12/</a></p></li><li><p>模型案例：<a href="https://arxiv.org/abs/1712.09444" target="_blank" rel="noopener">https://arxiv.org/abs/1712.09444</a></p></li></ul><h5 id="VoxCeleb"><a href="#VoxCeleb" class="headerlink" title="VoxCeleb"></a>VoxCeleb</h5><ul><li><p>介绍：</p><p>VoxCeleb是一个大型的说话人识别数据集。它包含约1,200名来自YouTube视频的约10万个话语。数据大部分是性别平衡的（男性占55％）。名人跨越不同的口音，职业和年龄。开发和测试集之间没有重叠。对于隔离和识别哪个超级巨星来说，这是一个有趣的用例。</p></li><li><p>数据集大小： 约150 M</p></li><li><p>记录数量： 1,251位名人的100,000条话语</p></li><li><p>官方网站：<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/" target="_blank" rel="noopener">http://www.robots.ox.ac.uk/~vgg/data/voxceleb/</a></p></li><li><p>模型案例：<a href="https://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf" target="_blank" rel="noopener">https://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf</a></p></li></ul><h4 id="比赛数据"><a href="#比赛数据" class="headerlink" title="比赛数据"></a>比赛数据</h4><h5 id="Twitter情绪分析数据"><a href="#Twitter情绪分析数据" class="headerlink" title="Twitter情绪分析数据"></a>Twitter情绪分析数据</h5><ul><li><p>介绍：</p><p>仇恨以种族主义和性别歧视为形式的言论已成为叽叽喳喳的麻烦，重要的是将这类推文与其他人分开。在这个实践问题中，我们提供既有正常又有仇恨推文的Twitter数据。您作为数据科学家的任务是确定推文是仇恨推文，哪些不是。</p></li><li><p>数据集大小： 约3 M</p></li><li><p>记录数量： 31,962条推文</p></li><li><p>官方网站：<a href="https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/" target="_blank" rel="noopener">https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/</a></p></li></ul><h5 id="印度演员的年龄检测"><a href="#印度演员的年龄检测" class="headerlink" title="印度演员的年龄检测"></a>印度演员的年龄检测</h5><ul><li><p>介绍：</p><p>对于任何深度学习爱好者来说，这是一个令人着迷的挑战。该数据集包含数千个印度演员的图像，你的任务是确定他们的年龄。所有图像都是手动选择的，并从视频帧中剪切，导致尺度，姿势，表情，照度，年龄，分辨率，遮挡和化妆的高度可变性。</p></li><li><p>数据集大小： 约48 M</p></li><li><p>记录数量： 训练集中的19,906幅图像和测试集中的6636幅图像</p></li><li><p>官方网站：<a href="https://datahack.analyticsvidhya.com/contest/practice-problem-age-detection/" target="_blank" rel="noopener">https://datahack.analyticsvidhya.com/contest/practice-problem-age-detection/</a></p></li></ul><h5 id="城市声音分类"><a href="#城市声音分类" class="headerlink" title="城市声音分类"></a>城市声音分类</h5><ul><li><p>介绍：</p><p>这个数据集包含超过8000个来自10个班级的城市声音摘录。这个实践问题旨在向您介绍常见分类方案中的音频处理。</p></li><li><p>数据集大小： 训练集 - 3 GB（压缩），测试集 - 2 GB（压缩）</p></li><li><p>记录数量： 来自10个班级的8732个城市声音标注的声音片段（&lt;= 4s）</p></li><li><p>官方网站：<a href="https://datahack.analyticsvidhya.com/contest/practice-problem-urban-sound-classification/" target="_blank" rel="noopener">https://datahack.analyticsvidhya.com/contest/practice-problem-urban-sound-classification/</a></p></li></ul><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p>【1】   <a href="https://www.tensorflow.org/versions/r1.1/get_started/mnist/beginners" target="_blank" rel="noopener">https://www.tensorflow.org/versions/r1.1/get_started/mnist/beginners</a></p><p>【2】  <a href="https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/</a></p><p>【3】  <a href="https://deeplearning4j.org/cn/opendata" target="_blank" rel="noopener">https://deeplearning4j.org/cn/opendata</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;数据科学研究的对象是数据，学习过程中需要相关数据集辅助大家练习、做实验。从而体会数据科学中算法方法论。中国古语云：巧妇难有无米之炊，说的就是
      
    
    </summary>
    
      <category term="datasets" scheme="https://zjrongxiang.github.io/categories/datasets/"/>
    
    
      <category term="datasets" scheme="https://zjrongxiang.github.io/tags/datasets/"/>
    
  </entry>
  
  <entry>
    <title>Hexo:解决Typora编辑table无法被解析问题</title>
    <link href="https://zjrongxiang.github.io/2018/04/01/2018-04-05-Hexo_table/"/>
    <id>https://zjrongxiang.github.io/2018/04/01/2018-04-05-Hexo_table/</id>
    <published>2018-04-01T15:30:00.000Z</published>
    <updated>2018-04-05T15:07:08.870Z</updated>
    
    <content type="html"><![CDATA[<h3 id="掉坑背景"><a href="#掉坑背景" class="headerlink" title="掉坑背景"></a>掉坑背景</h3><p>使用Typora编辑Makedown文件，添加表格，但是提交给Hexo渲染网页，无法正常解析显示，而是显示源码。例如：<br>| Table Header 1 | Table Header 2 |<br>| ————– | ————– |<br>| Division 1     | Division 2     |<br>| Division 1     | Division 2     |</p><h3 id="爬坑过程和解决办法"><a href="#爬坑过程和解决办法" class="headerlink" title="爬坑过程和解决办法"></a>爬坑过程和解决办法</h3><p>一开始认为是Hexo的bug，Google也没人遇到类似情况，都准备在github上建问题单了。最后本着严谨的态度，以文本的格式打开文档，发现表格源码和正文之间没有空行！！！！！</p><p>这尼玛坑爹呀，所以Hexo无法解析，但是Typora能正常解析。空出一行后正常解析：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">正文</span>&gt;</span></span></span><br><span class="line">(空一行)</span><br><span class="line">| Table Header 1 | Table Header 2 |</span><br><span class="line">| - | - | </span><br><span class="line">| Division 1 | Division 2 | </span><br><span class="line">| Division 1 | Division 2 |</span><br></pre></td></tr></table></figure><table><thead><tr><th>Table Header 1</th><th>Table Header 2</th></tr></thead><tbody><tr><td>Division 1</td><td>Division 2</td></tr><tr><td>Division 1</td><td>Division 2</td></tr></tbody></table><p>这一点Typora做的不够兼容（只怪他太过于强大的解析能力。。。。）。Tyopra不服了，我强大也有错？？哈哈哈</p><p>记录该坑供掉坑小伙伴参考。</p><h3 id="提示"><a href="#提示" class="headerlink" title="提示"></a>提示</h3><p>如果掉坑小伙伴，上面办法没解决。用文本方式打开文件，逐个排查原因。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;掉坑背景&quot;&gt;&lt;a href=&quot;#掉坑背景&quot; class=&quot;headerlink&quot; title=&quot;掉坑背景&quot;&gt;&lt;/a&gt;掉坑背景&lt;/h3&gt;&lt;p&gt;使用Typora编辑Makedown文件，添加表格，但是提交给Hexo渲染网页，无法正常解析显示，而是显示源码。例如：&lt;br
      
    
    </summary>
    
      <category term="hexo" scheme="https://zjrongxiang.github.io/categories/hexo/"/>
    
    
      <category term="hexo" scheme="https://zjrongxiang.github.io/tags/hexo/"/>
    
      <category term="Typora" scheme="https://zjrongxiang.github.io/tags/Typora/"/>
    
      <category term="table" scheme="https://zjrongxiang.github.io/tags/table/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu挂载新的硬盘（2T以上）</title>
    <link href="https://zjrongxiang.github.io/2018/04/01/2018-04-01-ubuntu_add_disk/"/>
    <id>https://zjrongxiang.github.io/2018/04/01/2018-04-01-ubuntu_add_disk/</id>
    <published>2018-04-01T15:30:00.000Z</published>
    <updated>2018-04-04T14:21:39.551Z</updated>
    
    <content type="html"><![CDATA[<h3 id="系统环境："><a href="#系统环境：" class="headerlink" title="系统环境："></a>系统环境：</h3><p>Linux version 4.13.0-37-generic (Ubuntu 5.4.0-6ubuntu1~16.04.9)</p><p>root用户登入操作</p><h3 id="查看硬盘信息"><a href="#查看硬盘信息" class="headerlink" title="查看硬盘信息"></a>查看硬盘信息</h3><p>机器断电时，接入硬盘。开机后用下面的命令查看硬盘状况（<font color="red">非root用户需sudo</font>）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:~# fdisk -l</span><br><span class="line">Disk /dev/sda: 465.8 GiB, 500107862016 bytes, 976773168 sectors</span><br><span class="line">Units: sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disklabel type: gpt</span><br><span class="line">Disk identifier: CC8004FC-D422-48FA-8ACF-54C3F48E860B</span><br><span class="line"></span><br><span class="line">Device         Start       End   Sectors   Size Type</span><br><span class="line">/dev/sda1       2048   1050623   1048576   512M EFI System</span><br><span class="line">/dev/sda2    1050624 909946879 908896256 433.4G Linux filesystem</span><br><span class="line">/dev/sda3  909946880 976771071  66824192  31.9G Linux swap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Disk /dev/sdb: 3.7 TiB, 4000787030016 bytes, 7814037168 sectors</span><br><span class="line">Units: sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 4096 bytes</span><br><span class="line">I/O size (minimum/optimal): 4096 bytes / 4096 bytes</span><br></pre></td></tr></table></figure><p>查看到系统由两块硬盘：/dev/sda和/dev/sdb，如果还有其他硬盘会继续sdc、sdd编号。</p><p>正在使用的系统盘sda已经有三个分区（sda1、sda2、sda3），新挂载的硬盘sdb位分区。</p><h3 id="新挂载硬盘分区"><a href="#新挂载硬盘分区" class="headerlink" title="新挂载硬盘分区"></a>新挂载硬盘分区</h3><p>新硬盘存储空间一共4T，我们对硬盘进行分区。划分为两个分区：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:~# fdisk /dev/sdb</span><br><span class="line"></span><br><span class="line">Welcome to fdisk (util-linux 2.27.1).</span><br><span class="line">Changes will remain in memory only, until you decide to write them.</span><br><span class="line">Be careful before using the write command.</span><br><span class="line"></span><br><span class="line">/dev/sdb: device contains a valid 'ext4' signature; it is strongly recommended to wipe the device with wipefs(8) if this is sible collisions</span><br><span class="line"></span><br><span class="line">Device does not contain a recognized partition table.</span><br><span class="line">The size of this disk is 3.7 TiB (4000787030016 bytes). DOS partition table format can not be used on drives for volumes lar512-byte sectors. Use GUID partition table format (GPT).</span><br><span class="line"></span><br><span class="line">Created a new DOS disklabel with disk identifier 0x6b028a17.</span><br><span class="line"></span><br><span class="line">Command (m for help):</span><br></pre></td></tr></table></figure><p>注意这里已经有警告：<em>The size of this disk is 3.7 TiB (4000787030016 bytes). DOS partition table format can not be used on drives for volumes lar512-byte sectors. Use GUID partition table format (GPT)</em></p><p>这里情况特殊，新加入的磁盘为4T。fdisk命令对于大于2T的分区无法划分。如果继续使用fdisk工具，最多只能分出2T的分区，剩下的空间无法利用。这不坑爹嘛。提示我们使用parted命令。</p><h3 id="使用parted分区"><a href="#使用parted分区" class="headerlink" title="使用parted分区"></a>使用parted分区</h3><p>parted命令可以划分单个分区大于2T的GPT格式的分区。</p><p>更改分区表类型：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:~# parted -s /dev/sdb mklabel gpt</span><br></pre></td></tr></table></figure><p>使用parted进行分区：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:~# parted /dev/sdb</span><br><span class="line">GNU Parted 3.2</span><br><span class="line">Using /dev/sdb</span><br><span class="line">Welcome to GNU Parted! Type 'help' to view a list of commands.</span><br><span class="line">(parted) print                                                            </span><br><span class="line">Model: ATA WDC WD40EFRX-68N (scsi)</span><br><span class="line">Disk /dev/sdb: 4001GB</span><br><span class="line">Sector size (logical/physical): 512B/4096B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br><span class="line"></span><br><span class="line">(parted) mklabel gpt                                                      </span><br><span class="line">Warning: The existing disk label on /dev/sdb will be destroyed and all data on this disk will be lost. Do you want to continue?</span><br><span class="line">Yes/No? yes                                                               </span><br><span class="line">(parted) mkpart                                                           </span><br><span class="line">Partition name?  []?                                                      </span><br><span class="line">File system type?  [ext2]? ext4                                           </span><br><span class="line">Start? 0%                                                                 </span><br><span class="line">End? 100%                                                                 </span><br><span class="line">(parted) print                                                            </span><br><span class="line">Model: ATA WDC WD40EFRX-68N (scsi)</span><br><span class="line">Disk /dev/sdb: 4001GB</span><br><span class="line">Sector size (logical/physical): 512B/4096B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start   End     Size    File system  Name  Flags</span><br><span class="line"> 1      1049kB  4001GB  4001GB  ext4</span><br><span class="line"></span><br><span class="line">(parted) quit                                                             </span><br><span class="line">Information: You may need to update /etc/fstab.</span><br></pre></td></tr></table></figure><p>最后我们验证一下，sdb1分区成功，提示我们要更新系统文件：/etc/fstab。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:~# ls /dev/sd*                                          </span><br><span class="line">/dev/sda  /dev/sda1  /dev/sda2  /dev/sda3  /dev/sdb  /dev/sdb1</span><br><span class="line">root@deeplearning:~# fdisk -l</span><br><span class="line">Disk /dev/sda: 465.8 GiB, 500107862016 bytes, 976773168 sectors</span><br><span class="line">Units: sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disklabel type: gpt</span><br><span class="line">Disk identifier: CC8004FC-D422-48FA-8ACF-54C3F48E860B</span><br><span class="line"></span><br><span class="line">Device         Start       End   Sectors   Size Type</span><br><span class="line">/dev/sda1       2048   1050623   1048576   512M EFI System</span><br><span class="line">/dev/sda2    1050624 909946879 908896256 433.4G Linux filesystem</span><br><span class="line">/dev/sda3  909946880 976771071  66824192  31.9G Linux swap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Disk /dev/sdb: 3.7 TiB, 4000787030016 bytes, 7814037168 sectors</span><br><span class="line">Units: sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 4096 bytes</span><br><span class="line">I/O size (minimum/optimal): 4096 bytes / 4096 bytes</span><br><span class="line">Disklabel type: gpt</span><br><span class="line">Disk identifier: 0D8B0FBC-83F6-4D77-ABDB-98875EC511E4</span><br><span class="line"></span><br><span class="line">Device     Start        End    Sectors  Size Type</span><br><span class="line">/dev/sdb1   2048 7814035455 7814033408  3.7T Linux filesystem</span><br></pre></td></tr></table></figure><h3 id="格式化新建分区"><a href="#格式化新建分区" class="headerlink" title="格式化新建分区"></a>格式化新建分区</h3><p>将分区格式化为ext4格式的文件系统。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:~# mkfs.ext4 /dev/sdb1</span><br><span class="line">mke2fs 1.42.13 (17-May-2015)</span><br><span class="line">Creating filesystem with 976754176 4k blocks and 244195328 inodes</span><br><span class="line">Filesystem UUID: dfcd419f-38a5-4a5c-9b93-9f236d2c2444</span><br><span class="line">Superblock backups stored on blocks: </span><br><span class="line">32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, </span><br><span class="line">4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, </span><br><span class="line">102400000, 214990848, 512000000, 550731776, 644972544</span><br><span class="line"></span><br><span class="line">Allocating group tables: done                            </span><br><span class="line">Writing inode tables: done                            </span><br><span class="line">Creating journal (32768 blocks): done</span><br><span class="line">Writing superblocks and filesystem accounting information:            </span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>如果有多个分区需要依次执行格式化。</p><h3 id="挂载分区"><a href="#挂载分区" class="headerlink" title="挂载分区"></a>挂载分区</h3><p>新建硬盘即将挂载的目录，然后将硬盘挂载到该目录下。并验证挂载成功，检查硬盘空间。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/# mkdir /data</span><br><span class="line">root@deeplearning:/# mount /dev/sdb1 /data</span><br><span class="line">root@deeplearning:/# df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">udev             16G     0   16G   0% /dev</span><br><span class="line">tmpfs           3.2G  9.3M  3.2G   1% /run</span><br><span class="line">/dev/sda2       427G   21G  385G   5% /</span><br><span class="line">tmpfs            16G     0   16G   0% /dev/shm</span><br><span class="line">tmpfs           5.0M  4.0K  5.0M   1% /run/lock</span><br><span class="line">tmpfs            16G     0   16G   0% /sys/fs/cgroup</span><br><span class="line">/dev/sda1       511M  3.5M  508M   1% /boot/efi</span><br><span class="line">tmpfs           3.2G   12K  3.2G   1% /run/user/1000</span><br><span class="line">/dev/sdb1       3.6T   68M  3.4T   1% /data</span><br></pre></td></tr></table></figure><p>上面我们把新的硬盘挂载到了/data目录，硬盘空间大小正常。</p><h3 id="配置开机自动挂载分区"><a href="#配置开机自动挂载分区" class="headerlink" title="配置开机自动挂载分区"></a>配置开机自动挂载分区</h3><h4 id="查看分区的UUID"><a href="#查看分区的UUID" class="headerlink" title="查看分区的UUID"></a>查看分区的UUID</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@deeplearning:/# blkid</span><br><span class="line"><span class="meta">#</span><span class="bash">（略）...</span></span><br><span class="line">/dev/sdb1: UUID="dfcd419f-38a5-4a5c-9b93-9f236d2c2444" TYPE="ext4" PARTUUID="fe373bd5-5b19-4ed0-8713-716455a8ebb4"</span><br></pre></td></tr></table></figure><h4 id="配置-etc-fstab"><a href="#配置-etc-fstab" class="headerlink" title="配置/etc/fstab"></a>配置/etc/fstab</h4><p>将分区信息写到/etc/fstab文件中让它永久挂载:</p><p>将下面的配置信息加入配置文件尾部：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UUID=dfcd419f-38a5-4a5c-9b93-9f236d2c2444 /data ext4 defaults 0 1</span><br></pre></td></tr></table></figure><h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><h4 id="etc-fstab配置说明"><a href="#etc-fstab配置说明" class="headerlink" title="/etc/fstab配置说明"></a>/etc/fstab配置说明</h4><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Use 'blkid' to print the universally unique identifier for a</span><br><span class="line"># device; this may be used with UUID= as a more robust way to name devices</span><br><span class="line"># that works even if disks are added and removed. See fstab(<span class="number">5</span>).</span><br><span class="line"></span><br><span class="line">&lt;file system&gt; &lt;mount point&gt;   &lt;type&gt;  &lt;options&gt;       &lt;dump&gt;      &lt;pass&gt;</span><br><span class="line">    <span class="number">1</span>               <span class="number">2</span>            <span class="number">3</span>         <span class="number">4</span>             <span class="number">5</span>            <span class="number">6</span></span><br><span class="line">对应参数说明：</span><br><span class="line"><span class="number">1</span>、指代文件系统的设备名。最初，该字段只包含待挂载分区的设备名（如/dev/sda1）。现在，除设备名外，还可以包含LABEL或UUID</span><br><span class="line"><span class="number">2</span>、文件系统挂载点。文件系统包含挂载点下整个目录树结构里的所有数据，除非其中某个目录又挂载了另一个文件系统</span><br><span class="line"><span class="number">3</span>、文件系统类型。下面是多数常见文件系统类型（ext3,tmpfs,devpts,sysfs,proc,swap,vfat）</span><br><span class="line"><span class="number">4</span>、mount命令选项。mount选项包括noauto（启动时不挂载该文件系统）和ro（只读方式挂载文件系统）等。在该字段里添加用户或属主选项，即可允许该用户挂载文件系统。多个选项之间必须用逗号隔开。其他选项的相关信息可参看mount命令手册页（-o选项处）</span><br><span class="line"><span class="number">5</span>、转储文件系统？该字段只在用dump备份时才有意义。数字<span class="number">1</span>表示该文件系统需要转储，<span class="number">0</span>表示不需要转储</span><br><span class="line"><span class="number">6</span>、文件系统检查？该字段里的数字表示文件系统是否需要用fsck检查。<span class="number">0</span>表示不必检查该文件系统，数字<span class="number">1</span>示意该文件系统需要先行检查（用于根文件系统）。数字<span class="number">2</span>则表示完成根文件系统检查后，再检查该文件系统。</span><br></pre></td></tr></table></figure><h4 id="Parted命令说明（本文使用交互模式完成配置）"><a href="#Parted命令说明（本文使用交互模式完成配置）" class="headerlink" title="Parted命令说明（本文使用交互模式完成配置）"></a>Parted命令说明（本文使用交互模式完成配置）</h4><p>Parted 命令分为两种模式：命令行模式和交互模式。</p><ul><li>命令行模式： parted [option] device [command] ,该模式可以直接在命令行下对磁盘进行分区操作，比较适合编程应用。</li><li>交互模式：parted [option] device 类似于使用fdisk /dev/xxx</li><li>MBR：MBR分区表(即主引导记录)大家都很熟悉。所支持的最大卷：2T，而且对分区有限制：最多4个主分区或3个主分区加一个扩展分区</li><li>GPT： GPT（即GUID分区表）。是源自EFI标准的一种较新的磁盘分区表结构的标准，是未来磁盘分区的主要形式。与MBR分区方式相比，具有如下优点。突破MBR 4个主分区限制，每个磁盘最多支持128个分区。支持大于2T的分区，最大卷可达18EB。</li></ul><p>parted是一个可以分区并进行分区调整的工具，他可以创建，破坏，移动，复制，调整ext2 linux-swap fat fat32 reiserfs类型的分区，可以创建，调整，移动Macintosh的HFS分区，检测jfs，ntfs，ufs，xfs分区。</p><p>使用方法：<code>parted [options] [device [command [options...]...]]</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">options</span><br><span class="line">-h  显示帮助信息</span><br><span class="line">-l  显示所有块设备上的分区</span><br><span class="line">device 对哪个块设备进行操作，如果没有指定则使用第一个块设备</span><br><span class="line">command [options...]</span><br><span class="line"></span><br><span class="line">check partition  对分区做一个简单的检测</span><br><span class="line"></span><br><span class="line">cp [source-device] source dest  复制source-device设备上的source分区到当前设备的dest分区</span><br><span class="line"></span><br><span class="line">mklabel label-type 创建新分区表类型，label-type可以是："bsd", "dvh", "gpt",  "loop","mac", "msdos", "pc98", or "sun" 一般的pc机都是msdos格式，如果分区大于2T则需要选用gpt格式的分区表。</span><br><span class="line"></span><br><span class="line">mkfs partition fs-type   在partition分区上创建一个fs-type文件系统，fs-type可以是："fat16", "fat32", "ext2", "linux-swap","reiserfs" 注意不支持ext3格式的文件系统，只能先分区然后用专有命令进行格式化。</span><br><span class="line"></span><br><span class="line">mkpart part-type [fs-type] start end  创建一个part-type类型的分区，part-type可以是："primary", "logical", or "extended" 如果指定fs-type则在创建分区的同时进行格式化。start和end指的是分区的起始位置，单位默认是M。</span><br><span class="line"></span><br><span class="line">eg：</span><br><span class="line">mkpart  primary  0  -1   0表示分区的开始  -1表示分区的结尾  意思是划分整个硬盘空间为主分区</span><br><span class="line">mkpartfs part-type fs-type start end 创建一个fs-type类型的part-type分区，不推荐使用，最好是使用mkpart分区完成后使用mke2fs进行格式化。</span><br><span class="line">name partition name 给分区设置一个名字，这种设置只能用在Mac, PC98, and GPT类型的分区表，设置时名字用引号括起来</span><br><span class="line">select device 在机器上有多个硬盘时，选择操作那个硬盘</span><br><span class="line">resize partition start end  调整分区大小</span><br><span class="line">rm partition  删除一个分区</span><br><span class="line">rescue start end  拯救一个位于stat和end之间的分区</span><br><span class="line">unit unit 在前面分区时，默认分区时数值的单位是M，这个参数卡伊改变默认单位，"kB", "MB",  "GB",  "TB"</span><br><span class="line">move partition start end 移动partition分区</span><br><span class="line">print  显示分区表信息  </span><br><span class="line">quit 退出parted</span><br></pre></td></tr></table></figure><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>【1】  <a href="http://magicmonster.com/kb/os/linux/large_hdd.html" target="_blank" rel="noopener">Setting up a large (2TB+) hard disk drive on Linux</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;系统环境：&quot;&gt;&lt;a href=&quot;#系统环境：&quot; class=&quot;headerlink&quot; title=&quot;系统环境：&quot;&gt;&lt;/a&gt;系统环境：&lt;/h3&gt;&lt;p&gt;Linux version 4.13.0-37-generic (Ubuntu 5.4.0-6ubuntu1~16.
      
    
    </summary>
    
      <category term="ubuntu" scheme="https://zjrongxiang.github.io/categories/ubuntu/"/>
    
    
      <category term="ubuntu" scheme="https://zjrongxiang.github.io/tags/ubuntu/"/>
    
      <category term="disk" scheme="https://zjrongxiang.github.io/tags/disk/"/>
    
      <category term="parted" scheme="https://zjrongxiang.github.io/tags/parted/"/>
    
  </entry>
  
  <entry>
    <title>jupyter notebook:主题和字体的美化</title>
    <link href="https://zjrongxiang.github.io/2018/03/19/2018-03-26-jupyter-notebook/"/>
    <id>https://zjrongxiang.github.io/2018/03/19/2018-03-26-jupyter-notebook/</id>
    <published>2018-03-19T11:30:00.000Z</published>
    <updated>2018-04-06T04:06:41.421Z</updated>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>Jupyter notebook是数据科学常用的代码交互式工具。通常在server端启jupyter进程（web服务），client端打开浏览器，jupyter提供代码编写和调试交互环境。非常方便。</p><p>但是jupyter提供的默认界面不够美观，特别是windows操作系统默认字体为浏览器默认字体–宋体（下图），另外默认主题太难看了，没有通常IDE提供的主题美观。</p><p><img src="\images\picture\jupyter01.jpg" alt=""></p><p>发现一个Jupyter的美化工具：<strong><a href="https://github.com/dunovank/jupyter-themes" target="_blank" rel="noopener">jupyterthemes</a></strong> ，和大家分享一下。简单介绍一下安装和配置。细节介绍参考项目的介绍文档。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>使用pip安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@vultr:~# pip install jupyterthemes</span><br></pre></td></tr></table></figure><p>或者使用<a href="https://anaconda.org/conda-forge/jupyterthemes" target="_blank" rel="noopener">Anaconda的conda安装</a>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@vultr:~# conda install -c conda-forge jupyterthemes</span><br></pre></td></tr></table></figure><h3 id="命令格式"><a href="#命令格式" class="headerlink" title="命令格式"></a>命令格式</h3><p>使用<code>jt -h</code>显示命令帮助说明：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">root@vultr:~# jt -h</span><br><span class="line">usage: jt [-h] [-l] [-t THEME] [-f MONOFONT] [-fs MONOSIZE] [-nf NBFONT]</span><br><span class="line">          [-nfs NBFONTSIZE] [-tf TCFONT] [-tfs TCFONTSIZE] [-dfs DFFONTSIZE]</span><br><span class="line">          [-ofs OUTFONTSIZE] [-mathfs MATHFONTSIZE] [-m MARGINS]</span><br><span class="line">          [-cursw CURSORWIDTH] [-cursc CURSORCOLOR] [-cellw CELLWIDTH]</span><br><span class="line">          [-lineh LINEHEIGHT] [-altp] [-altmd] [-altout] [-P] [-T] [-N] [-vim]</span><br><span class="line">          [-r] [-dfonts]</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --help            show this help message and exit</span><br><span class="line"><span class="meta">  #</span><span class="bash">-h，--<span class="built_in">help</span>显示此帮助信息并退出</span></span><br><span class="line">  -l, --list            list available themes</span><br><span class="line"><span class="meta">  #</span><span class="bash">-l， 列出可用主题</span></span><br><span class="line">  -t THEME, --theme THEME</span><br><span class="line">                        theme name to install（配置需要安装的主题）</span><br><span class="line">  -f MONOFONT, --monofont MONOFONT</span><br><span class="line">                        monospace code font（代码的字体）</span><br><span class="line">  -fs MONOSIZE, --monosize MONOSIZE</span><br><span class="line">                        code font-size（代码字体大小）</span><br><span class="line">  -nf NBFONT, --nbfont NBFONT</span><br><span class="line">                        notebook font（notebook 字体）</span><br><span class="line">  -nfs NBFONTSIZE, --nbfontsize NBFONTSIZE</span><br><span class="line">                        notebook fontsize（notebook 字体大小）</span><br><span class="line">  -tf TCFONT, --tcfont TCFONT</span><br><span class="line">                        txtcell font（文本的字体）</span><br><span class="line">  -tfs TCFONTSIZE, --tcfontsize TCFONTSIZE</span><br><span class="line">                        txtcell fontsize（文本的字体大小）</span><br><span class="line">  -dfs DFFONTSIZE, --dffontsize DFFONTSIZE</span><br><span class="line">                        pandas dataframe fontsize（pandas类型的字体大小）</span><br><span class="line">  -ofs OUTFONTSIZE, --outfontsize OUTFONTSIZE</span><br><span class="line">                        output area fontsize（输出区域字体大小）</span><br><span class="line">  -mathfs MATHFONTSIZE, --mathfontsize MATHFONTSIZE</span><br><span class="line">                        mathjax fontsize (in %)（数学公式字体大小）</span><br><span class="line">  -m MARGINS, --margins MARGINS</span><br><span class="line">                        fix margins of main intro page</span><br><span class="line">  -cursw CURSORWIDTH, --cursorwidth CURSORWIDTH</span><br><span class="line">                        set cursorwidth (px)（设置光标宽度）</span><br><span class="line">  -cursc CURSORCOLOR, --cursorcolor CURSORCOLOR</span><br><span class="line">                        cursor color (r, b, g, p)（设置光标颜色）</span><br><span class="line">  -cellw CELLWIDTH, --cellwidth CELLWIDTH</span><br><span class="line">                        set cell width (px or %)（单元的宽度）</span><br><span class="line">  -lineh LINEHEIGHT, --lineheight LINEHEIGHT</span><br><span class="line">                        code/text line-height (%)（行高）</span><br><span class="line">  -altp, --altprompt    alt input prompt style</span><br><span class="line">  -altmd, --altmarkdown</span><br><span class="line">                        alt markdown cell style</span><br><span class="line">  -altout, --altoutput  set output bg color to notebook bg</span><br><span class="line">  -P, --hideprompt      hide cell input prompt</span><br><span class="line">  -T, --toolbar         make toolbar visible（工具栏可见）</span><br><span class="line">  -N, --nbname          nb name/logo visible</span><br><span class="line">  -vim, --vimext        toggle styles for vim</span><br><span class="line">  -r, --reset           reset to default theme（设置成默认主题）</span><br><span class="line">  -dfonts, --defaultfonts</span><br><span class="line">                        force fonts to browser default（设置成浏览器默认字体）</span><br></pre></td></tr></table></figure><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><p>例如下面的命令完成效果：</p><p>使用的主题是：monokai，工具栏可见，命名笔记本的选项，代码的字体为13，代码的字体为consolamono。</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root<span class="variable">@vultr</span><span class="symbol">:~</span><span class="comment"># jt -t monokai -T -N -fs 13 -f consolamono</span></span><br></pre></td></tr></table></figure><p>如果jupyter进程已启，需要重新启进程后生效。</p><p>实现的效果截图：</p><p><img src="\images\picture\jupyter2.jpg" alt=""></p><p>其他主题效果大家可以自己尝试。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;Jupyter notebook是数据科学常用的代码交互式工具。通常在server端启jupyter进程（web服务），client端打开浏
      
    
    </summary>
    
      <category term="jupyter_notebook" scheme="https://zjrongxiang.github.io/categories/jupyter-notebook/"/>
    
    
      <category term="jupyter notebook" scheme="https://zjrongxiang.github.io/tags/jupyter-notebook/"/>
    
  </entry>
  
  <entry>
    <title>树莓派：使用USB摄像头自制家庭监控</title>
    <link href="https://zjrongxiang.github.io/2018/03/18/2018-03-18-raspberry_usb_camera/"/>
    <id>https://zjrongxiang.github.io/2018/03/18/2018-03-18-raspberry_usb_camera/</id>
    <published>2018-03-18T11:30:00.000Z</published>
    <updated>2018-04-04T14:31:39.756Z</updated>
    
    <content type="html"><![CDATA[<h3 id="树莓派摄像头和USB摄像头"><a href="#树莓派摄像头和USB摄像头" class="headerlink" title="树莓派摄像头和USB摄像头"></a>树莓派摄像头和USB摄像头</h3><blockquote><p>树莓派有配套的摄像头模块（Raspberry Pi camera board），如下图。</p><p>另外树莓派也支持USB摄像头。关于树莓派支持的USB摄像头有个<a href="https://elinux.org/RPi_USB_Webcams" target="_blank" rel="noopener">清单参考</a>（需要梯子）。大家购买前最好确认一下是否在兼容清单中。</p></blockquote><p><img src="\images\picture\camera_pi.png" alt="camera_pi"></p><h3 id="部署步骤"><a href="#部署步骤" class="headerlink" title="部署步骤"></a>部署步骤</h3><h4 id="第一步：检查USB摄像头和树莓派的兼容性"><a href="#第一步：检查USB摄像头和树莓派的兼容性" class="headerlink" title="第一步：检查USB摄像头和树莓派的兼容性"></a>第一步：检查USB摄像头和树莓派的兼容性</h4><p>将USB摄像头和树莓派连接，查看USB接口连接情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@raspberrypi:/# lsusb</span><br><span class="line">Bus 001 Device 004: ID 046d:0825 Logitech, Inc. Webcam C270</span><br><span class="line">Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. SMSC9512/9514 Fast Ethernet Adapter</span><br><span class="line">Bus 001 Device 002: ID 0424:9514 Standard Microsystems Corp.</span><br><span class="line">Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</span><br></pre></td></tr></table></figure><p>发现004口上面连接并识别了摄像头（我的是Logitech 270摄像头）。</p><blockquote><p>如果没有识别出来，需要查看USB兼容清单是否有该型号。另外由于树莓派供电功率较小，也有可能是USB供电功率不足，需要有外置电源的USB摄像头。</p></blockquote><p>另外还可以查看设备驱动情况：</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root<span class="variable">@raspberrypi</span><span class="symbol">:/</span><span class="comment"># ls /dev/vid*</span></span><br><span class="line">/dev/video<span class="number">0</span></span><br></pre></td></tr></table></figure><p>发现video0设备，说明识别了USB摄像头（罗技的c270i）</p><h4 id="第二步：MOTION软件实现"><a href="#第二步：MOTION软件实现" class="headerlink" title="第二步：MOTION软件实现"></a>第二步：MOTION软件实现</h4><p>对于USB摄像头，有多种软件包可以实现拍照和摄像等功能，这里使用<a href="https://github.com/Motion-Project/motion" target="_blank" rel="noopener">motion</a>。</p><h5 id="安装motion"><a href="#安装motion" class="headerlink" title="安装motion"></a>安装motion</h5>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@raspberrypi:/# sudo apt-get install motion</span><br></pre></td></tr></table></figure><h5 id="编辑配置配置文件"><a href="#编辑配置配置文件" class="headerlink" title="编辑配置配置文件"></a>编辑配置配置文件</h5>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@raspberrypi:~# vi /etc/motion/motion.conf</span><br></pre></td></tr></table></figure><h5 id="调整相关参数"><a href="#调整相关参数" class="headerlink" title="调整相关参数"></a>调整相关参数</h5>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The mini-http server listens to this port <span class="keyword">for</span> requests (default: 0 = disabled)</span></span><br><span class="line">stream_port 8082</span><br><span class="line"><span class="meta">#</span><span class="bash"> web界面访问端口</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> TCP/IP port <span class="keyword">for</span> the http server to listen on (default: 0 = disabled)</span></span><br><span class="line">webcontrol_port 8080</span><br><span class="line"><span class="meta">#</span><span class="bash">控制端口</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Restrict control connections to localhost only (default: on)</span></span><br><span class="line">webcontrol_localhost off</span><br><span class="line"><span class="meta">#</span><span class="bash"> Target base directory <span class="keyword">for</span> pictures and films</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Recommended to use absolute path. (Default: current working directory)</span></span><br><span class="line">target_dir /var/lib/motion</span><br><span class="line"><span class="meta">#</span><span class="bash">照片及视频存放路径</span></span><br></pre></td></tr></table></figure><p>其他参数调整如下：</p><p>​          <em>ffmpeg_output_movies</em>=off</p><p>​           <em>stream_localhost</em>=off</p><p>​           <em>webcontrol_localhost</em>=off</p><p>  ​         <em>locate_motion_mode</em>=peview</p><p>  ​         <em>locate_motion_style</em>=redbox</p><p>​           <em>text_changes</em>=on</p><h5 id="开启motion进程"><a href="#开启motion进程" class="headerlink" title="开启motion进程"></a>开启motion进程</h5><p>修改motion文件，设置为守护进程运行（即参数配置为：yes）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vi /etc/default/motion</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">set</span> to <span class="string">'yes'</span> to <span class="built_in">enable</span> the motion daemon</span></span><br><span class="line">start_motion_daemon=yes</span><br></pre></td></tr></table></figure><p>启进程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@raspberrypi:/etc/init.d# motion start</span><br><span class="line">[0] [NTC] [ALL] conf_load: Processing thread 0 - config file /etc/motion/motion.conf</span><br><span class="line">[0] [ALR] [ALL] conf_cmdparse: Unknown config option "sdl_threadnr"</span><br><span class="line">[0] [NTC] [ALL] motion_startup: Motion 3.2.12+git20140228 Started</span><br><span class="line">[0] [NTC] [ALL] motion_startup: Logging to syslog</span><br><span class="line">[0] [NTC] [ALL] motion_startup: Using log type (ALL) log level (NTC)</span><br><span class="line">[0] [NTC] [ALL] become_daemon: Motion going to daemon mode</span><br></pre></td></tr></table></figure><h5 id="查看监控画面"><a href="#查看监控画面" class="headerlink" title="查看监控画面"></a>查看监控画面</h5><p>地址栏中输入地址和端口号（IP：8082），上面配置的web界面访问端口为8082：</p><p><img src="\images\picture\example.png" alt=""></p><h5 id="查看监控数据存放目录"><a href="#查看监控数据存放目录" class="headerlink" title="查看监控数据存放目录"></a>查看监控数据存放目录</h5><p>另外目录/var/lib/motion中存放历史数据。</p><h4 id="第三步：内网穿透（外网访问web监控界面）"><a href="#第三步：内网穿透（外网访问web监控界面）" class="headerlink" title="第三步：内网穿透（外网访问web监控界面）"></a>第三步：内网穿透（外网访问web监控界面）</h4><p>实现上面的步骤，你只能在家里本地局域网访问监控界面，意义不大。</p><p>由于目前中国宽带服务公司都不会给家庭网络外网地址。所以需要内网穿透，实现外网访问家庭内网。</p><p>具体可以使用frp软件实现内网穿透。具体做法参考的博客中另一篇介绍frp的分享文章。</p><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p>【1】 <a href="https://www.bouvet.no/bouvet-deler/utbrudd/building-a-motion-activated-security-camera-with-the-raspberry-pi-zero" target="_blank" rel="noopener">https://www.bouvet.no/bouvet-deler/utbrudd/building-a-motion-activated-security-camera-with-the-raspberry-pi-zero</a></p><p>【2】 <a href="https://medium.com/@Cvrsor/how-to-make-a-diy-home-alarm-system-with-a-raspberry-pi-and-a-webcam-2d5a2d61da3d" target="_blank" rel="noopener">https://medium.com/@Cvrsor/how-to-make-a-diy-home-alarm-system-with-a-raspberry-pi-and-a-webcam-2d5a2d61da3d</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;树莓派摄像头和USB摄像头&quot;&gt;&lt;a href=&quot;#树莓派摄像头和USB摄像头&quot; class=&quot;headerlink&quot; title=&quot;树莓派摄像头和USB摄像头&quot;&gt;&lt;/a&gt;树莓派摄像头和USB摄像头&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;树莓派有配套的摄像头模块（
      
    
    </summary>
    
      <category term="raspberry" scheme="https://zjrongxiang.github.io/categories/raspberry/"/>
    
    
      <category term="raspberry" scheme="https://zjrongxiang.github.io/tags/raspberry/"/>
    
  </entry>
  
</feed>
