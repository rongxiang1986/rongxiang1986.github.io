<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RongXiang</title>
  
  <subtitle>我的烂笔头</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zjrongxiang.github.io/"/>
  <updated>2020-12-31T13:55:57.646Z</updated>
  <id>https://zjrongxiang.github.io/</id>
  
  <author>
    <name>rong xiang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hbase使用总结</title>
    <link href="https://zjrongxiang.github.io/2020/12/30/2020-12-11-Hbase%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2020/12/30/2020-12-11-Hbase使用总结/</id>
    <published>2020-12-30T13:30:00.000Z</published>
    <updated>2020-12-31T13:55:57.646Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  环境依赖</li><li>第二部分 交互接口</li><li>第三部分 任务提交</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>HBase是一个高可靠性、高性能、可伸缩的分布式存储系统：<br>• HBase是一个构建在HDFS上的分布式NoSQL数据库；<br>• HBase是基于Google BigTable模型开发的，典型的key/value系统；<br>• 具有松散的表结构；原生海量数据分布式存储；支持随机查询、范围查询<br>• 高吞吐，低延迟；<br>• 列存储，多版本，增量导入，多维删除</p><p>• HDFS Vs HBase：<br>• Hadoop是一个高容错、高延时的分布式文件系统和高并发的批处理系统，<br>不适用于提供实时计算；<br>• HBase是可以提供实时计算的分布式数据库，数据被保存在HDFS分布式文件<br>系统上，由HDFS保证其高容错性</p><p>HDFS Vs HBase：<br>• HBase上的数据是以StoreFile(HFile)二进制流的形式存储在HDFS上block块儿中<br>• HDFS并不知道的hbase存的是什么，它只把存储文件视为二进制文件，也就<br>是说，hbase的存储数据对于HDFS文件系统是透明的</p><p>HBase特性：<br>• 大：一个表可以有数十亿行，上百万列；<br>• 无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张<br>表中不同的行可以有截然不同的列；<br>• 面向列：面向列（族）的存储和权限控制，列（族）独立检索；<br>• 稀疏、多维、排序的map：空（null）列并不占用存储空间，表可以设计的非常稀疏；每个<br>单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；<br>• 数据类型单一：Hbase中的数据都是字符串，没有类型。<br>• 强一致性读写:HBase 不是“最终一致性”数据存储。这让它很适合高速计数聚合类任务；<br>• 自动分片(Automatic sharding)： HBase 表通过 region 分布在集群中。数据增长时，region<br>会自动分割并重新分布；<br>• RegionServer 自动故障转移和负载均衡；<br>• Hadoop/HDFS 集成：HBase 支持开箱即用地支持 HDFS 作为它的分布式文件系统；<br>• MapReduce： HBase 通过 MapReduce 支持大并发处理；<br>• 实时、随机地大数据访问；HBase内部使用LSM-tree(log-structured merge-tree)作为数据存<br>储架构，LSM-tree周期性地合并小文件到较大的文件，以减少硬盘寻址<br>• Java 客户端 API：HBase 支持易于使用的 Java API 进行编程访问；<br>• Thrift/REST API：HBase 也支持 Thrift 和 REST 作为非 Java 前端的访问；<br>• Block Cache 和 Bloom Filter：对于大容量查询优化， HBase 支持 Block Cache 和 Bloom Filter<br>• 快照支持</p><p>面向列的数据存储 Vs 面向行的数据存储：<br>HBase的逻辑视图<br>面向行的数据存储面向列的数据存储<br>对于记录的增加/修改效率较高对于读取数据效率较高<br>读取包含整个行的页面只需要读取列<br>最适合用于OLTP 对于OLTP还没有优化<br>将一行中所有的值一起序列化，<br>然后是下一行的值 ，等等<br>将列中的值一起序列化，依次类<br>推<br>行数据存储在内存或磁盘中的连<br>续页面中<br>列以页面的列式存储在内存或磁<br>盘中</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、 Apache Spark support，链接</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  环境依赖&lt;/li&gt;
&lt;li&gt;第二部分 交互接口&lt;/li&gt;
&lt;li&gt;第三部分 任务提交&lt;/li
      
    
    </summary>
    
      <category term="hadoop" scheme="https://zjrongxiang.github.io/categories/hadoop/"/>
    
    
  </entry>
  
  <entry>
    <title>Zeppelin安装部署介绍</title>
    <link href="https://zjrongxiang.github.io/2020/12/12/2020-12-19-Zeppelin%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E4%BB%8B%E7%BB%8D/"/>
    <id>https://zjrongxiang.github.io/2020/12/12/2020-12-19-Zeppelin安装部署介绍/</id>
    <published>2020-12-12T04:42:00.000Z</published>
    <updated>2020-12-19T14:17:50.975Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分  <code>alias</code>配置和取消</p></li><li><p>第二部分 <code>alias</code>查看</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Reindex from a remote cluster，链接：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分  &lt;code&gt;alias&lt;/code&gt;配置和取消&lt;/p&gt;
&lt;/li&gt;
&lt;
      
    
    </summary>
    
      <category term="Zeppelin" scheme="https://zjrongxiang.github.io/categories/Zeppelin/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux系统中命令别名设置</title>
    <link href="https://zjrongxiang.github.io/2020/12/12/2020-12-12-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%91%BD%E4%BB%A4%E5%88%AB%E5%90%8D%E8%AE%BE%E7%BD%AE/"/>
    <id>https://zjrongxiang.github.io/2020/12/12/2020-12-12-Linux系统中命令别名设置/</id>
    <published>2020-12-12T04:42:00.000Z</published>
    <updated>2020-12-12T09:21:20.671Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分  <code>alias</code>配置和取消</p></li><li><p>第二部分 <code>alias</code>查看</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><code>linux</code>中提供用户自定义命名别名，即<code>alias</code>命令。</p><h2 id="第一部分-alias配置和取消"><a href="#第一部分-alias配置和取消" class="headerlink" title="第一部分 alias配置和取消"></a>第一部分 <code>alias</code>配置和取消</h2><h3 id="1-1-配置"><a href="#1-1-配置" class="headerlink" title="1.1 配置"></a>1.1 配置</h3><ul><li><p>临时设置</p><p>使用下面的命令格式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alias 新的命令='原命令 -选项/参数'</span><br></pre></td></tr></table></figure><p>例如下面的例子中，我们将命令<code>cd ..</code>简化成<code>..</code>，方便使用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alias ..='cd ..'</span><br></pre></td></tr></table></figure></li></ul><p>​       但是这种设置是临时在当前shell中生效了，重启开启新的shell就会失效。</p><ul><li><p>永久生效</p><p>如果需要永久生效就需要将设置配置在环境变量中。需要注意的是环境变量有效范围（用户环境和系统环境变量）。例如系统环境变量中，在<code>/etc/profile</code>中追加：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alias rm='rm –i'</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>所有用户shell均具有该命令别名。</p></li></ul><h3 id="1-2-取消"><a href="#1-2-取消" class="headerlink" title="1.2 取消"></a>1.2 取消</h3><p>如果需要取消命令别名，可以使用下面的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unalias rm='rm –i'</span><br></pre></td></tr></table></figure><p>对于配置在环境变量中的就需要手动注释，并source生效（当前shell）。</p><h2 id="第二部分-alias查看"><a href="#第二部分-alias查看" class="headerlink" title="第二部分 alias查看"></a>第二部分 <code>alias</code>查看</h2><p>如果需要参看当前shell环境已经配置的命令别名，可以直接使用命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@VM-0-5-ubuntu:~# alias</span><br><span class="line">alias egrep='egrep --color=auto'</span><br><span class="line">alias fgrep='fgrep --color=auto'</span><br><span class="line">alias grep='grep --color=auto'</span><br><span class="line">alias l='ls -CF'</span><br><span class="line">alias la='ls -A'</span><br><span class="line">alias ll='ls -alF'</span><br><span class="line">alias ls='ls --color=auto'</span><br></pre></td></tr></table></figure><p>上面是<code>ubuntu</code>系统自带的命令别名。另外比较常用的还有：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">alias ..='cd ..'</span><br><span class="line">alias ...='cd ../..'</span><br><span class="line">alias ....='cd ../../../'</span><br></pre></td></tr></table></figure><p>日常操作中目录的进退是常用了，上面的命令别名大大提高了输入效率。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Using alias Command in Linux to Improve Your Efficiency，链接：<a href="https://linuxhandbook.com/linux-alias-command/" target="_blank" rel="noopener">https://linuxhandbook.com/linux-alias-command/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分  &lt;code&gt;alias&lt;/code&gt;配置和取消&lt;/p&gt;
&lt;/li&gt;
&lt;
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Anaconda相关信息汇总</title>
    <link href="https://zjrongxiang.github.io/2020/12/12/2020-12-13-Anaconda%E7%9B%B8%E5%85%B3%E4%BF%A1%E6%81%AF%E6%B1%87%E6%80%BB/"/>
    <id>https://zjrongxiang.github.io/2020/12/12/2020-12-13-Anaconda相关信息汇总/</id>
    <published>2020-12-12T04:42:00.000Z</published>
    <updated>2020-12-13T05:45:49.906Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>登记Anaconda相关的下载信息，备用查阅。</p><table><thead><tr><th></th><th>anaconda</th><th></th></tr></thead><tbody><tr><td>镜像下载地址</td><td><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=N&amp;O=D" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=N&amp;O=D</a></td><td><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/?C=N&amp;O=D" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/?C=N&amp;O=D</a></td></tr><tr><td>官方首页</td><td><a href="https://www.anaconda.com/distribution/" target="_blank" rel="noopener">https://www.anaconda.com/distribution/</a></td><td><a href="https://docs.conda.io/en/latest/miniconda.html" target="_blank" rel="noopener">https://docs.conda.io/en/latest/miniconda.html</a></td></tr><tr><td>官方下载地址</td><td><a href="https://repo.anaconda.com/archive/" target="_blank" rel="noopener">https://repo.anaconda.com/archive/</a></td><td><a href="https://repo.anaconda.com/miniconda/" target="_blank" rel="noopener">https://repo.anaconda.com/miniconda/</a></td></tr><tr><td>官方文档</td><td><a href="https://docs.anaconda.com/anaconda/" target="_blank" rel="noopener">https://docs.anaconda.com/anaconda/</a></td><td></td></tr><tr><td>old package lists</td><td><a href="https://docs.anaconda.com/anaconda/packages/oldpkglists/" target="_blank" rel="noopener">https://docs.anaconda.com/anaconda/packages/oldpkglists/</a></td><td></td></tr><tr><td>release notes</td><td><a href="https://docs.anaconda.com/anaconda/reference/release-notes/" target="_blank" rel="noopener">https://docs.anaconda.com/anaconda/reference/release-notes/</a></td></tr></tbody></table><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、官网文档，链接：<a href="https://docs.anaconda.com/" target="_blank" rel="noopener">https://docs.anaconda.com/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;参考文献及资料&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;背景&quot;&gt;&lt;a hr
      
    
    </summary>
    
      <category term="Python" scheme="https://zjrongxiang.github.io/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>使用Reindex实现Elasticsearch数据迁移</title>
    <link href="https://zjrongxiang.github.io/2020/12/12/2020-12-13-%E4%BD%BF%E7%94%A8Reindex%E5%AE%9E%E7%8E%B0Elasticsearch%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2020/12/12/2020-12-13-使用Reindex实现Elasticsearch数据迁移总结/</id>
    <published>2020-12-12T04:42:00.000Z</published>
    <updated>2020-12-13T05:53:36.411Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分  <code>alias</code>配置和取消</p></li><li><p>第二部分 <code>alias</code>查看</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在实际生产环境中，需要对线上<code>Elasticsearch</code>集群进行置换。业务上需要集群对外服务不中断的前提上，将原集群的数据迁移至新集群。目前迁移有多种方案，主要有：<code>elasticsearch-dump</code>、<code>logstash</code>、<code>reindex</code>、<code>snapshot</code>等方式。</p><table><thead><tr><th style="text-align:left"></th><th style="text-align:left">elasticsearch-dump</th><th style="text-align:left">logstashsnapshot</th><th style="text-align:left">snapshotreindex</th><th>reindex</th></tr></thead><tbody><tr><td style="text-align:left">基本原理</td><td style="text-align:left">逻辑备份，类似mysqldump将数据一条一条导出后再执行导入</td><td style="text-align:left">从一个 ES 集群中读取数据然后写入到另一个 ES 集群</td><td style="text-align:left">从源 ES 集群通过备份api创建数据快照，然后在目标 ES 集群中进行恢复</td><td>reindex是Elasticsearch提供的一个api接口，可以把数据从一个集群迁移到另外一个集群</td></tr><tr><td style="text-align:left">网络要求</td><td style="text-align:left">网络需要互通</td><td style="text-align:left">网络需要互通</td><td style="text-align:left">无网络互通要求</td><td>网络需要互通</td></tr><tr><td style="text-align:left">迁移速度</td><td style="text-align:left">慢</td><td style="text-align:left">一般</td><td style="text-align:left">快</td><td>一般</td></tr><tr><td style="text-align:left">运维配置复杂度</td><td style="text-align:left">复杂，索引的分片数量和副本数量需要对每个索引单独进行迁移，或者直接在目标集群提前将索引创建完成，再迁移数据</td><td style="text-align:left">复杂，需要提前在目标集群创建mapping和setting等，再迁移数据</td><td style="text-align:left">简单</td><td>需要在目标ES集群中配置reindex.remote.whitelist参数，指明能够reindex的远程集群的白名单</td></tr><tr><td style="text-align:left">适合场景</td><td style="text-align:left">适用于数据量小的场景</td><td style="text-align:left">适用于数据量一般，近实时数据传输</td><td style="text-align:left">适用于数据量大，接受离线数据迁移的场景</td><td>本地索引更新Mapping实现索引层面迁移，或者跨集群的索引迁移</td></tr></tbody></table><ul><li>scroll query + bulk: 批量读取旧集群的数据然后再批量写入新集群，elasticsearch-dump、logstash、reindex都是采用这种方式</li><li>snapshot: 直接把旧集群的底层的文件进行备份，在新的集群中恢复出来，相比较scroll query + bulk的方式，snapshot的方式迁移速度最快。</li></ul><p><a href="https://cloud.tencent.com/developer/article/1611786" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1611786</a></p><h3 id="第一部分-Reindex接口介绍"><a href="#第一部分-Reindex接口介绍" class="headerlink" title="第一部分 Reindex接口介绍"></a>第一部分 Reindex接口介绍</h3><p>reindex 是 ES 提供的一个 api 接口，可以把数据从源 ES 集群导入到当前 ES 集群，实现集群内部或跨集群同步数据。</p><p>但仅限于腾讯云 ES 的实现方式（跨集群迁移需要elasticsearch.yml中加上ip白名单，并重启集群），所以腾讯云ES不支持 reindex 操作。具体见官方文档说明：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.3/reindex-upgrade-remote.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/7.3/reindex-upgrade-remote.html</a></p><h3 id="第三部分-Reindex迁移方案"><a href="#第三部分-Reindex迁移方案" class="headerlink" title="第三部分 Reindex迁移方案"></a>第三部分 Reindex迁移方案</h3><p><strong>下面简单介绍 reindex 接口的使用方法：</strong></p><p>1) 配置 elasticsearch.yml中的reindex.remote.whitelist 参数</p><p>需要在目标 ES 集群中配置该参数，指明能够 reindex 的远程集群的白名单。</p><p>2) 调用 reindex api</p><p>以下操作表示从源 ES 集群中查询名为 test1 的索引，查询条件为 title 字段为 elasticsearch，将结果写入当前集群的 test2 索引。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line"> &#123;</span><br><span class="line">     <span class="string">"source"</span>: &#123;</span><br><span class="line">        <span class="string">"remote"</span>: &#123;</span><br><span class="line">            <span class="string">"host"</span>: <span class="string">"http://172.16.0.39:9200"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"index"</span>: <span class="string">"test1"</span>,</span><br><span class="line">        <span class="string">"query"</span>: &#123;</span><br><span class="line">            <span class="string">"match"</span>: &#123;</span><br><span class="line">                <span class="string">"title"</span>: <span class="string">"elasticsearch"</span></span><br><span class="line">            &#125;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;,</span><br><span class="line">     <span class="string">"dest"</span>: &#123;</span><br><span class="line">         <span class="string">"index"</span>: <span class="string">"test2"</span></span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>从源索引中提取文档源，并将文档索引到目标索引中。可以将所有文档复制到目标索引，或为文档的子集重新索引。_reindex获取源索引的快照，但是其目标必须是其他索引，因此不会发生版本冲突。</p><h3 id="第四部分-参数调优"><a href="#第四部分-参数调优" class="headerlink" title="第四部分 参数调优"></a>第四部分 参数调优</h3><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html</a></p><p><a href="https://www.cnblogs.com/Ace-suiyuan008/p/9985249.html" target="_blank" rel="noopener">https://www.cnblogs.com/Ace-suiyuan008/p/9985249.html</a></p><p><a href="https://elkguide.elasticsearch.cn/elasticsearch/api/reindex.html" target="_blank" rel="noopener">https://elkguide.elasticsearch.cn/elasticsearch/api/reindex.html</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Reindex from a remote cluster，链接：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分  &lt;code&gt;alias&lt;/code&gt;配置和取消&lt;/p&gt;
&lt;/li&gt;
&lt;
      
    
    </summary>
    
      <category term="Elasticsearch" scheme="https://zjrongxiang.github.io/categories/Elasticsearch/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark Mllib模块的学习总结</title>
    <link href="https://zjrongxiang.github.io/2020/12/12/2020-12-19-Spark%20Mllib%E6%A8%A1%E5%9D%97%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2020/12/12/2020-12-19-Spark Mllib模块的学习总结/</id>
    <published>2020-12-12T04:42:00.000Z</published>
    <updated>2020-12-19T08:30:54.719Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分  <code>alias</code>配置和取消</p></li><li><p>第二部分 <code>alias</code>查看</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>MLlib 是 Spark 的机器学习库，旨在简化机器学习的工程实践工作，并方便扩展到更大规模。</p><p>MLlib 由一些通用的学习算法和工具组成，包括分类、回归、聚类、协同过滤、降维等，同时还包括底层的优化原语和高层的管道 API。</p><p>本节将对 Spark MLlib 进行简单介绍，在介绍数据挖掘算法时，将使用 Spark MLlib 提供的算法进行实例讲解。</p><h2 id="Spark-MLlib的构成"><a href="#Spark-MLlib的构成" class="headerlink" title="Spark MLlib的构成"></a>Spark MLlib的构成</h2><p>Spark 是基于内存计算的，天然适应于数据挖掘的迭代式计算，但是对于普通开发者来说，实现分布式的数据挖掘算法仍然具有极大的挑战性。因此，Spark 提供了一个基于海量数据的机器学习库 MLlib，它提供了常用数据挖掘算法的分布式实现功能。</p><p>开发者只需要有 Spark 基础并且了解数据挖掘算法的原理，以及算法参数的含义，就可以通过调用相应的算法的 API 来实现基于海量数据的挖掘过程。</p><p>MLlib 由 4 部分组成：数据类型，数学统计计算库，算法评测和机器学习算法。</p><table><thead><tr><th>名称</th><th>说明</th></tr></thead><tbody><tr><td>数据类型</td><td>向量、带类别的向量、矩阵等</td></tr><tr><td>数学统计计算库</td><td>基本统计量、相关分析、随机数产生器、假设检验等</td></tr><tr><td>算法评测</td><td>AUC、准确率、召回率、F-Measure 等</td></tr><tr><td>机器学习算法</td><td>分类算法、回归算法、聚类算法、协同过滤等</td></tr></tbody></table><p>具体来讲，分类算法和回归算法包括逻辑回归、SVM、朴素贝叶斯、决策树和随机森林等算法。用于聚类算法包括 k-means 和 LDA 算法。协同过滤算法包括交替最小二乘法（ALS）算法。</p><p>Spark 机器学习库从 1.2 版本以后被分为两个包：</p><ul><li><code>spark.mllib</code>包含基于RDD的原始算法API。Spark MLlib 历史比较长，在1.0 以前的版本即已经包含了，提供的算法实现都是基于原始的 RDD。</li><li><a href="http://spark.apache.org/docs/latest/ml-guide.html" target="_blank" rel="noopener"><code>spark.ml</code></a> 则提供了基于<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes" target="_blank" rel="noopener">DataFrames</a> 高层次的API，可以用来构建机器学习工作流（PipeLine）。ML Pipeline 弥补了原始 MLlib 库的不足，向用户提供了一个基于 DataFrame 的机器学习工作流式 API 套件。</li></ul><p>使用 ML Pipeline API可以很方便的把数据处理，特征转换，正则化，以及多个机器学习算法联合起来，构建一个单一完整的机器学习流水线。这种方式给我们提供了更灵活的方法，更符合机器学习过程的特点，也更容易从其他语言迁移。Spark官方推荐使用spark.ml。如果新的算法能够适用于机器学习管道的概念，就应该将其放到spark.ml包中，如：特征提取器和转换器。开发者需要注意的是，从Spark2.0开始，基于RDD的API进入维护模式（即不增加任何新的特性），并预期于3.0版本的时候被移除出MLLib。</p><p>Spark在机器学习方面的发展非常快，目前已经支持了主流的统计和机器学习算法。纵观所有基于分布式架构的开源机器学习库，MLlib可以算是计算效率最高的。MLlib目前支持4种常见的机器学习问题: 分类、回归、聚类和协同过滤。下表列出了目前MLlib支持的主要的机器学习算法： </p><p><img src="http://mocom.xmu.edu.cn/blog/58481a13e083c990247075a4.png" alt="img"></p><h2 id="Spark-MLlib-的优势"><a href="#Spark-MLlib-的优势" class="headerlink" title="Spark MLlib 的优势"></a>Spark MLlib 的优势</h2><p>相比于基于 Hadoop MapReduce 实现的机器学习算法（如 Hadoop Manhout），Spark MLlib 在机器学习方面具有一些得天独厚的优势。</p><p>首先，机器学习算法一般都有由多个步骤组成迭代计算的过程，机器学习的计算需要在多次迭代后获得足够小的误差或者足够收敛时才会停止。如果迭代时使用 Hadoop MapReduce 计算框架，则每次计算都要读/写磁盘及完成任务的启动等工作，从而会导致非常大的 I/O 和 CPU 消耗。</p><p>而 Spark 基于内存的计算模型就是针对迭代计算而设计的，多个迭代直接在内存中完成，只有在必要时才会操作磁盘和网络，所以说，Spark MLlib 正是机器学习的理想的平台。其次，Spark 具有出色而高效的 Akka 和 Netty 通信系统，通信效率高于 Hadoop MapReduce 计算框架的通信机制。</p><p>在 Spark 官方首页中展示了 Logistic Regression 算法在 Spark 和 Hadoop 中运行的性能比较，可以看出 Spark 比 Hadoop 要快 100 倍以上。</p><p>MLlib(Machine Learnig lib) 是Spark对常用的机器学习算法的实现库，同时包括相关的测试和数据生成器。Spark的设计初衷就是为了支持一些迭代的Job, 这正好符合很多机器学习算法的特点。在Spark官方首页中展示了Logistic Regression算法在Spark和Hadoop中运行的性能比较，如图下图所示。</p><p> <img src="https://images0.cnblogs.com/blog/107289/201508/211415584105513.jpg" alt="img"></p><p><img src="https://images0.cnblogs.com/blog/107289/201508/211415580036455.gif" alt="img"></p><h3 id="第四部分-参数调优"><a href="#第四部分-参数调优" class="headerlink" title="第四部分 参数调优"></a>第四部分 参数调优</h3><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Reindex from a remote cluster，链接：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分  &lt;code&gt;alias&lt;/code&gt;配置和取消&lt;/p&gt;
&lt;/li&gt;
&lt;
      
    
    </summary>
    
      <category term="Elasticsearch" scheme="https://zjrongxiang.github.io/categories/Elasticsearch/"/>
    
    
  </entry>
  
  <entry>
    <title>nohup命令的日志重定向总结</title>
    <link href="https://zjrongxiang.github.io/2020/12/12/2020-12-19-nohup%E5%91%BD%E4%BB%A4%E7%9A%84%E6%97%A5%E5%BF%97%E9%87%8D%E5%AE%9A%E5%90%91%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2020/12/12/2020-12-19-nohup命令的日志重定向总结/</id>
    <published>2020-12-12T04:42:00.000Z</published>
    <updated>2020-12-19T09:42:08.252Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p>背景</p></li><li><p>第一部分  <code>alias</code>配置和取消</p></li><li><p>第二部分 <code>alias</code>查看</p></li><li><p>参考文献及资料</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://www.cnblogs.com/taosim/articles/2610170.html" target="_blank" rel="noopener">https://www.cnblogs.com/taosim/articles/2610170.html</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Reindex from a remote cluster，链接：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;背景&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;第一部分  &lt;code&gt;alias&lt;/code&gt;配置和取消&lt;/p&gt;
&lt;/li&gt;
&lt;
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>如何将文件数据导入hive表中</title>
    <link href="https://zjrongxiang.github.io/2020/11/23/2020-12-23-%E5%A6%82%E4%BD%95%E5%B0%86%E6%96%87%E4%BB%B6%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5hive%E8%A1%A8%E4%B8%AD/"/>
    <id>https://zjrongxiang.github.io/2020/11/23/2020-12-23-如何将文件数据导入hive表中/</id>
    <published>2020-11-23T13:30:00.000Z</published>
    <updated>2020-12-24T05:25:18.476Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  环境依赖</li><li>第二部分 交互接口</li><li>第三部分 任务提交</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>使用head命令看一下。</p><h2 id="第一步-创建库和hive表"><a href="#第一步-创建库和hive表" class="headerlink" title="第一步 创建库和hive表"></a>第一步 创建库和hive表</h2><p>使用下面的命令进入hive shell交互模式。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@hadoop01:/opt/hive/bin/#hive</span><br></pre></td></tr></table></figure><p>创建库：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">database</span> cda;</span><br></pre></td></tr></table></figure><p>创建表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> cda.users (</span><br><span class="line">user_id <span class="keyword">string</span>,</span><br><span class="line">item_id <span class="keyword">string</span>,</span><br><span class="line">cat_id <span class="keyword">string</span>,</span><br><span class="line">merchant_id <span class="keyword">string</span>,</span><br><span class="line">brand_id <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">month</span> <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">day</span> <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">action</span> <span class="keyword">string</span>,</span><br><span class="line">age_range <span class="keyword">string</span>,</span><br><span class="line">gender <span class="keyword">string</span>,</span><br><span class="line">province <span class="keyword">string</span>   </span><br><span class="line">)<span class="keyword">COMMENT</span> <span class="string">'user_log.csv Table'</span> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span> tblproperties(<span class="string">"skip.header.line.count"</span>=<span class="string">"1"</span>);</span><br></pre></td></tr></table></figure><p>如果需要分区：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> cda.users (</span><br><span class="line">user_id <span class="keyword">string</span>,</span><br><span class="line">item_id <span class="keyword">string</span>,</span><br><span class="line">cat_id <span class="keyword">string</span>,</span><br><span class="line">merchant_id <span class="keyword">string</span>,</span><br><span class="line">brand_id <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">action</span> <span class="keyword">string</span>,</span><br><span class="line">age_range <span class="keyword">string</span>,</span><br><span class="line">gender <span class="keyword">string</span>,</span><br><span class="line">province <span class="keyword">string</span>   </span><br><span class="line">)PARTITIONED <span class="keyword">BY</span>(<span class="keyword">month</span> <span class="keyword">string</span>,<span class="keyword">day</span> <span class="keyword">string</span>) <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span> tblproperties(<span class="string">"skip.header.line.count"</span>=<span class="string">"1"</span>);</span><br></pre></td></tr></table></figure><h2 id="第二部分-准备数据和导入"><a href="#第二部分-准备数据和导入" class="headerlink" title="第二部分 准备数据和导入"></a>第二部分 准备数据和导入</h2><p>将数据上传hdfs：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@hadoop03:/opt/data# hdfs dfs -put user_log.csv /data/user_log.csv</span><br></pre></td></tr></table></figure><p>导入数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> INPATH <span class="string">'hdfs:///data/user_log.csv'</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> cda.users;</span><br></pre></td></tr></table></figure><h2 id="第三部分-查询"><a href="#第三部分-查询" class="headerlink" title="第三部分 查询"></a>第三部分 查询</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root@hadoop01:/opt/hive/bin# hive</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/opt/hbase-1.4.13/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-2.3.7.jar!/hive-log4j2.properties Async: true</span><br><span class="line">Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from cda.users <span class="built_in">limit</span> 10;</span></span><br></pre></td></tr></table></figure><p>回显数据，完成导入。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、 hive-load-csv-file-into-table，链接:<a href="https://sparkbyexamples.com/apache-hive/hive-load-csv-file-into-table/" target="_blank" rel="noopener">https://sparkbyexamples.com/apache-hive/hive-load-csv-file-into-table/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  环境依赖&lt;/li&gt;
&lt;li&gt;第二部分 交互接口&lt;/li&gt;
&lt;li&gt;第三部分 任务提交&lt;/li
      
    
    </summary>
    
      <category term="hive" scheme="https://zjrongxiang.github.io/categories/hive/"/>
    
    
  </entry>
  
  <entry>
    <title>使用Pyspark进行机器学习</title>
    <link href="https://zjrongxiang.github.io/2020/11/23/2020-12-24-%E4%BD%BF%E7%94%A8Pyspark%E8%BF%9B%E8%A1%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>https://zjrongxiang.github.io/2020/11/23/2020-12-24-使用Pyspark进行机器学习/</id>
    <published>2020-11-23T13:30:00.000Z</published>
    <updated>2020-12-31T14:25:28.839Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  环境依赖</li><li>第二部分 交互接口</li><li>第三部分 任务提交</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>配置和启动<code>jupyter notebook</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@hadoop01:/opt# jupyter notebook --generate-config</span><br><span class="line">Writing default config to: /root/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">root@hadoop01:/opt# vi /root/anaconda3/share/jupyter/kernels/python3/kernel.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"> "argv": [</span><br><span class="line">  "/root/anaconda3/bin/python",</span><br><span class="line">  "-m",</span><br><span class="line">  "ipykernel_launcher",</span><br><span class="line">  "-f",</span><br><span class="line">  "&#123;connection_file&#125;"</span><br><span class="line"> ],</span><br><span class="line"> "display_name": "Python 3",</span><br><span class="line"> "language": "python",</span><br><span class="line">"env": &#123;</span><br><span class="line">"SPARK_HOME": "/opt/spark-2.3.2/",</span><br><span class="line">"PYSPARK_PYTHON": "/root/anaconda3/bin/python",</span><br><span class="line">"PYSPARK_DRIVER_PYTHON": "ipython3",                       </span><br><span class="line">"PYTHONPATH": "/opt/spark-2.3.2/python/:/opt/spark-2.3.2/python/lib/py4j-0.10.7-src.zip",</span><br><span class="line">"PYTHONSTARTUP": "/opt/spark-2.3.2/python/pyspark/shell.py",</span><br><span class="line">"PYSPARK_SUBMIT_ARGS": "--name pyspark --master local pyspark-shell"</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">'SparkByExamples.com'</span>).getOrCreate()</span><br></pre></td></tr></table></figure><h2 id="第一部分-数据的读取"><a href="#第一部分-数据的读取" class="headerlink" title="第一部分 数据的读取"></a>第一部分 数据的读取</h2><p>通常数据存储在csv文件中，需要使用pyspark进行读取。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">df</span> = spark.read.option(<span class="string">"header"</span>,<span class="literal">True</span>).csv(<span class="string">"/data/test.csv"</span>)</span><br></pre></td></tr></table></figure><p>如果是从hdfs文件系统读取：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = spark.<span class="keyword">read</span>.<span class="keyword">options</span>(<span class="keyword">header</span>=<span class="string">'True'</span>, delimiter=<span class="string">','</span>).csv("hdfs://hadoop01:9000/data/test.csv")</span><br></pre></td></tr></table></figure><p>可以使用show方法进行查看：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">df</span><span class="selector-class">.show</span>()</span><br></pre></td></tr></table></figure><h2 id="第二部分-数据清洗"><a href="#第二部分-数据清洗" class="headerlink" title="第二部分 数据清洗"></a>第二部分 数据清洗</h2><p>数据的清理，去除含有空字段的记录。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> isnull, when, count, col</span><br><span class="line"></span><br><span class="line"><span class="comment"># |user_id|age_range|gender|merchant_id|label|</span></span><br><span class="line"><span class="comment"># 去除字段为空的异常数据</span></span><br><span class="line">df = df.filter(df.label.isNotNull()&amp;df.merchant_id.isNotNull()&amp;df.gender.isNotNull()&amp;df.age_range.isNotNull()&amp;df.user_id.isNotNull())</span><br></pre></td></tr></table></figure><p>另外还有些时候我们需要替换指定条件的。比如替换空值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用数字0替换null字段</span></span><br><span class="line">df = df.na.fill(value=<span class="number">0</span>).show()</span><br><span class="line">df = df.fillna(value=<span class="number">0</span>,subset=[<span class="string">"population"</span>]).show()</span><br></pre></td></tr></table></figure><p>对于一些不需要的列进行去除。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 去除列</span></span><br><span class="line">df = df.drop(<span class="string">'user_id'</span>)</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><p>列值转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_columns</span><span class="params">(df, col_list)</span>:</span></span><br><span class="line">indexers = [</span><br><span class="line">StringIndexer(inputCol=c, outputCol=<span class="string">f'<span class="subst">&#123;c&#125;</span>_indexed'</span>).setHandleInvalid(<span class="string">"keep"</span>)</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> col_list</span><br><span class="line">]</span><br><span class="line">encoder = OneHotEncoderEstimator(</span><br><span class="line">inputCols = [indexer.getOutputCol()) <span class="keyword">for</span> index <span class="keyword">in</span> indexers]) <span class="comment">#.setDropLast(False)</span></span><br><span class="line">newColumns = []</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> col_list:</span><br><span class="line">colMap = df.select(<span class="string">f'<span class="subst">&#123;f&#125;</span>'</span>, <span class="string">f'<span class="subst">&#123;f&#125;</span>_indexed'</span>).distinct().rdd.collectAsMap()</span><br><span class="line">colTuple = sorted( (v, <span class="string">f'<span class="subst">&#123;f&#125;</span>_<span class="subst">&#123;k&#125;</span>'</span>) <span class="keyword">for</span> k,v <span class="keyword">in</span> colMap.items())</span><br><span class="line">newColumns.append(v[<span class="number">1</span>] <span class="keyword">for</span> v <span class="keyword">in</span> colTuple)</span><br><span class="line"></span><br><span class="line">pipeline = Pipeline(stages =indexers + [encoder])</span><br><span class="line">piped_encoder = pipeline.fit(df)</span><br><span class="line">encoded_df = piped_encoder.transfrom(df)</span><br><span class="line"><span class="keyword">return</span> piped_encoder, encoded_df, newColumns</span><br><span class="line"></span><br><span class="line">df = encode_columns(df, [<span class="string">'user_id'</span>])</span><br></pre></td></tr></table></figure><p>特征数据向量化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assemble all the features with VectorAssembler</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"></span><br><span class="line">required_features = [<span class="string">'age_range'</span>,<span class="string">'gender'</span>,<span class="string">'merchant_id'</span>]</span><br><span class="line">assembler = VectorAssembler(inputCols=required_features, outputCol=<span class="string">'features'</span>)</span><br><span class="line"></span><br><span class="line">df = assembler.transform(df)</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><p>切分数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Decide on the split between training and testing data from the dataframe</span></span><br><span class="line">trainingFraction = <span class="number">0.7</span></span><br><span class="line">testingFraction = (<span class="number">1</span>-trainingFraction)</span><br><span class="line">seed = <span class="number">1234</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the dataframe into test and training dataframes</span></span><br><span class="line">training_data, test_data = df.randomSplit([trainingFraction, testingFraction], seed=seed)</span><br></pre></td></tr></table></figure><h2 id="第三部分-模型训练"><a href="#第三部分-模型训练" class="headerlink" title="第三部分 模型训练"></a>第三部分 模型训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">rf = RandomForestClassifier(labelCol=<span class="string">'label'</span>, </span><br><span class="line">                            featuresCol=<span class="string">'features'</span>,</span><br><span class="line">                            maxDepth=<span class="number">5</span>)</span><br><span class="line">model = rf.fit(training_data)</span><br><span class="line">rf_predictions = model.transform(test_data)</span><br></pre></td></tr></table></figure><h2 id="第四部分-模型预测评分"><a href="#第四部分-模型预测评分" class="headerlink" title="第四部分 模型预测评分"></a>第四部分 模型预测评分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> MulticlassClassificationEvaluator</span><br><span class="line"></span><br><span class="line">multi_evaluator = MulticlassClassificationEvaluator(labelCol = <span class="string">'label'</span>, metricName = <span class="string">'accuracy'</span>)</span><br><span class="line">print(<span class="string">'Random Forest classifier Accuracy:'</span>, multi_evaluator.evaluate(rf_predictions))</span><br></pre></td></tr></table></figure><h2 id="第五部分-模型应用"><a href="#第五部分-模型应用" class="headerlink" title="第五部分 模型应用"></a>第五部分 模型应用</h2><p>数据准备：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.<span class="keyword">write</span>.<span class="keyword">option</span>("header",<span class="keyword">True</span>).csv("/data/output.csv")</span><br><span class="line">df.<span class="keyword">write</span>.<span class="keyword">option</span>("header",<span class="keyword">True</span>).csv("hdfs://hadoop01:9000/data/output")</span><br></pre></td></tr></table></figure><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、 案例，链接：<a href="https://hackernoon.com/building-a-machine-learning-model-with-pyspark-a-step-by-step-guide-1z2d3ycd" target="_blank" rel="noopener">https://hackernoon.com/building-a-machine-learning-model-with-pyspark-a-step-by-step-guide-1z2d3ycd</a></p><p>2、案例，链接：<a href="https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-machine-learning-mllib-notebook" target="_blank" rel="noopener">https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-machine-learning-mllib-notebook</a></p><p>3、pyspark介绍，链接：<a href="https://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/" target="_blank" rel="noopener">https://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  环境依赖&lt;/li&gt;
&lt;li&gt;第二部分 交互接口&lt;/li&gt;
&lt;li&gt;第三部分 任务提交&lt;/li
      
    
    </summary>
    
      <category term="pyspark" scheme="https://zjrongxiang.github.io/categories/pyspark/"/>
    
    
      <category term="pyspark" scheme="https://zjrongxiang.github.io/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>Yarn上长任务报Token失效问题总结(Invalid AMRMToken)</title>
    <link href="https://zjrongxiang.github.io/2020/11/12/2020-11-28-ubuntu%E6%90%AD%E5%BB%BAhadoop/"/>
    <id>https://zjrongxiang.github.io/2020/11/12/2020-11-28-ubuntu搭建hadoop/</id>
    <published>2020-11-12T13:30:00.000Z</published>
    <updated>2020-12-13T05:17:49.981Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  环境依赖</li><li>第二部分 交互接口</li><li>第三部分 任务提交</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://cloud.tencent.com/developer/article/1350441" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1350441</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、 Apache Spark support，链接</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  环境依赖&lt;/li&gt;
&lt;li&gt;第二部分 交互接口&lt;/li&gt;
&lt;li&gt;第三部分 任务提交&lt;/li
      
    
    </summary>
    
      <category term="HDFS" scheme="https://zjrongxiang.github.io/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>Yarn上长任务报Token失效问题总结(Invalid AMRMToken)</title>
    <link href="https://zjrongxiang.github.io/2020/11/12/2020-11-28-Yarn%E4%B8%8A%E9%95%BF%E4%BB%BB%E5%8A%A1%E6%8A%A5Token%E5%A4%B1%E6%95%88%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93(Invalid%20AMRMToken)/"/>
    <id>https://zjrongxiang.github.io/2020/11/12/2020-11-28-Yarn上长任务报Token失效问题总结(Invalid AMRMToken)/</id>
    <published>2020-11-12T13:30:00.000Z</published>
    <updated>2020-11-30T11:50:05.121Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  环境依赖</li><li>第二部分 交互接口</li><li>第三部分 任务提交</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://www.shuzhiduo.com/A/xl5600X0zr/" target="_blank" rel="noopener">https://www.shuzhiduo.com/A/xl5600X0zr/</a></p><p><a href="http://flume.cn/2016/11/24/Spark%E8%B8%A9%E5%9D%91%E4%B9%8BStreaming%E5%9C%A8Kerberos%E7%9A%84hadoop%E4%B8%ADrenew%E5%A4%B1%E8%B4%A5/" target="_blank" rel="noopener">http://flume.cn/2016/11/24/Spark%E8%B8%A9%E5%9D%91%E4%B9%8BStreaming%E5%9C%A8Kerberos%E7%9A%84hadoop%E4%B8%ADrenew%E5%A4%B1%E8%B4%A5/</a></p><p><a href="http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/" target="_blank" rel="noopener">http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/</a></p><p><a href="https://www.shuzhiduo.com/A/xl5600X0zr/" target="_blank" rel="noopener">https://www.shuzhiduo.com/A/xl5600X0zr/</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、 Apache Spark support，链接</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  环境依赖&lt;/li&gt;
&lt;li&gt;第二部分 交互接口&lt;/li&gt;
&lt;li&gt;第三部分 任务提交&lt;/li
      
    
    </summary>
    
      <category term="HDFS" scheme="https://zjrongxiang.github.io/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>Yarn上长任务报Token失效问题总结(Invalid AMRMToken)</title>
    <link href="https://zjrongxiang.github.io/2020/11/12/2020-12-11-%E5%A6%82%E4%BD%95%E4%BB%8Ehive%E8%A1%A8%E4%B8%AD%E5%88%A0%E9%99%A4%E6%95%B0%E6%8D%AE/"/>
    <id>https://zjrongxiang.github.io/2020/11/12/2020-12-11-如何从hive表中删除数据/</id>
    <published>2020-11-12T13:30:00.000Z</published>
    <updated>2020-12-11T06:01:30.987Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  环境依赖</li><li>第二部分 交互接口</li><li>第三部分 任务提交</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>\4. hive 从非分区表插入数据到分区表时出错：</p><p>Cannot insert into target table because column number/types are different ‘’分区名’’: Table insclause-0 has 2 columns, but query has 3 columns.</p><p>首先解释一下这个错误：因为hive的分区列是作为元信息存放在mysql中的，<strong>他们并不在数据文件中</strong>，相反他们以子目录的名字被使用，因此你的<strong>分区表实际含有的数据列，注意是数据列是不包含分区列的</strong>，所以在你向分区表插入数据时，不能插入分区列；</p><p>举个简单的例子：体育课上站队时，老师经常让男生、女生各站一队，你觉得有必要再给他们每一个人加上一个性别的标签吗？</p><p>下面是stackoverflow上大神关于这个问题的解释：</p><p> <strong>*\</strong>*In Hive the partitioning “columns” are managed as** **metadata** **&gt;&gt; they are not included in the data files, instead they are used as sub-directory names. So your partitioned table has just 2 real columns, and you must feed just 2 columns with your SELECT.**<em>**</em></p><p>比如你的非分区表non_part的内容如下：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">id</span><span class="string">namesex</span></span><br><span class="line"><span class="attr">1</span><span class="string">tom M</span></span><br><span class="line"><span class="attr">2</span><span class="string">maryF</span></span><br></pre></td></tr></table></figure><p><strong>假如你的分区表part是以sex来分区的，当你想把以上非分区表中的数据插入到分区表中：</strong></p><p><strong>你应该：insert into table part partition(sex=’M’) select id,name from non_part where sex=’M’;（对，你要插入的就是两列，分区列不作为数据保存在数据表中）</strong></p><p><strong><em>\</em>而不是\</strong>*<em>：****insert into table part partition(sex=’M’) select  \</em>  from non_part where sex=’M’;****</p><p>另外需要注意的地方：</p><p>1.从非分区表插入数据到分区表时hive会将HiveQL转换为MR来执行的,官方提示Hive-on-MR在将来可能不再被支持；</p><p>2.只要是向分区表内装数据，无论是load还是insert都要<strong>在表名后指明分区名</strong>；而且load时，会将你要load的文件内的所有内容放在指定的分区下；</p><p><a href="https://www.cnblogs.com/lemonu/p/11279979.html" target="_blank" rel="noopener">https://www.cnblogs.com/lemonu/p/11279979.html</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、 Apache Spark support，链接</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  环境依赖&lt;/li&gt;
&lt;li&gt;第二部分 交互接口&lt;/li&gt;
&lt;li&gt;第三部分 任务提交&lt;/li
      
    
    </summary>
    
      <category term="HDFS" scheme="https://zjrongxiang.github.io/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>Unix和Windows中文本行末结束符</title>
    <link href="https://zjrongxiang.github.io/2020/11/12/2020-11-12-Unix%E5%92%8CWindows%E4%B8%AD%E6%96%87%E6%9C%AC%E8%A1%8C%E6%9C%AB%E7%BB%93%E6%9D%9F%E7%AC%A6/"/>
    <id>https://zjrongxiang.github.io/2020/11/12/2020-11-12-Unix和Windows中文本行末结束符/</id>
    <published>2020-11-12T13:30:00.000Z</published>
    <updated>2020-12-12T13:16:46.766Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  回车和换行</li><li>第二部分 兼容性问题解决</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>或许你遇到过这样的坑。当你信心满满将自己编写的程序文件或配置文件上传到生产环境（<code>linux</code>），却发现无法运行或者生效。但是明明在本地（<code>Windows</code>）测试运行没有问题。那么很大几率遇到文本行末结束符的坑。</p><p>Unix和Windows中文本行末结束符是不同的。本文将详细讲解这个坑的背景、产生原因和解决办法。</p><h2 id="第一部分-回车和换行"><a href="#第一部分-回车和换行" class="headerlink" title="第一部分 回车和换行"></a>第一部分 回车和换行</h2><h3 id="1-1-历史背景"><a href="#1-1-历史背景" class="headerlink" title="1.1 历史背景"></a>1.1 历史背景</h3><p>关于回车（Carriage Return）和换行（Line Feed）的来历，需要从英文打字机讲起。在机械英文打字机中有个叫“字车”的部件，每打印一个英文字符，“字车”前进一位。但是纸张每行是有限制的，打满一行后，“字车”需要重置回到起始位置。从机械角度看，打印机有两个响应动作：（1）“字车”归位；（2）纸张的滚筒上卷一行，开始新的一行。这里“字车”归位就是“回车”，而滚筒上卷一行就是“换行”。</p><p>到了电传打印机时候，需要使用控制字符来通知打印机执行非打印操作的指令（回车（CR）和换行（NL））。而在ASCII码中分别使用<code>\r</code>（值13）和<code>\n</code>（值10）表示。</p><p>再后来计算机发明后，两个概念也就被搬到计算机中（计算机通常需要和打印机交互，保持兼容性）。考虑到当时存储资源的昂贵，就有人提出来文本中使用两个操作符表示行末结束较为浪费，于是分歧就产生了。</p><h3 id="1-2-分歧"><a href="#1-2-分歧" class="headerlink" title="1.2 分歧"></a>1.2 分歧</h3><p>目前主流操作系统（Unix、Windows、Mac）中分歧如下：</p><table><thead><tr><th>操作系统</th><th>系统行末结束符</th><th>备注</th></tr></thead><tbody><tr><td>UNIX</td><td>\n</td><td></td></tr><tr><td>window</td><td>\n\r</td><td></td></tr><tr><td>MAC OS</td><td>\n</td><td><code>v9</code> 之前 Mac OS 用 ‘\r’</td></tr></tbody></table><blockquote><p>注：从2001年3月发布的Mac OS 10.0开始，系统行末结束符采用”\n”。</p></blockquote><p>这种分歧就导致不同操作系统之间兼容问题。</p><h2 id="第二部分-兼容性问题解决"><a href="#第二部分-兼容性问题解决" class="headerlink" title="第二部分 兼容性问题解决"></a>第二部分 兼容性问题解决</h2><p>这种兼容问题通常发生在：不同操作系统之间传输纯文本文件。</p><ul><li>Unix/Mac系统创建的文件在Windows里打开，文字会变成一行。因为没有<code>\r</code>。</li><li>Windows里的文件在Unix/Mac下打开的话，在每行会多出一个<code>^M</code>符号。多了<code>\r</code>。</li></ul><h3 id="2-1-Windows文件上传Linux问题"><a href="#2-1-Windows文件上传Linux问题" class="headerlink" title="2.1 Windows文件上传Linux问题"></a>2.1 Windows文件上传Linux问题</h3><p>我们经常遇到的问题是：Windows下编写的Shell脚本或者Python脚本，放到Linux下执行会出错。通常上传文件前，使用<code>UE</code>（<code>Ultraedit</code>）或者<code>Nodepad++</code>来转换。</p><ul><li><p><code>UE</code>中，执行“File-&gt;conversions-&gt;Dos to Unix”，将文件中<code>\n\r</code>转换成<code>\n</code>。</p></li><li><p><code>Nodepad++</code>中，执行“编辑-&gt;档案格式转换-&gt;转换为UNIX格式”。</p></li></ul><p>上传到Linux后还可以使用下面的命令查看是否准确：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat -v test.txt</span><br></pre></td></tr></table></figure><p>如果每行换行处都有<code>^M</code>，这说明仍然是Windows下的文本文件。</p><blockquote><p>注：cat -A<code>命令：显示不可见字符。如换行符显示为“$”，TAB 显示为</code>^I<code>等。在这种模式下，回车（</code>\r<code>）字符将显示为</code>^M</p></blockquote><p>可以使用下面的命令进行统一替换：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i 's/^M$//g' test.txt</span><br></pre></td></tr></table></figure><h3 id="2-2-Linux文件上传Windows问题"><a href="#2-2-Linux文件上传Windows问题" class="headerlink" title="2.2 Linux文件上传Windows问题"></a>2.2 Linux文件上传Windows问题</h3><p>在Windows中使用<code>Ultraedit</code>或者<code>Nodepad++</code>文本编辑器查看Linux文件，而不是系统自带的记事本。</p><h3 id="2-3-FTP中传输问题"><a href="#2-3-FTP中传输问题" class="headerlink" title="2.3 FTP中传输问题"></a>2.3 FTP中传输问题</h3><p>FTP软件在传输文件的时候，通常有两种模式：文本模式（ASCII模式）和二进制模式（BINARY模式）。两种模式的区别就是行末结束符的处理，BINARY模式不会对数据进行任何处理。而ASCII模式将行末结束符转换为本机操作系统的行末结束符。例如Windows系统将文本文件上传至Linux，就会将<code>\r\n</code>替换成<code>\n</code>。</p><p>在使用过程中需要注意两种模式的差别。特别的上实际生产环境上线投产过程中建议统一使用二进制模式，避免FTP对文件进行转换。</p><h3 id="2-4-编程语言中"><a href="#2-4-编程语言中" class="headerlink" title="2.4 编程语言中"></a>2.4 编程语言中</h3><ul><li>Python 使用 “<a href="http://www.python.org/dev/peps/pep-0278/" target="_blank" rel="noopener">Universal Newline</a>“ 处理这个问题。文本使用 open() 方法打开时，会对行末结束符进行识别并一致处理成 ‘\n’，在文件写入的时候，使用 write(‘\n’) 即可，Python 会根据当前程序执行的操作系统自动处理。</li></ul><ul><li>Java中行末结束符使用下面的函数方法统一处理：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">System.getProperty(<span class="string">"line.separator"</span>)</span><br></pre></td></tr></table></figure><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、Line_feed，链接：<a href="https://nl.wikipedia.org/wiki/Line_feed" target="_blank" rel="noopener">https://nl.wikipedia.org/wiki/Line_feed</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  回车和换行&lt;/li&gt;
&lt;li&gt;第二部分 兼容性问题解决&lt;/li&gt;
&lt;li&gt;参考文献及资料&lt;/
      
    
    </summary>
    
      <category term="Linux" scheme="https://zjrongxiang.github.io/categories/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>HDFS Federation架构介绍</title>
    <link href="https://zjrongxiang.github.io/2020/11/12/2020-11-28-HDFS%20Federation%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D/"/>
    <id>https://zjrongxiang.github.io/2020/11/12/2020-11-28-HDFS Federation架构介绍/</id>
    <published>2020-11-12T13:30:00.000Z</published>
    <updated>2020-11-28T03:12:51.905Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  环境依赖</li><li>第二部分 交互接口</li><li>第三部分 任务提交</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="http://dongxicheng.org/mapreduce/hdfs-federation-introduction/" target="_blank" rel="noopener">http://dongxicheng.org/mapreduce/hdfs-federation-introduction/</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、 Apache Spark support，链接：<a href="http://dongxicheng.org/mapreduce/hdfs-federation-introduction/" target="_blank" rel="noopener">http://dongxicheng.org/mapreduce/hdfs-federation-introduction/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  环境依赖&lt;/li&gt;
&lt;li&gt;第二部分 交互接口&lt;/li&gt;
&lt;li&gt;第三部分 任务提交&lt;/li
      
    
    </summary>
    
      <category term="HDFS" scheme="https://zjrongxiang.github.io/categories/HDFS/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark和oracle交互总结</title>
    <link href="https://zjrongxiang.github.io/2020/11/05/2020-11-05-Spark%E5%92%8Coracle%E4%BA%A4%E4%BA%92%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2020/11/05/2020-11-05-Spark和oracle交互总结/</id>
    <published>2020-11-05T13:30:00.000Z</published>
    <updated>2020-11-09T12:09:37.799Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  环境依赖</li><li>第二部分 交互接口</li><li>第三部分 任务提交</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://issues.apache.org/jira/browse/SPARK-10909" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/SPARK-10909</a></p><ol><li><a href="https://issues.apache.org/jira/browse/SPARK" target="_blank" rel="noopener">Spark</a></li><li><a href="https://issues.apache.org/jira/browse/SPARK-10909" target="_blank" rel="noopener">SPARK-10909</a></li></ol><p>Spark sql jdbc fails for Oracle NUMBER type columns</p><p><a href="https://blog.csdn.net/cuichunchi/article/details/107838633" target="_blank" rel="noopener">https://blog.csdn.net/cuichunchi/article/details/107838633</a></p><p><a href="https://stackoverflow.com/questions/34067124/the-java-lang-illegalargumentexception-requirement-failed-overflowed-precisio" target="_blank" rel="noopener">https://stackoverflow.com/questions/34067124/the-java-lang-illegalargumentexception-requirement-failed-overflowed-precisio</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、 Apache Spark support，链接：<a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html</a></p><p>2、elasticsearch-hadoop项目，链接：<a href="https://github.com/elastic/elasticsearch-hadoop" target="_blank" rel="noopener">https://github.com/elastic/elasticsearch-hadoop</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  环境依赖&lt;/li&gt;
&lt;li&gt;第二部分 交互接口&lt;/li&gt;
&lt;li&gt;第三部分 任务提交&lt;/li
      
    
    </summary>
    
      <category term="Spark" scheme="https://zjrongxiang.github.io/categories/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive的分区和分桶总结</title>
    <link href="https://zjrongxiang.github.io/2020/11/05/2020-11-05-Hive%E7%9A%84%E5%88%86%E5%8C%BA%E5%92%8C%E5%88%86%E6%A1%B6%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2020/11/05/2020-11-05-Hive的分区和分桶总结/</id>
    <published>2020-11-05T13:30:00.000Z</published>
    <updated>2020-11-05T11:58:53.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  环境依赖</li><li>第二部分 交互接口</li><li>第三部分 任务提交</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://data-flair.training/blogs/hive-partitioning-vs-bucketing/" target="_blank" rel="noopener">https://data-flair.training/blogs/hive-partitioning-vs-bucketing/</a></p><p><a href="https://data-flair.training/blogs/hive-partitioning-vs-bucketing/" target="_blank" rel="noopener">https://data-flair.training/blogs/hive-partitioning-vs-bucketing/</a></p><p><a href="https://blog.csdn.net/whdxjbw/article/details/82219022" target="_blank" rel="noopener">https://blog.csdn.net/whdxjbw/article/details/82219022</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、 Apache Spark support，链接：<a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html</a></p><p>2、elasticsearch-hadoop项目，链接：<a href="https://github.com/elastic/elasticsearch-hadoop" target="_blank" rel="noopener">https://github.com/elastic/elasticsearch-hadoop</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  环境依赖&lt;/li&gt;
&lt;li&gt;第二部分 交互接口&lt;/li&gt;
&lt;li&gt;第三部分 任务提交&lt;/li
      
    
    </summary>
    
      <category term="Hive" scheme="https://zjrongxiang.github.io/categories/Hive/"/>
    
    
  </entry>
  
  <entry>
    <title>云服务器部署WordPress介绍</title>
    <link href="https://zjrongxiang.github.io/2020/10/31/2020-10-31-%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2WordPress%E4%BB%8B%E7%BB%8D/"/>
    <id>https://zjrongxiang.github.io/2020/10/31/2020-10-31-云服务器部署WordPress介绍/</id>
    <published>2020-10-31T13:30:00.000Z</published>
    <updated>2020-11-01T05:12:24.885Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  环境依赖</li><li>第二部分 交互接口</li><li>第三部分 任务提交</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h2 id="第一部分-环境部署"><a href="#第一部分-环境部署" class="headerlink" title="第一部分 环境部署"></a>第一部分 环境部署</h2><p>笔者购买了腾讯云的云主机，操作系统环境为：<code>Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-88-generic x86_64)</code>。主机具有互联网环境，所以依赖组件的安装将依赖在线安装。安装用户使用<code>root</code>用户。首先更新操作系统：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get update</span><br></pre></td></tr></table></figure><h3 id="1-1-部署Apache环境"><a href="#1-1-部署Apache环境" class="headerlink" title="1.1 部署Apache环境"></a>1.1 部署<code>Apache</code>环境</h3><p>使用apt安装：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt <span class="keyword">install</span> apache2</span><br></pre></td></tr></table></figure><p>使用下面的命令检查进程状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">root@VM-0-5-ubuntu:/apphome# systemctl status apache2</span><br><span class="line">● apache2.service - The Apache HTTP Server</span><br><span class="line">   Loaded: loaded (/lib/systemd/system/apache2.service; enabled; vendor preset: enabled)</span><br><span class="line">  Drop-In: /lib/systemd/system/apache2.service.d</span><br><span class="line">           └─apache2-systemd.conf</span><br><span class="line">   Active: active (running) since Sun 2020-11-01 12:26:24 CST; 1min 9s ago</span><br><span class="line"> Main PID: 704 (apache2)</span><br><span class="line">    Tasks: 55 (limit: 2122)</span><br><span class="line">   CGroup: /system.slice/apache2.service</span><br><span class="line">           ├─704 /usr/sbin/apache2 -k start</span><br><span class="line">           ├─706 /usr/sbin/apache2 -k start</span><br><span class="line">           └─707 /usr/sbin/apache2 -k start</span><br><span class="line"></span><br><span class="line">Nov 01 12:26:23 VM-0-5-ubuntu systemd[1]: Starting The Apache HTTP Server...</span><br><span class="line">Nov 01 12:26:24 VM-0-5-ubuntu systemd[1]: Started The Apache HTTP Server.</span><br></pre></td></tr></table></figure><p>浏览器地址栏：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://主机公网IP/</span><br></pre></td></tr></table></figure><p>可以看到默认的Apache 欢迎页面。</p><h3 id="1-2-部署Mysql环境"><a href="#1-2-部署Mysql环境" class="headerlink" title="1.2 部署Mysql环境"></a>1.2 部署<code>Mysql</code>环境</h3><h3 id="1-3-部署PHP环境"><a href="#1-3-部署PHP环境" class="headerlink" title="1.3 部署PHP环境"></a>1.3 部署<code>PHP</code>环境</h3><h2 id="第二部分-部署和配置WordPress"><a href="#第二部分-部署和配置WordPress" class="headerlink" title="第二部分 部署和配置WordPress"></a>第二部分 部署和配置<code>WordPress</code></h2><p><a href="https://www.cnblogs.com/jiangfeilong/p/11142181.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiangfeilong/p/11142181.html</a></p><p><a href="https://cloud.tencent.com/developer/article/1627432#:~:text=%20%E5%A6%82%E4%BD%95%E5%9C%A8%20Ubuntu%2020.04%20%E4%B8%8A%E5%AE%89%E8%A3%85%20Apache%20%201,5%20%E4%BA%94%E3%80%81%E8%AE%BE%E7%BD%AE%E4%B8%80%206%20%E8%99%9A%E6%8B%9F%E4%B8%BB%E6%9C%BA%207%20%E5%85%AD%E3%80%81%E6%80%BB%E7%BB%93%20More%20" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1627432#:~:text=%20%E5%A6%82%E4%BD%95%E5%9C%A8%20Ubuntu%2020.04%20%E4%B8%8A%E5%AE%89%E8%A3%85%20Apache%20%201,5%20%E4%BA%94%E3%80%81%E8%AE%BE%E7%BD%AE%E4%B8%80%206%20%E8%99%9A%E6%8B%9F%E4%B8%BB%E6%9C%BA%207%20%E5%85%AD%E3%80%81%E6%80%BB%E7%BB%93%20More%20</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、 Apache Spark support，链接：<a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  环境依赖&lt;/li&gt;
&lt;li&gt;第二部分 交互接口&lt;/li&gt;
&lt;li&gt;第三部分 任务提交&lt;/li
      
    
    </summary>
    
      <category term="WordPress" scheme="https://zjrongxiang.github.io/categories/WordPress/"/>
    
    
  </entry>
  
  <entry>
    <title>Hive的Metastore介绍</title>
    <link href="https://zjrongxiang.github.io/2020/10/27/2020-10-27-Hive%E7%9A%84Metastore%E4%BB%8B%E7%BB%8D/"/>
    <id>https://zjrongxiang.github.io/2020/10/27/2020-10-27-Hive的Metastore介绍/</id>
    <published>2020-10-27T13:30:00.000Z</published>
    <updated>2020-10-27T13:44:11.611Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  环境依赖</li><li>第二部分 交互接口</li><li>第三部分 任务提交</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Hive Metastore，也称为HCatalog，是一个关系数据库存储库，其中包含有关您在Hive中创建的对象的元数据。创建Hive表时，表定义（列名，数据类型，注释等）存储在Hive Metastore中。这是自动的，只是Hive架构的一部分。Hive Metastore之所以如此重要，是因为它充当中央架构存储库，可供其他访问工具（如Spark和Pig）使用。此外，通过Hiveserver2，您可以使用ODBC和JDBC连接访问Hive Metastore。这将为可视化工具（如PowerBi或Tableau）打开架构。</p><p><a href="https://bbs.huaweicloud.com/forum/viewthreaduni-66881-filter-reply-orderby-lastpost-page-7-1.html" target="_blank" rel="noopener">https://bbs.huaweicloud.com/forum/viewthreaduni-66881-filter-reply-orderby-lastpost-page-7-1.html</a></p><p><a href="https://blog.csdn.net/lalaguozhe/article/details/9070203" target="_blank" rel="noopener">https://blog.csdn.net/lalaguozhe/article/details/9070203</a></p><p><a href="https://www.infoq.cn/article/lXJisUVTgOjgHzRMSIBW" target="_blank" rel="noopener">https://www.infoq.cn/article/lXJisUVTgOjgHzRMSIBW</a></p><p><a href="https://www.codeobj.com/2019/01/hive-metastore%e5%b8%b8%e7%94%a8%e7%9a%84%e5%85%83%e6%95%b0%e6%8d%ae%e5%9c%a8mysql%e4%b8%ad%e5%af%b9%e5%ba%94%e7%9a%84%e8%a1%a8/" target="_blank" rel="noopener">https://www.codeobj.com/2019/01/hive-metastore%e5%b8%b8%e7%94%a8%e7%9a%84%e5%85%83%e6%95%b0%e6%8d%ae%e5%9c%a8mysql%e4%b8%ad%e5%af%b9%e5%ba%94%e7%9a%84%e8%a1%a8/</a></p><p>Hive Metastore Federation 在滴滴的实践</p><p><a href="https://blog.didiyun.com/index.php/2019/03/25/hive-metastore-federation/" target="_blank" rel="noopener">https://blog.didiyun.com/index.php/2019/03/25/hive-metastore-federation/</a></p><p>网易杭研大数据实践：Apache Hive稳定性测试</p><p><a href="https://dun.163.com/news/p/83abc4931c1349b086c73dfbad0fb57f" target="_blank" rel="noopener">https://dun.163.com/news/p/83abc4931c1349b086c73dfbad0fb57f</a></p><p>如何使用带有大量SPARK分区的HIVE表</p><p><a href="https://andr83.io/en/1090/" target="_blank" rel="noopener">https://andr83.io/en/1090/</a></p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、 Apache Spark support，链接：<a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html</a></p><p>2、elasticsearch-hadoop项目，链接：<a href="https://github.com/elastic/elasticsearch-hadoop" target="_blank" rel="noopener">https://github.com/elastic/elasticsearch-hadoop</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  环境依赖&lt;/li&gt;
&lt;li&gt;第二部分 交互接口&lt;/li&gt;
&lt;li&gt;第三部分 任务提交&lt;/li
      
    
    </summary>
    
      <category term="Hive" scheme="https://zjrongxiang.github.io/categories/Hive/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark和Elasticsearch交互实践总结</title>
    <link href="https://zjrongxiang.github.io/2020/10/13/2020-10-13-Spark%E5%92%8CElasticsearch%E4%BA%A4%E4%BA%92%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93/"/>
    <id>https://zjrongxiang.github.io/2020/10/13/2020-10-13-Spark和Elasticsearch交互实践总结/</id>
    <published>2020-10-13T13:30:00.000Z</published>
    <updated>2020-10-29T04:52:05.348Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  环境依赖</li><li>第二部分 交互接口</li><li>第三部分 任务提交</li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>为了更好的支持Spark应用和<code>Elasticsearch</code>交互，<code>Elasticsearch</code>官方推出了<code>elasticsearch-hadoop</code>项目。本文将详细介绍Spark Java应用和<code>Elasticsearch</code>的交互细节。</p><h2 id="第一部分-环境依赖"><a href="#第一部分-环境依赖" class="headerlink" title="第一部分 环境依赖"></a>第一部分 环境依赖</h2><h3 id="1-1-配置Maven依赖"><a href="#1-1-配置Maven依赖" class="headerlink" title="1.1 配置Maven依赖"></a>1.1 配置Maven依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.elasticsearch<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>elasticsearch-spark-13_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>6.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>需要注意Spark版本和<code>elasticsearch-hadoop</code>版本的兼容性，参考版本对照表：</p><table><thead><tr><th><code>Spark Version</code></th><th><code>Scala Version</code></th><th><code>ES-Hadoop Artifact ID</code></th></tr></thead><tbody><tr><td>1.0 - 1.2</td><td>2.10</td><td><unsupported></unsupported></td></tr><tr><td>1.0 - 1.2</td><td>2.11</td><td><unsupported></unsupported></td></tr><tr><td>1.3 - 1.6</td><td>2.10</td><td><code>elasticsearch-spark-13_2.10</code></td></tr><tr><td>1.3 - 1.6</td><td>2.11</td><td><code>elasticsearch-spark-13_2.11</code></td></tr><tr><td>2.0+</td><td>2.10</td><td><code>elasticsearch-spark-20_2.10</code></td></tr><tr><td>2.0+</td><td>2.11</td><td><code>elasticsearch-spark-20_2.11</code></td></tr></tbody></table><h3 id="1-2-Spark配置"><a href="#1-2-Spark配置" class="headerlink" title="1.2 Spark配置"></a>1.2 Spark配置</h3><p>关于<code>elasticsearch</code>集群的交互配置，定义在<code>SparkConf</code>中，例如下面的案例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"></span><br><span class="line">SparkConf sparkConf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"JavaSpark"</span>).setMaster(<span class="string">"local"</span>);</span><br><span class="line"><span class="comment">//config elasticsearch</span></span><br><span class="line">sparkConf.set(<span class="string">"es.nodes"</span>,<span class="string">"192.168.31.3:9200"</span>);</span><br><span class="line">sparkConf.set(<span class="string">"es.port"</span>,<span class="string">"9200"</span>);</span><br><span class="line">sparkConf.set(<span class="string">"es.index.auto.create"</span>,<span class="string">"true"</span>);</span><br><span class="line"></span><br><span class="line">JavaSparkContext jsc = <span class="keyword">new</span> JavaSparkContext(sparkConf);</span><br></pre></td></tr></table></figure><ul><li><code>es.nodes</code>，集群节点；</li><li><code>es.port</code>，服务端口；</li><li><code>es.index.auto.create</code>，参数指定<code>index</code>是否自动创建；</li></ul><p>其他配置参考官方文档：<a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html</a></p><h2 id="第二部分-交互接口"><a href="#第二部分-交互接口" class="headerlink" title="第二部分 交互接口"></a>第二部分 交互接口</h2><h3 id="2-1-自定义id的写入"><a href="#2-1-自定义id的写入" class="headerlink" title="2.1 自定义id的写入"></a>2.1 自定义id的写入</h3><p>在业务数据写入<code>elasticsearch</code>集群的时候，需要数据去重。这时候就需要自己指定元数据字段中的<code>_id</code>。<code>elasticsearch</code>在处理<code>_id</code>相同的数据时，会覆盖写入。例如下面的例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.spark.rdd.Metadata;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.spark.rdd.api.java.JavaEsSpark;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.elasticsearch.spark.rdd.Metadata.ID;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">      ArrayList&lt;Tuple2&lt;HashMap,HashMap&gt;&gt; metaList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    </span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">100</span>;i++) &#123;</span><br><span class="line">         HashMap&lt;String, String&gt; map = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span><br><span class="line">         map.put(<span class="string">"id"</span>, String.valueOf(i));</span><br><span class="line">         map.put(<span class="string">"name"</span>, <span class="string">"one"</span>);</span><br><span class="line"></span><br><span class="line">         HashMap&lt;Metadata, String&gt; metamap = <span class="keyword">new</span> HashMap&lt;Metadata, String&gt;();</span><br><span class="line">         metamap.put(ID, String.valueOf(i));</span><br><span class="line"></span><br><span class="line">         metaList.add(<span class="keyword">new</span> Tuple2(metamap, map));</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">      JavaPairRDD&lt;?, ?&gt; pairRdd = jsc.parallelizePairs(metaList);</span><br><span class="line">      JavaEsSpark.saveToEsWithMeta(pairRdd,<span class="string">"spark/doc"</span>);</span><br><span class="line"></span><br><span class="line">     &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">        System.out.println(<span class="string">"finish!"</span>);</span><br><span class="line">        jsc.stop();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>例子中我们使用<code>ArrayList&lt;Tuple2&lt;HashMap,HashMap&gt;&gt;</code>数据结构来存储待写入的数据，然后构造<code>RDD</code>，最后使用<code>JavaEsSpark.saveToEsWithMeta</code>方法写入。需要注意这里构造的两个<code>HashMap</code>:</p><ul><li>数据<code>HashMap</code>，数据结构为：<code>HashMap&lt;String, String&gt;</code>，用于存储数据键值对。</li><li>元数据<code>HashMap</code>，数据结构为：<code>HashMap&lt;Metadata, String&gt;</code>，用于存储元数据键值对。例如<code>ID</code>即为<code>_id</code>。</li></ul><p>其他类型读写可以参考官方网站：<a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html</a></p><h2 id="第三部分-任务提交"><a href="#第三部分-任务提交" class="headerlink" title="第三部分 任务提交"></a>第三部分 任务提交</h2><p>最后编译运行。主要是<code>setMaster()</code>指定运行方式，分为如下几种。</p><table><thead><tr><th>运行模式</th><th>说明</th></tr></thead><tbody><tr><td><code>local</code></td><td>Run Spark locally with one worker thread (i.e. no parallelism at all).</td></tr><tr><td><code>local[K]</code></td><td>Run Spark locally with K worker threads (ideally, set this to the number of cores on your machine).</td></tr><tr><td><code>local[*]</code></td><td>Run Spark locally with as many worker threads as logical cores on your machine.</td></tr><tr><td><code>spark://HOST:PORT</code></td><td>Connect to the given <a href="http://spark.apache.org/docs/1.6.3/spark-standalone.html" target="_blank" rel="noopener">Spark standalone cluster</a> master. The port must be whichever one your master is configured to use, which is 7077 by default.</td></tr><tr><td><code>mesos://HOST:PORT</code></td><td>Connect to the given <a href="http://spark.apache.org/docs/1.6.3/running-on-mesos.html" target="_blank" rel="noopener">Mesos</a> cluster. The port must be whichever one your is configured to use, which is 5050 by default. Or, for a Mesos cluster using ZooKeeper, use <code>mesos://zk://...</code>. To submit with <code>--deploy-mode cluster</code>, the HOST:PORT should be configured to connect to the <a href="http://spark.apache.org/docs/1.6.3/running-on-mesos.html#cluster-mode" target="_blank" rel="noopener">MesosClusterDispatcher</a>.</td></tr><tr><td><code>yarn</code></td><td>Connect to a <a href="http://spark.apache.org/docs/1.6.3/running-on-yarn.html" target="_blank" rel="noopener">YARN </a>cluster in <code>client</code> or <code>cluster</code> mode depending on the value of <code>--deploy-mode</code>. The cluster location will be found based on the <code>HADOOP_CONF_DIR</code> or <code>YARN_CONF_DIR</code> variable.</td></tr><tr><td><code>yarn-client</code></td><td>Equivalent to <code>yarn</code> with <code>--deploy-mode client</code>, which is preferred to <code>yarn-client</code></td></tr><tr><td><code>yarn-cluster</code></td><td>Equivalent to <code>yarn</code> with <code>--deploy-mode cluster</code>, which is preferred to <code>yarn-cluster</code></td></tr></tbody></table><p>除了在eclipse、Intellij中运行local模式的任务，也可以打成jar包，使用spark-submit来进行任务提交。</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、 Apache Spark support，链接：<a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html</a></p><p>2、elasticsearch-hadoop项目，链接：<a href="https://github.com/elastic/elasticsearch-hadoop" target="_blank" rel="noopener">https://github.com/elastic/elasticsearch-hadoop</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  环境依赖&lt;/li&gt;
&lt;li&gt;第二部分 交互接口&lt;/li&gt;
&lt;li&gt;第三部分 任务提交&lt;/li
      
    
    </summary>
    
      <category term="Spark" scheme="https://zjrongxiang.github.io/categories/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Python中局部函数</title>
    <link href="https://zjrongxiang.github.io/2020/10/10/2020-12-13-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Python%E4%B8%AD%E5%B1%80%E9%83%A8%E5%87%BD%E6%95%B0/"/>
    <id>https://zjrongxiang.github.io/2020/10/10/2020-12-13-Python系列文章-Python中局部函数/</id>
    <published>2020-10-10T05:30:00.000Z</published>
    <updated>2020-12-13T06:46:35.233Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>背景</li><li>第一部分  </li><li>第二部分 </li><li>参考文献及资料</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>通过前面的学习我们知道，<a href="http://c.biancheng.net/python/" target="_blank" rel="noopener">Python</a> 函数内部可以定义变量，这样就产生了局部变量，有读者可能会问，Python 函数内部能定义函数吗？答案是肯定的。Python 支持在函数内部定义函数，此类函数又称为局部函数。</p><p>那么，局部函数有哪些特征，在使用时需要注意什么呢？接下来就给读者详细介绍 Python 局部函数的用法。</p><p>首先，和局部变量一样，默认情况下局部函数只能在其所在函数的作用域内使用。举个例子：</p><figure class="highlight flix"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#全局函数<span class="function"><span class="keyword">def</span> <span class="title">outdef</span> </span>():    #局部函数    <span class="function"><span class="keyword">def</span> <span class="title">indef</span></span>():        print(<span class="string">"http://c.biancheng.net/python/"</span>)    #调用局部函数    indef()#调用全局函数outdef()</span><br></pre></td></tr></table></figure><p>程序执行结果为：</p><p><a href="http://c.biancheng.net/python/" target="_blank" rel="noopener">http://c.biancheng.net/python/</a></p><p>就如同全局函数返回其局部变量，就可以扩大该变量的作用域一样，通过将局部函数作为所在函数的返回值，也可以扩大局部函数的使用范围。例如，修改上面程序为：</p><figure class="highlight flix"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#全局函数<span class="function"><span class="keyword">def</span> <span class="title">outdef</span> </span>():    #局部函数    <span class="function"><span class="keyword">def</span> <span class="title">indef</span></span>():        print(<span class="string">"调用局部函数"</span>)    #调用局部函数    return indef#调用全局函数new_indef = outdef()调用全局函数中的局部函数new_indef()</span><br></pre></td></tr></table></figure><p>程序执行结果为：</p><p>调用局部函数</p><p>因此，对于局部函数的作用域，可以总结为：如果所在函数没有返回局部函数，则局部函数的可用范围仅限于所在函数内部；反之，如果所在函数将局部函数作为返回值，则局部函数的作用域就会扩大，既可以在所在函数内部使用，也可以在所在函数的作用域中使用。</p><p>以上面程序中的 outdef() 和 indef() 为例，如果 outdef() 不将 indef 作为返回值，则 indef() 只能在 outdef() 函数内部使用；反之，则 indef() 函数既可以在 outdef() 函数内部使用，也可以在 outdef() 函数的作用域，也就是全局范围内使用。</p><blockquote><p>有关函数返回函数，更详细的讲解，可阅读《<a href="http://c.biancheng.net/view/2261.html" target="_blank" rel="noopener">Python函数高级方法</a>》一节。</p></blockquote><p>另外值得一提的是，如果局部函数中定义有和所在函数中变量同名的变量，也会发生“遮蔽”的问题。例如：</p><figure class="highlight flix"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#全局函数<span class="function"><span class="keyword">def</span> <span class="title">outdef</span> </span>():    name = <span class="string">"所在函数中定义的 name 变量"</span>    #局部函数    <span class="function"><span class="keyword">def</span> <span class="title">indef</span></span>():        print(name)        name = <span class="string">"局部函数中定义的 name 变量"</span>    indef()#调用全局函数outdef()</span><br></pre></td></tr></table></figure><p>执行此程序，Python 解释器会报如下错误：</p><p>UnboundLocalError: local variable ‘name’ referenced before assignment</p><p>此错误直译过来的意思是“局部变量 name 还没定义就使用”。导致该错误的原因就在于，局部函数 indef() 中定义的 name 变量遮蔽了所在函数 outdef() 中定义的 name 变量。再加上，indef() 函数中 name 变量的定义位于 print() 输出语句之后，导致 print(name) 语句在执行时找不到定义的 name 变量，因此程序报错。</p><p>由于这里的 name 变量也是局部变量，因此前面章节讲解的 globals() 函数或者 globals 关键字，并不适用于解决此问题。这里可以使用 Python 提供的 nonlocal 关键字。</p><p>例如，修改上面程序为：</p><figure class="highlight flix"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#全局函数<span class="function"><span class="keyword">def</span> <span class="title">outdef</span> </span>():    name = <span class="string">"所在函数中定义的 name 变量"</span>    #局部函数    <span class="function"><span class="keyword">def</span> <span class="title">indef</span></span>():        nonlocal name        print(name)        #修改name变量的值        name = <span class="string">"局部函数中定义的 name 变量"</span>    indef()#调用全局函数outdef()</span><br></pre></td></tr></table></figure><p>程序执行结果为：</p><p>所在函数中定义的 name 变量</p><h2 id="参考文献及资料"><a href="#参考文献及资料" class="headerlink" title="参考文献及资料"></a>参考文献及资料</h2><p>1、YARN Application Security，链接：<a href="https://hadoop.apache.org/docs/r2.7.4/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/r2.7.4/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;背景&lt;/li&gt;
&lt;li&gt;第一部分  &lt;/li&gt;
&lt;li&gt;第二部分 &lt;/li&gt;
&lt;li&gt;参考文献及资料&lt;/li&gt;
&lt;/ul&gt;
&lt;h
      
    
    </summary>
    
      <category term="Python" scheme="https://zjrongxiang.github.io/categories/Python/"/>
    
    
  </entry>
  
</feed>
